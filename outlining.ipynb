{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"requirements_outlines.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947168e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import outlines\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import gc\n",
    "import ast\n",
    "from outlines import from_transformers, Generator\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfdf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a chunk for clearing model cache if it becomes necessary to switch to another model without having to reset\n",
    "'''\n",
    "\n",
    "# Delete the model object\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# Clear PyTorch cache on GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# This is a comment to test git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d42c6f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available\n",
      "GPU memory allocated: 3.50 GB\n",
      "GPU memory reserved: 3.51 GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Cuda available\")\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Call this before and after model loading\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7ab6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:34<00:00,  8.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is the DeepSeek 14b model, which at first glance seems to perform better than the Llama model. \n",
    "Definitely worth considering if this should be used instead.\n",
    "'''\n",
    "\n",
    "model = from_transformers(\n",
    "    transformers.AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", device_map=\"auto\", torch_dtype=torch.bfloat16),\n",
    "    transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dad94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pydantic class which ensures the structured output from the llm\n",
    "class BlameAnalysis(BaseModel):\n",
    "    text: str = Field(description=\"The exact original sentence being analyzed\")\n",
    "    blame: bool = Field(description=\"Whether blame is present in the sentence\")\n",
    "    blamee: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Who or what is being blamed (must not be empty if blame=true)\"\n",
    "    )\n",
    "    arguments: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"What the blamee is being blamed for - the specific negative outcome (must not be empty if blame=true)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce2be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences after flattening: 399018\n",
      "First sentence: Mødet er åbnet.\n",
      "First sentence length: 15 chars\n",
      "Longest sentence: 1804 chars\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/annotation_data_translated_version_03_10.csv\", encoding='utf-8')\n",
    "\n",
    "# Parse and flatten the sentences\n",
    "all_sentences = []\n",
    "for text in data[\"da_segmented_text\"]:\n",
    "    if pd.notna(text):  # Skip NaN values\n",
    "        try:\n",
    "            # Parse the string representation of a list into actual list\n",
    "            sentence_list = ast.literal_eval(text)\n",
    "            if isinstance(sentence_list, list):\n",
    "                all_sentences.extend(sentence_list)\n",
    "            else:\n",
    "                # If it's not a list, treat it as a single sentence\n",
    "                all_sentences.append(str(sentence_list))\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If parsing fails, treat the whole thing as one sentence\n",
    "            all_sentences.append(text)\n",
    "\n",
    "sentences = all_sentences\n",
    "\n",
    "print(f\"Total sentences after flattening: {len(sentences)}\")\n",
    "print(f\"First sentence: {sentences[0]}\")\n",
    "print(f\"First sentence length: {len(sentences[0])} chars\")\n",
    "print(f\"Longest sentence: {max(len(s) for s in sentences)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6aa4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, BlameAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e78ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tqdm.tqdm(sentences, desc = \"Deepseek blame (GPU)\"):\n",
    "    prompt = f\"\"\"Perform blame identification on the following sentence.\n",
    "    Sentence: {sentence}\n",
    "\n",
    "    Rules:\n",
    "    - Start by determining whether blame is present at all in the sentence\n",
    "    - Identify who is being blamed, what they are being blamed for, and the arguments used\n",
    "    - Set blame=true ONLY if someone/something is being blamed for causing a negative outcome\n",
    "    - The \"text\" field must be EXACTLY the sentence provided above - do not modify it\n",
    "    - If blame=true, \"blamee\" must NOT be empty and \"arguments\" must contain the specific outcome they are blamed for\n",
    "    - Do not leave arguments as an empty string\n",
    "\n",
    "    Semantic roles:\n",
    "    - Blamee: The patient receiving the blame (who or what is being blamed)\n",
    "    - Argument: What is the blamee being blamed for (the negative outcome)\n",
    "\n",
    "    Output your analysis in JSON format.\"\"\"\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        result = generator(prompt, max_new_tokens=256, use_cache=False)\n",
    "\n",
    "    data = json.loads(result)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    # Parsing json for saving\n",
    "    result_out = BlameAnalysis.model_validate_json(result)\n",
    "    # (Over)Writing to file to avoid duplicates\n",
    "    with open(\"result_blame.json\", \"a\") as f:\n",
    "       json.dump(result_out.model_dump(), f, indent=2)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
