{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"requirements_outlines.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947168e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import outlines\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import gc\n",
    "import ast\n",
    "from outlines import from_transformers, Generator\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = from_transformers(\n",
    " #   transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\"),\n",
    "  #  transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\"),\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a chunk for clearing model cache if it becomes necessary to switch to another model without having to reset\n",
    "'''\n",
    "\n",
    "# Delete the model object\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# Clear PyTorch cache on GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# This is a comment to test git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42c6f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Call this before and after model loading\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7ab6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:37<00:00,  9.31s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is the DeepSeek 14b model, which at first glance seems to perform better than the Llama model. \n",
    "Definitely worth considering if this should be used instead.\n",
    "'''\n",
    "\n",
    "model = from_transformers(\n",
    "    transformers.AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", device_map=\"auto\"),\n",
    "    transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895539b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pydantic class which ensures the structured output from the llm\n",
    "class BlameAnalysis(BaseModel):\n",
    "    text: str = Field(description=\"The exact original sentence being analyzed\")\n",
    "    blame: bool = Field(description=\"Whether blame is present in the sentence\")\n",
    "    blamee: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Who or what is being blamed (must not be empty if blame=true)\"\n",
    "    )\n",
    "    arguments: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"What the blamee is being blamed for - the specific negative outcome (must not be empty if blame=true)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812429c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/annotation_data_translated_version_03_10.csv\")\n",
    "sentences = data[\"da_segmented_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce2be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences after flattening: 399018\n",
      "First sentence: Mødet er åbnet.\n",
      "First sentence length: 15 chars\n",
      "Longest sentence: 1804 chars\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/annotation_data_translated_version_03_10.csv\")\n",
    "\n",
    "# Parse and flatten the sentences\n",
    "all_sentences = []\n",
    "for text in data[\"da_segmented_text\"]:\n",
    "    if pd.notna(text):  # Skip NaN values\n",
    "        try:\n",
    "            # Parse the string representation of a list into actual list\n",
    "            sentence_list = ast.literal_eval(text)\n",
    "            if isinstance(sentence_list, list):\n",
    "                all_sentences.extend(sentence_list)\n",
    "            else:\n",
    "                # If it's not a list, treat it as a single sentence\n",
    "                all_sentences.append(str(sentence_list))\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If parsing fails, treat the whole thing as one sentence\n",
    "            all_sentences.append(text)\n",
    "\n",
    "sentences = all_sentences\n",
    "\n",
    "print(f\"Total sentences after flattening: {len(sentences)}\")\n",
    "print(f\"First sentence: {sentences[0]}\")\n",
    "print(f\"First sentence length: {len(sentences[0])} chars\")\n",
    "print(f\"Longest sentence: {max(len(s) for s in sentences)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6aa4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, BlameAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16b3939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deepseek blame (GPU):   0%|          | 0/399018 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 1/399018 [00:07<848:40:35,  7.66s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 2/399018 [00:14<768:45:48,  6.94s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 3/399018 [00:24<966:27:47,  8.72s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 4/399018 [00:43<1386:46:50, 12.51s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 5/399018 [00:50<1158:16:41, 10.45s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 6/399018 [00:57<1038:40:46,  9.37s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 7/399018 [02:11<3383:51:33, 30.53s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 8/399018 [02:20<2639:53:31, 23.82s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 9/399018 [02:30<2132:46:21, 19.24s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 10/399018 [02:43<1918:00:27, 17.30s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 11/399018 [02:53<1696:26:00, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Deepseek blame (GPU):   0%|          | 11/399018 [03:04<1862:27:02, 16.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      2\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mPerform blame identification on the following sentence.\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mSentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33mOutput your analysis in JSON format.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():  \u001b[38;5;66;03m# Disable gradient tracking\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     result = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m data = json.loads(result)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#print(json.dumps(data, indent=2))\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Parsing json for saving\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/outlines/generator.py:297\u001b[39m, in \u001b[36mSteerableGenerator.__call__\u001b[39m\u001b[34m(self, prompt, **inference_kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.logits_processor.reset()\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/outlines/models/transformers.py:326\u001b[39m, in \u001b[36mTransformers.generate\u001b[39m\u001b[34m(self, model_input, output_type, **inference_kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m prompts, inputs = \u001b[38;5;28mself\u001b[39m._prepare_model_inputs(model_input, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    324\u001b[39m logits_processor = \u001b[38;5;28mself\u001b[39m.type_adapter.format_output_type(output_type)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m generated_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_output_seq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# required for multi-modal models that return a 2D tensor even when\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# num_return_sequences is 1\u001b[39;00m\n\u001b[32m    335\u001b[39m num_samples = inference_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mnum_return_sequences\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/outlines/models/transformers.py:375\u001b[39m, in \u001b[36mTransformers._generate_output_seq\u001b[39m\u001b[34m(self, prompts, inputs, **inference_kwargs)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_output_seq\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts, inputs, **inference_kwargs):\n\u001b[32m    373\u001b[39m     input_ids = inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     \u001b[38;5;66;03m# encoder-decoder returns output_ids only, decoder-only returns full seq ids\u001b[39;00m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/generation/utils.py:2873\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2870\u001b[39m     outputs = model_forward(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2872\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2873\u001b[39m model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_model_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2874\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2877\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[32m   2879\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/generation/utils.py:997\u001b[39m, in \u001b[36mGenerationMixin._update_model_kwargs_for_generation\u001b[39m\u001b[34m(self, outputs, model_kwargs, is_encoder_decoder, num_new_tokens)\u001b[39m\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    996\u001b[39m     past_positions = model_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     new_positions = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.to(past_positions.device)\n\u001b[32m   1000\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat((past_positions, new_positions))\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for sentence in tqdm.tqdm(sentences, desc = \"Deepseek blame (GPU)\"):\n",
    "    prompt = f\"\"\"Perform blame identification on the following sentence.\n",
    "    Sentence: {sentence}\n",
    "\n",
    "    Rules:\n",
    "    - Start by determining whether blame is present at all in the sentence\n",
    "    - Identify who is being blamed, what they are being blamed for, and the arguments used\n",
    "    - Set blame=true ONLY if someone/something is being blamed for causing a negative outcome\n",
    "    - The \"text\" field must be EXACTLY the sentence provided above - do not modify it\n",
    "    - If blame=true, \"blamee\" must NOT be empty and \"arguments\" must contain the specific outcome they are blamed for\n",
    "    - Do not leave arguments as an empty string\n",
    "\n",
    "    Semantic roles:\n",
    "    - Blamee: The patient receiving the blame (who or what is being blamed)\n",
    "    - Argument: What is the blamee being blamed for (the negative outcome)\n",
    "\n",
    "    Output your analysis in JSON format.\"\"\"\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        result = generator(prompt, max_new_tokens=256, use_cache=False)\n",
    "\n",
    "    data = json.loads(result)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    # Parsing json for saving\n",
    "    result_out = BlameAnalysis.model_validate_json(result)\n",
    "    # (Over)Writing to file to avoid duplicates\n",
    "    with open(\"result_blame.json\", \"a\") as f:\n",
    "       json.dump(result_out.model_dump(), f, indent=2)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2177cd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(result)\n",
    "print(json.dumps(data, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
