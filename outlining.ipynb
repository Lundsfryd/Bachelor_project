{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"requirements_outlines.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947168e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import outlines\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import gc\n",
    "import ast\n",
    "from outlines import from_transformers, Generator, models\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfdf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a chunk for clearing model cache if it becomes necessary to switch to another model without having to reset\n",
    "'''\n",
    "\n",
    "# Delete the model object\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# Clear PyTorch cache on GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# This is a comment to test git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d42c6f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available\n",
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Cuda available\")\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# Call this before and after model loading\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79e2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = from_transformers(\n",
    "    transformers.AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\", dtype=torch.bfloat16),\n",
    "    transformers.AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7ab6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:34<00:00,  8.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is the DeepSeek 14b model, which at first glance seems to perform better than the Llama model. \n",
    "Definitely worth considering if this should be used instead.\n",
    "'''\n",
    "\n",
    "model = from_transformers(\n",
    "    transformers.AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", device_map=\"auto\", torch_dtype=torch.bfloat16),\n",
    "    transformers.AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5dad94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pydantic class which ensures the structured output from the llm\n",
    "class BlameAnalysis(BaseModel):\n",
    "    text: str = Field(description=\"The exact original sentence being analyzed\")\n",
    "    blame: bool = Field(description=\"Whether blame is present in the sentence\")\n",
    "    blamee: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Who or what is being blamed (must not be empty if blame=true)\"\n",
    "    )\n",
    "    arguments: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"What the blamee is being blamed for - the specific negative outcome (must not be empty if blame=true)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4c5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pydantic class which ensures the structured output from the llm\n",
    "class BlameAnalysis(BaseModel):\n",
    "    text: str = Field(description=\"The exact original sentence being analyzed\")\n",
    "    blame: bool = Field(description=\"Whether blame is present in the sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db14139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/annotation_data_translated_version_03_10.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce2be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph_entry = {}\n",
    "for i, text in enumerate(text_data[\"da_segmented_text\"]): #check if i is sctually number\n",
    "\n",
    "\n",
    "    da_segmented_sentences = ast.literal_eval(text_data.loc[i][\"da_segmented_text\"])\n",
    "\n",
    "    sentece_entry = {}\n",
    "    for p, sentence in enumerate(da_segmented_sentences):\n",
    "        sentece_entry[p] = sentence\n",
    "    \n",
    "    paragraph_entry[i] = sentece_entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6aa4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, BlameAnalysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce12b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLama blame (GPU):   0%|          | 2/36314 [13:40<4137:20:03, 410.18s/it]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for BlameAnalysis\n  Invalid JSON: EOF while parsing a string at line 1 column 891 [type=json_invalid, input_value='{\"text\": \"Rules: Rules: ...s: Rules: Rules: Rules:', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     20\u001b[39m     result = generator(prompt, max_new_tokens=\u001b[32m256\u001b[39m, use_cache=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m result_out = \u001b[43mBlameAnalysis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Store in nested structure matching input\u001b[39;00m\n\u001b[32m     25\u001b[39m json_out[outer_key][inner_key] = result_out.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/pydantic/main.py:746\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, context, by_alias, by_name)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    742\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    743\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    744\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for BlameAnalysis\n  Invalid JSON: EOF while parsing a string at line 1 column 891 [type=json_invalid, input_value='{\"text\": \"Rules: Rules: ...s: Rules: Rules: Rules:', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid"
     ]
    }
   ],
   "source": [
    "json_out = {}\n",
    "\n",
    "# Iterate through outer keys (0 to 36000)\n",
    "for outer_key, inner_dict in tqdm.tqdm(paragraph_entry.items(), desc=\"LLama blame (GPU)\"):\n",
    "    json_out[outer_key] = {}  # Initialize nested dict for this outer key\n",
    "    \n",
    "    # Iterate through inner keys (0, 1, 2, ...)\n",
    "    for inner_key, sentence in inner_dict.items():\n",
    "        prompt = f\"\"\"Perform blame identification on the following sentence.\n",
    "        Sentence: {sentence}\n",
    "\n",
    "        Rules:\n",
    "        - Start by determining whether blame is present at all in the sentence\n",
    "        - Set blame=true ONLY if someone/something is being blamed for causing a negative outcome\n",
    "        - The \"text\" field must be EXACTLY the sentence provided above - do not modify it\n",
    "\n",
    "        Output your analysis in JSON format. /no_think\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result = generator(prompt, max_new_tokens=256, use_cache=False)\n",
    "        \n",
    "        result_out = BlameAnalysis.model_validate_json(result)\n",
    "        \n",
    "        # Store in nested structure matching input\n",
    "        json_out[outer_key][inner_key] = result_out.model_dump()\n",
    "        \n",
    "        # Append to file\n",
    "        with open(\"result_blame.json\", \"a\") as f:\n",
    "            json.dump({\n",
    "                \"paragraph\": outer_key,\n",
    "                \"sentence\": inner_key,\n",
    "                \"result\": result_out.model_dump()\n",
    "            }, f, indent=2)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Save complete nested structure\n",
    "with open(\"result_blame_complete.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=2)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f0b2b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:05, 65.08s/it]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for BlameAnalysis\n  Invalid JSON: EOF while parsing a string at line 1 column 641 [type=json_invalid, input_value='{\"text\": \"Sentence: Jeg ...kerørsel på en fisker', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     50\u001b[39m         results.append(result)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (outer_key, inner_key, _), result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, results):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     result_out = \u001b[43mBlameAnalysis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     json_out.setdefault(outer_key, {})[inner_key] = result_out.model_dump()\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# incremental write\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/pydantic/main.py:746\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, context, by_alias, by_name)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    742\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    743\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    744\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for BlameAnalysis\n  Invalid JSON: EOF while parsing a string at line 1 column 641 [type=json_invalid, input_value='{\"text\": \"Sentence: Jeg ...kerørsel på en fisker', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "from itertools import islice\n",
    "\n",
    "# 1. Initialize accelerator\n",
    "accelerator = Accelerator()  # auto-detects GPUs, mixed precision, etc.\n",
    "device = accelerator.device\n",
    "\n",
    "# 2. Move your model to the accelerator\n",
    "#generator = generator.to(device)  # or use pipeline(..., device_map='auto')\n",
    "generator = accelerator.prepare(generator)\n",
    "\n",
    "# 3. Helper for batching\n",
    "def batch_iterable(iterable, batch_size):\n",
    "    it = iter(iterable)\n",
    "    while batch := list(islice(it, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "# 4. Flatten input data\n",
    "flat_sentences = []\n",
    "for outer_key, inner_dict in paragraph_entry.items():\n",
    "    for inner_key, sentence in inner_dict.items():\n",
    "        flat_sentences.append((outer_key, inner_key, sentence))\n",
    "\n",
    "# 5. Batched inference\n",
    "BATCH_SIZE = 8\n",
    "json_out = {}\n",
    "\n",
    "for batch in tqdm.tqdm(batch_iterable(flat_sentences, BATCH_SIZE)):\n",
    "    prompts = [\n",
    "        f\"\"\"Perform blame identification on the following sentence.\n",
    "        Sentence: {sentence}\n",
    "\n",
    "        Rules:\n",
    "        - Start by determining whether blame is present at all in the sentence\n",
    "        - Set blame=true ONLY if someone/something is being blamed for causing a negative outcome\n",
    "        - The \"text\" field must be EXACTLY the sentence provided above - do not modify it\n",
    "\n",
    "        Output your analysis in JSON format. /no_think\"\"\"\n",
    "        for (_, _, sentence) in batch\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # accelerator handles distributing model and data automatically\n",
    "        results = []\n",
    "        for p in prompts:\n",
    "            result = generator(p, max_new_tokens=256, use_cache=False)\n",
    "            results.append(result)\n",
    "\n",
    "\n",
    "    for (outer_key, inner_key, _), result in zip(batch, results):\n",
    "        result_out = BlameAnalysis.model_validate_json(result)\n",
    "        json_out.setdefault(outer_key, {})[inner_key] = result_out.model_dump()\n",
    "\n",
    "        # incremental write\n",
    "        with open(\"result_blame.json\", \"a\") as f:\n",
    "            json.dump({\n",
    "                \"paragraph\": outer_key,\n",
    "                \"sentence\": inner_key,\n",
    "                \"result\": result_out.model_dump()\n",
    "            }, f, indent=2)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# save full results\n",
    "with open(\"result_blame_complete.json\", \"w\") as f:\n",
    "    json.dump(json_out, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1876fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLama blame (multi-GPU):   0%|          | 0/49877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLama blame (multi-GPU):   0%|          | 5/49877 [00:53<148:09:07, 10.69s/it]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for BlameAnalysis\n  Invalid JSON: EOF while parsing a string at line 1 column 891 [type=json_invalid, input_value='{\"text\": \"Rules: Rules: ...s: Rules: Rules: Rules:', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Process and buffer results\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (outer_key, inner_key, _), result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch, results):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     result_out = \u001b[43mBlameAnalysis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m     json_out.setdefault(outer_key, {})[inner_key] = result_out.model_dump()\n\u001b[32m     89\u001b[39m     buffer.append({\n\u001b[32m     90\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparagraph\u001b[39m\u001b[33m\"\u001b[39m: outer_key,\n\u001b[32m     91\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m\"\u001b[39m: inner_key,\n\u001b[32m     92\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m: result_out.model_dump()\n\u001b[32m     93\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/pydantic/main.py:746\u001b[39m, in \u001b[36mBaseModel.model_validate_json\u001b[39m\u001b[34m(cls, json_data, strict, context, by_alias, by_name)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    742\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    743\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    744\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for BlameAnalysis\n  Invalid JSON: EOF while parsing a string at line 1 column 891 [type=json_invalid, input_value='{\"text\": \"Rules: Rules: ...s: Rules: Rules: Rules:', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "from itertools import islice\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "BATCH_SIZE = 8                      # Adjust based on GPU memory\n",
    "SAVE_INTERVAL = 200                 # Write every 200 results\n",
    "CHECKPOINT_INTERVAL_SEC = 300       # Also checkpoint full dict every 5 min\n",
    "OUTPUT_FILE = \"result_blame.json\"\n",
    "CHECKPOINT_FILE = \"result_blame_complete.json\"\n",
    "# -----------------------------\n",
    "\n",
    "# ---------- SETUP ----------\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "#generator = generator.to(device)\n",
    "generator = accelerator.prepare(generator)\n",
    "\n",
    "# Flatten nested dict structure\n",
    "flat_sentences = []\n",
    "for outer_key, inner_dict in paragraph_entry.items():\n",
    "    for inner_key, sentence in inner_dict.items():\n",
    "        flat_sentences.append((outer_key, inner_key, sentence))\n",
    "\n",
    "def batch_iterable(iterable, batch_size):\n",
    "    it = iter(iterable)\n",
    "    while batch := list(islice(it, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "# ---------- ASYNC WRITER ----------\n",
    "write_queue = Queue()\n",
    "\n",
    "def writer_thread():\n",
    "    \"\"\"Background writer to handle async JSON appends.\"\"\"\n",
    "    with open(OUTPUT_FILE, \"a\") as f:\n",
    "        while True:\n",
    "            batch = write_queue.get()\n",
    "            if batch is None:\n",
    "                break\n",
    "            for item in batch:\n",
    "                json.dump(item, f)\n",
    "                f.write(\"\\n\")\n",
    "            f.flush()\n",
    "\n",
    "writer = threading.Thread(target=writer_thread, daemon=True)\n",
    "writer.start()\n",
    "\n",
    "# ---------- MAIN LOOP ----------\n",
    "json_out = {}\n",
    "buffer = []\n",
    "last_checkpoint_time = time.time()\n",
    "\n",
    "for batch in tqdm.tqdm(\n",
    "    batch_iterable(flat_sentences, BATCH_SIZE),\n",
    "    total=len(flat_sentences) // BATCH_SIZE,\n",
    "    desc=\"LLama blame (multi-GPU)\"\n",
    "):\n",
    "    prompts = [\n",
    "        f\"\"\"Perform blame identification on the following sentence.\n",
    "        Sentence: {sentence}\n",
    "\n",
    "        Rules:\n",
    "        - Start by determining whether blame is present at all in the sentence\n",
    "        - Set blame=true ONLY if someone/something is being blamed for causing a negative outcome\n",
    "        - The \"text\" field must be EXACTLY the sentence provided above - do not modify it\n",
    "\n",
    "        Output your analysis in JSON format. /no_think\"\"\"\n",
    "        for (_, _, sentence) in batch\n",
    "    ]\n",
    "\n",
    "    # Outlines generator only supports single-string input → iterate\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for p in prompts:\n",
    "            result = generator(p, max_new_tokens=256, use_cache=False)\n",
    "            results.append(result)\n",
    "\n",
    "    # Process and buffer results\n",
    "    for (outer_key, inner_key, _), result in zip(batch, results):\n",
    "        result_out = BlameAnalysis.model_validate_json(result)\n",
    "        json_out.setdefault(outer_key, {})[inner_key] = result_out.model_dump()\n",
    "\n",
    "        buffer.append({\n",
    "            \"paragraph\": outer_key,\n",
    "            \"sentence\": inner_key,\n",
    "            \"result\": result_out.model_dump()\n",
    "        })\n",
    "\n",
    "        # Flush to async writer when buffer fills\n",
    "        if len(buffer) >= SAVE_INTERVAL:\n",
    "            write_queue.put(buffer.copy())\n",
    "            buffer.clear()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Periodic checkpoint of full dict\n",
    "    if (time.time() - last_checkpoint_time) > CHECKPOINT_INTERVAL_SEC:\n",
    "        with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "            json.dump(json_out, f, indent=2)\n",
    "        last_checkpoint_time = time.time()\n",
    "\n",
    "# ---------- FINAL FLUSH ----------\n",
    "if buffer:\n",
    "    write_queue.put(buffer.copy())\n",
    "\n",
    "write_queue.put(None)  # tell writer to stop\n",
    "writer.join()\n",
    "\n",
    "# Final save of complete structure\n",
    "with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "    json.dump(json_out, f, indent=2)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(\"✅ Processing complete — results written to disk.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
