{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wzyQgO9PJGY5",
        "outputId": "fec30aab-45e3-4acc-a40b-73348c84916d"
      },
      "outputs": [],
      "source": [
        "%pip install -r \"requirements_bert.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ PyTorch OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\runet\\anaconda3\\envs\\training\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Datasets OK\n",
            "✓ Transformers OK\n",
            "✓ BnB OK\n",
            "✓ PEFT OK\n",
            "✓ NumPy OK\n"
          ]
        }
      ],
      "source": [
        "# Test each import individually\n",
        "try:\n",
        "    import torch\n",
        "    print(\"✓ PyTorch OK\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ PyTorch failed: {e}\")\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    print(\"✓ Datasets OK\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Datasets failed: {e}\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM\n",
        "    print(\"✓ Transformers OK\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Transformers failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(\"✓ BnB OK\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ BNb failed: {e}\")\n",
        "\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    print(\"✓ PEFT OK\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ PEFT failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(\"✓ NumPy OK\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ NumPy failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0+cu129\n",
            "CUDA available: True\n",
            "CUDA version used by PyTorch: 12.9\n",
            "Number of GPUs: 1\n",
            "GPU name: NVIDIA GeForce RTX 4070 SUPER\n"
          ]
        }
      ],
      "source": [
        "#import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version used by PyTorch: {torch.version.cuda}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU name: {device}\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1036"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory allocated: 0.87 GB\n",
            "GPU memory reserved: 10.79 GB\n"
          ]
        }
      ],
      "source": [
        "def print_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
        "\n",
        "# Call this before and after model loading\n",
        "print_gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MEIj_2U2Mqrd",
        "outputId": "44716c46-dec9-424d-a173-a545f2cab3f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"jhu-clsp/mmBERT-base\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "                                        load_in_4bit=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                                         bnb_4bit_quant_type=\"nf4\",\n",
        "                                         bnb_4bit_use_double_quant=True,\n",
        "                                         )\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    #dtype=torch.float16,\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NqLpCX0EkUGh",
        "outputId": "e5b40dad-fafd-414e-d6f4-e68fe342ea2d"
      },
      "outputs": [],
      "source": [
        "from transformers import Conv1D\n",
        "\n",
        "def get_specific_layer_names(model):\n",
        "    # Create a list to store the layer names\n",
        "    layer_names = []\n",
        "\n",
        "    # Recursively visit all modules and submodules\n",
        "    for name, module in model.named_modules():\n",
        "        # Check if the module is an instance of the specified layers\n",
        "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
        "            # model name parsing\n",
        "\n",
        "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
        "\n",
        "    return layer_names\n",
        "\n",
        "list(set(get_specific_layer_names(model)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtvlR2bNO5GW",
        "outputId": "27dce38a-71fc-407b-8201-a6405c03bf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 540,672 || all params: 308,072,450 || trainable%: 0.1755\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,  # Low-rank dimension\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"Wqkv\"],  # Fine-tuning the attention layer specifically\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"mlburnham/Pol_NLI\")\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"premise\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"entailment\", \"labels\") # Rename entailment column to labels (which is standard lookup for evaluation in the transmformers trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['premise', 'hypothesis', 'entailment', 'dataset', 'task', 'augmented_hypothesis'],\n",
              "    num_rows: 171289\n",
              "})"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"]\n",
        "# Premise is the context, hypothesis is the statement to verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])  # Take the first 100 samples\n",
        "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].shuffle(seed=42).select([i for i in list(range(20))])  # Take the first 20 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Column(['This text describes a hostage taking (kidnapping)', 'This text is attacking people for their place of origin.', 'This text is about defense r&d.', 'This text is about voting rights.', 'This text is dehumanizing people for their race.'])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "i2PnPqvvmhMG"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,  # Start small, increase gradually\n",
        "    gradient_accumulation_steps=12,  # Simulate larger batch size\n",
        "\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,  # Enable mixed precision\n",
        "    dataloader_pin_memory=False,\n",
        "    remove_unused_columns=False,\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    disable_tqdm=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions),\n",
        "        'f1': f1_score(labels, predictions, average='weighted')\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "2yaHNhbTmnh0",
        "outputId": "f0ffde3c-d418-4761-b2ca-7d4c07781e55"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/27 06:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.109100</td>\n",
              "      <td>0.985361</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.457333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.083000</td>\n",
              "      <td>0.974652</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.457333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.682100</td>\n",
              "      <td>0.971707</td>\n",
              "      <td>0.450000</td>\n",
              "      <td>0.457333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=27, training_loss=10.236043294270834, metrics={'train_runtime': 417.4004, 'train_samples_per_second': 0.719, 'train_steps_per_second': 0.065, 'total_flos': 1643610193920000.0, 'train_loss': 10.236043294270834, 'epoch': 3.0})"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "lora_model.save_pretrained(f\"output/mmBERT/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766,
          "referenced_widgets": [
            "4e96343e0b424f009ad53c9527f0eb5c",
            "5a3986810ff94802a4db2022638bcb4d",
            "248924edcab74bf29cec5ceec4da8422",
            "3a3bd95e902d4a19b5f58d0961aed973",
            "144f800dc7a440678b856ffcba10c290",
            "89d836fe02a14970bc93965242230a86",
            "21a58c23b7224f61a4e84e2bbe60a40e",
            "757a64c05dc549cbb954d87a75c9fd44",
            "58ba5ead5cf040958578e582a18ac8f6",
            "33f0d6fabf3543b7a63f3cbbe35ffa04",
            "1ab01e194b224daea06c239ce0871490"
          ]
        },
        "id": "-Uz32l_faghm",
        "outputId": "db6ae241-2a7e-4ec1-c98b-b426a6193829"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions),\n",
        "        'f1': f1_score(labels, predictions, average='weighted')\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    model_name = \"jhu-clsp/mmBERT-base\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=3\n",
        "    )\n",
        "\n",
        "    dataset = load_dataset(\"xnli\", \"all_languages\")\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        texts = [f\"{p} {tokenizer.sep_token} {h}\"\n",
        "                for p, h in zip(examples[\"premise\"], examples[\"hypothesis\"])]\n",
        "\n",
        "        return tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "    train_dataset = dataset[\"train\"].map(tokenize_function, batched=True)\n",
        "    eval_dataset = dataset[\"validation\"].map(tokenize_function, batched=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./mmbert-xnli\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Features, Value, Sequence\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# Load dataset with correct schema\n",
        "features = Features({\n",
        "    'sent_id': Value('string'),\n",
        "    'doc_id': Value('string'),\n",
        "    'text': Value('string'),\n",
        "    'tokens': Sequence(Value('string')),\n",
        "    'clusters': Sequence(Sequence(Value('int64')))\n",
        "})\n",
        "\n",
        "dataset = load_dataset(\"alexandrainst/dacoref\", features=features)\n",
        "train_dataset = dataset['train']\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"], \n",
        "        padding=\"max_length\", \n",
        "        truncation=True, \n",
        "        max_length=512\n",
        "    )\n",
        "    # For causal LM, labels = input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove string columns that can't be converted to tensors\n",
        "columns_to_remove = ['sent_id', 'doc_id', 'text', 'tokens', 'clusters']\n",
        "final_dataset = tokenized_dataset.remove_columns(columns_to_remove)\n",
        "\n",
        "# Verify the dataset structure\n",
        "print(\"Final columns:\", final_dataset.column_names)\n",
        "print(\"Sample item keys:\", final_dataset[0].keys())\n",
        "print(\"Input IDs shape:\", len(final_dataset[0]['input_ids']))\n",
        "\n",
        "# Create data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 171289/171289 [00:00<00:00, 601641.98 examples/s]\n",
            "Generating validation split: 100%|██████████| 15036/15036 [00:00<00:00, 541832.88 examples/s]\n",
            "Generating test split: 100%|██████████| 15366/15366 [00:00<00:00, 727422.97 examples/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "premise",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "entailment",
                  "rawType": "int64",
                  "type": "integer"
                }
              ],
              "ref": "1b77a621-7c2e-45f9-9a17-7148ce5a2856",
              "rows": [
                [
                  "0",
                  "The soldiers storming the beaches on D-Day may as well have gassed those jews themselves.",
                  "0"
                ],
                [
                  "1",
                  "Regime warplanes and helicopters targeted Al-Latamna city in Hama. The city was targeted with chlorine gas, which caused suffocation symptoms among civilians. No fatalities reported.",
                  "1"
                ],
                [
                  "2",
                  "rt @scottwalker first up this morning at #ncsc22 #yafcon @yaf congressman jim jordan",
                  "1"
                ],
                [
                  "3",
                  "With protection from the Taliban, al Qaeda and its associates trained, indoctrinated, and sent forth thousands of killers to set up terror cells in dozens of countries, including our own.",
                  "0"
                ],
                [
                  "4",
                  "@aiyegbayo @KadariaAhmed LOL what nigerian project? there is nothing like inclusiveness it's a ruse Those people are there from themselves You have nothing to gain just like me",
                  "1"
                ]
              ],
              "shape": {
                "columns": 2,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>entailment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The soldiers storming the beaches on D-Day may...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regime warplanes and helicopters targeted Al-L...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rt @scottwalker first up this morning at #ncsc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With protection from the Taliban, al Qaeda and...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@aiyegbayo @KadariaAhmed LOL what nigerian pro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             premise  entailment\n",
              "0  The soldiers storming the beaches on D-Day may...           0\n",
              "1  Regime warplanes and helicopters targeted Al-L...           1\n",
              "2  rt @scottwalker first up this morning at #ncsc...           1\n",
              "3  With protection from the Taliban, al Qaeda and...           0\n",
              "4  @aiyegbayo @KadariaAhmed LOL what nigerian pro...           1"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds = load_dataset(\"mlburnham/Pol_NLI\")\n",
        "test = ds['test'].to_pandas()\n",
        "# we'll use a random sample of 1,000 documents for this example\n",
        "test = test[['premise', 'hypothesis', 'entailment', 'task']].sample(1000, random_state = 1)\n",
        "test.reset_index(drop = True, inplace = True)\n",
        "test[['premise', 'entailment']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"zero-shot-classification\", model='mlburnham/Political_DEBATE_large_v1.0', device = device, batch_size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "C:\\Users\\runet\\AppData\\Local\\Temp\\ipykernel_14412\\1507637166.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test[colname].replace({0:1, 1:0}, inplace = True) # in our data entailment is labeled as 0 and not entailment is 1, so we recode the 0 and 1 probabilities to match the entailment labels\n"
          ]
        }
      ],
      "source": [
        "colname = 'debate_label' # the name of the column where we will assign out labels to\n",
        "test[colname] = 0\n",
        "\n",
        "for i in test.index:\n",
        "    hypothesis = test.loc[i, 'hypothesis'] # get the right entailment hypothesis\n",
        "    sample = test.loc[i, 'premise'] # get the document to be classified\n",
        "    res = pipe(sample, hypothesis, hypothesis_template = '{}') # classify the document-hypothesis pair\n",
        "    test.loc[i, colname] = round(res['scores'][0]) # here we extract the probability from the resulting dictionary, round the number to 0 or 1, and assign it to the dataframe\n",
        "test[colname].replace({0:1, 1:0}, inplace = True) # in our data entailment is labeled as 0 and not entailment is 1, so we recode the 0 and 1 probabilities to match the entailment labels\n",
        "test[colname] = test[colname].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.9027057940562578)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_corrcoef(test['entailment'], test['debate_label'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "144f800dc7a440678b856ffcba10c290": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab01e194b224daea06c239ce0871490": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21a58c23b7224f61a4e84e2bbe60a40e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "248924edcab74bf29cec5ceec4da8422": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_757a64c05dc549cbb954d87a75c9fd44",
            "max": 2490,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58ba5ead5cf040958578e582a18ac8f6",
            "value": 2490
          }
        },
        "33f0d6fabf3543b7a63f3cbbe35ffa04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3bd95e902d4a19b5f58d0961aed973": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33f0d6fabf3543b7a63f3cbbe35ffa04",
            "placeholder": "​",
            "style": "IPY_MODEL_1ab01e194b224daea06c239ce0871490",
            "value": " 2490/2490 [00:05&lt;00:00, 433.54 examples/s]"
          }
        },
        "4e96343e0b424f009ad53c9527f0eb5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a3986810ff94802a4db2022638bcb4d",
              "IPY_MODEL_248924edcab74bf29cec5ceec4da8422",
              "IPY_MODEL_3a3bd95e902d4a19b5f58d0961aed973"
            ],
            "layout": "IPY_MODEL_144f800dc7a440678b856ffcba10c290"
          }
        },
        "58ba5ead5cf040958578e582a18ac8f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a3986810ff94802a4db2022638bcb4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89d836fe02a14970bc93965242230a86",
            "placeholder": "​",
            "style": "IPY_MODEL_21a58c23b7224f61a4e84e2bbe60a40e",
            "value": "Map: 100%"
          }
        },
        "757a64c05dc549cbb954d87a75c9fd44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d836fe02a14970bc93965242230a86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
