{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ddde3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68cee196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to be used\n",
    "def read_labeled_data_reformat(input_file, output_file):\n",
    "    # --- PROCESSING ---\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    flattened = []\n",
    "    for entry in data:\n",
    "        flat_entry = {} \n",
    "\n",
    "        # Add all key-value pairs from the \"data\" dict\n",
    "        flat_entry.update(entry.get(\"data\", {}))\n",
    "\n",
    "        # Extract the first \"choices\" value from annotations â†’ result\n",
    "        choices = None\n",
    "        for ann in entry.get(\"annotations\", []):\n",
    "            for res in ann.get(\"result\", []):\n",
    "                val = res.get(\"value\", {})\n",
    "                if \"choices\" in val:\n",
    "                    # if multiple choices exist, join them\n",
    "                    choices = \", \".join(val[\"choices\"])\n",
    "                    break\n",
    "            if choices:\n",
    "                break\n",
    "\n",
    "        flat_entry[\"choices\"] = choices\n",
    "\n",
    "        flattened.append(flat_entry)\n",
    "\n",
    "    # --- SAVE OUTPUT ---\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flattened, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def get_choices(json_data, paragraph, sentence_nr, key = \"choices\"):\n",
    "    \"\"\"\n",
    "    Returns the 'choices' value for a given paragraph and sentence number\n",
    "    from the flattened JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        json_file (str): Path to the flattened JSON file\n",
    "        paragraph (str or int): Paragraph number to look up\n",
    "        sentence_nr (str or int): Sentence number to look up\n",
    "\n",
    "    Returns:\n",
    "        str or None: The choices value if found, otherwise None\n",
    "    \"\"\"\n",
    "\n",
    "    for entry in json_data:\n",
    "        if str(entry.get(\"paragraph\")) == str(paragraph) and str(entry.get(\"sentence_nr\")) == str(sentence_nr):\n",
    "            return entry.get(key)\n",
    "\n",
    "    return None  # If no match found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_markus = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/markus_labeling.json\"\n",
    "output_file_markus = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/cleaned_labelled_markus.json\"\n",
    "\n",
    "read_labeled_data_reformat(input_file_markus, output_file_markus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02218753",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_rune = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/labelled_rune.json\"\n",
    "output_file_rune = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/cleaned_labelled_rune.json\"\n",
    "\n",
    "read_labeled_data_reformat(input_file_rune, output_file_rune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "122c5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inter annotater agreement\n",
    "\n",
    "#load data\n",
    "with open(output_file_markus, \"r\", encoding=\"utf-8\") as f:\n",
    "        markus_data = json.load(f)\n",
    "\n",
    "with open(output_file_rune, \"r\", encoding=\"utf-8\") as f:\n",
    "        rune_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67863ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_annotater_agreement = []\n",
    "inter_annotater_disagreement = []\n",
    "\n",
    "for entry in markus_data:\n",
    "    paragraph_nr = entry['paragraph']\n",
    "    sentence_nr = entry['sentence_nr']\n",
    "\n",
    "    model_blame = entry[\"Blame\"]\n",
    "\n",
    "    markus_choice = entry[\"choices\"]\n",
    "    \n",
    "    rune_choice = get_choices(rune_data, paragraph_nr, sentence_nr)\n",
    "\n",
    "    if markus_choice != rune_choice:\n",
    "        inter_annotater_disagreement.append((int(paragraph_nr), int(sentence_nr)))\n",
    "\n",
    "    elif markus_choice == rune_choice:\n",
    "        inter_annotater_agreement.append((int(paragraph_nr), int(sentence_nr)))\n",
    "    \n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0551e213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(inter_annotater_agreement))\n",
    "print(len(inter_annotater_disagreement))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "inter-annotater agreement of 86%\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b9b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_with_model = []\n",
    "disagreement_with_model = []\n",
    "\n",
    "for entry in markus_data:\n",
    "    paragraph_nr = entry['paragraph']\n",
    "    sentence_nr = entry['sentence_nr']\n",
    "\n",
    "    model_blame = entry[\"Blame\"]\n",
    "\n",
    "    markus_choice = entry[\"choices\"]\n",
    "    \n",
    "    rune_choice = get_choices(rune_data, paragraph_nr, sentence_nr)\n",
    "\n",
    "    annotater_choice = 0\n",
    "\n",
    "    if (markus_choice == \"Blame\") or (rune_choice == \"Blame\"):\n",
    "        annotater_choice = 1\n",
    "\n",
    "    if annotater_choice == int(model_blame):\n",
    "        agreement_with_model.append((int(paragraph_nr), int(sentence_nr)))\n",
    "    \n",
    "    elif annotater_choice != int(model_blame):\n",
    "        disagreement_with_model.append((int(paragraph_nr), int(sentence_nr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4acb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(agreement_with_model))\n",
    "print(len(disagreement_with_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c05797",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "One or more annotaters agreee with the model 78% of the samples\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a134de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.23255813953489\n"
     ]
    }
   ],
   "source": [
    "#Find in how great a proportion of the times where both annotaters agreed, \n",
    "# did they also agree with the model\n",
    "\n",
    "inter_ann_aggee_and_model = 0\n",
    "for i in inter_annotater_agreement:\n",
    "    if i in agreement_with_model:\n",
    "        inter_ann_aggee_and_model += 1\n",
    "\n",
    "\n",
    "print((inter_ann_aggee_and_model/len(inter_annotater_agreement))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334995a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in 80 percent of all cases where both annotaters agreed, \n",
    "# the labels of the PolDebate model also agreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db833895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversly if the annotaters did not agree, \n",
    "# how is the distribution of blame/no blame by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ec58b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.28571428571429\n"
     ]
    }
   ],
   "source": [
    "blame = 0\n",
    "no_blame = 0\n",
    "\n",
    "for para, sent in inter_annotater_disagreement:\n",
    "    model_choice = get_choices(markus_data, para, sent, key= 'Blame')\n",
    "\n",
    "    if model_choice == 1:\n",
    "        blame+=1\n",
    "    elif model_choice == 0:\n",
    "        no_blame += 1\n",
    "\n",
    "print((blame/(no_blame+blame))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd739d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "a rather even, at least to some extend, distribution of blame/no_blame in \n",
    "inter-annotater disagreeement, where 64% were classified as blame by the PolDebate \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A note about the sentences classified as blame by the PolDebate model.\n",
    "During annotation quite a bit of the sentences would be negative but \n",
    "manually annotated as blame. Thus the PolDebate model might have a tendence \n",
    "(understandable) to highten the probability of a sentence containing blame simply due\n",
    "to the sentiment of the sentence. However, the data used for training \n",
    "(sentences classified as blame by all five templates) is assumed to be more robust and \n",
    "thus some of these misclassifications are assumed to be watered down enough. \n",
    "The validation of the model on the gold-labeled dataset will show to what degree \n",
    "this was achieved.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ebf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hell yearh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
