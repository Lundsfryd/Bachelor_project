{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 3,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "0ddde3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "68cee196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to be used\n",
    "def read_labeled_data_reformat(input_file, output_file):\n",
    "    # --- PROCESSING ---\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    flattened = []\n",
    "    for entry in data:\n",
    "        flat_entry = {} \n",
    "\n",
    "        # Add all key-value pairs from the \"data\" dict\n",
    "        flat_entry.update(entry.get(\"data\", {}))\n",
    "\n",
    "        # Extract the first \"choices\" value from annotations â†’ result\n",
    "        choices = None\n",
    "        for ann in entry.get(\"annotations\", []):\n",
    "            for res in ann.get(\"result\", []):\n",
    "                val = res.get(\"value\", {})\n",
    "                if \"choices\" in val:\n",
    "                    # if multiple choices exist, join them\n",
    "                    choices = \", \".join(val[\"choices\"])\n",
    "                    break\n",
    "            if choices:\n",
    "                break\n",
    "\n",
    "        flat_entry[\"choices\"] = choices\n",
    "\n",
    "        flattened.append(flat_entry)\n",
    "\n",
    "    # --- SAVE OUTPUT ---\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flattened, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def get_choices(json_data, paragraph, sentence_nr, key = \"choices\"):\n",
    "    \"\"\"\n",
    "    Returns the 'choices' value for a given paragraph and sentence number\n",
    "    from the flattened JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        json_file (str): Path to the flattened JSON file\n",
    "        paragraph (str or int): Paragraph number to look up\n",
    "        sentence_nr (str or int): Sentence number to look up\n",
    "\n",
    "    Returns:\n",
    "        str or None: The choices value if found, otherwise None\n",
    "    \"\"\"\n",
    "\n",
    "    for entry in json_data:\n",
    "        if str(entry.get(\"paragraph\")) == str(paragraph) and str(entry.get(\"sentence_nr\")) == str(sentence_nr):\n",
    "            return entry.get(key)\n",
    "\n",
    "    return None  # If no match found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "80fcac79",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "input_file_markus = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/markus_labeling.json\"\n",
=======
    "input_file_markus = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/v2_Gold_markus.json\"\n",
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
    "output_file_markus = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/cleaned_labelled_markus.json\"\n",
    "\n",
    "read_labeled_data_reformat(input_file_markus, output_file_markus)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "02218753",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "input_file_rune = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/labelled_rune.json\"\n",
=======
    "input_file_rune = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/v2_labelled_rune.json\"\n",
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
    "output_file_rune = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/cleaned_labelled_rune.json\"\n",
    "\n",
    "read_labeled_data_reformat(input_file_rune, output_file_rune)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 7,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "122c5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inter annotater agreement\n",
    "\n",
    "#load data\n",
    "with open(output_file_markus, \"r\", encoding=\"utf-8\") as f:\n",
    "        markus_data = json.load(f)\n",
    "\n",
    "with open(output_file_rune, \"r\", encoding=\"utf-8\") as f:\n",
    "        rune_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 8,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "67863ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_annotater_agreement = []\n",
    "inter_annotater_disagreement = []\n",
    "\n",
    "for entry in markus_data:\n",
    "    paragraph_nr = entry['paragraph']\n",
    "    sentence_nr = entry['sentence_nr']\n",
    "\n",
    "    model_blame = entry[\"Blame\"]\n",
    "\n",
    "    markus_choice = entry[\"choices\"]\n",
    "    \n",
    "    rune_choice = get_choices(rune_data, paragraph_nr, sentence_nr)\n",
    "\n",
    "    if markus_choice != rune_choice:\n",
    "        inter_annotater_disagreement.append((int(paragraph_nr), int(sentence_nr)))\n",
    "\n",
    "    elif markus_choice == rune_choice:\n",
    "        inter_annotater_agreement.append((int(paragraph_nr), int(sentence_nr)))\n",
    "    \n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 10,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "0551e213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "86\n",
      "14\n"
=======
      "88.0\n",
      "12.0\n"
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(len(inter_annotater_agreement))\n",
    "print(len(inter_annotater_disagreement))"
=======
    "print((len(inter_annotater_agreement)/200)*100)\n",
    "print((len(inter_annotater_disagreement)/200)*100)\n",
    "\n"
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
<<<<<<< HEAD
    "inter-annotater agreement of 86%\n",
=======
    "inter-annotater agreement of 88%\n",
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
    "'''"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 11,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "e5b9b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_with_model = []\n",
    "disagreement_with_model = []\n",
    "\n",
    "for entry in markus_data:\n",
    "    paragraph_nr = entry['paragraph']\n",
    "    sentence_nr = entry['sentence_nr']\n",
    "\n",
    "    model_blame = entry[\"Blame\"]\n",
    "\n",
    "    markus_choice = entry[\"choices\"]\n",
    "    \n",
    "    rune_choice = get_choices(rune_data, paragraph_nr, sentence_nr)\n",
    "\n",
    "    annotater_choice = 0\n",
    "\n",
    "    if (markus_choice == \"Blame\") or (rune_choice == \"Blame\"):\n",
    "        annotater_choice = 1\n",
    "\n",
    "    if annotater_choice == int(model_blame):\n",
    "        agreement_with_model.append((int(paragraph_nr), int(sentence_nr)))\n",
    "    \n",
    "    elif annotater_choice != int(model_blame):\n",
    "        disagreement_with_model.append((int(paragraph_nr), int(sentence_nr)))\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 13,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "c4acb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "78\n",
      "22\n"
=======
      "76.5\n",
      "23.5\n"
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(len(agreement_with_model))\n",
    "print(len(disagreement_with_model))"
=======
    "print((len(agreement_with_model)/200)*100)\n",
    "print((len(disagreement_with_model)/200)*100)"
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c05797",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
<<<<<<< HEAD
    "One or more annotaters agreee with the model 78% of the samples\n",
=======
    "One or more annotaters agreee with the model 76.5% of the samples\n",
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
    "'''"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 16,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "a134de77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "80.23255813953489\n"
=======
      "77.8409090909091\n"
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "#Find in how great a proportion of the times where both annotaters agreed, \n",
=======
    "#Find out how great a proportion of the times where both annotaters agreed, \n",
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
    "# did they also agree with the model\n",
    "\n",
    "inter_ann_aggee_and_model = 0\n",
    "for i in inter_annotater_agreement:\n",
    "    if i in agreement_with_model:\n",
    "        inter_ann_aggee_and_model += 1\n",
    "\n",
    "\n",
    "print((inter_ann_aggee_and_model/len(inter_annotater_agreement))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334995a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
    "#in 80 percent of all cases where both annotaters agreed, \n",
=======
    "#in 77.8 percent of all cases where both annotaters agreed, \n",
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
    "# the labels of the PolDebate model also agreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db833895",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversly if the annotaters did not agree, \n",
    "# how is the distribution of blame/no blame by the model?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 17,
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "id": "7ec58b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "64.28571428571429\n"
=======
      "66.66666666666666\n"
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
     ]
    }
   ],
   "source": [
    "blame = 0\n",
    "no_blame = 0\n",
    "\n",
    "for para, sent in inter_annotater_disagreement:\n",
    "    model_choice = get_choices(markus_data, para, sent, key= 'Blame')\n",
    "\n",
    "    if model_choice == 1:\n",
    "        blame+=1\n",
    "    elif model_choice == 0:\n",
    "        no_blame += 1\n",
    "\n",
    "print((blame/(no_blame+blame))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd739d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "a rather even, at least to some extend, distribution of blame/no_blame in \n",
<<<<<<< HEAD
    "inter-annotater disagreeement, where 64% were classified as blame by the PolDebate \n",
=======
    "inter-annotater disagreeement, where 66% were classified as blame by the PolDebate \n",
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "f6c20b1e",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 18,
   "id": "f6c20b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.72727272727273\n"
     ]
    }
   ],
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
   "source": [
    "blame = 0\n",
    "no_blame = 0\n",
    "\n",
    "for para, sent in inter_annotater_agreement:\n",
    "    model_choice = get_choices(markus_data, para, sent, key= 'Blame')\n",
    "\n",
    "    if model_choice == 1:\n",
    "        blame+=1\n",
    "    elif model_choice == 0:\n",
    "        no_blame += 1\n",
    "\n",
    "print((blame/(no_blame+blame))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A note about the sentences classified as blame by the PolDebate model.\n",
    "During annotation quite a bit of the sentences would be negative but \n",
    "manually annotated as blame. Thus the PolDebate model might have a tendence \n",
    "(understandable) to highten the probability of a sentence containing blame simply due\n",
    "to the sentiment of the sentence. However, the data used for training \n",
    "(sentences classified as blame by all five templates) is assumed to be more robust and \n",
    "thus some of these misclassifications are assumed to be watered down enough. \n",
    "The validation of the model on the gold-labeled dataset will show to what degree \n",
    "this was achieved.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ebf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hell yearh"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa34690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 176 matching entries to filtered_output.json.\n"
     ]
    }
   ],
   "source": [
    "# Only keep sentences where both annotaters agreed and save file in model data\n",
    "\n",
    "\n",
    "# --- Input data ---\n",
    "matches = inter_annotater_agreement\n",
    "\n",
    "# Convert to a set of tuples with string values, since the JSON stores them as strings\n",
    "match_set = {(str(p), str(s)) for p, s in matches}\n",
    "\n",
    "# --- Load the JSON file ---\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/cleaned_labelled_markus.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- Filter matching entries ---\n",
    "# --- Filter and modify entries ---\n",
    "filtered_entries = []\n",
    "for entry in data:\n",
    "    if (entry.get(\"paragraph\"), entry.get(\"sentence_nr\")) in match_set:\n",
    "        # Add the new \"label\" attribute\n",
    "        if entry.get(\"choices\") == \"Blame\":\n",
    "            entry[\"label\"] = 1\n",
    "        elif entry.get(\"choices\") == \"No blame\":\n",
    "            entry[\"label\"] = 0\n",
    "        else:\n",
    "            entry[\"label\"] = None  # Optional fallback in case of missing or unexpected value\n",
    "\n",
    "        # Delete \"choices\" and \"Blame\" fields if present\n",
    "        entry.pop(\"choices\", None)\n",
    "        entry.pop(\"Blame\", None)\n",
    "\n",
    "        filtered_entries.append(entry)\n",
    "\n",
    "\n",
    "\n",
    "# --- Save to new JSON file ---\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Model_data/validation_set.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_entries, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Saved {len(filtered_entries)} matching entries to filtered_output.json.\")\n"
   ]
>>>>>>> 4e31ccdbd704986d8ba2cd8388461ae9b3341c13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
