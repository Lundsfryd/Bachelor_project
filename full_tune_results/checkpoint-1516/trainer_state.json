{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1516,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006596306068601583,
      "grad_norm": 4.531986236572266,
      "learning_rate": 0.0001,
      "loss": 1.0457,
      "step": 1
    },
    {
      "epoch": 0.0013192612137203166,
      "grad_norm": 5.067681789398193,
      "learning_rate": 9.997801231310466e-05,
      "loss": 0.9627,
      "step": 2
    },
    {
      "epoch": 0.001978891820580475,
      "grad_norm": 5.772594451904297,
      "learning_rate": 9.995602462620933e-05,
      "loss": 0.7771,
      "step": 3
    },
    {
      "epoch": 0.002638522427440633,
      "grad_norm": 6.303614616394043,
      "learning_rate": 9.993403693931399e-05,
      "loss": 0.8937,
      "step": 4
    },
    {
      "epoch": 0.0032981530343007917,
      "grad_norm": 2.969001531600952,
      "learning_rate": 9.991204925241865e-05,
      "loss": 0.6488,
      "step": 5
    },
    {
      "epoch": 0.00395778364116095,
      "grad_norm": 9.095396041870117,
      "learning_rate": 9.98900615655233e-05,
      "loss": 0.6489,
      "step": 6
    },
    {
      "epoch": 0.004617414248021108,
      "grad_norm": 16.392295837402344,
      "learning_rate": 9.986807387862797e-05,
      "loss": 0.8172,
      "step": 7
    },
    {
      "epoch": 0.005277044854881266,
      "grad_norm": 4.317264556884766,
      "learning_rate": 9.984608619173263e-05,
      "loss": 0.6442,
      "step": 8
    },
    {
      "epoch": 0.005936675461741424,
      "grad_norm": 5.794140815734863,
      "learning_rate": 9.982409850483729e-05,
      "loss": 0.8768,
      "step": 9
    },
    {
      "epoch": 0.006596306068601583,
      "grad_norm": 5.2939019203186035,
      "learning_rate": 9.980211081794196e-05,
      "loss": 0.7239,
      "step": 10
    },
    {
      "epoch": 0.007255936675461741,
      "grad_norm": 6.291360855102539,
      "learning_rate": 9.978012313104662e-05,
      "loss": 0.7026,
      "step": 11
    },
    {
      "epoch": 0.0079155672823219,
      "grad_norm": 4.184798240661621,
      "learning_rate": 9.975813544415129e-05,
      "loss": 0.7767,
      "step": 12
    },
    {
      "epoch": 0.008575197889182058,
      "grad_norm": 3.3040435314178467,
      "learning_rate": 9.973614775725594e-05,
      "loss": 0.6402,
      "step": 13
    },
    {
      "epoch": 0.009234828496042216,
      "grad_norm": 2.6878457069396973,
      "learning_rate": 9.971416007036061e-05,
      "loss": 0.8372,
      "step": 14
    },
    {
      "epoch": 0.009894459102902375,
      "grad_norm": 16.032752990722656,
      "learning_rate": 9.969217238346527e-05,
      "loss": 1.0326,
      "step": 15
    },
    {
      "epoch": 0.010554089709762533,
      "grad_norm": 5.763377666473389,
      "learning_rate": 9.967018469656993e-05,
      "loss": 0.717,
      "step": 16
    },
    {
      "epoch": 0.011213720316622692,
      "grad_norm": 4.459600448608398,
      "learning_rate": 9.964819700967458e-05,
      "loss": 1.0234,
      "step": 17
    },
    {
      "epoch": 0.011873350923482849,
      "grad_norm": 4.630609035491943,
      "learning_rate": 9.962620932277925e-05,
      "loss": 0.78,
      "step": 18
    },
    {
      "epoch": 0.012532981530343008,
      "grad_norm": 3.39028263092041,
      "learning_rate": 9.960422163588391e-05,
      "loss": 0.7027,
      "step": 19
    },
    {
      "epoch": 0.013192612137203167,
      "grad_norm": 5.187651634216309,
      "learning_rate": 9.958223394898857e-05,
      "loss": 0.9289,
      "step": 20
    },
    {
      "epoch": 0.013852242744063324,
      "grad_norm": 12.003926277160645,
      "learning_rate": 9.956024626209324e-05,
      "loss": 0.7179,
      "step": 21
    },
    {
      "epoch": 0.014511873350923483,
      "grad_norm": 5.073107719421387,
      "learning_rate": 9.95382585751979e-05,
      "loss": 0.8415,
      "step": 22
    },
    {
      "epoch": 0.015171503957783642,
      "grad_norm": 7.556689262390137,
      "learning_rate": 9.951627088830255e-05,
      "loss": 0.873,
      "step": 23
    },
    {
      "epoch": 0.0158311345646438,
      "grad_norm": 6.803813457489014,
      "learning_rate": 9.949428320140721e-05,
      "loss": 0.7995,
      "step": 24
    },
    {
      "epoch": 0.016490765171503958,
      "grad_norm": 8.11173152923584,
      "learning_rate": 9.947229551451188e-05,
      "loss": 0.9972,
      "step": 25
    },
    {
      "epoch": 0.017150395778364115,
      "grad_norm": 10.537644386291504,
      "learning_rate": 9.945030782761654e-05,
      "loss": 0.6363,
      "step": 26
    },
    {
      "epoch": 0.017810026385224276,
      "grad_norm": 3.1294376850128174,
      "learning_rate": 9.94283201407212e-05,
      "loss": 0.7346,
      "step": 27
    },
    {
      "epoch": 0.018469656992084433,
      "grad_norm": 2.057481527328491,
      "learning_rate": 9.940633245382587e-05,
      "loss": 0.6061,
      "step": 28
    },
    {
      "epoch": 0.01912928759894459,
      "grad_norm": 1.193145990371704,
      "learning_rate": 9.938434476693052e-05,
      "loss": 0.5502,
      "step": 29
    },
    {
      "epoch": 0.01978891820580475,
      "grad_norm": 2.5944647789001465,
      "learning_rate": 9.936235708003518e-05,
      "loss": 0.6297,
      "step": 30
    },
    {
      "epoch": 0.020448548812664908,
      "grad_norm": 3.0091919898986816,
      "learning_rate": 9.934036939313984e-05,
      "loss": 0.6462,
      "step": 31
    },
    {
      "epoch": 0.021108179419525065,
      "grad_norm": 3.257093667984009,
      "learning_rate": 9.93183817062445e-05,
      "loss": 0.7555,
      "step": 32
    },
    {
      "epoch": 0.021767810026385226,
      "grad_norm": 3.5682806968688965,
      "learning_rate": 9.929639401934916e-05,
      "loss": 0.7987,
      "step": 33
    },
    {
      "epoch": 0.022427440633245383,
      "grad_norm": 1.2468340396881104,
      "learning_rate": 9.927440633245383e-05,
      "loss": 0.5583,
      "step": 34
    },
    {
      "epoch": 0.02308707124010554,
      "grad_norm": 2.471144676208496,
      "learning_rate": 9.925241864555849e-05,
      "loss": 0.6557,
      "step": 35
    },
    {
      "epoch": 0.023746701846965697,
      "grad_norm": 2.374011993408203,
      "learning_rate": 9.923043095866316e-05,
      "loss": 0.5802,
      "step": 36
    },
    {
      "epoch": 0.024406332453825858,
      "grad_norm": 2.9550600051879883,
      "learning_rate": 9.920844327176782e-05,
      "loss": 0.644,
      "step": 37
    },
    {
      "epoch": 0.025065963060686015,
      "grad_norm": 2.389709234237671,
      "learning_rate": 9.918645558487248e-05,
      "loss": 0.6308,
      "step": 38
    },
    {
      "epoch": 0.025725593667546173,
      "grad_norm": 3.5071465969085693,
      "learning_rate": 9.916446789797715e-05,
      "loss": 0.7297,
      "step": 39
    },
    {
      "epoch": 0.026385224274406333,
      "grad_norm": 8.733741760253906,
      "learning_rate": 9.91424802110818e-05,
      "loss": 0.7281,
      "step": 40
    },
    {
      "epoch": 0.02704485488126649,
      "grad_norm": 2.8885374069213867,
      "learning_rate": 9.912049252418646e-05,
      "loss": 0.6053,
      "step": 41
    },
    {
      "epoch": 0.027704485488126648,
      "grad_norm": 3.98757266998291,
      "learning_rate": 9.909850483729112e-05,
      "loss": 0.5921,
      "step": 42
    },
    {
      "epoch": 0.02836411609498681,
      "grad_norm": 2.6323204040527344,
      "learning_rate": 9.907651715039579e-05,
      "loss": 0.5386,
      "step": 43
    },
    {
      "epoch": 0.029023746701846966,
      "grad_norm": 2.8174002170562744,
      "learning_rate": 9.905452946350044e-05,
      "loss": 0.565,
      "step": 44
    },
    {
      "epoch": 0.029683377308707123,
      "grad_norm": 6.7299065589904785,
      "learning_rate": 9.90325417766051e-05,
      "loss": 0.6746,
      "step": 45
    },
    {
      "epoch": 0.030343007915567283,
      "grad_norm": 2.758044481277466,
      "learning_rate": 9.901055408970977e-05,
      "loss": 0.4941,
      "step": 46
    },
    {
      "epoch": 0.03100263852242744,
      "grad_norm": 1.7152965068817139,
      "learning_rate": 9.898856640281443e-05,
      "loss": 0.5033,
      "step": 47
    },
    {
      "epoch": 0.0316622691292876,
      "grad_norm": 1.4907680749893188,
      "learning_rate": 9.896657871591909e-05,
      "loss": 0.5776,
      "step": 48
    },
    {
      "epoch": 0.032321899736147755,
      "grad_norm": 1.680080533027649,
      "learning_rate": 9.894459102902374e-05,
      "loss": 0.5173,
      "step": 49
    },
    {
      "epoch": 0.032981530343007916,
      "grad_norm": 1.9971681833267212,
      "learning_rate": 9.892260334212841e-05,
      "loss": 0.6037,
      "step": 50
    },
    {
      "epoch": 0.033641160949868076,
      "grad_norm": 9.688085556030273,
      "learning_rate": 9.890061565523307e-05,
      "loss": 0.5462,
      "step": 51
    },
    {
      "epoch": 0.03430079155672823,
      "grad_norm": 3.970235824584961,
      "learning_rate": 9.887862796833773e-05,
      "loss": 0.6729,
      "step": 52
    },
    {
      "epoch": 0.03496042216358839,
      "grad_norm": 2.120004653930664,
      "learning_rate": 9.88566402814424e-05,
      "loss": 0.5057,
      "step": 53
    },
    {
      "epoch": 0.03562005277044855,
      "grad_norm": 10.357929229736328,
      "learning_rate": 9.883465259454706e-05,
      "loss": 0.5715,
      "step": 54
    },
    {
      "epoch": 0.036279683377308705,
      "grad_norm": 9.115762710571289,
      "learning_rate": 9.881266490765173e-05,
      "loss": 0.4746,
      "step": 55
    },
    {
      "epoch": 0.036939313984168866,
      "grad_norm": 5.591172218322754,
      "learning_rate": 9.879067722075638e-05,
      "loss": 0.6536,
      "step": 56
    },
    {
      "epoch": 0.037598944591029027,
      "grad_norm": 2.4644813537597656,
      "learning_rate": 9.876868953386105e-05,
      "loss": 0.5088,
      "step": 57
    },
    {
      "epoch": 0.03825857519788918,
      "grad_norm": 6.665093421936035,
      "learning_rate": 9.874670184696571e-05,
      "loss": 0.8179,
      "step": 58
    },
    {
      "epoch": 0.03891820580474934,
      "grad_norm": 5.4693498611450195,
      "learning_rate": 9.872471416007037e-05,
      "loss": 0.5125,
      "step": 59
    },
    {
      "epoch": 0.0395778364116095,
      "grad_norm": 2.9178969860076904,
      "learning_rate": 9.870272647317502e-05,
      "loss": 0.5517,
      "step": 60
    },
    {
      "epoch": 0.040237467018469655,
      "grad_norm": 2.3586549758911133,
      "learning_rate": 9.86807387862797e-05,
      "loss": 0.4732,
      "step": 61
    },
    {
      "epoch": 0.040897097625329816,
      "grad_norm": 2.713927745819092,
      "learning_rate": 9.865875109938435e-05,
      "loss": 0.4758,
      "step": 62
    },
    {
      "epoch": 0.04155672823218998,
      "grad_norm": 1.9510691165924072,
      "learning_rate": 9.863676341248901e-05,
      "loss": 0.3906,
      "step": 63
    },
    {
      "epoch": 0.04221635883905013,
      "grad_norm": 3.3277153968811035,
      "learning_rate": 9.861477572559368e-05,
      "loss": 0.4649,
      "step": 64
    },
    {
      "epoch": 0.04287598944591029,
      "grad_norm": 4.11034631729126,
      "learning_rate": 9.859278803869834e-05,
      "loss": 0.546,
      "step": 65
    },
    {
      "epoch": 0.04353562005277045,
      "grad_norm": 5.576740741729736,
      "learning_rate": 9.857080035180299e-05,
      "loss": 0.5581,
      "step": 66
    },
    {
      "epoch": 0.044195250659630606,
      "grad_norm": 4.263465404510498,
      "learning_rate": 9.854881266490765e-05,
      "loss": 0.574,
      "step": 67
    },
    {
      "epoch": 0.044854881266490766,
      "grad_norm": 3.7017147541046143,
      "learning_rate": 9.852682497801232e-05,
      "loss": 0.3851,
      "step": 68
    },
    {
      "epoch": 0.04551451187335093,
      "grad_norm": 3.178976058959961,
      "learning_rate": 9.850483729111698e-05,
      "loss": 0.3864,
      "step": 69
    },
    {
      "epoch": 0.04617414248021108,
      "grad_norm": 8.117331504821777,
      "learning_rate": 9.848284960422163e-05,
      "loss": 0.5726,
      "step": 70
    },
    {
      "epoch": 0.04683377308707124,
      "grad_norm": 3.4088168144226074,
      "learning_rate": 9.846086191732629e-05,
      "loss": 0.4248,
      "step": 71
    },
    {
      "epoch": 0.047493403693931395,
      "grad_norm": 2.6418685913085938,
      "learning_rate": 9.843887423043096e-05,
      "loss": 0.3513,
      "step": 72
    },
    {
      "epoch": 0.048153034300791556,
      "grad_norm": 1.60867178440094,
      "learning_rate": 9.841688654353562e-05,
      "loss": 0.3322,
      "step": 73
    },
    {
      "epoch": 0.048812664907651716,
      "grad_norm": 3.885395050048828,
      "learning_rate": 9.839489885664029e-05,
      "loss": 0.3567,
      "step": 74
    },
    {
      "epoch": 0.04947229551451187,
      "grad_norm": 2.9260194301605225,
      "learning_rate": 9.837291116974495e-05,
      "loss": 0.3209,
      "step": 75
    },
    {
      "epoch": 0.05013192612137203,
      "grad_norm": 6.384348392486572,
      "learning_rate": 9.835092348284962e-05,
      "loss": 0.3784,
      "step": 76
    },
    {
      "epoch": 0.05079155672823219,
      "grad_norm": 2.570908784866333,
      "learning_rate": 9.832893579595427e-05,
      "loss": 0.2482,
      "step": 77
    },
    {
      "epoch": 0.051451187335092345,
      "grad_norm": 16.984237670898438,
      "learning_rate": 9.830694810905893e-05,
      "loss": 1.384,
      "step": 78
    },
    {
      "epoch": 0.052110817941952506,
      "grad_norm": 3.1267828941345215,
      "learning_rate": 9.82849604221636e-05,
      "loss": 0.3955,
      "step": 79
    },
    {
      "epoch": 0.052770448548812667,
      "grad_norm": 61.00532150268555,
      "learning_rate": 9.826297273526826e-05,
      "loss": 1.3551,
      "step": 80
    },
    {
      "epoch": 0.05343007915567282,
      "grad_norm": 3.1040701866149902,
      "learning_rate": 9.824098504837292e-05,
      "loss": 0.3821,
      "step": 81
    },
    {
      "epoch": 0.05408970976253298,
      "grad_norm": 4.638526439666748,
      "learning_rate": 9.821899736147759e-05,
      "loss": 0.5077,
      "step": 82
    },
    {
      "epoch": 0.05474934036939314,
      "grad_norm": 6.687178611755371,
      "learning_rate": 9.819700967458224e-05,
      "loss": 0.3525,
      "step": 83
    },
    {
      "epoch": 0.055408970976253295,
      "grad_norm": 13.174142837524414,
      "learning_rate": 9.81750219876869e-05,
      "loss": 0.5467,
      "step": 84
    },
    {
      "epoch": 0.056068601583113456,
      "grad_norm": 1.9921770095825195,
      "learning_rate": 9.815303430079156e-05,
      "loss": 0.2906,
      "step": 85
    },
    {
      "epoch": 0.05672823218997362,
      "grad_norm": 3.9600632190704346,
      "learning_rate": 9.813104661389623e-05,
      "loss": 0.2594,
      "step": 86
    },
    {
      "epoch": 0.05738786279683377,
      "grad_norm": 3.9642388820648193,
      "learning_rate": 9.810905892700088e-05,
      "loss": 0.3104,
      "step": 87
    },
    {
      "epoch": 0.05804749340369393,
      "grad_norm": 1.7085200548171997,
      "learning_rate": 9.808707124010554e-05,
      "loss": 0.1688,
      "step": 88
    },
    {
      "epoch": 0.05870712401055409,
      "grad_norm": 4.156135559082031,
      "learning_rate": 9.80650835532102e-05,
      "loss": 0.2328,
      "step": 89
    },
    {
      "epoch": 0.059366754617414245,
      "grad_norm": 5.6058878898620605,
      "learning_rate": 9.804309586631487e-05,
      "loss": 0.3186,
      "step": 90
    },
    {
      "epoch": 0.060026385224274406,
      "grad_norm": 6.102548599243164,
      "learning_rate": 9.802110817941953e-05,
      "loss": 0.2986,
      "step": 91
    },
    {
      "epoch": 0.06068601583113457,
      "grad_norm": 16.016244888305664,
      "learning_rate": 9.799912049252418e-05,
      "loss": 0.399,
      "step": 92
    },
    {
      "epoch": 0.06134564643799472,
      "grad_norm": 8.95863151550293,
      "learning_rate": 9.797713280562885e-05,
      "loss": 0.4655,
      "step": 93
    },
    {
      "epoch": 0.06200527704485488,
      "grad_norm": 3.887929916381836,
      "learning_rate": 9.795514511873351e-05,
      "loss": 0.2281,
      "step": 94
    },
    {
      "epoch": 0.06266490765171503,
      "grad_norm": 4.085494518280029,
      "learning_rate": 9.793315743183817e-05,
      "loss": 0.2395,
      "step": 95
    },
    {
      "epoch": 0.0633245382585752,
      "grad_norm": 17.440839767456055,
      "learning_rate": 9.791116974494284e-05,
      "loss": 1.9235,
      "step": 96
    },
    {
      "epoch": 0.06398416886543536,
      "grad_norm": 8.297568321228027,
      "learning_rate": 9.78891820580475e-05,
      "loss": 0.6553,
      "step": 97
    },
    {
      "epoch": 0.06464379947229551,
      "grad_norm": 3.6152451038360596,
      "learning_rate": 9.786719437115217e-05,
      "loss": 0.284,
      "step": 98
    },
    {
      "epoch": 0.06530343007915568,
      "grad_norm": 12.869908332824707,
      "learning_rate": 9.784520668425682e-05,
      "loss": 0.3809,
      "step": 99
    },
    {
      "epoch": 0.06596306068601583,
      "grad_norm": 8.249813079833984,
      "learning_rate": 9.782321899736149e-05,
      "loss": 0.3895,
      "step": 100
    },
    {
      "epoch": 0.06662269129287599,
      "grad_norm": 7.127250671386719,
      "learning_rate": 9.780123131046615e-05,
      "loss": 0.3677,
      "step": 101
    },
    {
      "epoch": 0.06728232189973615,
      "grad_norm": 11.182333946228027,
      "learning_rate": 9.777924362357081e-05,
      "loss": 0.7908,
      "step": 102
    },
    {
      "epoch": 0.0679419525065963,
      "grad_norm": 22.779510498046875,
      "learning_rate": 9.775725593667546e-05,
      "loss": 0.8439,
      "step": 103
    },
    {
      "epoch": 0.06860158311345646,
      "grad_norm": 3.75848650932312,
      "learning_rate": 9.773526824978013e-05,
      "loss": 0.21,
      "step": 104
    },
    {
      "epoch": 0.06926121372031663,
      "grad_norm": 11.627514839172363,
      "learning_rate": 9.771328056288479e-05,
      "loss": 0.4251,
      "step": 105
    },
    {
      "epoch": 0.06992084432717678,
      "grad_norm": 17.779827117919922,
      "learning_rate": 9.769129287598945e-05,
      "loss": 0.9118,
      "step": 106
    },
    {
      "epoch": 0.07058047493403694,
      "grad_norm": 6.356856822967529,
      "learning_rate": 9.76693051890941e-05,
      "loss": 0.3861,
      "step": 107
    },
    {
      "epoch": 0.0712401055408971,
      "grad_norm": 5.841317176818848,
      "learning_rate": 9.764731750219878e-05,
      "loss": 0.4019,
      "step": 108
    },
    {
      "epoch": 0.07189973614775726,
      "grad_norm": 4.2984514236450195,
      "learning_rate": 9.762532981530343e-05,
      "loss": 0.2313,
      "step": 109
    },
    {
      "epoch": 0.07255936675461741,
      "grad_norm": 1.7280741930007935,
      "learning_rate": 9.760334212840809e-05,
      "loss": 0.2054,
      "step": 110
    },
    {
      "epoch": 0.07321899736147758,
      "grad_norm": 8.101102828979492,
      "learning_rate": 9.758135444151276e-05,
      "loss": 0.2712,
      "step": 111
    },
    {
      "epoch": 0.07387862796833773,
      "grad_norm": 8.789128303527832,
      "learning_rate": 9.755936675461742e-05,
      "loss": 0.5046,
      "step": 112
    },
    {
      "epoch": 0.07453825857519789,
      "grad_norm": 6.344821929931641,
      "learning_rate": 9.753737906772207e-05,
      "loss": 0.2059,
      "step": 113
    },
    {
      "epoch": 0.07519788918205805,
      "grad_norm": 20.985668182373047,
      "learning_rate": 9.751539138082673e-05,
      "loss": 1.1103,
      "step": 114
    },
    {
      "epoch": 0.0758575197889182,
      "grad_norm": 10.115891456604004,
      "learning_rate": 9.74934036939314e-05,
      "loss": 0.2982,
      "step": 115
    },
    {
      "epoch": 0.07651715039577836,
      "grad_norm": 11.470943450927734,
      "learning_rate": 9.747141600703606e-05,
      "loss": 0.6502,
      "step": 116
    },
    {
      "epoch": 0.07717678100263853,
      "grad_norm": 3.588383436203003,
      "learning_rate": 9.744942832014073e-05,
      "loss": 0.225,
      "step": 117
    },
    {
      "epoch": 0.07783641160949868,
      "grad_norm": 3.0699503421783447,
      "learning_rate": 9.742744063324539e-05,
      "loss": 0.2426,
      "step": 118
    },
    {
      "epoch": 0.07849604221635884,
      "grad_norm": 4.775482654571533,
      "learning_rate": 9.740545294635006e-05,
      "loss": 0.2953,
      "step": 119
    },
    {
      "epoch": 0.079155672823219,
      "grad_norm": 4.584918975830078,
      "learning_rate": 9.738346525945471e-05,
      "loss": 0.3863,
      "step": 120
    },
    {
      "epoch": 0.07981530343007916,
      "grad_norm": 5.9860758781433105,
      "learning_rate": 9.736147757255937e-05,
      "loss": 0.2795,
      "step": 121
    },
    {
      "epoch": 0.08047493403693931,
      "grad_norm": 4.324174880981445,
      "learning_rate": 9.733948988566404e-05,
      "loss": 0.4418,
      "step": 122
    },
    {
      "epoch": 0.08113456464379948,
      "grad_norm": 9.555084228515625,
      "learning_rate": 9.73175021987687e-05,
      "loss": 0.6746,
      "step": 123
    },
    {
      "epoch": 0.08179419525065963,
      "grad_norm": 13.854504585266113,
      "learning_rate": 9.729551451187336e-05,
      "loss": 0.6313,
      "step": 124
    },
    {
      "epoch": 0.08245382585751979,
      "grad_norm": 8.088510513305664,
      "learning_rate": 9.727352682497801e-05,
      "loss": 0.3207,
      "step": 125
    },
    {
      "epoch": 0.08311345646437995,
      "grad_norm": 6.426117420196533,
      "learning_rate": 9.725153913808268e-05,
      "loss": 0.2712,
      "step": 126
    },
    {
      "epoch": 0.08377308707124011,
      "grad_norm": 4.953434467315674,
      "learning_rate": 9.722955145118734e-05,
      "loss": 0.3994,
      "step": 127
    },
    {
      "epoch": 0.08443271767810026,
      "grad_norm": 4.749808311462402,
      "learning_rate": 9.7207563764292e-05,
      "loss": 0.2691,
      "step": 128
    },
    {
      "epoch": 0.08509234828496043,
      "grad_norm": 6.768006801605225,
      "learning_rate": 9.718557607739667e-05,
      "loss": 0.2721,
      "step": 129
    },
    {
      "epoch": 0.08575197889182058,
      "grad_norm": 3.2697718143463135,
      "learning_rate": 9.716358839050132e-05,
      "loss": 0.1877,
      "step": 130
    },
    {
      "epoch": 0.08641160949868074,
      "grad_norm": 10.125407218933105,
      "learning_rate": 9.714160070360598e-05,
      "loss": 0.8895,
      "step": 131
    },
    {
      "epoch": 0.0870712401055409,
      "grad_norm": 5.645936489105225,
      "learning_rate": 9.711961301671064e-05,
      "loss": 0.26,
      "step": 132
    },
    {
      "epoch": 0.08773087071240106,
      "grad_norm": 5.156286239624023,
      "learning_rate": 9.709762532981531e-05,
      "loss": 0.3277,
      "step": 133
    },
    {
      "epoch": 0.08839050131926121,
      "grad_norm": 4.345258712768555,
      "learning_rate": 9.707563764291997e-05,
      "loss": 0.3462,
      "step": 134
    },
    {
      "epoch": 0.08905013192612138,
      "grad_norm": 2.528203248977661,
      "learning_rate": 9.705364995602462e-05,
      "loss": 0.2541,
      "step": 135
    },
    {
      "epoch": 0.08970976253298153,
      "grad_norm": 3.7870678901672363,
      "learning_rate": 9.70316622691293e-05,
      "loss": 0.2907,
      "step": 136
    },
    {
      "epoch": 0.09036939313984169,
      "grad_norm": 2.0809385776519775,
      "learning_rate": 9.700967458223395e-05,
      "loss": 0.1854,
      "step": 137
    },
    {
      "epoch": 0.09102902374670185,
      "grad_norm": 10.749056816101074,
      "learning_rate": 9.698768689533861e-05,
      "loss": 0.417,
      "step": 138
    },
    {
      "epoch": 0.09168865435356201,
      "grad_norm": 4.189787864685059,
      "learning_rate": 9.696569920844328e-05,
      "loss": 0.2488,
      "step": 139
    },
    {
      "epoch": 0.09234828496042216,
      "grad_norm": 7.159988880157471,
      "learning_rate": 9.694371152154795e-05,
      "loss": 0.2837,
      "step": 140
    },
    {
      "epoch": 0.09300791556728233,
      "grad_norm": 3.8593969345092773,
      "learning_rate": 9.69217238346526e-05,
      "loss": 0.2673,
      "step": 141
    },
    {
      "epoch": 0.09366754617414248,
      "grad_norm": 58.34790802001953,
      "learning_rate": 9.689973614775726e-05,
      "loss": 0.2787,
      "step": 142
    },
    {
      "epoch": 0.09432717678100264,
      "grad_norm": 16.038251876831055,
      "learning_rate": 9.687774846086192e-05,
      "loss": 0.8106,
      "step": 143
    },
    {
      "epoch": 0.09498680738786279,
      "grad_norm": 14.047736167907715,
      "learning_rate": 9.685576077396659e-05,
      "loss": 0.6585,
      "step": 144
    },
    {
      "epoch": 0.09564643799472296,
      "grad_norm": 22.948694229125977,
      "learning_rate": 9.683377308707125e-05,
      "loss": 0.8468,
      "step": 145
    },
    {
      "epoch": 0.09630606860158311,
      "grad_norm": 1.706349492073059,
      "learning_rate": 9.68117854001759e-05,
      "loss": 0.1385,
      "step": 146
    },
    {
      "epoch": 0.09696569920844327,
      "grad_norm": 4.248431205749512,
      "learning_rate": 9.678979771328057e-05,
      "loss": 0.1875,
      "step": 147
    },
    {
      "epoch": 0.09762532981530343,
      "grad_norm": 5.504576206207275,
      "learning_rate": 9.676781002638523e-05,
      "loss": 0.1686,
      "step": 148
    },
    {
      "epoch": 0.09828496042216359,
      "grad_norm": 10.182625770568848,
      "learning_rate": 9.674582233948989e-05,
      "loss": 0.7761,
      "step": 149
    },
    {
      "epoch": 0.09894459102902374,
      "grad_norm": 5.391607761383057,
      "learning_rate": 9.672383465259455e-05,
      "loss": 0.1835,
      "step": 150
    },
    {
      "epoch": 0.09960422163588391,
      "grad_norm": 9.352992057800293,
      "learning_rate": 9.670184696569922e-05,
      "loss": 0.5852,
      "step": 151
    },
    {
      "epoch": 0.10026385224274406,
      "grad_norm": 6.914687156677246,
      "learning_rate": 9.667985927880387e-05,
      "loss": 0.4032,
      "step": 152
    },
    {
      "epoch": 0.10092348284960422,
      "grad_norm": 14.83975887298584,
      "learning_rate": 9.665787159190853e-05,
      "loss": 0.9284,
      "step": 153
    },
    {
      "epoch": 0.10158311345646438,
      "grad_norm": 6.7429728507995605,
      "learning_rate": 9.66358839050132e-05,
      "loss": 0.2489,
      "step": 154
    },
    {
      "epoch": 0.10224274406332454,
      "grad_norm": 3.148801803588867,
      "learning_rate": 9.661389621811786e-05,
      "loss": 0.306,
      "step": 155
    },
    {
      "epoch": 0.10290237467018469,
      "grad_norm": 3.0552284717559814,
      "learning_rate": 9.659190853122251e-05,
      "loss": 0.3134,
      "step": 156
    },
    {
      "epoch": 0.10356200527704486,
      "grad_norm": 9.207420349121094,
      "learning_rate": 9.656992084432717e-05,
      "loss": 0.8167,
      "step": 157
    },
    {
      "epoch": 0.10422163588390501,
      "grad_norm": 6.983640193939209,
      "learning_rate": 9.654793315743184e-05,
      "loss": 0.2557,
      "step": 158
    },
    {
      "epoch": 0.10488126649076517,
      "grad_norm": 3.1181278228759766,
      "learning_rate": 9.65259454705365e-05,
      "loss": 0.1565,
      "step": 159
    },
    {
      "epoch": 0.10554089709762533,
      "grad_norm": 8.010306358337402,
      "learning_rate": 9.650395778364117e-05,
      "loss": 0.3395,
      "step": 160
    },
    {
      "epoch": 0.10620052770448549,
      "grad_norm": 2.4721341133117676,
      "learning_rate": 9.648197009674583e-05,
      "loss": 0.125,
      "step": 161
    },
    {
      "epoch": 0.10686015831134564,
      "grad_norm": 1.9041739702224731,
      "learning_rate": 9.64599824098505e-05,
      "loss": 0.2007,
      "step": 162
    },
    {
      "epoch": 0.10751978891820581,
      "grad_norm": 3.5505146980285645,
      "learning_rate": 9.643799472295515e-05,
      "loss": 0.2043,
      "step": 163
    },
    {
      "epoch": 0.10817941952506596,
      "grad_norm": 13.114124298095703,
      "learning_rate": 9.641600703605981e-05,
      "loss": 0.8276,
      "step": 164
    },
    {
      "epoch": 0.10883905013192612,
      "grad_norm": 2.7680492401123047,
      "learning_rate": 9.639401934916448e-05,
      "loss": 0.1985,
      "step": 165
    },
    {
      "epoch": 0.10949868073878628,
      "grad_norm": 2.6186463832855225,
      "learning_rate": 9.637203166226914e-05,
      "loss": 0.1169,
      "step": 166
    },
    {
      "epoch": 0.11015831134564644,
      "grad_norm": 3.694247007369995,
      "learning_rate": 9.63500439753738e-05,
      "loss": 0.2272,
      "step": 167
    },
    {
      "epoch": 0.11081794195250659,
      "grad_norm": 5.367670059204102,
      "learning_rate": 9.632805628847845e-05,
      "loss": 0.2045,
      "step": 168
    },
    {
      "epoch": 0.11147757255936676,
      "grad_norm": 6.287727355957031,
      "learning_rate": 9.630606860158312e-05,
      "loss": 0.2035,
      "step": 169
    },
    {
      "epoch": 0.11213720316622691,
      "grad_norm": 15.984766006469727,
      "learning_rate": 9.628408091468778e-05,
      "loss": 1.1333,
      "step": 170
    },
    {
      "epoch": 0.11279683377308707,
      "grad_norm": 19.921527862548828,
      "learning_rate": 9.626209322779244e-05,
      "loss": 0.6229,
      "step": 171
    },
    {
      "epoch": 0.11345646437994723,
      "grad_norm": 1.4522851705551147,
      "learning_rate": 9.624010554089711e-05,
      "loss": 0.0885,
      "step": 172
    },
    {
      "epoch": 0.11411609498680739,
      "grad_norm": 1.608290195465088,
      "learning_rate": 9.621811785400176e-05,
      "loss": 0.0936,
      "step": 173
    },
    {
      "epoch": 0.11477572559366754,
      "grad_norm": 22.7802791595459,
      "learning_rate": 9.619613016710642e-05,
      "loss": 1.5382,
      "step": 174
    },
    {
      "epoch": 0.11543535620052771,
      "grad_norm": 20.197134017944336,
      "learning_rate": 9.617414248021108e-05,
      "loss": 0.6578,
      "step": 175
    },
    {
      "epoch": 0.11609498680738786,
      "grad_norm": 31.607677459716797,
      "learning_rate": 9.615215479331575e-05,
      "loss": 1.0615,
      "step": 176
    },
    {
      "epoch": 0.11675461741424802,
      "grad_norm": 15.968056678771973,
      "learning_rate": 9.61301671064204e-05,
      "loss": 0.5403,
      "step": 177
    },
    {
      "epoch": 0.11741424802110818,
      "grad_norm": 0.9046594500541687,
      "learning_rate": 9.610817941952506e-05,
      "loss": 0.0837,
      "step": 178
    },
    {
      "epoch": 0.11807387862796834,
      "grad_norm": 12.26724624633789,
      "learning_rate": 9.608619173262972e-05,
      "loss": 0.7243,
      "step": 179
    },
    {
      "epoch": 0.11873350923482849,
      "grad_norm": 3.6950275897979736,
      "learning_rate": 9.606420404573439e-05,
      "loss": 0.2389,
      "step": 180
    },
    {
      "epoch": 0.11939313984168866,
      "grad_norm": 9.74068546295166,
      "learning_rate": 9.604221635883906e-05,
      "loss": 0.4203,
      "step": 181
    },
    {
      "epoch": 0.12005277044854881,
      "grad_norm": 4.175169944763184,
      "learning_rate": 9.602022867194372e-05,
      "loss": 0.269,
      "step": 182
    },
    {
      "epoch": 0.12071240105540897,
      "grad_norm": 3.911198377609253,
      "learning_rate": 9.599824098504839e-05,
      "loss": 0.378,
      "step": 183
    },
    {
      "epoch": 0.12137203166226913,
      "grad_norm": 7.8163533210754395,
      "learning_rate": 9.597625329815305e-05,
      "loss": 0.4011,
      "step": 184
    },
    {
      "epoch": 0.12203166226912929,
      "grad_norm": 11.546730995178223,
      "learning_rate": 9.59542656112577e-05,
      "loss": 0.5726,
      "step": 185
    },
    {
      "epoch": 0.12269129287598944,
      "grad_norm": 7.248900413513184,
      "learning_rate": 9.593227792436236e-05,
      "loss": 0.5427,
      "step": 186
    },
    {
      "epoch": 0.12335092348284961,
      "grad_norm": 6.656569957733154,
      "learning_rate": 9.591029023746703e-05,
      "loss": 0.327,
      "step": 187
    },
    {
      "epoch": 0.12401055408970976,
      "grad_norm": 8.585644721984863,
      "learning_rate": 9.588830255057169e-05,
      "loss": 0.7683,
      "step": 188
    },
    {
      "epoch": 0.12467018469656992,
      "grad_norm": 2.7941060066223145,
      "learning_rate": 9.586631486367634e-05,
      "loss": 0.1743,
      "step": 189
    },
    {
      "epoch": 0.12532981530343007,
      "grad_norm": 3.8324575424194336,
      "learning_rate": 9.584432717678101e-05,
      "loss": 0.2373,
      "step": 190
    },
    {
      "epoch": 0.12598944591029024,
      "grad_norm": 5.790744781494141,
      "learning_rate": 9.582233948988567e-05,
      "loss": 0.2279,
      "step": 191
    },
    {
      "epoch": 0.1266490765171504,
      "grad_norm": 2.7161808013916016,
      "learning_rate": 9.580035180299033e-05,
      "loss": 0.1262,
      "step": 192
    },
    {
      "epoch": 0.12730870712401055,
      "grad_norm": 16.125776290893555,
      "learning_rate": 9.577836411609499e-05,
      "loss": 0.6914,
      "step": 193
    },
    {
      "epoch": 0.1279683377308707,
      "grad_norm": 14.788344383239746,
      "learning_rate": 9.575637642919966e-05,
      "loss": 0.4566,
      "step": 194
    },
    {
      "epoch": 0.12862796833773088,
      "grad_norm": 19.527645111083984,
      "learning_rate": 9.573438874230431e-05,
      "loss": 0.9014,
      "step": 195
    },
    {
      "epoch": 0.12928759894459102,
      "grad_norm": 11.289031982421875,
      "learning_rate": 9.571240105540897e-05,
      "loss": 0.9914,
      "step": 196
    },
    {
      "epoch": 0.1299472295514512,
      "grad_norm": 4.1000871658325195,
      "learning_rate": 9.569041336851363e-05,
      "loss": 0.1791,
      "step": 197
    },
    {
      "epoch": 0.13060686015831136,
      "grad_norm": 6.532097339630127,
      "learning_rate": 9.56684256816183e-05,
      "loss": 0.2298,
      "step": 198
    },
    {
      "epoch": 0.1312664907651715,
      "grad_norm": 5.43147611618042,
      "learning_rate": 9.564643799472295e-05,
      "loss": 0.2111,
      "step": 199
    },
    {
      "epoch": 0.13192612137203166,
      "grad_norm": 7.010229110717773,
      "learning_rate": 9.562445030782761e-05,
      "loss": 0.602,
      "step": 200
    },
    {
      "epoch": 0.13258575197889183,
      "grad_norm": 14.178595542907715,
      "learning_rate": 9.560246262093228e-05,
      "loss": 0.808,
      "step": 201
    },
    {
      "epoch": 0.13324538258575197,
      "grad_norm": 7.512713432312012,
      "learning_rate": 9.558047493403694e-05,
      "loss": 0.6022,
      "step": 202
    },
    {
      "epoch": 0.13390501319261214,
      "grad_norm": 6.7358927726745605,
      "learning_rate": 9.555848724714161e-05,
      "loss": 0.3095,
      "step": 203
    },
    {
      "epoch": 0.1345646437994723,
      "grad_norm": 3.4137418270111084,
      "learning_rate": 9.553649956024627e-05,
      "loss": 0.2505,
      "step": 204
    },
    {
      "epoch": 0.13522427440633245,
      "grad_norm": 5.655755043029785,
      "learning_rate": 9.551451187335094e-05,
      "loss": 0.6017,
      "step": 205
    },
    {
      "epoch": 0.1358839050131926,
      "grad_norm": 8.7196626663208,
      "learning_rate": 9.54925241864556e-05,
      "loss": 0.378,
      "step": 206
    },
    {
      "epoch": 0.13654353562005278,
      "grad_norm": 4.970999717712402,
      "learning_rate": 9.547053649956025e-05,
      "loss": 0.3696,
      "step": 207
    },
    {
      "epoch": 0.13720316622691292,
      "grad_norm": 2.659224271774292,
      "learning_rate": 9.544854881266492e-05,
      "loss": 0.2462,
      "step": 208
    },
    {
      "epoch": 0.1378627968337731,
      "grad_norm": 2.564129114151001,
      "learning_rate": 9.542656112576958e-05,
      "loss": 0.2685,
      "step": 209
    },
    {
      "epoch": 0.13852242744063326,
      "grad_norm": 10.508207321166992,
      "learning_rate": 9.540457343887424e-05,
      "loss": 0.5707,
      "step": 210
    },
    {
      "epoch": 0.1391820580474934,
      "grad_norm": 8.377464294433594,
      "learning_rate": 9.538258575197889e-05,
      "loss": 0.3179,
      "step": 211
    },
    {
      "epoch": 0.13984168865435356,
      "grad_norm": 3.788947343826294,
      "learning_rate": 9.536059806508356e-05,
      "loss": 0.2135,
      "step": 212
    },
    {
      "epoch": 0.14050131926121373,
      "grad_norm": 6.8638691902160645,
      "learning_rate": 9.533861037818822e-05,
      "loss": 0.3873,
      "step": 213
    },
    {
      "epoch": 0.14116094986807387,
      "grad_norm": 3.6801187992095947,
      "learning_rate": 9.531662269129288e-05,
      "loss": 0.2075,
      "step": 214
    },
    {
      "epoch": 0.14182058047493404,
      "grad_norm": 7.366370677947998,
      "learning_rate": 9.529463500439753e-05,
      "loss": 0.2979,
      "step": 215
    },
    {
      "epoch": 0.1424802110817942,
      "grad_norm": 1.507960557937622,
      "learning_rate": 9.52726473175022e-05,
      "loss": 0.1739,
      "step": 216
    },
    {
      "epoch": 0.14313984168865435,
      "grad_norm": 3.6374645233154297,
      "learning_rate": 9.525065963060686e-05,
      "loss": 0.2495,
      "step": 217
    },
    {
      "epoch": 0.1437994722955145,
      "grad_norm": 5.5132269859313965,
      "learning_rate": 9.522867194371152e-05,
      "loss": 0.2764,
      "step": 218
    },
    {
      "epoch": 0.14445910290237468,
      "grad_norm": 3.053514242172241,
      "learning_rate": 9.520668425681619e-05,
      "loss": 0.217,
      "step": 219
    },
    {
      "epoch": 0.14511873350923482,
      "grad_norm": 3.836074113845825,
      "learning_rate": 9.518469656992085e-05,
      "loss": 0.1745,
      "step": 220
    },
    {
      "epoch": 0.145778364116095,
      "grad_norm": 4.121550559997559,
      "learning_rate": 9.51627088830255e-05,
      "loss": 0.2166,
      "step": 221
    },
    {
      "epoch": 0.14643799472295516,
      "grad_norm": 15.297948837280273,
      "learning_rate": 9.514072119613017e-05,
      "loss": 1.0627,
      "step": 222
    },
    {
      "epoch": 0.1470976253298153,
      "grad_norm": 13.965232849121094,
      "learning_rate": 9.511873350923483e-05,
      "loss": 1.1596,
      "step": 223
    },
    {
      "epoch": 0.14775725593667546,
      "grad_norm": 12.847553253173828,
      "learning_rate": 9.50967458223395e-05,
      "loss": 0.5558,
      "step": 224
    },
    {
      "epoch": 0.14841688654353563,
      "grad_norm": 22.347904205322266,
      "learning_rate": 9.507475813544416e-05,
      "loss": 0.9866,
      "step": 225
    },
    {
      "epoch": 0.14907651715039577,
      "grad_norm": 4.095053672790527,
      "learning_rate": 9.505277044854883e-05,
      "loss": 0.151,
      "step": 226
    },
    {
      "epoch": 0.14973614775725594,
      "grad_norm": 14.058609962463379,
      "learning_rate": 9.503078276165349e-05,
      "loss": 0.9183,
      "step": 227
    },
    {
      "epoch": 0.1503957783641161,
      "grad_norm": 11.485191345214844,
      "learning_rate": 9.500879507475814e-05,
      "loss": 0.4986,
      "step": 228
    },
    {
      "epoch": 0.15105540897097625,
      "grad_norm": 8.885944366455078,
      "learning_rate": 9.49868073878628e-05,
      "loss": 0.3998,
      "step": 229
    },
    {
      "epoch": 0.1517150395778364,
      "grad_norm": 3.3462908267974854,
      "learning_rate": 9.496481970096747e-05,
      "loss": 0.1675,
      "step": 230
    },
    {
      "epoch": 0.15237467018469658,
      "grad_norm": 4.375722408294678,
      "learning_rate": 9.494283201407213e-05,
      "loss": 0.3403,
      "step": 231
    },
    {
      "epoch": 0.15303430079155672,
      "grad_norm": 6.561213970184326,
      "learning_rate": 9.492084432717678e-05,
      "loss": 0.3421,
      "step": 232
    },
    {
      "epoch": 0.1536939313984169,
      "grad_norm": 4.641203880310059,
      "learning_rate": 9.489885664028144e-05,
      "loss": 0.2862,
      "step": 233
    },
    {
      "epoch": 0.15435356200527706,
      "grad_norm": 3.8651139736175537,
      "learning_rate": 9.487686895338611e-05,
      "loss": 0.1935,
      "step": 234
    },
    {
      "epoch": 0.1550131926121372,
      "grad_norm": 1.7850981950759888,
      "learning_rate": 9.485488126649077e-05,
      "loss": 0.2116,
      "step": 235
    },
    {
      "epoch": 0.15567282321899736,
      "grad_norm": 6.068543434143066,
      "learning_rate": 9.483289357959543e-05,
      "loss": 0.2744,
      "step": 236
    },
    {
      "epoch": 0.15633245382585753,
      "grad_norm": 2.1093294620513916,
      "learning_rate": 9.48109058927001e-05,
      "loss": 0.1222,
      "step": 237
    },
    {
      "epoch": 0.15699208443271767,
      "grad_norm": 3.2206156253814697,
      "learning_rate": 9.478891820580475e-05,
      "loss": 0.2057,
      "step": 238
    },
    {
      "epoch": 0.15765171503957784,
      "grad_norm": 5.696338653564453,
      "learning_rate": 9.476693051890941e-05,
      "loss": 0.2303,
      "step": 239
    },
    {
      "epoch": 0.158311345646438,
      "grad_norm": 7.671870708465576,
      "learning_rate": 9.474494283201407e-05,
      "loss": 0.321,
      "step": 240
    },
    {
      "epoch": 0.15897097625329815,
      "grad_norm": 3.0645029544830322,
      "learning_rate": 9.472295514511874e-05,
      "loss": 0.1358,
      "step": 241
    },
    {
      "epoch": 0.15963060686015831,
      "grad_norm": 8.630577087402344,
      "learning_rate": 9.47009674582234e-05,
      "loss": 0.4683,
      "step": 242
    },
    {
      "epoch": 0.16029023746701848,
      "grad_norm": 9.467769622802734,
      "learning_rate": 9.467897977132805e-05,
      "loss": 0.3776,
      "step": 243
    },
    {
      "epoch": 0.16094986807387862,
      "grad_norm": 3.119511127471924,
      "learning_rate": 9.465699208443272e-05,
      "loss": 0.1365,
      "step": 244
    },
    {
      "epoch": 0.1616094986807388,
      "grad_norm": 4.410305976867676,
      "learning_rate": 9.463500439753739e-05,
      "loss": 0.1863,
      "step": 245
    },
    {
      "epoch": 0.16226912928759896,
      "grad_norm": 11.90566349029541,
      "learning_rate": 9.461301671064205e-05,
      "loss": 0.7895,
      "step": 246
    },
    {
      "epoch": 0.1629287598944591,
      "grad_norm": 15.504525184631348,
      "learning_rate": 9.45910290237467e-05,
      "loss": 1.0234,
      "step": 247
    },
    {
      "epoch": 0.16358839050131926,
      "grad_norm": 1.4994410276412964,
      "learning_rate": 9.456904133685138e-05,
      "loss": 0.1622,
      "step": 248
    },
    {
      "epoch": 0.16424802110817943,
      "grad_norm": 3.1784205436706543,
      "learning_rate": 9.454705364995603e-05,
      "loss": 0.3086,
      "step": 249
    },
    {
      "epoch": 0.16490765171503957,
      "grad_norm": 2.8643293380737305,
      "learning_rate": 9.452506596306069e-05,
      "loss": 0.2153,
      "step": 250
    },
    {
      "epoch": 0.16556728232189974,
      "grad_norm": 4.11081600189209,
      "learning_rate": 9.450307827616535e-05,
      "loss": 0.3581,
      "step": 251
    },
    {
      "epoch": 0.1662269129287599,
      "grad_norm": 4.917235374450684,
      "learning_rate": 9.448109058927002e-05,
      "loss": 0.2695,
      "step": 252
    },
    {
      "epoch": 0.16688654353562005,
      "grad_norm": 2.669201135635376,
      "learning_rate": 9.445910290237468e-05,
      "loss": 0.2481,
      "step": 253
    },
    {
      "epoch": 0.16754617414248021,
      "grad_norm": 7.605030536651611,
      "learning_rate": 9.443711521547933e-05,
      "loss": 0.3376,
      "step": 254
    },
    {
      "epoch": 0.16820580474934038,
      "grad_norm": 5.632042407989502,
      "learning_rate": 9.4415127528584e-05,
      "loss": 0.2278,
      "step": 255
    },
    {
      "epoch": 0.16886543535620052,
      "grad_norm": 12.501486778259277,
      "learning_rate": 9.439313984168866e-05,
      "loss": 0.8338,
      "step": 256
    },
    {
      "epoch": 0.1695250659630607,
      "grad_norm": 9.749971389770508,
      "learning_rate": 9.437115215479332e-05,
      "loss": 0.5013,
      "step": 257
    },
    {
      "epoch": 0.17018469656992086,
      "grad_norm": 17.205278396606445,
      "learning_rate": 9.434916446789797e-05,
      "loss": 1.3322,
      "step": 258
    },
    {
      "epoch": 0.170844327176781,
      "grad_norm": 15.249499320983887,
      "learning_rate": 9.432717678100264e-05,
      "loss": 0.9385,
      "step": 259
    },
    {
      "epoch": 0.17150395778364116,
      "grad_norm": 10.776744842529297,
      "learning_rate": 9.43051890941073e-05,
      "loss": 0.46,
      "step": 260
    },
    {
      "epoch": 0.17216358839050133,
      "grad_norm": 12.72671890258789,
      "learning_rate": 9.428320140721196e-05,
      "loss": 0.7001,
      "step": 261
    },
    {
      "epoch": 0.17282321899736147,
      "grad_norm": 6.652371406555176,
      "learning_rate": 9.426121372031663e-05,
      "loss": 0.2714,
      "step": 262
    },
    {
      "epoch": 0.17348284960422164,
      "grad_norm": 6.954916000366211,
      "learning_rate": 9.423922603342129e-05,
      "loss": 0.2861,
      "step": 263
    },
    {
      "epoch": 0.1741424802110818,
      "grad_norm": 7.584397792816162,
      "learning_rate": 9.421723834652594e-05,
      "loss": 0.3263,
      "step": 264
    },
    {
      "epoch": 0.17480211081794195,
      "grad_norm": 3.30351185798645,
      "learning_rate": 9.419525065963061e-05,
      "loss": 0.3144,
      "step": 265
    },
    {
      "epoch": 0.17546174142480211,
      "grad_norm": 8.403555870056152,
      "learning_rate": 9.417326297273527e-05,
      "loss": 0.4162,
      "step": 266
    },
    {
      "epoch": 0.17612137203166228,
      "grad_norm": 5.725770950317383,
      "learning_rate": 9.415127528583994e-05,
      "loss": 0.322,
      "step": 267
    },
    {
      "epoch": 0.17678100263852242,
      "grad_norm": 9.461005210876465,
      "learning_rate": 9.41292875989446e-05,
      "loss": 0.5001,
      "step": 268
    },
    {
      "epoch": 0.1774406332453826,
      "grad_norm": 3.0130867958068848,
      "learning_rate": 9.410729991204925e-05,
      "loss": 0.4525,
      "step": 269
    },
    {
      "epoch": 0.17810026385224276,
      "grad_norm": 7.698887348175049,
      "learning_rate": 9.408531222515393e-05,
      "loss": 0.4358,
      "step": 270
    },
    {
      "epoch": 0.1787598944591029,
      "grad_norm": 2.1622262001037598,
      "learning_rate": 9.406332453825858e-05,
      "loss": 0.1798,
      "step": 271
    },
    {
      "epoch": 0.17941952506596306,
      "grad_norm": 2.9267311096191406,
      "learning_rate": 9.404133685136324e-05,
      "loss": 0.1884,
      "step": 272
    },
    {
      "epoch": 0.18007915567282323,
      "grad_norm": 3.0657594203948975,
      "learning_rate": 9.401934916446791e-05,
      "loss": 0.3082,
      "step": 273
    },
    {
      "epoch": 0.18073878627968337,
      "grad_norm": 7.003755569458008,
      "learning_rate": 9.399736147757257e-05,
      "loss": 0.7174,
      "step": 274
    },
    {
      "epoch": 0.18139841688654354,
      "grad_norm": 4.500641822814941,
      "learning_rate": 9.397537379067722e-05,
      "loss": 0.1894,
      "step": 275
    },
    {
      "epoch": 0.1820580474934037,
      "grad_norm": 2.134341239929199,
      "learning_rate": 9.395338610378188e-05,
      "loss": 0.1062,
      "step": 276
    },
    {
      "epoch": 0.18271767810026385,
      "grad_norm": 2.0787932872772217,
      "learning_rate": 9.393139841688655e-05,
      "loss": 0.1192,
      "step": 277
    },
    {
      "epoch": 0.18337730870712401,
      "grad_norm": 6.1616387367248535,
      "learning_rate": 9.390941072999121e-05,
      "loss": 0.3163,
      "step": 278
    },
    {
      "epoch": 0.18403693931398418,
      "grad_norm": 13.011774063110352,
      "learning_rate": 9.388742304309587e-05,
      "loss": 1.3315,
      "step": 279
    },
    {
      "epoch": 0.18469656992084432,
      "grad_norm": 10.852892875671387,
      "learning_rate": 9.386543535620054e-05,
      "loss": 0.5583,
      "step": 280
    },
    {
      "epoch": 0.1853562005277045,
      "grad_norm": 17.953712463378906,
      "learning_rate": 9.384344766930519e-05,
      "loss": 1.3629,
      "step": 281
    },
    {
      "epoch": 0.18601583113456466,
      "grad_norm": 11.034554481506348,
      "learning_rate": 9.382145998240985e-05,
      "loss": 0.7461,
      "step": 282
    },
    {
      "epoch": 0.1866754617414248,
      "grad_norm": 5.567540645599365,
      "learning_rate": 9.379947229551451e-05,
      "loss": 0.2415,
      "step": 283
    },
    {
      "epoch": 0.18733509234828497,
      "grad_norm": 7.338798522949219,
      "learning_rate": 9.377748460861918e-05,
      "loss": 0.3696,
      "step": 284
    },
    {
      "epoch": 0.1879947229551451,
      "grad_norm": 13.176653861999512,
      "learning_rate": 9.375549692172383e-05,
      "loss": 0.9469,
      "step": 285
    },
    {
      "epoch": 0.18865435356200527,
      "grad_norm": 1.9688154458999634,
      "learning_rate": 9.37335092348285e-05,
      "loss": 0.1837,
      "step": 286
    },
    {
      "epoch": 0.18931398416886544,
      "grad_norm": 5.629026412963867,
      "learning_rate": 9.371152154793316e-05,
      "loss": 0.4105,
      "step": 287
    },
    {
      "epoch": 0.18997361477572558,
      "grad_norm": 6.030405044555664,
      "learning_rate": 9.368953386103783e-05,
      "loss": 0.6806,
      "step": 288
    },
    {
      "epoch": 0.19063324538258575,
      "grad_norm": 9.411407470703125,
      "learning_rate": 9.366754617414249e-05,
      "loss": 0.5073,
      "step": 289
    },
    {
      "epoch": 0.19129287598944592,
      "grad_norm": 3.406062126159668,
      "learning_rate": 9.364555848724715e-05,
      "loss": 0.2882,
      "step": 290
    },
    {
      "epoch": 0.19195250659630606,
      "grad_norm": 7.303451061248779,
      "learning_rate": 9.362357080035182e-05,
      "loss": 0.4168,
      "step": 291
    },
    {
      "epoch": 0.19261213720316622,
      "grad_norm": 8.242011070251465,
      "learning_rate": 9.360158311345647e-05,
      "loss": 0.5135,
      "step": 292
    },
    {
      "epoch": 0.1932717678100264,
      "grad_norm": 3.9878385066986084,
      "learning_rate": 9.357959542656113e-05,
      "loss": 0.2935,
      "step": 293
    },
    {
      "epoch": 0.19393139841688653,
      "grad_norm": 2.068594217300415,
      "learning_rate": 9.355760773966579e-05,
      "loss": 0.4069,
      "step": 294
    },
    {
      "epoch": 0.1945910290237467,
      "grad_norm": 2.4738967418670654,
      "learning_rate": 9.353562005277046e-05,
      "loss": 0.2338,
      "step": 295
    },
    {
      "epoch": 0.19525065963060687,
      "grad_norm": 8.180070877075195,
      "learning_rate": 9.351363236587512e-05,
      "loss": 0.6877,
      "step": 296
    },
    {
      "epoch": 0.195910290237467,
      "grad_norm": 2.0007987022399902,
      "learning_rate": 9.349164467897977e-05,
      "loss": 0.1935,
      "step": 297
    },
    {
      "epoch": 0.19656992084432717,
      "grad_norm": 3.8324484825134277,
      "learning_rate": 9.346965699208444e-05,
      "loss": 0.1823,
      "step": 298
    },
    {
      "epoch": 0.19722955145118734,
      "grad_norm": 10.783690452575684,
      "learning_rate": 9.34476693051891e-05,
      "loss": 0.7957,
      "step": 299
    },
    {
      "epoch": 0.19788918205804748,
      "grad_norm": 3.433708667755127,
      "learning_rate": 9.342568161829376e-05,
      "loss": 0.2222,
      "step": 300
    },
    {
      "epoch": 0.19854881266490765,
      "grad_norm": 0.9049488306045532,
      "learning_rate": 9.340369393139841e-05,
      "loss": 0.0715,
      "step": 301
    },
    {
      "epoch": 0.19920844327176782,
      "grad_norm": 8.10256290435791,
      "learning_rate": 9.338170624450308e-05,
      "loss": 0.5624,
      "step": 302
    },
    {
      "epoch": 0.19986807387862796,
      "grad_norm": 1.9499095678329468,
      "learning_rate": 9.335971855760774e-05,
      "loss": 0.0875,
      "step": 303
    },
    {
      "epoch": 0.20052770448548812,
      "grad_norm": 4.462866306304932,
      "learning_rate": 9.33377308707124e-05,
      "loss": 0.2162,
      "step": 304
    },
    {
      "epoch": 0.2011873350923483,
      "grad_norm": 4.302006244659424,
      "learning_rate": 9.331574318381706e-05,
      "loss": 0.1939,
      "step": 305
    },
    {
      "epoch": 0.20184696569920843,
      "grad_norm": 2.0927228927612305,
      "learning_rate": 9.329375549692173e-05,
      "loss": 0.1185,
      "step": 306
    },
    {
      "epoch": 0.2025065963060686,
      "grad_norm": 2.592996597290039,
      "learning_rate": 9.327176781002638e-05,
      "loss": 0.1053,
      "step": 307
    },
    {
      "epoch": 0.20316622691292877,
      "grad_norm": 7.068577289581299,
      "learning_rate": 9.324978012313105e-05,
      "loss": 0.2732,
      "step": 308
    },
    {
      "epoch": 0.2038258575197889,
      "grad_norm": 1.071905493736267,
      "learning_rate": 9.322779243623571e-05,
      "loss": 0.0875,
      "step": 309
    },
    {
      "epoch": 0.20448548812664907,
      "grad_norm": 8.733232498168945,
      "learning_rate": 9.320580474934038e-05,
      "loss": 0.5305,
      "step": 310
    },
    {
      "epoch": 0.20514511873350924,
      "grad_norm": 12.489777565002441,
      "learning_rate": 9.318381706244504e-05,
      "loss": 1.2819,
      "step": 311
    },
    {
      "epoch": 0.20580474934036938,
      "grad_norm": 4.520515441894531,
      "learning_rate": 9.31618293755497e-05,
      "loss": 0.1965,
      "step": 312
    },
    {
      "epoch": 0.20646437994722955,
      "grad_norm": 5.594666481018066,
      "learning_rate": 9.313984168865437e-05,
      "loss": 0.3413,
      "step": 313
    },
    {
      "epoch": 0.20712401055408972,
      "grad_norm": 1.7970514297485352,
      "learning_rate": 9.311785400175902e-05,
      "loss": 0.1111,
      "step": 314
    },
    {
      "epoch": 0.20778364116094986,
      "grad_norm": 1.7292300462722778,
      "learning_rate": 9.309586631486368e-05,
      "loss": 0.1642,
      "step": 315
    },
    {
      "epoch": 0.20844327176781002,
      "grad_norm": 3.2029671669006348,
      "learning_rate": 9.307387862796835e-05,
      "loss": 0.2253,
      "step": 316
    },
    {
      "epoch": 0.2091029023746702,
      "grad_norm": 1.8017809391021729,
      "learning_rate": 9.305189094107301e-05,
      "loss": 0.1845,
      "step": 317
    },
    {
      "epoch": 0.20976253298153033,
      "grad_norm": 1.6349616050720215,
      "learning_rate": 9.302990325417766e-05,
      "loss": 0.1695,
      "step": 318
    },
    {
      "epoch": 0.2104221635883905,
      "grad_norm": 6.758336544036865,
      "learning_rate": 9.300791556728232e-05,
      "loss": 0.3865,
      "step": 319
    },
    {
      "epoch": 0.21108179419525067,
      "grad_norm": 4.430947303771973,
      "learning_rate": 9.298592788038699e-05,
      "loss": 0.2053,
      "step": 320
    },
    {
      "epoch": 0.2117414248021108,
      "grad_norm": 5.5015339851379395,
      "learning_rate": 9.296394019349165e-05,
      "loss": 0.1682,
      "step": 321
    },
    {
      "epoch": 0.21240105540897097,
      "grad_norm": 2.217189073562622,
      "learning_rate": 9.29419525065963e-05,
      "loss": 0.1016,
      "step": 322
    },
    {
      "epoch": 0.21306068601583114,
      "grad_norm": 12.043814659118652,
      "learning_rate": 9.291996481970096e-05,
      "loss": 1.1503,
      "step": 323
    },
    {
      "epoch": 0.21372031662269128,
      "grad_norm": 6.839044570922852,
      "learning_rate": 9.289797713280563e-05,
      "loss": 0.1388,
      "step": 324
    },
    {
      "epoch": 0.21437994722955145,
      "grad_norm": 16.582380294799805,
      "learning_rate": 9.287598944591029e-05,
      "loss": 1.7686,
      "step": 325
    },
    {
      "epoch": 0.21503957783641162,
      "grad_norm": 1.3707796335220337,
      "learning_rate": 9.285400175901495e-05,
      "loss": 0.0611,
      "step": 326
    },
    {
      "epoch": 0.21569920844327176,
      "grad_norm": 2.1243717670440674,
      "learning_rate": 9.283201407211962e-05,
      "loss": 0.0823,
      "step": 327
    },
    {
      "epoch": 0.21635883905013192,
      "grad_norm": 10.42601490020752,
      "learning_rate": 9.281002638522427e-05,
      "loss": 0.5142,
      "step": 328
    },
    {
      "epoch": 0.2170184696569921,
      "grad_norm": 4.671543598175049,
      "learning_rate": 9.278803869832894e-05,
      "loss": 0.1488,
      "step": 329
    },
    {
      "epoch": 0.21767810026385223,
      "grad_norm": 24.10990333557129,
      "learning_rate": 9.27660510114336e-05,
      "loss": 1.3161,
      "step": 330
    },
    {
      "epoch": 0.2183377308707124,
      "grad_norm": 1.9906197786331177,
      "learning_rate": 9.274406332453827e-05,
      "loss": 0.0859,
      "step": 331
    },
    {
      "epoch": 0.21899736147757257,
      "grad_norm": 0.7828160524368286,
      "learning_rate": 9.272207563764293e-05,
      "loss": 0.0303,
      "step": 332
    },
    {
      "epoch": 0.2196569920844327,
      "grad_norm": 1.4524950981140137,
      "learning_rate": 9.270008795074759e-05,
      "loss": 0.0591,
      "step": 333
    },
    {
      "epoch": 0.22031662269129287,
      "grad_norm": 0.5822636485099792,
      "learning_rate": 9.267810026385226e-05,
      "loss": 0.0153,
      "step": 334
    },
    {
      "epoch": 0.22097625329815304,
      "grad_norm": 18.789011001586914,
      "learning_rate": 9.265611257695691e-05,
      "loss": 1.8068,
      "step": 335
    },
    {
      "epoch": 0.22163588390501318,
      "grad_norm": 28.460872650146484,
      "learning_rate": 9.263412489006157e-05,
      "loss": 1.4046,
      "step": 336
    },
    {
      "epoch": 0.22229551451187335,
      "grad_norm": 15.232007026672363,
      "learning_rate": 9.261213720316623e-05,
      "loss": 1.105,
      "step": 337
    },
    {
      "epoch": 0.22295514511873352,
      "grad_norm": 13.255441665649414,
      "learning_rate": 9.25901495162709e-05,
      "loss": 1.8761,
      "step": 338
    },
    {
      "epoch": 0.22361477572559366,
      "grad_norm": 25.39190673828125,
      "learning_rate": 9.256816182937556e-05,
      "loss": 2.7647,
      "step": 339
    },
    {
      "epoch": 0.22427440633245382,
      "grad_norm": 20.97213363647461,
      "learning_rate": 9.254617414248021e-05,
      "loss": 1.0953,
      "step": 340
    },
    {
      "epoch": 0.224934036939314,
      "grad_norm": 11.386876106262207,
      "learning_rate": 9.252418645558487e-05,
      "loss": 0.4688,
      "step": 341
    },
    {
      "epoch": 0.22559366754617413,
      "grad_norm": 3.085404634475708,
      "learning_rate": 9.250219876868954e-05,
      "loss": 0.0994,
      "step": 342
    },
    {
      "epoch": 0.2262532981530343,
      "grad_norm": 15.992936134338379,
      "learning_rate": 9.24802110817942e-05,
      "loss": 0.9143,
      "step": 343
    },
    {
      "epoch": 0.22691292875989447,
      "grad_norm": 12.405535697937012,
      "learning_rate": 9.245822339489885e-05,
      "loss": 1.3734,
      "step": 344
    },
    {
      "epoch": 0.2275725593667546,
      "grad_norm": 7.5888471603393555,
      "learning_rate": 9.243623570800352e-05,
      "loss": 0.9666,
      "step": 345
    },
    {
      "epoch": 0.22823218997361477,
      "grad_norm": 13.028181076049805,
      "learning_rate": 9.241424802110818e-05,
      "loss": 0.7829,
      "step": 346
    },
    {
      "epoch": 0.22889182058047494,
      "grad_norm": 6.226950168609619,
      "learning_rate": 9.239226033421284e-05,
      "loss": 0.5331,
      "step": 347
    },
    {
      "epoch": 0.22955145118733508,
      "grad_norm": 3.5844147205352783,
      "learning_rate": 9.23702726473175e-05,
      "loss": 0.3613,
      "step": 348
    },
    {
      "epoch": 0.23021108179419525,
      "grad_norm": 3.3005380630493164,
      "learning_rate": 9.234828496042217e-05,
      "loss": 0.3476,
      "step": 349
    },
    {
      "epoch": 0.23087071240105542,
      "grad_norm": 2.001451015472412,
      "learning_rate": 9.232629727352682e-05,
      "loss": 0.3811,
      "step": 350
    },
    {
      "epoch": 0.23153034300791556,
      "grad_norm": 2.5706796646118164,
      "learning_rate": 9.23043095866315e-05,
      "loss": 0.3328,
      "step": 351
    },
    {
      "epoch": 0.23218997361477572,
      "grad_norm": 2.068180799484253,
      "learning_rate": 9.228232189973616e-05,
      "loss": 0.3622,
      "step": 352
    },
    {
      "epoch": 0.2328496042216359,
      "grad_norm": 5.141445159912109,
      "learning_rate": 9.226033421284082e-05,
      "loss": 0.3676,
      "step": 353
    },
    {
      "epoch": 0.23350923482849603,
      "grad_norm": 5.901932716369629,
      "learning_rate": 9.223834652594548e-05,
      "loss": 0.9401,
      "step": 354
    },
    {
      "epoch": 0.2341688654353562,
      "grad_norm": 6.831417083740234,
      "learning_rate": 9.221635883905013e-05,
      "loss": 0.4863,
      "step": 355
    },
    {
      "epoch": 0.23482849604221637,
      "grad_norm": 3.3002476692199707,
      "learning_rate": 9.21943711521548e-05,
      "loss": 0.4143,
      "step": 356
    },
    {
      "epoch": 0.2354881266490765,
      "grad_norm": 2.08404278755188,
      "learning_rate": 9.217238346525946e-05,
      "loss": 0.3246,
      "step": 357
    },
    {
      "epoch": 0.23614775725593667,
      "grad_norm": 1.6094441413879395,
      "learning_rate": 9.215039577836412e-05,
      "loss": 0.2499,
      "step": 358
    },
    {
      "epoch": 0.23680738786279684,
      "grad_norm": 3.754490613937378,
      "learning_rate": 9.212840809146878e-05,
      "loss": 0.4671,
      "step": 359
    },
    {
      "epoch": 0.23746701846965698,
      "grad_norm": 6.186951160430908,
      "learning_rate": 9.210642040457345e-05,
      "loss": 0.421,
      "step": 360
    },
    {
      "epoch": 0.23812664907651715,
      "grad_norm": 6.24514102935791,
      "learning_rate": 9.20844327176781e-05,
      "loss": 0.4959,
      "step": 361
    },
    {
      "epoch": 0.23878627968337732,
      "grad_norm": 6.280337810516357,
      "learning_rate": 9.206244503078276e-05,
      "loss": 0.7579,
      "step": 362
    },
    {
      "epoch": 0.23944591029023746,
      "grad_norm": 6.125627040863037,
      "learning_rate": 9.204045734388743e-05,
      "loss": 0.6258,
      "step": 363
    },
    {
      "epoch": 0.24010554089709762,
      "grad_norm": 1.4763375520706177,
      "learning_rate": 9.201846965699209e-05,
      "loss": 0.2026,
      "step": 364
    },
    {
      "epoch": 0.2407651715039578,
      "grad_norm": 1.2312299013137817,
      "learning_rate": 9.199648197009675e-05,
      "loss": 0.1782,
      "step": 365
    },
    {
      "epoch": 0.24142480211081793,
      "grad_norm": 4.073061466217041,
      "learning_rate": 9.19744942832014e-05,
      "loss": 0.3927,
      "step": 366
    },
    {
      "epoch": 0.2420844327176781,
      "grad_norm": 1.8968424797058105,
      "learning_rate": 9.195250659630607e-05,
      "loss": 0.2511,
      "step": 367
    },
    {
      "epoch": 0.24274406332453827,
      "grad_norm": 3.5699281692504883,
      "learning_rate": 9.193051890941073e-05,
      "loss": 0.4078,
      "step": 368
    },
    {
      "epoch": 0.2434036939313984,
      "grad_norm": 1.662182331085205,
      "learning_rate": 9.190853122251539e-05,
      "loss": 0.2973,
      "step": 369
    },
    {
      "epoch": 0.24406332453825857,
      "grad_norm": 4.712750434875488,
      "learning_rate": 9.188654353562006e-05,
      "loss": 0.3627,
      "step": 370
    },
    {
      "epoch": 0.24472295514511874,
      "grad_norm": 2.797128438949585,
      "learning_rate": 9.186455584872471e-05,
      "loss": 0.4041,
      "step": 371
    },
    {
      "epoch": 0.24538258575197888,
      "grad_norm": 3.2958638668060303,
      "learning_rate": 9.184256816182938e-05,
      "loss": 0.4272,
      "step": 372
    },
    {
      "epoch": 0.24604221635883905,
      "grad_norm": 2.5601389408111572,
      "learning_rate": 9.182058047493404e-05,
      "loss": 0.3361,
      "step": 373
    },
    {
      "epoch": 0.24670184696569922,
      "grad_norm": 3.2534117698669434,
      "learning_rate": 9.179859278803871e-05,
      "loss": 0.3199,
      "step": 374
    },
    {
      "epoch": 0.24736147757255936,
      "grad_norm": 2.973487615585327,
      "learning_rate": 9.177660510114337e-05,
      "loss": 0.2336,
      "step": 375
    },
    {
      "epoch": 0.24802110817941952,
      "grad_norm": 2.1475892066955566,
      "learning_rate": 9.175461741424803e-05,
      "loss": 0.2939,
      "step": 376
    },
    {
      "epoch": 0.2486807387862797,
      "grad_norm": 5.3070149421691895,
      "learning_rate": 9.173262972735268e-05,
      "loss": 0.6215,
      "step": 377
    },
    {
      "epoch": 0.24934036939313983,
      "grad_norm": 8.021796226501465,
      "learning_rate": 9.171064204045735e-05,
      "loss": 0.4619,
      "step": 378
    },
    {
      "epoch": 0.25,
      "grad_norm": 4.105077743530273,
      "learning_rate": 9.168865435356201e-05,
      "loss": 0.3498,
      "step": 379
    },
    {
      "epoch": 0.25065963060686014,
      "grad_norm": 5.693858623504639,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.357,
      "step": 380
    },
    {
      "epoch": 0.25131926121372034,
      "grad_norm": 13.102690696716309,
      "learning_rate": 9.164467897977134e-05,
      "loss": 1.0718,
      "step": 381
    },
    {
      "epoch": 0.2519788918205805,
      "grad_norm": 7.079848766326904,
      "learning_rate": 9.1622691292876e-05,
      "loss": 0.4513,
      "step": 382
    },
    {
      "epoch": 0.2526385224274406,
      "grad_norm": 1.3577038049697876,
      "learning_rate": 9.160070360598065e-05,
      "loss": 0.2105,
      "step": 383
    },
    {
      "epoch": 0.2532981530343008,
      "grad_norm": 3.447260856628418,
      "learning_rate": 9.157871591908531e-05,
      "loss": 0.1994,
      "step": 384
    },
    {
      "epoch": 0.25395778364116095,
      "grad_norm": 4.097410202026367,
      "learning_rate": 9.155672823218998e-05,
      "loss": 0.2412,
      "step": 385
    },
    {
      "epoch": 0.2546174142480211,
      "grad_norm": 0.7140834927558899,
      "learning_rate": 9.153474054529464e-05,
      "loss": 0.0977,
      "step": 386
    },
    {
      "epoch": 0.2552770448548813,
      "grad_norm": 4.85173225402832,
      "learning_rate": 9.15127528583993e-05,
      "loss": 0.2108,
      "step": 387
    },
    {
      "epoch": 0.2559366754617414,
      "grad_norm": 14.205138206481934,
      "learning_rate": 9.149076517150396e-05,
      "loss": 0.5197,
      "step": 388
    },
    {
      "epoch": 0.25659630606860157,
      "grad_norm": 11.7460355758667,
      "learning_rate": 9.146877748460862e-05,
      "loss": 0.5943,
      "step": 389
    },
    {
      "epoch": 0.25725593667546176,
      "grad_norm": 1.0060454607009888,
      "learning_rate": 9.144678979771328e-05,
      "loss": 0.0846,
      "step": 390
    },
    {
      "epoch": 0.2579155672823219,
      "grad_norm": 10.49466609954834,
      "learning_rate": 9.142480211081795e-05,
      "loss": 0.384,
      "step": 391
    },
    {
      "epoch": 0.25857519788918204,
      "grad_norm": 1.5218459367752075,
      "learning_rate": 9.14028144239226e-05,
      "loss": 0.1276,
      "step": 392
    },
    {
      "epoch": 0.25923482849604224,
      "grad_norm": 3.103940010070801,
      "learning_rate": 9.138082673702728e-05,
      "loss": 0.1844,
      "step": 393
    },
    {
      "epoch": 0.2598944591029024,
      "grad_norm": 1.5325329303741455,
      "learning_rate": 9.135883905013193e-05,
      "loss": 0.1148,
      "step": 394
    },
    {
      "epoch": 0.2605540897097625,
      "grad_norm": 5.3918070793151855,
      "learning_rate": 9.13368513632366e-05,
      "loss": 0.2928,
      "step": 395
    },
    {
      "epoch": 0.2612137203166227,
      "grad_norm": 2.7727065086364746,
      "learning_rate": 9.131486367634126e-05,
      "loss": 0.1757,
      "step": 396
    },
    {
      "epoch": 0.26187335092348285,
      "grad_norm": 4.099031925201416,
      "learning_rate": 9.129287598944592e-05,
      "loss": 0.1723,
      "step": 397
    },
    {
      "epoch": 0.262532981530343,
      "grad_norm": 9.121962547302246,
      "learning_rate": 9.127088830255057e-05,
      "loss": 0.481,
      "step": 398
    },
    {
      "epoch": 0.2631926121372032,
      "grad_norm": 9.713727951049805,
      "learning_rate": 9.124890061565525e-05,
      "loss": 0.644,
      "step": 399
    },
    {
      "epoch": 0.2638522427440633,
      "grad_norm": 1.7543517351150513,
      "learning_rate": 9.12269129287599e-05,
      "loss": 0.0918,
      "step": 400
    },
    {
      "epoch": 0.26451187335092347,
      "grad_norm": 17.678936004638672,
      "learning_rate": 9.120492524186456e-05,
      "loss": 1.0611,
      "step": 401
    },
    {
      "epoch": 0.26517150395778366,
      "grad_norm": 3.0806643962860107,
      "learning_rate": 9.118293755496922e-05,
      "loss": 0.1757,
      "step": 402
    },
    {
      "epoch": 0.2658311345646438,
      "grad_norm": 2.215322971343994,
      "learning_rate": 9.116094986807389e-05,
      "loss": 0.1003,
      "step": 403
    },
    {
      "epoch": 0.26649076517150394,
      "grad_norm": 10.542194366455078,
      "learning_rate": 9.113896218117854e-05,
      "loss": 1.5662,
      "step": 404
    },
    {
      "epoch": 0.26715039577836414,
      "grad_norm": 6.559330463409424,
      "learning_rate": 9.11169744942832e-05,
      "loss": 0.3628,
      "step": 405
    },
    {
      "epoch": 0.2678100263852243,
      "grad_norm": 2.5351755619049072,
      "learning_rate": 9.109498680738787e-05,
      "loss": 0.1248,
      "step": 406
    },
    {
      "epoch": 0.2684696569920844,
      "grad_norm": 1.4812602996826172,
      "learning_rate": 9.107299912049253e-05,
      "loss": 0.0844,
      "step": 407
    },
    {
      "epoch": 0.2691292875989446,
      "grad_norm": 14.78693962097168,
      "learning_rate": 9.105101143359719e-05,
      "loss": 1.0042,
      "step": 408
    },
    {
      "epoch": 0.26978891820580475,
      "grad_norm": 4.528454303741455,
      "learning_rate": 9.102902374670184e-05,
      "loss": 0.1381,
      "step": 409
    },
    {
      "epoch": 0.2704485488126649,
      "grad_norm": 1.2214003801345825,
      "learning_rate": 9.100703605980651e-05,
      "loss": 0.0987,
      "step": 410
    },
    {
      "epoch": 0.2711081794195251,
      "grad_norm": 3.645660877227783,
      "learning_rate": 9.098504837291117e-05,
      "loss": 0.1157,
      "step": 411
    },
    {
      "epoch": 0.2717678100263852,
      "grad_norm": 13.388525009155273,
      "learning_rate": 9.096306068601583e-05,
      "loss": 0.8872,
      "step": 412
    },
    {
      "epoch": 0.27242744063324537,
      "grad_norm": 7.8114705085754395,
      "learning_rate": 9.09410729991205e-05,
      "loss": 0.921,
      "step": 413
    },
    {
      "epoch": 0.27308707124010556,
      "grad_norm": 24.907121658325195,
      "learning_rate": 9.091908531222515e-05,
      "loss": 1.7651,
      "step": 414
    },
    {
      "epoch": 0.2737467018469657,
      "grad_norm": 2.359736442565918,
      "learning_rate": 9.089709762532982e-05,
      "loss": 0.0901,
      "step": 415
    },
    {
      "epoch": 0.27440633245382584,
      "grad_norm": 1.7397414445877075,
      "learning_rate": 9.087510993843448e-05,
      "loss": 0.0821,
      "step": 416
    },
    {
      "epoch": 0.27506596306068604,
      "grad_norm": 12.62512493133545,
      "learning_rate": 9.085312225153915e-05,
      "loss": 0.9629,
      "step": 417
    },
    {
      "epoch": 0.2757255936675462,
      "grad_norm": 5.5620856285095215,
      "learning_rate": 9.083113456464381e-05,
      "loss": 0.294,
      "step": 418
    },
    {
      "epoch": 0.2763852242744063,
      "grad_norm": 16.266460418701172,
      "learning_rate": 9.080914687774847e-05,
      "loss": 0.6693,
      "step": 419
    },
    {
      "epoch": 0.2770448548812665,
      "grad_norm": 20.879619598388672,
      "learning_rate": 9.078715919085312e-05,
      "loss": 0.8273,
      "step": 420
    },
    {
      "epoch": 0.27770448548812665,
      "grad_norm": 5.133668899536133,
      "learning_rate": 9.07651715039578e-05,
      "loss": 0.2798,
      "step": 421
    },
    {
      "epoch": 0.2783641160949868,
      "grad_norm": 4.368912220001221,
      "learning_rate": 9.074318381706245e-05,
      "loss": 0.2447,
      "step": 422
    },
    {
      "epoch": 0.279023746701847,
      "grad_norm": 1.742450475692749,
      "learning_rate": 9.072119613016711e-05,
      "loss": 0.1889,
      "step": 423
    },
    {
      "epoch": 0.2796833773087071,
      "grad_norm": 4.552252292633057,
      "learning_rate": 9.069920844327178e-05,
      "loss": 0.4499,
      "step": 424
    },
    {
      "epoch": 0.28034300791556727,
      "grad_norm": 3.0797879695892334,
      "learning_rate": 9.067722075637644e-05,
      "loss": 0.3575,
      "step": 425
    },
    {
      "epoch": 0.28100263852242746,
      "grad_norm": 1.2407945394515991,
      "learning_rate": 9.065523306948109e-05,
      "loss": 0.1936,
      "step": 426
    },
    {
      "epoch": 0.2816622691292876,
      "grad_norm": 11.210232734680176,
      "learning_rate": 9.063324538258575e-05,
      "loss": 1.3216,
      "step": 427
    },
    {
      "epoch": 0.28232189973614774,
      "grad_norm": 2.6400132179260254,
      "learning_rate": 9.061125769569042e-05,
      "loss": 0.1574,
      "step": 428
    },
    {
      "epoch": 0.28298153034300794,
      "grad_norm": 2.098757266998291,
      "learning_rate": 9.058927000879508e-05,
      "loss": 0.2319,
      "step": 429
    },
    {
      "epoch": 0.2836411609498681,
      "grad_norm": 4.432528972625732,
      "learning_rate": 9.056728232189973e-05,
      "loss": 0.3111,
      "step": 430
    },
    {
      "epoch": 0.2843007915567282,
      "grad_norm": 3.9680166244506836,
      "learning_rate": 9.05452946350044e-05,
      "loss": 0.2568,
      "step": 431
    },
    {
      "epoch": 0.2849604221635884,
      "grad_norm": 2.7051398754119873,
      "learning_rate": 9.052330694810906e-05,
      "loss": 0.2808,
      "step": 432
    },
    {
      "epoch": 0.28562005277044855,
      "grad_norm": 3.7328131198883057,
      "learning_rate": 9.050131926121372e-05,
      "loss": 0.2765,
      "step": 433
    },
    {
      "epoch": 0.2862796833773087,
      "grad_norm": 4.417738437652588,
      "learning_rate": 9.047933157431839e-05,
      "loss": 0.3619,
      "step": 434
    },
    {
      "epoch": 0.2869393139841689,
      "grad_norm": 10.154789924621582,
      "learning_rate": 9.045734388742305e-05,
      "loss": 0.7811,
      "step": 435
    },
    {
      "epoch": 0.287598944591029,
      "grad_norm": 3.5787034034729004,
      "learning_rate": 9.043535620052772e-05,
      "loss": 0.3104,
      "step": 436
    },
    {
      "epoch": 0.28825857519788917,
      "grad_norm": 5.45831823348999,
      "learning_rate": 9.041336851363237e-05,
      "loss": 0.2671,
      "step": 437
    },
    {
      "epoch": 0.28891820580474936,
      "grad_norm": 8.759632110595703,
      "learning_rate": 9.039138082673703e-05,
      "loss": 1.0447,
      "step": 438
    },
    {
      "epoch": 0.2895778364116095,
      "grad_norm": 21.6894588470459,
      "learning_rate": 9.03693931398417e-05,
      "loss": 0.7846,
      "step": 439
    },
    {
      "epoch": 0.29023746701846964,
      "grad_norm": 4.7036967277526855,
      "learning_rate": 9.034740545294636e-05,
      "loss": 0.2917,
      "step": 440
    },
    {
      "epoch": 0.29089709762532984,
      "grad_norm": 2.0186500549316406,
      "learning_rate": 9.032541776605101e-05,
      "loss": 0.1595,
      "step": 441
    },
    {
      "epoch": 0.29155672823219,
      "grad_norm": 5.234262943267822,
      "learning_rate": 9.030343007915569e-05,
      "loss": 0.4472,
      "step": 442
    },
    {
      "epoch": 0.2922163588390501,
      "grad_norm": 4.342545032501221,
      "learning_rate": 9.028144239226034e-05,
      "loss": 0.2755,
      "step": 443
    },
    {
      "epoch": 0.2928759894459103,
      "grad_norm": 4.516271591186523,
      "learning_rate": 9.0259454705365e-05,
      "loss": 0.3599,
      "step": 444
    },
    {
      "epoch": 0.29353562005277045,
      "grad_norm": 1.2405375242233276,
      "learning_rate": 9.023746701846966e-05,
      "loss": 0.1536,
      "step": 445
    },
    {
      "epoch": 0.2941952506596306,
      "grad_norm": 4.88320779800415,
      "learning_rate": 9.021547933157433e-05,
      "loss": 0.2947,
      "step": 446
    },
    {
      "epoch": 0.2948548812664908,
      "grad_norm": 2.9292290210723877,
      "learning_rate": 9.019349164467898e-05,
      "loss": 0.1801,
      "step": 447
    },
    {
      "epoch": 0.2955145118733509,
      "grad_norm": 6.61607027053833,
      "learning_rate": 9.017150395778364e-05,
      "loss": 0.4074,
      "step": 448
    },
    {
      "epoch": 0.29617414248021107,
      "grad_norm": 8.811742782592773,
      "learning_rate": 9.014951627088831e-05,
      "loss": 0.5815,
      "step": 449
    },
    {
      "epoch": 0.29683377308707126,
      "grad_norm": 4.767643928527832,
      "learning_rate": 9.012752858399297e-05,
      "loss": 0.3542,
      "step": 450
    },
    {
      "epoch": 0.2974934036939314,
      "grad_norm": 6.349925518035889,
      "learning_rate": 9.010554089709763e-05,
      "loss": 0.6576,
      "step": 451
    },
    {
      "epoch": 0.29815303430079154,
      "grad_norm": 1.1197857856750488,
      "learning_rate": 9.008355321020228e-05,
      "loss": 0.1046,
      "step": 452
    },
    {
      "epoch": 0.29881266490765174,
      "grad_norm": 8.122859954833984,
      "learning_rate": 9.006156552330695e-05,
      "loss": 0.5536,
      "step": 453
    },
    {
      "epoch": 0.2994722955145119,
      "grad_norm": 4.923213958740234,
      "learning_rate": 9.003957783641161e-05,
      "loss": 0.2103,
      "step": 454
    },
    {
      "epoch": 0.300131926121372,
      "grad_norm": 9.104116439819336,
      "learning_rate": 9.001759014951627e-05,
      "loss": 0.4111,
      "step": 455
    },
    {
      "epoch": 0.3007915567282322,
      "grad_norm": 8.334168434143066,
      "learning_rate": 8.999560246262094e-05,
      "loss": 0.7942,
      "step": 456
    },
    {
      "epoch": 0.30145118733509235,
      "grad_norm": 6.207230567932129,
      "learning_rate": 8.997361477572561e-05,
      "loss": 0.6918,
      "step": 457
    },
    {
      "epoch": 0.3021108179419525,
      "grad_norm": 3.2731363773345947,
      "learning_rate": 8.995162708883026e-05,
      "loss": 0.2109,
      "step": 458
    },
    {
      "epoch": 0.3027704485488127,
      "grad_norm": 2.555839776992798,
      "learning_rate": 8.992963940193492e-05,
      "loss": 0.2763,
      "step": 459
    },
    {
      "epoch": 0.3034300791556728,
      "grad_norm": 3.9017181396484375,
      "learning_rate": 8.990765171503959e-05,
      "loss": 0.3016,
      "step": 460
    },
    {
      "epoch": 0.30408970976253297,
      "grad_norm": 7.232858657836914,
      "learning_rate": 8.988566402814425e-05,
      "loss": 0.3308,
      "step": 461
    },
    {
      "epoch": 0.30474934036939316,
      "grad_norm": 2.9371776580810547,
      "learning_rate": 8.98636763412489e-05,
      "loss": 0.2372,
      "step": 462
    },
    {
      "epoch": 0.3054089709762533,
      "grad_norm": 2.3194684982299805,
      "learning_rate": 8.984168865435356e-05,
      "loss": 0.1596,
      "step": 463
    },
    {
      "epoch": 0.30606860158311344,
      "grad_norm": 2.3812754154205322,
      "learning_rate": 8.981970096745823e-05,
      "loss": 0.2009,
      "step": 464
    },
    {
      "epoch": 0.30672823218997364,
      "grad_norm": 2.9123404026031494,
      "learning_rate": 8.979771328056289e-05,
      "loss": 0.2676,
      "step": 465
    },
    {
      "epoch": 0.3073878627968338,
      "grad_norm": 3.5506749153137207,
      "learning_rate": 8.977572559366755e-05,
      "loss": 0.1697,
      "step": 466
    },
    {
      "epoch": 0.3080474934036939,
      "grad_norm": 1.8470388650894165,
      "learning_rate": 8.975373790677222e-05,
      "loss": 0.1461,
      "step": 467
    },
    {
      "epoch": 0.3087071240105541,
      "grad_norm": 2.2053351402282715,
      "learning_rate": 8.973175021987688e-05,
      "loss": 0.108,
      "step": 468
    },
    {
      "epoch": 0.30936675461741425,
      "grad_norm": 9.822685241699219,
      "learning_rate": 8.970976253298153e-05,
      "loss": 0.7805,
      "step": 469
    },
    {
      "epoch": 0.3100263852242744,
      "grad_norm": 2.2294838428497314,
      "learning_rate": 8.968777484608619e-05,
      "loss": 0.1213,
      "step": 470
    },
    {
      "epoch": 0.3106860158311346,
      "grad_norm": 6.62475061416626,
      "learning_rate": 8.966578715919086e-05,
      "loss": 0.3155,
      "step": 471
    },
    {
      "epoch": 0.3113456464379947,
      "grad_norm": 15.616621971130371,
      "learning_rate": 8.964379947229552e-05,
      "loss": 1.9674,
      "step": 472
    },
    {
      "epoch": 0.31200527704485487,
      "grad_norm": 9.56424331665039,
      "learning_rate": 8.962181178540017e-05,
      "loss": 1.0543,
      "step": 473
    },
    {
      "epoch": 0.31266490765171506,
      "grad_norm": 7.450915813446045,
      "learning_rate": 8.959982409850483e-05,
      "loss": 0.3146,
      "step": 474
    },
    {
      "epoch": 0.3133245382585752,
      "grad_norm": 13.64261245727539,
      "learning_rate": 8.95778364116095e-05,
      "loss": 0.8299,
      "step": 475
    },
    {
      "epoch": 0.31398416886543534,
      "grad_norm": 11.607853889465332,
      "learning_rate": 8.955584872471416e-05,
      "loss": 0.822,
      "step": 476
    },
    {
      "epoch": 0.31464379947229554,
      "grad_norm": 1.1483771800994873,
      "learning_rate": 8.953386103781883e-05,
      "loss": 0.113,
      "step": 477
    },
    {
      "epoch": 0.3153034300791557,
      "grad_norm": 14.526041984558105,
      "learning_rate": 8.951187335092349e-05,
      "loss": 0.699,
      "step": 478
    },
    {
      "epoch": 0.3159630606860158,
      "grad_norm": 1.1255441904067993,
      "learning_rate": 8.948988566402816e-05,
      "loss": 0.0791,
      "step": 479
    },
    {
      "epoch": 0.316622691292876,
      "grad_norm": 3.346407413482666,
      "learning_rate": 8.946789797713281e-05,
      "loss": 0.2259,
      "step": 480
    },
    {
      "epoch": 0.31728232189973615,
      "grad_norm": 10.267210960388184,
      "learning_rate": 8.944591029023747e-05,
      "loss": 0.9328,
      "step": 481
    },
    {
      "epoch": 0.3179419525065963,
      "grad_norm": 8.919878005981445,
      "learning_rate": 8.942392260334214e-05,
      "loss": 1.0718,
      "step": 482
    },
    {
      "epoch": 0.3186015831134565,
      "grad_norm": 1.6874467134475708,
      "learning_rate": 8.94019349164468e-05,
      "loss": 0.2607,
      "step": 483
    },
    {
      "epoch": 0.31926121372031663,
      "grad_norm": 4.967100620269775,
      "learning_rate": 8.937994722955145e-05,
      "loss": 0.3271,
      "step": 484
    },
    {
      "epoch": 0.31992084432717677,
      "grad_norm": 5.107692241668701,
      "learning_rate": 8.935795954265613e-05,
      "loss": 0.3764,
      "step": 485
    },
    {
      "epoch": 0.32058047493403696,
      "grad_norm": 7.359992027282715,
      "learning_rate": 8.933597185576078e-05,
      "loss": 0.5064,
      "step": 486
    },
    {
      "epoch": 0.3212401055408971,
      "grad_norm": 3.0724503993988037,
      "learning_rate": 8.931398416886544e-05,
      "loss": 0.1893,
      "step": 487
    },
    {
      "epoch": 0.32189973614775724,
      "grad_norm": 2.3116583824157715,
      "learning_rate": 8.92919964819701e-05,
      "loss": 0.1585,
      "step": 488
    },
    {
      "epoch": 0.32255936675461744,
      "grad_norm": 2.5600361824035645,
      "learning_rate": 8.927000879507477e-05,
      "loss": 0.2154,
      "step": 489
    },
    {
      "epoch": 0.3232189973614776,
      "grad_norm": 4.08428430557251,
      "learning_rate": 8.924802110817942e-05,
      "loss": 0.2304,
      "step": 490
    },
    {
      "epoch": 0.3238786279683377,
      "grad_norm": 3.712628126144409,
      "learning_rate": 8.922603342128408e-05,
      "loss": 0.3426,
      "step": 491
    },
    {
      "epoch": 0.3245382585751979,
      "grad_norm": 6.688517093658447,
      "learning_rate": 8.920404573438874e-05,
      "loss": 0.4188,
      "step": 492
    },
    {
      "epoch": 0.32519788918205805,
      "grad_norm": 4.609042644500732,
      "learning_rate": 8.918205804749341e-05,
      "loss": 0.4682,
      "step": 493
    },
    {
      "epoch": 0.3258575197889182,
      "grad_norm": 2.709615707397461,
      "learning_rate": 8.916007036059806e-05,
      "loss": 0.1498,
      "step": 494
    },
    {
      "epoch": 0.3265171503957784,
      "grad_norm": 3.9310314655303955,
      "learning_rate": 8.913808267370272e-05,
      "loss": 0.264,
      "step": 495
    },
    {
      "epoch": 0.32717678100263853,
      "grad_norm": 6.242100238800049,
      "learning_rate": 8.911609498680739e-05,
      "loss": 0.2887,
      "step": 496
    },
    {
      "epoch": 0.32783641160949867,
      "grad_norm": 9.558406829833984,
      "learning_rate": 8.909410729991205e-05,
      "loss": 0.6099,
      "step": 497
    },
    {
      "epoch": 0.32849604221635886,
      "grad_norm": 1.541845440864563,
      "learning_rate": 8.907211961301672e-05,
      "loss": 0.1616,
      "step": 498
    },
    {
      "epoch": 0.329155672823219,
      "grad_norm": 7.824613571166992,
      "learning_rate": 8.905013192612138e-05,
      "loss": 0.5916,
      "step": 499
    },
    {
      "epoch": 0.32981530343007914,
      "grad_norm": 1.1538023948669434,
      "learning_rate": 8.902814423922605e-05,
      "loss": 0.1405,
      "step": 500
    },
    {
      "epoch": 0.33047493403693934,
      "grad_norm": 4.675868034362793,
      "learning_rate": 8.90061565523307e-05,
      "loss": 0.2508,
      "step": 501
    },
    {
      "epoch": 0.3311345646437995,
      "grad_norm": 2.808488130569458,
      "learning_rate": 8.898416886543536e-05,
      "loss": 0.269,
      "step": 502
    },
    {
      "epoch": 0.3317941952506596,
      "grad_norm": 2.50015926361084,
      "learning_rate": 8.896218117854003e-05,
      "loss": 0.225,
      "step": 503
    },
    {
      "epoch": 0.3324538258575198,
      "grad_norm": 3.033926248550415,
      "learning_rate": 8.894019349164469e-05,
      "loss": 0.2308,
      "step": 504
    },
    {
      "epoch": 0.33311345646437995,
      "grad_norm": 12.407211303710938,
      "learning_rate": 8.891820580474935e-05,
      "loss": 0.5624,
      "step": 505
    },
    {
      "epoch": 0.3337730870712401,
      "grad_norm": 8.017393112182617,
      "learning_rate": 8.8896218117854e-05,
      "loss": 0.6654,
      "step": 506
    },
    {
      "epoch": 0.3344327176781003,
      "grad_norm": 3.2478342056274414,
      "learning_rate": 8.887423043095867e-05,
      "loss": 0.24,
      "step": 507
    },
    {
      "epoch": 0.33509234828496043,
      "grad_norm": 6.669556140899658,
      "learning_rate": 8.885224274406333e-05,
      "loss": 0.3439,
      "step": 508
    },
    {
      "epoch": 0.33575197889182057,
      "grad_norm": 6.317419052124023,
      "learning_rate": 8.883025505716799e-05,
      "loss": 0.4633,
      "step": 509
    },
    {
      "epoch": 0.33641160949868076,
      "grad_norm": 2.7643561363220215,
      "learning_rate": 8.880826737027264e-05,
      "loss": 0.1982,
      "step": 510
    },
    {
      "epoch": 0.3370712401055409,
      "grad_norm": 6.673772811889648,
      "learning_rate": 8.878627968337731e-05,
      "loss": 0.3843,
      "step": 511
    },
    {
      "epoch": 0.33773087071240104,
      "grad_norm": 2.2649452686309814,
      "learning_rate": 8.876429199648197e-05,
      "loss": 0.2258,
      "step": 512
    },
    {
      "epoch": 0.33839050131926124,
      "grad_norm": 2.1154727935791016,
      "learning_rate": 8.874230430958663e-05,
      "loss": 0.1475,
      "step": 513
    },
    {
      "epoch": 0.3390501319261214,
      "grad_norm": 3.808337926864624,
      "learning_rate": 8.87203166226913e-05,
      "loss": 0.348,
      "step": 514
    },
    {
      "epoch": 0.3397097625329815,
      "grad_norm": 4.505814075469971,
      "learning_rate": 8.869832893579596e-05,
      "loss": 0.2869,
      "step": 515
    },
    {
      "epoch": 0.3403693931398417,
      "grad_norm": 6.735334396362305,
      "learning_rate": 8.867634124890061e-05,
      "loss": 0.4025,
      "step": 516
    },
    {
      "epoch": 0.34102902374670185,
      "grad_norm": 9.057083129882812,
      "learning_rate": 8.865435356200527e-05,
      "loss": 0.6653,
      "step": 517
    },
    {
      "epoch": 0.341688654353562,
      "grad_norm": 5.825721263885498,
      "learning_rate": 8.863236587510994e-05,
      "loss": 0.3076,
      "step": 518
    },
    {
      "epoch": 0.3423482849604222,
      "grad_norm": 7.597251892089844,
      "learning_rate": 8.86103781882146e-05,
      "loss": 0.6528,
      "step": 519
    },
    {
      "epoch": 0.34300791556728233,
      "grad_norm": 2.8945202827453613,
      "learning_rate": 8.858839050131927e-05,
      "loss": 0.2284,
      "step": 520
    },
    {
      "epoch": 0.34366754617414247,
      "grad_norm": 4.102789878845215,
      "learning_rate": 8.856640281442393e-05,
      "loss": 0.2612,
      "step": 521
    },
    {
      "epoch": 0.34432717678100266,
      "grad_norm": 4.723498344421387,
      "learning_rate": 8.85444151275286e-05,
      "loss": 0.1803,
      "step": 522
    },
    {
      "epoch": 0.3449868073878628,
      "grad_norm": 2.0659050941467285,
      "learning_rate": 8.852242744063325e-05,
      "loss": 0.1405,
      "step": 523
    },
    {
      "epoch": 0.34564643799472294,
      "grad_norm": 5.03515100479126,
      "learning_rate": 8.850043975373791e-05,
      "loss": 0.3891,
      "step": 524
    },
    {
      "epoch": 0.34630606860158314,
      "grad_norm": 1.8038629293441772,
      "learning_rate": 8.847845206684258e-05,
      "loss": 0.1845,
      "step": 525
    },
    {
      "epoch": 0.3469656992084433,
      "grad_norm": 12.87426471710205,
      "learning_rate": 8.845646437994724e-05,
      "loss": 0.752,
      "step": 526
    },
    {
      "epoch": 0.3476253298153034,
      "grad_norm": 5.238137722015381,
      "learning_rate": 8.84344766930519e-05,
      "loss": 0.4134,
      "step": 527
    },
    {
      "epoch": 0.3482849604221636,
      "grad_norm": 9.478986740112305,
      "learning_rate": 8.841248900615655e-05,
      "loss": 0.575,
      "step": 528
    },
    {
      "epoch": 0.34894459102902375,
      "grad_norm": 1.8734304904937744,
      "learning_rate": 8.839050131926122e-05,
      "loss": 0.1769,
      "step": 529
    },
    {
      "epoch": 0.3496042216358839,
      "grad_norm": 3.548351287841797,
      "learning_rate": 8.836851363236588e-05,
      "loss": 0.2154,
      "step": 530
    },
    {
      "epoch": 0.3502638522427441,
      "grad_norm": 2.498629570007324,
      "learning_rate": 8.834652594547054e-05,
      "loss": 0.1617,
      "step": 531
    },
    {
      "epoch": 0.35092348284960423,
      "grad_norm": 4.70318603515625,
      "learning_rate": 8.83245382585752e-05,
      "loss": 0.418,
      "step": 532
    },
    {
      "epoch": 0.35158311345646437,
      "grad_norm": 3.1421802043914795,
      "learning_rate": 8.830255057167986e-05,
      "loss": 0.3674,
      "step": 533
    },
    {
      "epoch": 0.35224274406332456,
      "grad_norm": 5.147050380706787,
      "learning_rate": 8.828056288478452e-05,
      "loss": 0.3988,
      "step": 534
    },
    {
      "epoch": 0.3529023746701847,
      "grad_norm": 5.162676811218262,
      "learning_rate": 8.825857519788918e-05,
      "loss": 0.464,
      "step": 535
    },
    {
      "epoch": 0.35356200527704484,
      "grad_norm": 7.638882637023926,
      "learning_rate": 8.823658751099385e-05,
      "loss": 0.5002,
      "step": 536
    },
    {
      "epoch": 0.35422163588390504,
      "grad_norm": 5.643865585327148,
      "learning_rate": 8.82145998240985e-05,
      "loss": 0.3346,
      "step": 537
    },
    {
      "epoch": 0.3548812664907652,
      "grad_norm": 5.350941181182861,
      "learning_rate": 8.819261213720316e-05,
      "loss": 0.394,
      "step": 538
    },
    {
      "epoch": 0.3555408970976253,
      "grad_norm": 4.610873222351074,
      "learning_rate": 8.817062445030783e-05,
      "loss": 0.3729,
      "step": 539
    },
    {
      "epoch": 0.3562005277044855,
      "grad_norm": 7.535212516784668,
      "learning_rate": 8.814863676341249e-05,
      "loss": 0.4187,
      "step": 540
    },
    {
      "epoch": 0.35686015831134565,
      "grad_norm": 3.2134227752685547,
      "learning_rate": 8.812664907651716e-05,
      "loss": 0.2752,
      "step": 541
    },
    {
      "epoch": 0.3575197889182058,
      "grad_norm": 4.723737716674805,
      "learning_rate": 8.810466138962182e-05,
      "loss": 0.2245,
      "step": 542
    },
    {
      "epoch": 0.358179419525066,
      "grad_norm": 1.6374021768569946,
      "learning_rate": 8.808267370272649e-05,
      "loss": 0.2232,
      "step": 543
    },
    {
      "epoch": 0.35883905013192613,
      "grad_norm": 5.564384937286377,
      "learning_rate": 8.806068601583114e-05,
      "loss": 0.4857,
      "step": 544
    },
    {
      "epoch": 0.35949868073878627,
      "grad_norm": 3.8699839115142822,
      "learning_rate": 8.80386983289358e-05,
      "loss": 0.2365,
      "step": 545
    },
    {
      "epoch": 0.36015831134564646,
      "grad_norm": 1.0916024446487427,
      "learning_rate": 8.801671064204046e-05,
      "loss": 0.1193,
      "step": 546
    },
    {
      "epoch": 0.3608179419525066,
      "grad_norm": 2.3746724128723145,
      "learning_rate": 8.799472295514513e-05,
      "loss": 0.2908,
      "step": 547
    },
    {
      "epoch": 0.36147757255936674,
      "grad_norm": 2.222017765045166,
      "learning_rate": 8.797273526824979e-05,
      "loss": 0.1399,
      "step": 548
    },
    {
      "epoch": 0.36213720316622694,
      "grad_norm": 8.104708671569824,
      "learning_rate": 8.795074758135444e-05,
      "loss": 0.367,
      "step": 549
    },
    {
      "epoch": 0.3627968337730871,
      "grad_norm": 2.0717368125915527,
      "learning_rate": 8.792875989445911e-05,
      "loss": 0.1045,
      "step": 550
    },
    {
      "epoch": 0.3634564643799472,
      "grad_norm": 1.8206852674484253,
      "learning_rate": 8.790677220756377e-05,
      "loss": 0.0858,
      "step": 551
    },
    {
      "epoch": 0.3641160949868074,
      "grad_norm": 1.395050048828125,
      "learning_rate": 8.788478452066843e-05,
      "loss": 0.0778,
      "step": 552
    },
    {
      "epoch": 0.36477572559366755,
      "grad_norm": 6.295565128326416,
      "learning_rate": 8.786279683377308e-05,
      "loss": 0.25,
      "step": 553
    },
    {
      "epoch": 0.3654353562005277,
      "grad_norm": 2.9075002670288086,
      "learning_rate": 8.784080914687775e-05,
      "loss": 0.1245,
      "step": 554
    },
    {
      "epoch": 0.3660949868073879,
      "grad_norm": 13.625897407531738,
      "learning_rate": 8.781882145998241e-05,
      "loss": 0.8693,
      "step": 555
    },
    {
      "epoch": 0.36675461741424803,
      "grad_norm": 4.761309623718262,
      "learning_rate": 8.779683377308707e-05,
      "loss": 0.1497,
      "step": 556
    },
    {
      "epoch": 0.36741424802110817,
      "grad_norm": 17.396320343017578,
      "learning_rate": 8.777484608619174e-05,
      "loss": 1.1062,
      "step": 557
    },
    {
      "epoch": 0.36807387862796836,
      "grad_norm": 16.49413299560547,
      "learning_rate": 8.77528583992964e-05,
      "loss": 1.5617,
      "step": 558
    },
    {
      "epoch": 0.3687335092348285,
      "grad_norm": 12.435603141784668,
      "learning_rate": 8.773087071240105e-05,
      "loss": 0.6724,
      "step": 559
    },
    {
      "epoch": 0.36939313984168864,
      "grad_norm": 11.790371894836426,
      "learning_rate": 8.770888302550571e-05,
      "loss": 1.4632,
      "step": 560
    },
    {
      "epoch": 0.37005277044854884,
      "grad_norm": 6.195546627044678,
      "learning_rate": 8.768689533861038e-05,
      "loss": 0.216,
      "step": 561
    },
    {
      "epoch": 0.370712401055409,
      "grad_norm": 0.7611521482467651,
      "learning_rate": 8.766490765171505e-05,
      "loss": 0.0563,
      "step": 562
    },
    {
      "epoch": 0.3713720316622691,
      "grad_norm": 0.7102899551391602,
      "learning_rate": 8.764291996481971e-05,
      "loss": 0.0626,
      "step": 563
    },
    {
      "epoch": 0.3720316622691293,
      "grad_norm": 9.8390531539917,
      "learning_rate": 8.762093227792437e-05,
      "loss": 0.5684,
      "step": 564
    },
    {
      "epoch": 0.37269129287598945,
      "grad_norm": 12.986096382141113,
      "learning_rate": 8.759894459102904e-05,
      "loss": 1.5401,
      "step": 565
    },
    {
      "epoch": 0.3733509234828496,
      "grad_norm": 2.567634344100952,
      "learning_rate": 8.757695690413369e-05,
      "loss": 0.129,
      "step": 566
    },
    {
      "epoch": 0.3740105540897098,
      "grad_norm": 6.2790913581848145,
      "learning_rate": 8.755496921723835e-05,
      "loss": 0.3093,
      "step": 567
    },
    {
      "epoch": 0.37467018469656993,
      "grad_norm": 1.7939209938049316,
      "learning_rate": 8.753298153034302e-05,
      "loss": 0.1018,
      "step": 568
    },
    {
      "epoch": 0.37532981530343007,
      "grad_norm": 0.9751856923103333,
      "learning_rate": 8.751099384344768e-05,
      "loss": 0.0926,
      "step": 569
    },
    {
      "epoch": 0.3759894459102902,
      "grad_norm": 6.794933795928955,
      "learning_rate": 8.748900615655233e-05,
      "loss": 0.2959,
      "step": 570
    },
    {
      "epoch": 0.3766490765171504,
      "grad_norm": 9.430303573608398,
      "learning_rate": 8.746701846965699e-05,
      "loss": 1.5925,
      "step": 571
    },
    {
      "epoch": 0.37730870712401055,
      "grad_norm": 1.7484562397003174,
      "learning_rate": 8.744503078276166e-05,
      "loss": 0.1052,
      "step": 572
    },
    {
      "epoch": 0.3779683377308707,
      "grad_norm": 2.570610761642456,
      "learning_rate": 8.742304309586632e-05,
      "loss": 0.158,
      "step": 573
    },
    {
      "epoch": 0.3786279683377309,
      "grad_norm": 10.450345039367676,
      "learning_rate": 8.740105540897098e-05,
      "loss": 0.6908,
      "step": 574
    },
    {
      "epoch": 0.379287598944591,
      "grad_norm": 1.6002728939056396,
      "learning_rate": 8.737906772207565e-05,
      "loss": 0.1427,
      "step": 575
    },
    {
      "epoch": 0.37994722955145116,
      "grad_norm": 1.00077223777771,
      "learning_rate": 8.73570800351803e-05,
      "loss": 0.1116,
      "step": 576
    },
    {
      "epoch": 0.38060686015831136,
      "grad_norm": 2.197131633758545,
      "learning_rate": 8.733509234828496e-05,
      "loss": 0.1748,
      "step": 577
    },
    {
      "epoch": 0.3812664907651715,
      "grad_norm": 2.261547088623047,
      "learning_rate": 8.731310466138962e-05,
      "loss": 0.1731,
      "step": 578
    },
    {
      "epoch": 0.38192612137203164,
      "grad_norm": 5.828131675720215,
      "learning_rate": 8.729111697449429e-05,
      "loss": 0.2255,
      "step": 579
    },
    {
      "epoch": 0.38258575197889183,
      "grad_norm": 0.9501099586486816,
      "learning_rate": 8.726912928759894e-05,
      "loss": 0.1566,
      "step": 580
    },
    {
      "epoch": 0.38324538258575197,
      "grad_norm": 1.2395963668823242,
      "learning_rate": 8.72471416007036e-05,
      "loss": 0.1212,
      "step": 581
    },
    {
      "epoch": 0.3839050131926121,
      "grad_norm": 19.275827407836914,
      "learning_rate": 8.722515391380827e-05,
      "loss": 1.1388,
      "step": 582
    },
    {
      "epoch": 0.3845646437994723,
      "grad_norm": 11.57835865020752,
      "learning_rate": 8.720316622691293e-05,
      "loss": 0.6881,
      "step": 583
    },
    {
      "epoch": 0.38522427440633245,
      "grad_norm": 2.531592607498169,
      "learning_rate": 8.71811785400176e-05,
      "loss": 0.1893,
      "step": 584
    },
    {
      "epoch": 0.3858839050131926,
      "grad_norm": 2.6495440006256104,
      "learning_rate": 8.715919085312226e-05,
      "loss": 0.1451,
      "step": 585
    },
    {
      "epoch": 0.3865435356200528,
      "grad_norm": 8.207498550415039,
      "learning_rate": 8.713720316622693e-05,
      "loss": 0.5733,
      "step": 586
    },
    {
      "epoch": 0.3872031662269129,
      "grad_norm": 2.630970001220703,
      "learning_rate": 8.711521547933158e-05,
      "loss": 0.1432,
      "step": 587
    },
    {
      "epoch": 0.38786279683377306,
      "grad_norm": 5.959112644195557,
      "learning_rate": 8.709322779243624e-05,
      "loss": 0.503,
      "step": 588
    },
    {
      "epoch": 0.38852242744063326,
      "grad_norm": 6.437242031097412,
      "learning_rate": 8.70712401055409e-05,
      "loss": 1.1141,
      "step": 589
    },
    {
      "epoch": 0.3891820580474934,
      "grad_norm": 1.5802713632583618,
      "learning_rate": 8.704925241864557e-05,
      "loss": 0.1206,
      "step": 590
    },
    {
      "epoch": 0.38984168865435354,
      "grad_norm": 4.1992082595825195,
      "learning_rate": 8.702726473175023e-05,
      "loss": 0.1988,
      "step": 591
    },
    {
      "epoch": 0.39050131926121373,
      "grad_norm": 11.666790008544922,
      "learning_rate": 8.700527704485488e-05,
      "loss": 0.925,
      "step": 592
    },
    {
      "epoch": 0.39116094986807387,
      "grad_norm": 9.101651191711426,
      "learning_rate": 8.698328935795955e-05,
      "loss": 0.8508,
      "step": 593
    },
    {
      "epoch": 0.391820580474934,
      "grad_norm": 6.387360095977783,
      "learning_rate": 8.696130167106421e-05,
      "loss": 0.6986,
      "step": 594
    },
    {
      "epoch": 0.3924802110817942,
      "grad_norm": 0.7755548357963562,
      "learning_rate": 8.693931398416887e-05,
      "loss": 0.083,
      "step": 595
    },
    {
      "epoch": 0.39313984168865435,
      "grad_norm": 0.8583356142044067,
      "learning_rate": 8.691732629727352e-05,
      "loss": 0.0951,
      "step": 596
    },
    {
      "epoch": 0.3937994722955145,
      "grad_norm": 1.6900720596313477,
      "learning_rate": 8.68953386103782e-05,
      "loss": 0.101,
      "step": 597
    },
    {
      "epoch": 0.3944591029023747,
      "grad_norm": 1.52247953414917,
      "learning_rate": 8.687335092348285e-05,
      "loss": 0.1792,
      "step": 598
    },
    {
      "epoch": 0.3951187335092348,
      "grad_norm": 1.4367378950119019,
      "learning_rate": 8.685136323658751e-05,
      "loss": 0.1197,
      "step": 599
    },
    {
      "epoch": 0.39577836411609496,
      "grad_norm": 10.097174644470215,
      "learning_rate": 8.682937554969217e-05,
      "loss": 0.8928,
      "step": 600
    },
    {
      "epoch": 0.39643799472295516,
      "grad_norm": 5.672211170196533,
      "learning_rate": 8.680738786279684e-05,
      "loss": 0.301,
      "step": 601
    },
    {
      "epoch": 0.3970976253298153,
      "grad_norm": 10.202777862548828,
      "learning_rate": 8.67854001759015e-05,
      "loss": 0.7057,
      "step": 602
    },
    {
      "epoch": 0.39775725593667544,
      "grad_norm": 8.287519454956055,
      "learning_rate": 8.676341248900616e-05,
      "loss": 0.6223,
      "step": 603
    },
    {
      "epoch": 0.39841688654353563,
      "grad_norm": 2.606781482696533,
      "learning_rate": 8.674142480211082e-05,
      "loss": 0.2158,
      "step": 604
    },
    {
      "epoch": 0.39907651715039577,
      "grad_norm": 1.0042611360549927,
      "learning_rate": 8.671943711521549e-05,
      "loss": 0.0995,
      "step": 605
    },
    {
      "epoch": 0.3997361477572559,
      "grad_norm": 10.3836030960083,
      "learning_rate": 8.669744942832015e-05,
      "loss": 0.9049,
      "step": 606
    },
    {
      "epoch": 0.4003957783641161,
      "grad_norm": 6.2848920822143555,
      "learning_rate": 8.66754617414248e-05,
      "loss": 0.6672,
      "step": 607
    },
    {
      "epoch": 0.40105540897097625,
      "grad_norm": 5.111202239990234,
      "learning_rate": 8.665347405452948e-05,
      "loss": 0.3969,
      "step": 608
    },
    {
      "epoch": 0.4017150395778364,
      "grad_norm": 3.5611448287963867,
      "learning_rate": 8.663148636763413e-05,
      "loss": 0.2368,
      "step": 609
    },
    {
      "epoch": 0.4023746701846966,
      "grad_norm": 1.600982904434204,
      "learning_rate": 8.660949868073879e-05,
      "loss": 0.134,
      "step": 610
    },
    {
      "epoch": 0.4030343007915567,
      "grad_norm": 3.067080020904541,
      "learning_rate": 8.658751099384346e-05,
      "loss": 0.3687,
      "step": 611
    },
    {
      "epoch": 0.40369393139841686,
      "grad_norm": 4.600856781005859,
      "learning_rate": 8.656552330694812e-05,
      "loss": 0.5529,
      "step": 612
    },
    {
      "epoch": 0.40435356200527706,
      "grad_norm": 2.090303659439087,
      "learning_rate": 8.654353562005277e-05,
      "loss": 0.2232,
      "step": 613
    },
    {
      "epoch": 0.4050131926121372,
      "grad_norm": 4.23403263092041,
      "learning_rate": 8.652154793315743e-05,
      "loss": 0.325,
      "step": 614
    },
    {
      "epoch": 0.40567282321899734,
      "grad_norm": 3.722106695175171,
      "learning_rate": 8.64995602462621e-05,
      "loss": 0.3519,
      "step": 615
    },
    {
      "epoch": 0.40633245382585753,
      "grad_norm": 1.7215821743011475,
      "learning_rate": 8.647757255936676e-05,
      "loss": 0.1491,
      "step": 616
    },
    {
      "epoch": 0.40699208443271767,
      "grad_norm": 3.2589852809906006,
      "learning_rate": 8.645558487247142e-05,
      "loss": 0.2597,
      "step": 617
    },
    {
      "epoch": 0.4076517150395778,
      "grad_norm": 1.977856159210205,
      "learning_rate": 8.643359718557607e-05,
      "loss": 0.1337,
      "step": 618
    },
    {
      "epoch": 0.408311345646438,
      "grad_norm": 8.176034927368164,
      "learning_rate": 8.641160949868074e-05,
      "loss": 0.4979,
      "step": 619
    },
    {
      "epoch": 0.40897097625329815,
      "grad_norm": 2.9159367084503174,
      "learning_rate": 8.63896218117854e-05,
      "loss": 0.1486,
      "step": 620
    },
    {
      "epoch": 0.4096306068601583,
      "grad_norm": 1.1946992874145508,
      "learning_rate": 8.636763412489006e-05,
      "loss": 0.1094,
      "step": 621
    },
    {
      "epoch": 0.4102902374670185,
      "grad_norm": 15.224732398986816,
      "learning_rate": 8.634564643799473e-05,
      "loss": 0.6547,
      "step": 622
    },
    {
      "epoch": 0.4109498680738786,
      "grad_norm": 15.671095848083496,
      "learning_rate": 8.632365875109938e-05,
      "loss": 0.7704,
      "step": 623
    },
    {
      "epoch": 0.41160949868073876,
      "grad_norm": 14.50412368774414,
      "learning_rate": 8.630167106420404e-05,
      "loss": 1.5145,
      "step": 624
    },
    {
      "epoch": 0.41226912928759896,
      "grad_norm": 11.830679893493652,
      "learning_rate": 8.627968337730871e-05,
      "loss": 0.9928,
      "step": 625
    },
    {
      "epoch": 0.4129287598944591,
      "grad_norm": 9.37365436553955,
      "learning_rate": 8.625769569041337e-05,
      "loss": 0.6977,
      "step": 626
    },
    {
      "epoch": 0.41358839050131924,
      "grad_norm": 9.755638122558594,
      "learning_rate": 8.623570800351804e-05,
      "loss": 0.8478,
      "step": 627
    },
    {
      "epoch": 0.41424802110817943,
      "grad_norm": 1.6091463565826416,
      "learning_rate": 8.62137203166227e-05,
      "loss": 0.0796,
      "step": 628
    },
    {
      "epoch": 0.41490765171503957,
      "grad_norm": 1.973203182220459,
      "learning_rate": 8.619173262972737e-05,
      "loss": 0.0934,
      "step": 629
    },
    {
      "epoch": 0.4155672823218997,
      "grad_norm": 4.059591293334961,
      "learning_rate": 8.616974494283202e-05,
      "loss": 0.2056,
      "step": 630
    },
    {
      "epoch": 0.4162269129287599,
      "grad_norm": 0.8210698366165161,
      "learning_rate": 8.614775725593668e-05,
      "loss": 0.0815,
      "step": 631
    },
    {
      "epoch": 0.41688654353562005,
      "grad_norm": 4.548077583312988,
      "learning_rate": 8.612576956904134e-05,
      "loss": 0.1858,
      "step": 632
    },
    {
      "epoch": 0.4175461741424802,
      "grad_norm": 9.904218673706055,
      "learning_rate": 8.610378188214601e-05,
      "loss": 0.6122,
      "step": 633
    },
    {
      "epoch": 0.4182058047493404,
      "grad_norm": 1.9664099216461182,
      "learning_rate": 8.608179419525067e-05,
      "loss": 0.1061,
      "step": 634
    },
    {
      "epoch": 0.4188654353562005,
      "grad_norm": 8.880142211914062,
      "learning_rate": 8.605980650835532e-05,
      "loss": 0.49,
      "step": 635
    },
    {
      "epoch": 0.41952506596306066,
      "grad_norm": 4.383951663970947,
      "learning_rate": 8.603781882145998e-05,
      "loss": 0.2107,
      "step": 636
    },
    {
      "epoch": 0.42018469656992086,
      "grad_norm": 1.916690707206726,
      "learning_rate": 8.601583113456465e-05,
      "loss": 0.1502,
      "step": 637
    },
    {
      "epoch": 0.420844327176781,
      "grad_norm": 1.9417039155960083,
      "learning_rate": 8.599384344766931e-05,
      "loss": 0.1021,
      "step": 638
    },
    {
      "epoch": 0.42150395778364114,
      "grad_norm": 3.0885074138641357,
      "learning_rate": 8.597185576077396e-05,
      "loss": 0.1217,
      "step": 639
    },
    {
      "epoch": 0.42216358839050133,
      "grad_norm": 3.5536720752716064,
      "learning_rate": 8.594986807387863e-05,
      "loss": 0.2131,
      "step": 640
    },
    {
      "epoch": 0.42282321899736147,
      "grad_norm": 9.881816864013672,
      "learning_rate": 8.592788038698329e-05,
      "loss": 0.8697,
      "step": 641
    },
    {
      "epoch": 0.4234828496042216,
      "grad_norm": 4.300847053527832,
      "learning_rate": 8.590589270008795e-05,
      "loss": 0.2207,
      "step": 642
    },
    {
      "epoch": 0.4241424802110818,
      "grad_norm": 3.8750839233398438,
      "learning_rate": 8.58839050131926e-05,
      "loss": 0.231,
      "step": 643
    },
    {
      "epoch": 0.42480211081794195,
      "grad_norm": 4.3404316902160645,
      "learning_rate": 8.586191732629728e-05,
      "loss": 0.2328,
      "step": 644
    },
    {
      "epoch": 0.4254617414248021,
      "grad_norm": 5.156894207000732,
      "learning_rate": 8.583992963940193e-05,
      "loss": 0.5454,
      "step": 645
    },
    {
      "epoch": 0.4261213720316623,
      "grad_norm": 4.097331523895264,
      "learning_rate": 8.58179419525066e-05,
      "loss": 0.2822,
      "step": 646
    },
    {
      "epoch": 0.4267810026385224,
      "grad_norm": 1.8232942819595337,
      "learning_rate": 8.579595426561126e-05,
      "loss": 0.2111,
      "step": 647
    },
    {
      "epoch": 0.42744063324538256,
      "grad_norm": 7.204232692718506,
      "learning_rate": 8.577396657871593e-05,
      "loss": 0.5235,
      "step": 648
    },
    {
      "epoch": 0.42810026385224276,
      "grad_norm": 4.2446513175964355,
      "learning_rate": 8.575197889182059e-05,
      "loss": 0.3654,
      "step": 649
    },
    {
      "epoch": 0.4287598944591029,
      "grad_norm": 1.7477812767028809,
      "learning_rate": 8.572999120492525e-05,
      "loss": 0.1841,
      "step": 650
    },
    {
      "epoch": 0.42941952506596304,
      "grad_norm": 1.983232855796814,
      "learning_rate": 8.570800351802992e-05,
      "loss": 0.1578,
      "step": 651
    },
    {
      "epoch": 0.43007915567282323,
      "grad_norm": 2.889765977859497,
      "learning_rate": 8.568601583113457e-05,
      "loss": 0.2428,
      "step": 652
    },
    {
      "epoch": 0.43073878627968337,
      "grad_norm": 4.3378424644470215,
      "learning_rate": 8.566402814423923e-05,
      "loss": 0.2469,
      "step": 653
    },
    {
      "epoch": 0.4313984168865435,
      "grad_norm": 6.2121076583862305,
      "learning_rate": 8.564204045734389e-05,
      "loss": 0.5114,
      "step": 654
    },
    {
      "epoch": 0.4320580474934037,
      "grad_norm": 2.541395902633667,
      "learning_rate": 8.562005277044856e-05,
      "loss": 0.1685,
      "step": 655
    },
    {
      "epoch": 0.43271767810026385,
      "grad_norm": 9.130796432495117,
      "learning_rate": 8.559806508355321e-05,
      "loss": 0.798,
      "step": 656
    },
    {
      "epoch": 0.433377308707124,
      "grad_norm": 5.35709285736084,
      "learning_rate": 8.557607739665787e-05,
      "loss": 0.3836,
      "step": 657
    },
    {
      "epoch": 0.4340369393139842,
      "grad_norm": 5.616968631744385,
      "learning_rate": 8.555408970976254e-05,
      "loss": 0.316,
      "step": 658
    },
    {
      "epoch": 0.4346965699208443,
      "grad_norm": 12.799691200256348,
      "learning_rate": 8.55321020228672e-05,
      "loss": 0.5408,
      "step": 659
    },
    {
      "epoch": 0.43535620052770446,
      "grad_norm": 7.587985992431641,
      "learning_rate": 8.551011433597186e-05,
      "loss": 0.4823,
      "step": 660
    },
    {
      "epoch": 0.43601583113456466,
      "grad_norm": 9.356327056884766,
      "learning_rate": 8.548812664907651e-05,
      "loss": 0.7358,
      "step": 661
    },
    {
      "epoch": 0.4366754617414248,
      "grad_norm": 4.119976043701172,
      "learning_rate": 8.546613896218118e-05,
      "loss": 0.2529,
      "step": 662
    },
    {
      "epoch": 0.43733509234828494,
      "grad_norm": 7.287176609039307,
      "learning_rate": 8.544415127528584e-05,
      "loss": 0.4298,
      "step": 663
    },
    {
      "epoch": 0.43799472295514513,
      "grad_norm": 4.719658374786377,
      "learning_rate": 8.54221635883905e-05,
      "loss": 0.249,
      "step": 664
    },
    {
      "epoch": 0.4386543535620053,
      "grad_norm": 5.719143390655518,
      "learning_rate": 8.540017590149517e-05,
      "loss": 0.3589,
      "step": 665
    },
    {
      "epoch": 0.4393139841688654,
      "grad_norm": 8.86751651763916,
      "learning_rate": 8.537818821459982e-05,
      "loss": 1.1464,
      "step": 666
    },
    {
      "epoch": 0.4399736147757256,
      "grad_norm": 2.6724648475646973,
      "learning_rate": 8.53562005277045e-05,
      "loss": 0.324,
      "step": 667
    },
    {
      "epoch": 0.44063324538258575,
      "grad_norm": 1.5311839580535889,
      "learning_rate": 8.533421284080915e-05,
      "loss": 0.2172,
      "step": 668
    },
    {
      "epoch": 0.4412928759894459,
      "grad_norm": 2.214045524597168,
      "learning_rate": 8.531222515391382e-05,
      "loss": 0.1895,
      "step": 669
    },
    {
      "epoch": 0.4419525065963061,
      "grad_norm": 2.0543224811553955,
      "learning_rate": 8.529023746701848e-05,
      "loss": 0.2533,
      "step": 670
    },
    {
      "epoch": 0.4426121372031662,
      "grad_norm": 1.672110676765442,
      "learning_rate": 8.526824978012314e-05,
      "loss": 0.1254,
      "step": 671
    },
    {
      "epoch": 0.44327176781002636,
      "grad_norm": 2.8828461170196533,
      "learning_rate": 8.52462620932278e-05,
      "loss": 0.1917,
      "step": 672
    },
    {
      "epoch": 0.44393139841688656,
      "grad_norm": 8.137594223022461,
      "learning_rate": 8.522427440633246e-05,
      "loss": 0.784,
      "step": 673
    },
    {
      "epoch": 0.4445910290237467,
      "grad_norm": 3.6377880573272705,
      "learning_rate": 8.520228671943712e-05,
      "loss": 0.2258,
      "step": 674
    },
    {
      "epoch": 0.44525065963060684,
      "grad_norm": 3.8083198070526123,
      "learning_rate": 8.518029903254178e-05,
      "loss": 0.2379,
      "step": 675
    },
    {
      "epoch": 0.44591029023746703,
      "grad_norm": 8.895240783691406,
      "learning_rate": 8.515831134564645e-05,
      "loss": 0.522,
      "step": 676
    },
    {
      "epoch": 0.4465699208443272,
      "grad_norm": 4.980997562408447,
      "learning_rate": 8.51363236587511e-05,
      "loss": 0.1919,
      "step": 677
    },
    {
      "epoch": 0.4472295514511873,
      "grad_norm": 6.29743766784668,
      "learning_rate": 8.511433597185576e-05,
      "loss": 0.4591,
      "step": 678
    },
    {
      "epoch": 0.4478891820580475,
      "grad_norm": 3.620671033859253,
      "learning_rate": 8.509234828496042e-05,
      "loss": 0.3099,
      "step": 679
    },
    {
      "epoch": 0.44854881266490765,
      "grad_norm": 13.95104694366455,
      "learning_rate": 8.507036059806509e-05,
      "loss": 0.1516,
      "step": 680
    },
    {
      "epoch": 0.4492084432717678,
      "grad_norm": 6.67484188079834,
      "learning_rate": 8.504837291116975e-05,
      "loss": 0.2538,
      "step": 681
    },
    {
      "epoch": 0.449868073878628,
      "grad_norm": 3.310877799987793,
      "learning_rate": 8.50263852242744e-05,
      "loss": 0.305,
      "step": 682
    },
    {
      "epoch": 0.4505277044854881,
      "grad_norm": 2.549614191055298,
      "learning_rate": 8.500439753737907e-05,
      "loss": 0.2698,
      "step": 683
    },
    {
      "epoch": 0.45118733509234826,
      "grad_norm": 2.139801025390625,
      "learning_rate": 8.498240985048373e-05,
      "loss": 0.2677,
      "step": 684
    },
    {
      "epoch": 0.45184696569920846,
      "grad_norm": 1.6720401048660278,
      "learning_rate": 8.496042216358839e-05,
      "loss": 0.2442,
      "step": 685
    },
    {
      "epoch": 0.4525065963060686,
      "grad_norm": 3.917452812194824,
      "learning_rate": 8.493843447669305e-05,
      "loss": 0.2846,
      "step": 686
    },
    {
      "epoch": 0.45316622691292874,
      "grad_norm": 2.357390880584717,
      "learning_rate": 8.491644678979772e-05,
      "loss": 0.2401,
      "step": 687
    },
    {
      "epoch": 0.45382585751978893,
      "grad_norm": 4.968775272369385,
      "learning_rate": 8.489445910290237e-05,
      "loss": 0.2984,
      "step": 688
    },
    {
      "epoch": 0.4544854881266491,
      "grad_norm": 3.5178682804107666,
      "learning_rate": 8.487247141600704e-05,
      "loss": 0.2806,
      "step": 689
    },
    {
      "epoch": 0.4551451187335092,
      "grad_norm": 7.626011848449707,
      "learning_rate": 8.48504837291117e-05,
      "loss": 0.6425,
      "step": 690
    },
    {
      "epoch": 0.4558047493403694,
      "grad_norm": 7.072492599487305,
      "learning_rate": 8.482849604221637e-05,
      "loss": 0.3183,
      "step": 691
    },
    {
      "epoch": 0.45646437994722955,
      "grad_norm": 4.356438636779785,
      "learning_rate": 8.480650835532103e-05,
      "loss": 0.2434,
      "step": 692
    },
    {
      "epoch": 0.4571240105540897,
      "grad_norm": 1.0624336004257202,
      "learning_rate": 8.478452066842569e-05,
      "loss": 0.123,
      "step": 693
    },
    {
      "epoch": 0.4577836411609499,
      "grad_norm": 3.313631057739258,
      "learning_rate": 8.476253298153036e-05,
      "loss": 0.1705,
      "step": 694
    },
    {
      "epoch": 0.45844327176781,
      "grad_norm": 8.258650779724121,
      "learning_rate": 8.474054529463501e-05,
      "loss": 0.5844,
      "step": 695
    },
    {
      "epoch": 0.45910290237467016,
      "grad_norm": 3.473341703414917,
      "learning_rate": 8.471855760773967e-05,
      "loss": 0.1949,
      "step": 696
    },
    {
      "epoch": 0.45976253298153036,
      "grad_norm": 2.259833335876465,
      "learning_rate": 8.469656992084433e-05,
      "loss": 0.1943,
      "step": 697
    },
    {
      "epoch": 0.4604221635883905,
      "grad_norm": 7.889392852783203,
      "learning_rate": 8.4674582233949e-05,
      "loss": 0.4887,
      "step": 698
    },
    {
      "epoch": 0.46108179419525064,
      "grad_norm": 5.564690589904785,
      "learning_rate": 8.465259454705365e-05,
      "loss": 0.3855,
      "step": 699
    },
    {
      "epoch": 0.46174142480211083,
      "grad_norm": 3.133204698562622,
      "learning_rate": 8.463060686015831e-05,
      "loss": 0.1727,
      "step": 700
    },
    {
      "epoch": 0.462401055408971,
      "grad_norm": 6.4977335929870605,
      "learning_rate": 8.460861917326298e-05,
      "loss": 0.3853,
      "step": 701
    },
    {
      "epoch": 0.4630606860158311,
      "grad_norm": 4.38666296005249,
      "learning_rate": 8.458663148636764e-05,
      "loss": 0.2063,
      "step": 702
    },
    {
      "epoch": 0.4637203166226913,
      "grad_norm": 1.6428747177124023,
      "learning_rate": 8.45646437994723e-05,
      "loss": 0.1958,
      "step": 703
    },
    {
      "epoch": 0.46437994722955145,
      "grad_norm": 3.9250547885894775,
      "learning_rate": 8.454265611257695e-05,
      "loss": 0.2024,
      "step": 704
    },
    {
      "epoch": 0.4650395778364116,
      "grad_norm": 7.32628870010376,
      "learning_rate": 8.452066842568162e-05,
      "loss": 0.9612,
      "step": 705
    },
    {
      "epoch": 0.4656992084432718,
      "grad_norm": 7.076186180114746,
      "learning_rate": 8.449868073878628e-05,
      "loss": 0.4701,
      "step": 706
    },
    {
      "epoch": 0.4663588390501319,
      "grad_norm": 1.1656415462493896,
      "learning_rate": 8.447669305189094e-05,
      "loss": 0.1018,
      "step": 707
    },
    {
      "epoch": 0.46701846965699206,
      "grad_norm": 2.779819965362549,
      "learning_rate": 8.445470536499561e-05,
      "loss": 0.2245,
      "step": 708
    },
    {
      "epoch": 0.46767810026385226,
      "grad_norm": 1.992388367652893,
      "learning_rate": 8.443271767810026e-05,
      "loss": 0.1652,
      "step": 709
    },
    {
      "epoch": 0.4683377308707124,
      "grad_norm": 2.6740000247955322,
      "learning_rate": 8.441072999120494e-05,
      "loss": 0.1916,
      "step": 710
    },
    {
      "epoch": 0.46899736147757254,
      "grad_norm": 11.493885040283203,
      "learning_rate": 8.438874230430959e-05,
      "loss": 0.5583,
      "step": 711
    },
    {
      "epoch": 0.46965699208443273,
      "grad_norm": 4.33714485168457,
      "learning_rate": 8.436675461741426e-05,
      "loss": 0.3577,
      "step": 712
    },
    {
      "epoch": 0.4703166226912929,
      "grad_norm": 7.468125820159912,
      "learning_rate": 8.434476693051892e-05,
      "loss": 0.3115,
      "step": 713
    },
    {
      "epoch": 0.470976253298153,
      "grad_norm": 2.683070659637451,
      "learning_rate": 8.432277924362358e-05,
      "loss": 0.1853,
      "step": 714
    },
    {
      "epoch": 0.4716358839050132,
      "grad_norm": 3.4519684314727783,
      "learning_rate": 8.430079155672823e-05,
      "loss": 0.2207,
      "step": 715
    },
    {
      "epoch": 0.47229551451187335,
      "grad_norm": 3.1449670791625977,
      "learning_rate": 8.42788038698329e-05,
      "loss": 0.1918,
      "step": 716
    },
    {
      "epoch": 0.4729551451187335,
      "grad_norm": 2.915945529937744,
      "learning_rate": 8.425681618293756e-05,
      "loss": 0.2811,
      "step": 717
    },
    {
      "epoch": 0.4736147757255937,
      "grad_norm": 1.2807800769805908,
      "learning_rate": 8.423482849604222e-05,
      "loss": 0.124,
      "step": 718
    },
    {
      "epoch": 0.4742744063324538,
      "grad_norm": 2.1772918701171875,
      "learning_rate": 8.421284080914689e-05,
      "loss": 0.1397,
      "step": 719
    },
    {
      "epoch": 0.47493403693931396,
      "grad_norm": 13.47558879852295,
      "learning_rate": 8.419085312225155e-05,
      "loss": 0.4071,
      "step": 720
    },
    {
      "epoch": 0.47559366754617416,
      "grad_norm": 9.535431861877441,
      "learning_rate": 8.41688654353562e-05,
      "loss": 0.4458,
      "step": 721
    },
    {
      "epoch": 0.4762532981530343,
      "grad_norm": 0.683192789554596,
      "learning_rate": 8.414687774846086e-05,
      "loss": 0.0643,
      "step": 722
    },
    {
      "epoch": 0.47691292875989444,
      "grad_norm": 1.7384600639343262,
      "learning_rate": 8.412489006156553e-05,
      "loss": 0.1188,
      "step": 723
    },
    {
      "epoch": 0.47757255936675463,
      "grad_norm": 2.6806564331054688,
      "learning_rate": 8.410290237467019e-05,
      "loss": 0.1416,
      "step": 724
    },
    {
      "epoch": 0.4782321899736148,
      "grad_norm": 8.589217185974121,
      "learning_rate": 8.408091468777484e-05,
      "loss": 0.3274,
      "step": 725
    },
    {
      "epoch": 0.4788918205804749,
      "grad_norm": 13.8891019821167,
      "learning_rate": 8.40589270008795e-05,
      "loss": 0.8671,
      "step": 726
    },
    {
      "epoch": 0.4795514511873351,
      "grad_norm": 10.130946159362793,
      "learning_rate": 8.403693931398417e-05,
      "loss": 0.5062,
      "step": 727
    },
    {
      "epoch": 0.48021108179419525,
      "grad_norm": 10.789441108703613,
      "learning_rate": 8.401495162708883e-05,
      "loss": 0.3942,
      "step": 728
    },
    {
      "epoch": 0.4808707124010554,
      "grad_norm": 8.165515899658203,
      "learning_rate": 8.399296394019349e-05,
      "loss": 0.2844,
      "step": 729
    },
    {
      "epoch": 0.4815303430079156,
      "grad_norm": 8.477927207946777,
      "learning_rate": 8.397097625329816e-05,
      "loss": 0.3236,
      "step": 730
    },
    {
      "epoch": 0.4821899736147757,
      "grad_norm": 1.4024276733398438,
      "learning_rate": 8.394898856640281e-05,
      "loss": 0.1563,
      "step": 731
    },
    {
      "epoch": 0.48284960422163586,
      "grad_norm": 5.369033336639404,
      "learning_rate": 8.392700087950748e-05,
      "loss": 0.1838,
      "step": 732
    },
    {
      "epoch": 0.48350923482849606,
      "grad_norm": 2.0200257301330566,
      "learning_rate": 8.390501319261214e-05,
      "loss": 0.1096,
      "step": 733
    },
    {
      "epoch": 0.4841688654353562,
      "grad_norm": 58.11643600463867,
      "learning_rate": 8.388302550571681e-05,
      "loss": 0.3966,
      "step": 734
    },
    {
      "epoch": 0.48482849604221634,
      "grad_norm": 3.1665260791778564,
      "learning_rate": 8.386103781882147e-05,
      "loss": 0.1614,
      "step": 735
    },
    {
      "epoch": 0.48548812664907653,
      "grad_norm": 7.957869052886963,
      "learning_rate": 8.383905013192613e-05,
      "loss": 0.6752,
      "step": 736
    },
    {
      "epoch": 0.4861477572559367,
      "grad_norm": 2.4993457794189453,
      "learning_rate": 8.38170624450308e-05,
      "loss": 0.1809,
      "step": 737
    },
    {
      "epoch": 0.4868073878627968,
      "grad_norm": 10.142062187194824,
      "learning_rate": 8.379507475813545e-05,
      "loss": 0.2613,
      "step": 738
    },
    {
      "epoch": 0.487467018469657,
      "grad_norm": 11.050564765930176,
      "learning_rate": 8.377308707124011e-05,
      "loss": 1.0889,
      "step": 739
    },
    {
      "epoch": 0.48812664907651715,
      "grad_norm": 4.517742156982422,
      "learning_rate": 8.375109938434477e-05,
      "loss": 0.2549,
      "step": 740
    },
    {
      "epoch": 0.4887862796833773,
      "grad_norm": 6.305844306945801,
      "learning_rate": 8.372911169744944e-05,
      "loss": 0.4024,
      "step": 741
    },
    {
      "epoch": 0.4894459102902375,
      "grad_norm": 2.1026499271392822,
      "learning_rate": 8.37071240105541e-05,
      "loss": 0.2599,
      "step": 742
    },
    {
      "epoch": 0.4901055408970976,
      "grad_norm": 4.216249942779541,
      "learning_rate": 8.368513632365875e-05,
      "loss": 0.2617,
      "step": 743
    },
    {
      "epoch": 0.49076517150395776,
      "grad_norm": 6.493701457977295,
      "learning_rate": 8.366314863676341e-05,
      "loss": 0.4518,
      "step": 744
    },
    {
      "epoch": 0.49142480211081796,
      "grad_norm": 6.630246639251709,
      "learning_rate": 8.364116094986808e-05,
      "loss": 0.435,
      "step": 745
    },
    {
      "epoch": 0.4920844327176781,
      "grad_norm": 2.083435535430908,
      "learning_rate": 8.361917326297274e-05,
      "loss": 0.1639,
      "step": 746
    },
    {
      "epoch": 0.49274406332453824,
      "grad_norm": 3.1521201133728027,
      "learning_rate": 8.359718557607739e-05,
      "loss": 0.2952,
      "step": 747
    },
    {
      "epoch": 0.49340369393139843,
      "grad_norm": 12.91403579711914,
      "learning_rate": 8.357519788918206e-05,
      "loss": 0.9237,
      "step": 748
    },
    {
      "epoch": 0.4940633245382586,
      "grad_norm": 17.318191528320312,
      "learning_rate": 8.355321020228672e-05,
      "loss": 0.9305,
      "step": 749
    },
    {
      "epoch": 0.4947229551451187,
      "grad_norm": 2.4589459896087646,
      "learning_rate": 8.353122251539138e-05,
      "loss": 0.1311,
      "step": 750
    },
    {
      "epoch": 0.4953825857519789,
      "grad_norm": 2.231827974319458,
      "learning_rate": 8.350923482849605e-05,
      "loss": 0.1279,
      "step": 751
    },
    {
      "epoch": 0.49604221635883905,
      "grad_norm": 12.755821228027344,
      "learning_rate": 8.34872471416007e-05,
      "loss": 0.7319,
      "step": 752
    },
    {
      "epoch": 0.4967018469656992,
      "grad_norm": 6.303048610687256,
      "learning_rate": 8.346525945470538e-05,
      "loss": 0.2614,
      "step": 753
    },
    {
      "epoch": 0.4973614775725594,
      "grad_norm": 7.545900344848633,
      "learning_rate": 8.344327176781003e-05,
      "loss": 0.3404,
      "step": 754
    },
    {
      "epoch": 0.4980211081794195,
      "grad_norm": 9.888914108276367,
      "learning_rate": 8.34212840809147e-05,
      "loss": 0.5513,
      "step": 755
    },
    {
      "epoch": 0.49868073878627966,
      "grad_norm": 13.955513954162598,
      "learning_rate": 8.339929639401936e-05,
      "loss": 0.9798,
      "step": 756
    },
    {
      "epoch": 0.49934036939313986,
      "grad_norm": 11.227166175842285,
      "learning_rate": 8.337730870712402e-05,
      "loss": 0.5435,
      "step": 757
    },
    {
      "epoch": 0.5,
      "grad_norm": 7.754158020019531,
      "learning_rate": 8.335532102022867e-05,
      "loss": 0.4945,
      "step": 758
    },
    {
      "epoch": 0.5006596306068601,
      "grad_norm": 1.1494914293289185,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.0692,
      "step": 759
    },
    {
      "epoch": 0.5013192612137203,
      "grad_norm": 8.00858211517334,
      "learning_rate": 8.3311345646438e-05,
      "loss": 0.3552,
      "step": 760
    },
    {
      "epoch": 0.5019788918205804,
      "grad_norm": 3.9932360649108887,
      "learning_rate": 8.328935795954266e-05,
      "loss": 0.2343,
      "step": 761
    },
    {
      "epoch": 0.5026385224274407,
      "grad_norm": 0.9175379872322083,
      "learning_rate": 8.326737027264733e-05,
      "loss": 0.1232,
      "step": 762
    },
    {
      "epoch": 0.5032981530343008,
      "grad_norm": 1.5915336608886719,
      "learning_rate": 8.324538258575199e-05,
      "loss": 0.1845,
      "step": 763
    },
    {
      "epoch": 0.503957783641161,
      "grad_norm": 1.2917506694793701,
      "learning_rate": 8.322339489885664e-05,
      "loss": 0.128,
      "step": 764
    },
    {
      "epoch": 0.5046174142480211,
      "grad_norm": 3.410529375076294,
      "learning_rate": 8.32014072119613e-05,
      "loss": 0.2335,
      "step": 765
    },
    {
      "epoch": 0.5052770448548812,
      "grad_norm": 1.582959771156311,
      "learning_rate": 8.317941952506597e-05,
      "loss": 0.175,
      "step": 766
    },
    {
      "epoch": 0.5059366754617414,
      "grad_norm": 2.9041779041290283,
      "learning_rate": 8.315743183817063e-05,
      "loss": 0.1806,
      "step": 767
    },
    {
      "epoch": 0.5065963060686016,
      "grad_norm": 2.046766996383667,
      "learning_rate": 8.313544415127528e-05,
      "loss": 0.2056,
      "step": 768
    },
    {
      "epoch": 0.5072559366754618,
      "grad_norm": 1.8777587413787842,
      "learning_rate": 8.311345646437994e-05,
      "loss": 0.1689,
      "step": 769
    },
    {
      "epoch": 0.5079155672823219,
      "grad_norm": 6.285266876220703,
      "learning_rate": 8.309146877748461e-05,
      "loss": 0.3846,
      "step": 770
    },
    {
      "epoch": 0.508575197889182,
      "grad_norm": 2.253115653991699,
      "learning_rate": 8.306948109058927e-05,
      "loss": 0.2251,
      "step": 771
    },
    {
      "epoch": 0.5092348284960422,
      "grad_norm": 3.0486576557159424,
      "learning_rate": 8.304749340369393e-05,
      "loss": 0.2096,
      "step": 772
    },
    {
      "epoch": 0.5098944591029023,
      "grad_norm": 1.384106159210205,
      "learning_rate": 8.30255057167986e-05,
      "loss": 0.1559,
      "step": 773
    },
    {
      "epoch": 0.5105540897097626,
      "grad_norm": 5.6139726638793945,
      "learning_rate": 8.300351802990327e-05,
      "loss": 0.4474,
      "step": 774
    },
    {
      "epoch": 0.5112137203166227,
      "grad_norm": 5.164905548095703,
      "learning_rate": 8.298153034300792e-05,
      "loss": 0.3982,
      "step": 775
    },
    {
      "epoch": 0.5118733509234829,
      "grad_norm": 2.5720717906951904,
      "learning_rate": 8.295954265611258e-05,
      "loss": 0.1732,
      "step": 776
    },
    {
      "epoch": 0.512532981530343,
      "grad_norm": 4.93599271774292,
      "learning_rate": 8.293755496921725e-05,
      "loss": 0.2552,
      "step": 777
    },
    {
      "epoch": 0.5131926121372031,
      "grad_norm": 0.9488899111747742,
      "learning_rate": 8.291556728232191e-05,
      "loss": 0.1077,
      "step": 778
    },
    {
      "epoch": 0.5138522427440633,
      "grad_norm": 3.5257153511047363,
      "learning_rate": 8.289357959542657e-05,
      "loss": 0.1795,
      "step": 779
    },
    {
      "epoch": 0.5145118733509235,
      "grad_norm": 0.5538566708564758,
      "learning_rate": 8.287159190853124e-05,
      "loss": 0.0683,
      "step": 780
    },
    {
      "epoch": 0.5151715039577837,
      "grad_norm": 1.5285289287567139,
      "learning_rate": 8.284960422163589e-05,
      "loss": 0.145,
      "step": 781
    },
    {
      "epoch": 0.5158311345646438,
      "grad_norm": 1.17994225025177,
      "learning_rate": 8.282761653474055e-05,
      "loss": 0.0744,
      "step": 782
    },
    {
      "epoch": 0.5164907651715039,
      "grad_norm": 18.709861755371094,
      "learning_rate": 8.28056288478452e-05,
      "loss": 1.5182,
      "step": 783
    },
    {
      "epoch": 0.5171503957783641,
      "grad_norm": 6.937044620513916,
      "learning_rate": 8.278364116094988e-05,
      "loss": 0.2421,
      "step": 784
    },
    {
      "epoch": 0.5178100263852242,
      "grad_norm": 1.4790068864822388,
      "learning_rate": 8.276165347405453e-05,
      "loss": 0.1088,
      "step": 785
    },
    {
      "epoch": 0.5184696569920845,
      "grad_norm": 1.493061900138855,
      "learning_rate": 8.273966578715919e-05,
      "loss": 0.0747,
      "step": 786
    },
    {
      "epoch": 0.5191292875989446,
      "grad_norm": 5.21042013168335,
      "learning_rate": 8.271767810026385e-05,
      "loss": 0.1987,
      "step": 787
    },
    {
      "epoch": 0.5197889182058048,
      "grad_norm": 0.6577551960945129,
      "learning_rate": 8.269569041336852e-05,
      "loss": 0.0298,
      "step": 788
    },
    {
      "epoch": 0.5204485488126649,
      "grad_norm": 8.040680885314941,
      "learning_rate": 8.267370272647318e-05,
      "loss": 0.5431,
      "step": 789
    },
    {
      "epoch": 0.521108179419525,
      "grad_norm": 12.845093727111816,
      "learning_rate": 8.265171503957783e-05,
      "loss": 0.4976,
      "step": 790
    },
    {
      "epoch": 0.5217678100263852,
      "grad_norm": 8.373961448669434,
      "learning_rate": 8.26297273526825e-05,
      "loss": 0.44,
      "step": 791
    },
    {
      "epoch": 0.5224274406332454,
      "grad_norm": 1.3464038372039795,
      "learning_rate": 8.260773966578716e-05,
      "loss": 0.0647,
      "step": 792
    },
    {
      "epoch": 0.5230870712401056,
      "grad_norm": 3.0908925533294678,
      "learning_rate": 8.258575197889182e-05,
      "loss": 0.0994,
      "step": 793
    },
    {
      "epoch": 0.5237467018469657,
      "grad_norm": 1.6547890901565552,
      "learning_rate": 8.256376429199649e-05,
      "loss": 0.1011,
      "step": 794
    },
    {
      "epoch": 0.5244063324538258,
      "grad_norm": 1.5940556526184082,
      "learning_rate": 8.254177660510114e-05,
      "loss": 0.0427,
      "step": 795
    },
    {
      "epoch": 0.525065963060686,
      "grad_norm": 0.47416016459465027,
      "learning_rate": 8.251978891820582e-05,
      "loss": 0.0433,
      "step": 796
    },
    {
      "epoch": 0.5257255936675461,
      "grad_norm": 13.011704444885254,
      "learning_rate": 8.249780123131047e-05,
      "loss": 1.5686,
      "step": 797
    },
    {
      "epoch": 0.5263852242744064,
      "grad_norm": 0.8302112817764282,
      "learning_rate": 8.247581354441514e-05,
      "loss": 0.0568,
      "step": 798
    },
    {
      "epoch": 0.5270448548812665,
      "grad_norm": 0.5757842063903809,
      "learning_rate": 8.24538258575198e-05,
      "loss": 0.0585,
      "step": 799
    },
    {
      "epoch": 0.5277044854881267,
      "grad_norm": 8.629563331604004,
      "learning_rate": 8.243183817062446e-05,
      "loss": 0.4018,
      "step": 800
    },
    {
      "epoch": 0.5283641160949868,
      "grad_norm": 1.0420278310775757,
      "learning_rate": 8.240985048372911e-05,
      "loss": 0.0719,
      "step": 801
    },
    {
      "epoch": 0.5290237467018469,
      "grad_norm": 1.4710733890533447,
      "learning_rate": 8.238786279683378e-05,
      "loss": 0.1058,
      "step": 802
    },
    {
      "epoch": 0.5296833773087071,
      "grad_norm": 8.229145050048828,
      "learning_rate": 8.236587510993844e-05,
      "loss": 0.5722,
      "step": 803
    },
    {
      "epoch": 0.5303430079155673,
      "grad_norm": 3.466040849685669,
      "learning_rate": 8.23438874230431e-05,
      "loss": 0.1719,
      "step": 804
    },
    {
      "epoch": 0.5310026385224275,
      "grad_norm": 1.3601408004760742,
      "learning_rate": 8.232189973614775e-05,
      "loss": 0.0975,
      "step": 805
    },
    {
      "epoch": 0.5316622691292876,
      "grad_norm": 1.2258623838424683,
      "learning_rate": 8.229991204925243e-05,
      "loss": 0.1137,
      "step": 806
    },
    {
      "epoch": 0.5323218997361477,
      "grad_norm": 4.365675926208496,
      "learning_rate": 8.227792436235708e-05,
      "loss": 0.2301,
      "step": 807
    },
    {
      "epoch": 0.5329815303430079,
      "grad_norm": 3.282942056655884,
      "learning_rate": 8.225593667546174e-05,
      "loss": 0.2045,
      "step": 808
    },
    {
      "epoch": 0.533641160949868,
      "grad_norm": 11.609478950500488,
      "learning_rate": 8.223394898856641e-05,
      "loss": 0.6779,
      "step": 809
    },
    {
      "epoch": 0.5343007915567283,
      "grad_norm": 11.387365341186523,
      "learning_rate": 8.221196130167107e-05,
      "loss": 1.3136,
      "step": 810
    },
    {
      "epoch": 0.5349604221635884,
      "grad_norm": 1.2844181060791016,
      "learning_rate": 8.218997361477572e-05,
      "loss": 0.0904,
      "step": 811
    },
    {
      "epoch": 0.5356200527704486,
      "grad_norm": 3.0390374660491943,
      "learning_rate": 8.216798592788038e-05,
      "loss": 0.1906,
      "step": 812
    },
    {
      "epoch": 0.5362796833773087,
      "grad_norm": 6.880462169647217,
      "learning_rate": 8.214599824098505e-05,
      "loss": 0.3057,
      "step": 813
    },
    {
      "epoch": 0.5369393139841688,
      "grad_norm": 14.775864601135254,
      "learning_rate": 8.212401055408971e-05,
      "loss": 1.2876,
      "step": 814
    },
    {
      "epoch": 0.537598944591029,
      "grad_norm": 3.0483670234680176,
      "learning_rate": 8.210202286719438e-05,
      "loss": 0.1814,
      "step": 815
    },
    {
      "epoch": 0.5382585751978892,
      "grad_norm": 15.551091194152832,
      "learning_rate": 8.208003518029904e-05,
      "loss": 1.1173,
      "step": 816
    },
    {
      "epoch": 0.5389182058047494,
      "grad_norm": 2.496610403060913,
      "learning_rate": 8.20580474934037e-05,
      "loss": 0.1645,
      "step": 817
    },
    {
      "epoch": 0.5395778364116095,
      "grad_norm": 1.428332805633545,
      "learning_rate": 8.203605980650836e-05,
      "loss": 0.0787,
      "step": 818
    },
    {
      "epoch": 0.5402374670184696,
      "grad_norm": 1.1645410060882568,
      "learning_rate": 8.201407211961302e-05,
      "loss": 0.1264,
      "step": 819
    },
    {
      "epoch": 0.5408970976253298,
      "grad_norm": 10.182646751403809,
      "learning_rate": 8.199208443271769e-05,
      "loss": 0.525,
      "step": 820
    },
    {
      "epoch": 0.5415567282321899,
      "grad_norm": 21.445581436157227,
      "learning_rate": 8.197009674582235e-05,
      "loss": 1.6631,
      "step": 821
    },
    {
      "epoch": 0.5422163588390502,
      "grad_norm": 1.01560640335083,
      "learning_rate": 8.1948109058927e-05,
      "loss": 0.0962,
      "step": 822
    },
    {
      "epoch": 0.5428759894459103,
      "grad_norm": 1.829500436782837,
      "learning_rate": 8.192612137203166e-05,
      "loss": 0.1249,
      "step": 823
    },
    {
      "epoch": 0.5435356200527705,
      "grad_norm": 9.010458946228027,
      "learning_rate": 8.190413368513633e-05,
      "loss": 0.6098,
      "step": 824
    },
    {
      "epoch": 0.5441952506596306,
      "grad_norm": 3.2889444828033447,
      "learning_rate": 8.188214599824099e-05,
      "loss": 0.1335,
      "step": 825
    },
    {
      "epoch": 0.5448548812664907,
      "grad_norm": 17.699501037597656,
      "learning_rate": 8.186015831134565e-05,
      "loss": 1.4261,
      "step": 826
    },
    {
      "epoch": 0.5455145118733509,
      "grad_norm": 1.4797042608261108,
      "learning_rate": 8.183817062445032e-05,
      "loss": 0.0949,
      "step": 827
    },
    {
      "epoch": 0.5461741424802111,
      "grad_norm": 11.64783763885498,
      "learning_rate": 8.181618293755497e-05,
      "loss": 1.707,
      "step": 828
    },
    {
      "epoch": 0.5468337730870713,
      "grad_norm": 0.6900449991226196,
      "learning_rate": 8.179419525065963e-05,
      "loss": 0.0532,
      "step": 829
    },
    {
      "epoch": 0.5474934036939314,
      "grad_norm": 11.382604598999023,
      "learning_rate": 8.177220756376429e-05,
      "loss": 1.0562,
      "step": 830
    },
    {
      "epoch": 0.5481530343007915,
      "grad_norm": 10.58116340637207,
      "learning_rate": 8.175021987686896e-05,
      "loss": 1.269,
      "step": 831
    },
    {
      "epoch": 0.5488126649076517,
      "grad_norm": 3.746049642562866,
      "learning_rate": 8.172823218997362e-05,
      "loss": 0.1658,
      "step": 832
    },
    {
      "epoch": 0.5494722955145118,
      "grad_norm": 1.335852861404419,
      "learning_rate": 8.170624450307827e-05,
      "loss": 0.0681,
      "step": 833
    },
    {
      "epoch": 0.5501319261213721,
      "grad_norm": 0.9521579742431641,
      "learning_rate": 8.168425681618294e-05,
      "loss": 0.0753,
      "step": 834
    },
    {
      "epoch": 0.5507915567282322,
      "grad_norm": 1.3966480493545532,
      "learning_rate": 8.16622691292876e-05,
      "loss": 0.0686,
      "step": 835
    },
    {
      "epoch": 0.5514511873350924,
      "grad_norm": 6.643986225128174,
      "learning_rate": 8.164028144239226e-05,
      "loss": 0.6037,
      "step": 836
    },
    {
      "epoch": 0.5521108179419525,
      "grad_norm": 15.072859764099121,
      "learning_rate": 8.161829375549693e-05,
      "loss": 1.6164,
      "step": 837
    },
    {
      "epoch": 0.5527704485488126,
      "grad_norm": 7.6694817543029785,
      "learning_rate": 8.159630606860158e-05,
      "loss": 0.2708,
      "step": 838
    },
    {
      "epoch": 0.5534300791556728,
      "grad_norm": 8.183963775634766,
      "learning_rate": 8.157431838170625e-05,
      "loss": 0.6714,
      "step": 839
    },
    {
      "epoch": 0.554089709762533,
      "grad_norm": 1.0781593322753906,
      "learning_rate": 8.155233069481091e-05,
      "loss": 0.1038,
      "step": 840
    },
    {
      "epoch": 0.5547493403693932,
      "grad_norm": 8.880059242248535,
      "learning_rate": 8.153034300791557e-05,
      "loss": 0.6287,
      "step": 841
    },
    {
      "epoch": 0.5554089709762533,
      "grad_norm": 1.1260709762573242,
      "learning_rate": 8.150835532102024e-05,
      "loss": 0.1207,
      "step": 842
    },
    {
      "epoch": 0.5560686015831134,
      "grad_norm": 2.6418700218200684,
      "learning_rate": 8.14863676341249e-05,
      "loss": 0.1778,
      "step": 843
    },
    {
      "epoch": 0.5567282321899736,
      "grad_norm": 1.7103650569915771,
      "learning_rate": 8.146437994722955e-05,
      "loss": 0.1209,
      "step": 844
    },
    {
      "epoch": 0.5573878627968337,
      "grad_norm": 3.587876558303833,
      "learning_rate": 8.144239226033422e-05,
      "loss": 0.2333,
      "step": 845
    },
    {
      "epoch": 0.558047493403694,
      "grad_norm": 6.15562105178833,
      "learning_rate": 8.142040457343888e-05,
      "loss": 0.448,
      "step": 846
    },
    {
      "epoch": 0.5587071240105541,
      "grad_norm": 10.017584800720215,
      "learning_rate": 8.139841688654354e-05,
      "loss": 0.9718,
      "step": 847
    },
    {
      "epoch": 0.5593667546174143,
      "grad_norm": 4.338993072509766,
      "learning_rate": 8.13764291996482e-05,
      "loss": 0.3933,
      "step": 848
    },
    {
      "epoch": 0.5600263852242744,
      "grad_norm": 2.7830042839050293,
      "learning_rate": 8.135444151275287e-05,
      "loss": 0.2001,
      "step": 849
    },
    {
      "epoch": 0.5606860158311345,
      "grad_norm": 2.846414804458618,
      "learning_rate": 8.133245382585752e-05,
      "loss": 0.2388,
      "step": 850
    },
    {
      "epoch": 0.5613456464379947,
      "grad_norm": 5.379751682281494,
      "learning_rate": 8.131046613896218e-05,
      "loss": 0.4963,
      "step": 851
    },
    {
      "epoch": 0.5620052770448549,
      "grad_norm": 2.3737001419067383,
      "learning_rate": 8.128847845206685e-05,
      "loss": 0.1967,
      "step": 852
    },
    {
      "epoch": 0.5626649076517151,
      "grad_norm": 1.8901705741882324,
      "learning_rate": 8.126649076517151e-05,
      "loss": 0.1933,
      "step": 853
    },
    {
      "epoch": 0.5633245382585752,
      "grad_norm": 2.280057668685913,
      "learning_rate": 8.124450307827616e-05,
      "loss": 0.2496,
      "step": 854
    },
    {
      "epoch": 0.5639841688654353,
      "grad_norm": 13.76511001586914,
      "learning_rate": 8.122251539138082e-05,
      "loss": 0.8798,
      "step": 855
    },
    {
      "epoch": 0.5646437994722955,
      "grad_norm": 3.3847827911376953,
      "learning_rate": 8.120052770448549e-05,
      "loss": 0.2272,
      "step": 856
    },
    {
      "epoch": 0.5653034300791556,
      "grad_norm": 6.6845011711120605,
      "learning_rate": 8.117854001759015e-05,
      "loss": 0.7103,
      "step": 857
    },
    {
      "epoch": 0.5659630606860159,
      "grad_norm": 0.9186415672302246,
      "learning_rate": 8.115655233069482e-05,
      "loss": 0.1279,
      "step": 858
    },
    {
      "epoch": 0.566622691292876,
      "grad_norm": 3.5516486167907715,
      "learning_rate": 8.113456464379948e-05,
      "loss": 0.3003,
      "step": 859
    },
    {
      "epoch": 0.5672823218997362,
      "grad_norm": 1.8111193180084229,
      "learning_rate": 8.111257695690415e-05,
      "loss": 0.1808,
      "step": 860
    },
    {
      "epoch": 0.5679419525065963,
      "grad_norm": 1.703894019126892,
      "learning_rate": 8.10905892700088e-05,
      "loss": 0.2071,
      "step": 861
    },
    {
      "epoch": 0.5686015831134564,
      "grad_norm": 2.405061960220337,
      "learning_rate": 8.106860158311346e-05,
      "loss": 0.1871,
      "step": 862
    },
    {
      "epoch": 0.5692612137203166,
      "grad_norm": 2.8390278816223145,
      "learning_rate": 8.104661389621813e-05,
      "loss": 0.2246,
      "step": 863
    },
    {
      "epoch": 0.5699208443271768,
      "grad_norm": 3.0829575061798096,
      "learning_rate": 8.102462620932279e-05,
      "loss": 0.2063,
      "step": 864
    },
    {
      "epoch": 0.570580474934037,
      "grad_norm": 2.7274324893951416,
      "learning_rate": 8.100263852242744e-05,
      "loss": 0.2529,
      "step": 865
    },
    {
      "epoch": 0.5712401055408971,
      "grad_norm": 2.440918207168579,
      "learning_rate": 8.09806508355321e-05,
      "loss": 0.1685,
      "step": 866
    },
    {
      "epoch": 0.5718997361477572,
      "grad_norm": 1.7544569969177246,
      "learning_rate": 8.095866314863677e-05,
      "loss": 0.1462,
      "step": 867
    },
    {
      "epoch": 0.5725593667546174,
      "grad_norm": 1.0617033243179321,
      "learning_rate": 8.093667546174143e-05,
      "loss": 0.1486,
      "step": 868
    },
    {
      "epoch": 0.5732189973614775,
      "grad_norm": 4.032690048217773,
      "learning_rate": 8.091468777484609e-05,
      "loss": 0.2825,
      "step": 869
    },
    {
      "epoch": 0.5738786279683378,
      "grad_norm": 1.1907655000686646,
      "learning_rate": 8.089270008795076e-05,
      "loss": 0.1196,
      "step": 870
    },
    {
      "epoch": 0.5745382585751979,
      "grad_norm": 1.4916976690292358,
      "learning_rate": 8.087071240105541e-05,
      "loss": 0.1241,
      "step": 871
    },
    {
      "epoch": 0.575197889182058,
      "grad_norm": 4.855848789215088,
      "learning_rate": 8.084872471416007e-05,
      "loss": 0.3089,
      "step": 872
    },
    {
      "epoch": 0.5758575197889182,
      "grad_norm": 10.963858604431152,
      "learning_rate": 8.082673702726473e-05,
      "loss": 1.0443,
      "step": 873
    },
    {
      "epoch": 0.5765171503957783,
      "grad_norm": 3.3197383880615234,
      "learning_rate": 8.08047493403694e-05,
      "loss": 0.2236,
      "step": 874
    },
    {
      "epoch": 0.5771767810026385,
      "grad_norm": 10.820455551147461,
      "learning_rate": 8.078276165347406e-05,
      "loss": 0.7766,
      "step": 875
    },
    {
      "epoch": 0.5778364116094987,
      "grad_norm": 1.363017201423645,
      "learning_rate": 8.076077396657871e-05,
      "loss": 0.1471,
      "step": 876
    },
    {
      "epoch": 0.5784960422163589,
      "grad_norm": 4.7707109451293945,
      "learning_rate": 8.073878627968337e-05,
      "loss": 0.2688,
      "step": 877
    },
    {
      "epoch": 0.579155672823219,
      "grad_norm": 2.9365077018737793,
      "learning_rate": 8.071679859278804e-05,
      "loss": 0.2315,
      "step": 878
    },
    {
      "epoch": 0.5798153034300791,
      "grad_norm": 5.155881404876709,
      "learning_rate": 8.069481090589271e-05,
      "loss": 0.3975,
      "step": 879
    },
    {
      "epoch": 0.5804749340369393,
      "grad_norm": 6.583662986755371,
      "learning_rate": 8.067282321899737e-05,
      "loss": 0.4597,
      "step": 880
    },
    {
      "epoch": 0.5811345646437994,
      "grad_norm": 2.2623348236083984,
      "learning_rate": 8.065083553210204e-05,
      "loss": 0.2928,
      "step": 881
    },
    {
      "epoch": 0.5817941952506597,
      "grad_norm": 7.089635848999023,
      "learning_rate": 8.06288478452067e-05,
      "loss": 0.4555,
      "step": 882
    },
    {
      "epoch": 0.5824538258575198,
      "grad_norm": 6.526857376098633,
      "learning_rate": 8.060686015831135e-05,
      "loss": 0.881,
      "step": 883
    },
    {
      "epoch": 0.58311345646438,
      "grad_norm": 1.6813597679138184,
      "learning_rate": 8.058487247141601e-05,
      "loss": 0.1862,
      "step": 884
    },
    {
      "epoch": 0.5837730870712401,
      "grad_norm": 4.6755146980285645,
      "learning_rate": 8.056288478452068e-05,
      "loss": 0.3931,
      "step": 885
    },
    {
      "epoch": 0.5844327176781002,
      "grad_norm": 6.419981002807617,
      "learning_rate": 8.054089709762534e-05,
      "loss": 0.4934,
      "step": 886
    },
    {
      "epoch": 0.5850923482849604,
      "grad_norm": 4.037771701812744,
      "learning_rate": 8.051890941073e-05,
      "loss": 0.367,
      "step": 887
    },
    {
      "epoch": 0.5857519788918206,
      "grad_norm": 3.436342477798462,
      "learning_rate": 8.049692172383466e-05,
      "loss": 0.3835,
      "step": 888
    },
    {
      "epoch": 0.5864116094986808,
      "grad_norm": 1.9861974716186523,
      "learning_rate": 8.047493403693932e-05,
      "loss": 0.2393,
      "step": 889
    },
    {
      "epoch": 0.5870712401055409,
      "grad_norm": 5.349773406982422,
      "learning_rate": 8.045294635004398e-05,
      "loss": 0.3729,
      "step": 890
    },
    {
      "epoch": 0.587730870712401,
      "grad_norm": 6.713428497314453,
      "learning_rate": 8.043095866314863e-05,
      "loss": 0.4789,
      "step": 891
    },
    {
      "epoch": 0.5883905013192612,
      "grad_norm": 5.769681930541992,
      "learning_rate": 8.04089709762533e-05,
      "loss": 0.3989,
      "step": 892
    },
    {
      "epoch": 0.5890501319261213,
      "grad_norm": 2.173945665359497,
      "learning_rate": 8.038698328935796e-05,
      "loss": 0.2033,
      "step": 893
    },
    {
      "epoch": 0.5897097625329816,
      "grad_norm": 4.096983909606934,
      "learning_rate": 8.036499560246262e-05,
      "loss": 0.2443,
      "step": 894
    },
    {
      "epoch": 0.5903693931398417,
      "grad_norm": 3.0052592754364014,
      "learning_rate": 8.034300791556728e-05,
      "loss": 0.201,
      "step": 895
    },
    {
      "epoch": 0.5910290237467019,
      "grad_norm": 0.6351921558380127,
      "learning_rate": 8.032102022867195e-05,
      "loss": 0.1096,
      "step": 896
    },
    {
      "epoch": 0.591688654353562,
      "grad_norm": 3.0675697326660156,
      "learning_rate": 8.02990325417766e-05,
      "loss": 0.2016,
      "step": 897
    },
    {
      "epoch": 0.5923482849604221,
      "grad_norm": 2.842473030090332,
      "learning_rate": 8.027704485488126e-05,
      "loss": 0.1149,
      "step": 898
    },
    {
      "epoch": 0.5930079155672823,
      "grad_norm": 7.444634914398193,
      "learning_rate": 8.025505716798593e-05,
      "loss": 0.5408,
      "step": 899
    },
    {
      "epoch": 0.5936675461741425,
      "grad_norm": 1.9643173217773438,
      "learning_rate": 8.023306948109059e-05,
      "loss": 0.1155,
      "step": 900
    },
    {
      "epoch": 0.5943271767810027,
      "grad_norm": 3.2139835357666016,
      "learning_rate": 8.021108179419526e-05,
      "loss": 0.188,
      "step": 901
    },
    {
      "epoch": 0.5949868073878628,
      "grad_norm": 1.359769582748413,
      "learning_rate": 8.018909410729992e-05,
      "loss": 0.0696,
      "step": 902
    },
    {
      "epoch": 0.5956464379947229,
      "grad_norm": 9.827093124389648,
      "learning_rate": 8.016710642040459e-05,
      "loss": 1.2749,
      "step": 903
    },
    {
      "epoch": 0.5963060686015831,
      "grad_norm": 9.357587814331055,
      "learning_rate": 8.014511873350924e-05,
      "loss": 0.6616,
      "step": 904
    },
    {
      "epoch": 0.5969656992084432,
      "grad_norm": 9.828413009643555,
      "learning_rate": 8.01231310466139e-05,
      "loss": 0.6054,
      "step": 905
    },
    {
      "epoch": 0.5976253298153035,
      "grad_norm": 11.59744644165039,
      "learning_rate": 8.010114335971857e-05,
      "loss": 1.44,
      "step": 906
    },
    {
      "epoch": 0.5982849604221636,
      "grad_norm": 2.791154384613037,
      "learning_rate": 8.007915567282323e-05,
      "loss": 0.1226,
      "step": 907
    },
    {
      "epoch": 0.5989445910290238,
      "grad_norm": 0.6947816610336304,
      "learning_rate": 8.005716798592788e-05,
      "loss": 0.067,
      "step": 908
    },
    {
      "epoch": 0.5996042216358839,
      "grad_norm": 1.6706640720367432,
      "learning_rate": 8.003518029903254e-05,
      "loss": 0.1314,
      "step": 909
    },
    {
      "epoch": 0.600263852242744,
      "grad_norm": 3.162368059158325,
      "learning_rate": 8.001319261213721e-05,
      "loss": 0.1834,
      "step": 910
    },
    {
      "epoch": 0.6009234828496042,
      "grad_norm": 0.7917658686637878,
      "learning_rate": 7.999120492524187e-05,
      "loss": 0.0949,
      "step": 911
    },
    {
      "epoch": 0.6015831134564644,
      "grad_norm": 8.68026065826416,
      "learning_rate": 7.996921723834653e-05,
      "loss": 0.4913,
      "step": 912
    },
    {
      "epoch": 0.6022427440633246,
      "grad_norm": 5.706587791442871,
      "learning_rate": 7.994722955145118e-05,
      "loss": 0.4152,
      "step": 913
    },
    {
      "epoch": 0.6029023746701847,
      "grad_norm": 1.5762215852737427,
      "learning_rate": 7.992524186455585e-05,
      "loss": 0.1588,
      "step": 914
    },
    {
      "epoch": 0.6035620052770448,
      "grad_norm": 2.1515586376190186,
      "learning_rate": 7.990325417766051e-05,
      "loss": 0.167,
      "step": 915
    },
    {
      "epoch": 0.604221635883905,
      "grad_norm": 2.5326318740844727,
      "learning_rate": 7.988126649076517e-05,
      "loss": 0.1882,
      "step": 916
    },
    {
      "epoch": 0.6048812664907651,
      "grad_norm": 5.20916223526001,
      "learning_rate": 7.985927880386984e-05,
      "loss": 0.5459,
      "step": 917
    },
    {
      "epoch": 0.6055408970976254,
      "grad_norm": 4.455801486968994,
      "learning_rate": 7.98372911169745e-05,
      "loss": 0.3506,
      "step": 918
    },
    {
      "epoch": 0.6062005277044855,
      "grad_norm": 3.090127944946289,
      "learning_rate": 7.981530343007915e-05,
      "loss": 0.2734,
      "step": 919
    },
    {
      "epoch": 0.6068601583113457,
      "grad_norm": 6.681058883666992,
      "learning_rate": 7.979331574318382e-05,
      "loss": 0.4279,
      "step": 920
    },
    {
      "epoch": 0.6075197889182058,
      "grad_norm": 1.0323739051818848,
      "learning_rate": 7.977132805628848e-05,
      "loss": 0.1477,
      "step": 921
    },
    {
      "epoch": 0.6081794195250659,
      "grad_norm": 1.4753963947296143,
      "learning_rate": 7.974934036939315e-05,
      "loss": 0.1427,
      "step": 922
    },
    {
      "epoch": 0.6088390501319261,
      "grad_norm": 2.5275847911834717,
      "learning_rate": 7.972735268249781e-05,
      "loss": 0.1886,
      "step": 923
    },
    {
      "epoch": 0.6094986807387863,
      "grad_norm": 2.8785853385925293,
      "learning_rate": 7.970536499560248e-05,
      "loss": 0.2256,
      "step": 924
    },
    {
      "epoch": 0.6101583113456465,
      "grad_norm": 4.018117427825928,
      "learning_rate": 7.968337730870713e-05,
      "loss": 0.3009,
      "step": 925
    },
    {
      "epoch": 0.6108179419525066,
      "grad_norm": 4.539083480834961,
      "learning_rate": 7.966138962181179e-05,
      "loss": 0.18,
      "step": 926
    },
    {
      "epoch": 0.6114775725593667,
      "grad_norm": 1.0869468450546265,
      "learning_rate": 7.963940193491645e-05,
      "loss": 0.1394,
      "step": 927
    },
    {
      "epoch": 0.6121372031662269,
      "grad_norm": 8.80084228515625,
      "learning_rate": 7.961741424802112e-05,
      "loss": 0.6948,
      "step": 928
    },
    {
      "epoch": 0.612796833773087,
      "grad_norm": 2.256512403488159,
      "learning_rate": 7.959542656112578e-05,
      "loss": 0.1297,
      "step": 929
    },
    {
      "epoch": 0.6134564643799473,
      "grad_norm": 1.5890642404556274,
      "learning_rate": 7.957343887423043e-05,
      "loss": 0.1015,
      "step": 930
    },
    {
      "epoch": 0.6141160949868074,
      "grad_norm": 1.316535234451294,
      "learning_rate": 7.955145118733509e-05,
      "loss": 0.1157,
      "step": 931
    },
    {
      "epoch": 0.6147757255936676,
      "grad_norm": 0.9953067302703857,
      "learning_rate": 7.952946350043976e-05,
      "loss": 0.0907,
      "step": 932
    },
    {
      "epoch": 0.6154353562005277,
      "grad_norm": 14.624872207641602,
      "learning_rate": 7.950747581354442e-05,
      "loss": 0.9191,
      "step": 933
    },
    {
      "epoch": 0.6160949868073878,
      "grad_norm": 1.2131636142730713,
      "learning_rate": 7.948548812664907e-05,
      "loss": 0.0648,
      "step": 934
    },
    {
      "epoch": 0.616754617414248,
      "grad_norm": 9.925443649291992,
      "learning_rate": 7.946350043975375e-05,
      "loss": 0.6189,
      "step": 935
    },
    {
      "epoch": 0.6174142480211082,
      "grad_norm": 0.9358710646629333,
      "learning_rate": 7.94415127528584e-05,
      "loss": 0.04,
      "step": 936
    },
    {
      "epoch": 0.6180738786279684,
      "grad_norm": 15.5637788772583,
      "learning_rate": 7.941952506596306e-05,
      "loss": 0.7879,
      "step": 937
    },
    {
      "epoch": 0.6187335092348285,
      "grad_norm": 1.7543128728866577,
      "learning_rate": 7.939753737906772e-05,
      "loss": 0.074,
      "step": 938
    },
    {
      "epoch": 0.6193931398416886,
      "grad_norm": 0.7455615997314453,
      "learning_rate": 7.937554969217239e-05,
      "loss": 0.0456,
      "step": 939
    },
    {
      "epoch": 0.6200527704485488,
      "grad_norm": 18.942581176757812,
      "learning_rate": 7.935356200527704e-05,
      "loss": 1.0371,
      "step": 940
    },
    {
      "epoch": 0.6207124010554089,
      "grad_norm": 19.99357795715332,
      "learning_rate": 7.93315743183817e-05,
      "loss": 0.8342,
      "step": 941
    },
    {
      "epoch": 0.6213720316622692,
      "grad_norm": 10.188753128051758,
      "learning_rate": 7.930958663148637e-05,
      "loss": 0.6472,
      "step": 942
    },
    {
      "epoch": 0.6220316622691293,
      "grad_norm": 12.600068092346191,
      "learning_rate": 7.928759894459103e-05,
      "loss": 0.8528,
      "step": 943
    },
    {
      "epoch": 0.6226912928759895,
      "grad_norm": 15.279541969299316,
      "learning_rate": 7.92656112576957e-05,
      "loss": 0.7909,
      "step": 944
    },
    {
      "epoch": 0.6233509234828496,
      "grad_norm": 12.661685943603516,
      "learning_rate": 7.924362357080036e-05,
      "loss": 0.5207,
      "step": 945
    },
    {
      "epoch": 0.6240105540897097,
      "grad_norm": 21.306596755981445,
      "learning_rate": 7.922163588390503e-05,
      "loss": 1.5871,
      "step": 946
    },
    {
      "epoch": 0.6246701846965699,
      "grad_norm": 0.5335755944252014,
      "learning_rate": 7.919964819700968e-05,
      "loss": 0.0556,
      "step": 947
    },
    {
      "epoch": 0.6253298153034301,
      "grad_norm": 2.8528363704681396,
      "learning_rate": 7.917766051011434e-05,
      "loss": 0.1131,
      "step": 948
    },
    {
      "epoch": 0.6259894459102903,
      "grad_norm": 16.75007438659668,
      "learning_rate": 7.9155672823219e-05,
      "loss": 2.2001,
      "step": 949
    },
    {
      "epoch": 0.6266490765171504,
      "grad_norm": 1.1333487033843994,
      "learning_rate": 7.913368513632367e-05,
      "loss": 0.1116,
      "step": 950
    },
    {
      "epoch": 0.6273087071240105,
      "grad_norm": 9.509688377380371,
      "learning_rate": 7.911169744942832e-05,
      "loss": 0.4198,
      "step": 951
    },
    {
      "epoch": 0.6279683377308707,
      "grad_norm": 0.796479344367981,
      "learning_rate": 7.908970976253298e-05,
      "loss": 0.0914,
      "step": 952
    },
    {
      "epoch": 0.6286279683377308,
      "grad_norm": 15.141663551330566,
      "learning_rate": 7.906772207563765e-05,
      "loss": 0.9725,
      "step": 953
    },
    {
      "epoch": 0.6292875989445911,
      "grad_norm": 4.277050495147705,
      "learning_rate": 7.904573438874231e-05,
      "loss": 0.3571,
      "step": 954
    },
    {
      "epoch": 0.6299472295514512,
      "grad_norm": 6.626742839813232,
      "learning_rate": 7.902374670184697e-05,
      "loss": 0.7528,
      "step": 955
    },
    {
      "epoch": 0.6306068601583114,
      "grad_norm": 4.68829345703125,
      "learning_rate": 7.900175901495162e-05,
      "loss": 0.3578,
      "step": 956
    },
    {
      "epoch": 0.6312664907651715,
      "grad_norm": 1.3070074319839478,
      "learning_rate": 7.89797713280563e-05,
      "loss": 0.1522,
      "step": 957
    },
    {
      "epoch": 0.6319261213720316,
      "grad_norm": 2.034397840499878,
      "learning_rate": 7.895778364116095e-05,
      "loss": 0.2446,
      "step": 958
    },
    {
      "epoch": 0.6325857519788918,
      "grad_norm": 5.413354396820068,
      "learning_rate": 7.893579595426561e-05,
      "loss": 0.3957,
      "step": 959
    },
    {
      "epoch": 0.633245382585752,
      "grad_norm": 18.802087783813477,
      "learning_rate": 7.891380826737028e-05,
      "loss": 0.84,
      "step": 960
    },
    {
      "epoch": 0.6339050131926122,
      "grad_norm": 5.001225471496582,
      "learning_rate": 7.889182058047494e-05,
      "loss": 0.3546,
      "step": 961
    },
    {
      "epoch": 0.6345646437994723,
      "grad_norm": 3.9555575847625732,
      "learning_rate": 7.886983289357959e-05,
      "loss": 0.326,
      "step": 962
    },
    {
      "epoch": 0.6352242744063324,
      "grad_norm": 1.1314188241958618,
      "learning_rate": 7.884784520668426e-05,
      "loss": 0.1045,
      "step": 963
    },
    {
      "epoch": 0.6358839050131926,
      "grad_norm": 1.334878921508789,
      "learning_rate": 7.882585751978892e-05,
      "loss": 0.2095,
      "step": 964
    },
    {
      "epoch": 0.6365435356200527,
      "grad_norm": 1.1509042978286743,
      "learning_rate": 7.880386983289359e-05,
      "loss": 0.1087,
      "step": 965
    },
    {
      "epoch": 0.637203166226913,
      "grad_norm": 5.682953834533691,
      "learning_rate": 7.878188214599825e-05,
      "loss": 0.6239,
      "step": 966
    },
    {
      "epoch": 0.6378627968337731,
      "grad_norm": 2.324690341949463,
      "learning_rate": 7.87598944591029e-05,
      "loss": 0.1768,
      "step": 967
    },
    {
      "epoch": 0.6385224274406333,
      "grad_norm": 3.1864209175109863,
      "learning_rate": 7.873790677220757e-05,
      "loss": 0.2334,
      "step": 968
    },
    {
      "epoch": 0.6391820580474934,
      "grad_norm": 1.3695348501205444,
      "learning_rate": 7.871591908531223e-05,
      "loss": 0.1434,
      "step": 969
    },
    {
      "epoch": 0.6398416886543535,
      "grad_norm": 5.425814151763916,
      "learning_rate": 7.869393139841689e-05,
      "loss": 0.3275,
      "step": 970
    },
    {
      "epoch": 0.6405013192612137,
      "grad_norm": 1.1858106851577759,
      "learning_rate": 7.867194371152156e-05,
      "loss": 0.1033,
      "step": 971
    },
    {
      "epoch": 0.6411609498680739,
      "grad_norm": 2.5600547790527344,
      "learning_rate": 7.864995602462622e-05,
      "loss": 0.156,
      "step": 972
    },
    {
      "epoch": 0.6418205804749341,
      "grad_norm": 3.952651023864746,
      "learning_rate": 7.862796833773087e-05,
      "loss": 0.2089,
      "step": 973
    },
    {
      "epoch": 0.6424802110817942,
      "grad_norm": 4.926373481750488,
      "learning_rate": 7.860598065083553e-05,
      "loss": 0.4624,
      "step": 974
    },
    {
      "epoch": 0.6431398416886543,
      "grad_norm": 4.689053058624268,
      "learning_rate": 7.85839929639402e-05,
      "loss": 0.2397,
      "step": 975
    },
    {
      "epoch": 0.6437994722955145,
      "grad_norm": 3.5382370948791504,
      "learning_rate": 7.856200527704486e-05,
      "loss": 0.2259,
      "step": 976
    },
    {
      "epoch": 0.6444591029023746,
      "grad_norm": 6.002260208129883,
      "learning_rate": 7.854001759014951e-05,
      "loss": 0.2546,
      "step": 977
    },
    {
      "epoch": 0.6451187335092349,
      "grad_norm": 2.735797643661499,
      "learning_rate": 7.851802990325419e-05,
      "loss": 0.1781,
      "step": 978
    },
    {
      "epoch": 0.645778364116095,
      "grad_norm": 1.293770670890808,
      "learning_rate": 7.849604221635884e-05,
      "loss": 0.1101,
      "step": 979
    },
    {
      "epoch": 0.6464379947229552,
      "grad_norm": 9.95289134979248,
      "learning_rate": 7.84740545294635e-05,
      "loss": 0.6975,
      "step": 980
    },
    {
      "epoch": 0.6470976253298153,
      "grad_norm": 1.5511865615844727,
      "learning_rate": 7.845206684256816e-05,
      "loss": 0.1025,
      "step": 981
    },
    {
      "epoch": 0.6477572559366754,
      "grad_norm": 8.106106758117676,
      "learning_rate": 7.843007915567283e-05,
      "loss": 1.0745,
      "step": 982
    },
    {
      "epoch": 0.6484168865435356,
      "grad_norm": 3.236959457397461,
      "learning_rate": 7.840809146877748e-05,
      "loss": 0.2419,
      "step": 983
    },
    {
      "epoch": 0.6490765171503958,
      "grad_norm": 1.950374722480774,
      "learning_rate": 7.838610378188215e-05,
      "loss": 0.1838,
      "step": 984
    },
    {
      "epoch": 0.649736147757256,
      "grad_norm": 2.6735873222351074,
      "learning_rate": 7.836411609498681e-05,
      "loss": 0.1961,
      "step": 985
    },
    {
      "epoch": 0.6503957783641161,
      "grad_norm": 4.606313228607178,
      "learning_rate": 7.834212840809148e-05,
      "loss": 0.2347,
      "step": 986
    },
    {
      "epoch": 0.6510554089709762,
      "grad_norm": 5.901972770690918,
      "learning_rate": 7.832014072119614e-05,
      "loss": 0.7527,
      "step": 987
    },
    {
      "epoch": 0.6517150395778364,
      "grad_norm": 7.111227989196777,
      "learning_rate": 7.82981530343008e-05,
      "loss": 0.4923,
      "step": 988
    },
    {
      "epoch": 0.6523746701846965,
      "grad_norm": 3.2108235359191895,
      "learning_rate": 7.827616534740547e-05,
      "loss": 0.209,
      "step": 989
    },
    {
      "epoch": 0.6530343007915568,
      "grad_norm": 2.377563238143921,
      "learning_rate": 7.825417766051012e-05,
      "loss": 0.1758,
      "step": 990
    },
    {
      "epoch": 0.6536939313984169,
      "grad_norm": 7.74413537979126,
      "learning_rate": 7.823218997361478e-05,
      "loss": 0.6252,
      "step": 991
    },
    {
      "epoch": 0.6543535620052771,
      "grad_norm": 3.0536892414093018,
      "learning_rate": 7.821020228671944e-05,
      "loss": 0.1885,
      "step": 992
    },
    {
      "epoch": 0.6550131926121372,
      "grad_norm": 2.0057616233825684,
      "learning_rate": 7.818821459982411e-05,
      "loss": 0.2181,
      "step": 993
    },
    {
      "epoch": 0.6556728232189973,
      "grad_norm": 5.60325813293457,
      "learning_rate": 7.816622691292876e-05,
      "loss": 0.7015,
      "step": 994
    },
    {
      "epoch": 0.6563324538258575,
      "grad_norm": 4.315613269805908,
      "learning_rate": 7.814423922603342e-05,
      "loss": 0.266,
      "step": 995
    },
    {
      "epoch": 0.6569920844327177,
      "grad_norm": 1.6200734376907349,
      "learning_rate": 7.812225153913809e-05,
      "loss": 0.131,
      "step": 996
    },
    {
      "epoch": 0.6576517150395779,
      "grad_norm": 1.6681832075119019,
      "learning_rate": 7.810026385224275e-05,
      "loss": 0.1977,
      "step": 997
    },
    {
      "epoch": 0.658311345646438,
      "grad_norm": 1.7149431705474854,
      "learning_rate": 7.80782761653474e-05,
      "loss": 0.1721,
      "step": 998
    },
    {
      "epoch": 0.6589709762532981,
      "grad_norm": 4.844252586364746,
      "learning_rate": 7.805628847845206e-05,
      "loss": 0.3003,
      "step": 999
    },
    {
      "epoch": 0.6596306068601583,
      "grad_norm": 0.9842169880867004,
      "learning_rate": 7.803430079155673e-05,
      "loss": 0.1115,
      "step": 1000
    },
    {
      "epoch": 0.6602902374670184,
      "grad_norm": 1.3315362930297852,
      "learning_rate": 7.801231310466139e-05,
      "loss": 0.1926,
      "step": 1001
    },
    {
      "epoch": 0.6609498680738787,
      "grad_norm": 2.432805299758911,
      "learning_rate": 7.799032541776605e-05,
      "loss": 0.2016,
      "step": 1002
    },
    {
      "epoch": 0.6616094986807388,
      "grad_norm": 2.927255868911743,
      "learning_rate": 7.79683377308707e-05,
      "loss": 0.204,
      "step": 1003
    },
    {
      "epoch": 0.662269129287599,
      "grad_norm": 9.391742706298828,
      "learning_rate": 7.794635004397538e-05,
      "loss": 0.9116,
      "step": 1004
    },
    {
      "epoch": 0.6629287598944591,
      "grad_norm": 2.4245035648345947,
      "learning_rate": 7.792436235708003e-05,
      "loss": 0.1643,
      "step": 1005
    },
    {
      "epoch": 0.6635883905013192,
      "grad_norm": 1.0191013813018799,
      "learning_rate": 7.79023746701847e-05,
      "loss": 0.1241,
      "step": 1006
    },
    {
      "epoch": 0.6642480211081794,
      "grad_norm": 0.9679897427558899,
      "learning_rate": 7.788038698328936e-05,
      "loss": 0.1152,
      "step": 1007
    },
    {
      "epoch": 0.6649076517150396,
      "grad_norm": 9.392921447753906,
      "learning_rate": 7.785839929639403e-05,
      "loss": 0.994,
      "step": 1008
    },
    {
      "epoch": 0.6655672823218998,
      "grad_norm": 4.272958278656006,
      "learning_rate": 7.783641160949869e-05,
      "loss": 0.2249,
      "step": 1009
    },
    {
      "epoch": 0.6662269129287599,
      "grad_norm": 12.045079231262207,
      "learning_rate": 7.781442392260334e-05,
      "loss": 1.2499,
      "step": 1010
    },
    {
      "epoch": 0.66688654353562,
      "grad_norm": 8.620349884033203,
      "learning_rate": 7.779243623570801e-05,
      "loss": 0.4221,
      "step": 1011
    },
    {
      "epoch": 0.6675461741424802,
      "grad_norm": 7.1377058029174805,
      "learning_rate": 7.777044854881267e-05,
      "loss": 0.4945,
      "step": 1012
    },
    {
      "epoch": 0.6682058047493403,
      "grad_norm": 7.252720355987549,
      "learning_rate": 7.774846086191733e-05,
      "loss": 0.4341,
      "step": 1013
    },
    {
      "epoch": 0.6688654353562006,
      "grad_norm": 0.855466365814209,
      "learning_rate": 7.7726473175022e-05,
      "loss": 0.0928,
      "step": 1014
    },
    {
      "epoch": 0.6695250659630607,
      "grad_norm": 5.823473930358887,
      "learning_rate": 7.770448548812666e-05,
      "loss": 0.3652,
      "step": 1015
    },
    {
      "epoch": 0.6701846965699209,
      "grad_norm": 1.7600139379501343,
      "learning_rate": 7.768249780123131e-05,
      "loss": 0.1443,
      "step": 1016
    },
    {
      "epoch": 0.670844327176781,
      "grad_norm": 8.690916061401367,
      "learning_rate": 7.766051011433597e-05,
      "loss": 0.8739,
      "step": 1017
    },
    {
      "epoch": 0.6715039577836411,
      "grad_norm": 5.236632823944092,
      "learning_rate": 7.763852242744064e-05,
      "loss": 0.7026,
      "step": 1018
    },
    {
      "epoch": 0.6721635883905013,
      "grad_norm": 2.117746114730835,
      "learning_rate": 7.76165347405453e-05,
      "loss": 0.2022,
      "step": 1019
    },
    {
      "epoch": 0.6728232189973615,
      "grad_norm": 3.0281031131744385,
      "learning_rate": 7.759454705364995e-05,
      "loss": 0.2892,
      "step": 1020
    },
    {
      "epoch": 0.6734828496042217,
      "grad_norm": 2.3062493801116943,
      "learning_rate": 7.757255936675461e-05,
      "loss": 0.2011,
      "step": 1021
    },
    {
      "epoch": 0.6741424802110818,
      "grad_norm": 6.052372932434082,
      "learning_rate": 7.755057167985928e-05,
      "loss": 0.4745,
      "step": 1022
    },
    {
      "epoch": 0.674802110817942,
      "grad_norm": 1.7145075798034668,
      "learning_rate": 7.752858399296394e-05,
      "loss": 0.1963,
      "step": 1023
    },
    {
      "epoch": 0.6754617414248021,
      "grad_norm": 22.130191802978516,
      "learning_rate": 7.75065963060686e-05,
      "loss": 0.5497,
      "step": 1024
    },
    {
      "epoch": 0.6761213720316622,
      "grad_norm": 12.286149024963379,
      "learning_rate": 7.748460861917327e-05,
      "loss": 0.416,
      "step": 1025
    },
    {
      "epoch": 0.6767810026385225,
      "grad_norm": 7.26080322265625,
      "learning_rate": 7.746262093227792e-05,
      "loss": 0.557,
      "step": 1026
    },
    {
      "epoch": 0.6774406332453826,
      "grad_norm": 2.3620874881744385,
      "learning_rate": 7.74406332453826e-05,
      "loss": 0.1856,
      "step": 1027
    },
    {
      "epoch": 0.6781002638522428,
      "grad_norm": 6.684014797210693,
      "learning_rate": 7.741864555848725e-05,
      "loss": 0.5078,
      "step": 1028
    },
    {
      "epoch": 0.6787598944591029,
      "grad_norm": 1.2564029693603516,
      "learning_rate": 7.739665787159192e-05,
      "loss": 0.1395,
      "step": 1029
    },
    {
      "epoch": 0.679419525065963,
      "grad_norm": 1.1266402006149292,
      "learning_rate": 7.737467018469658e-05,
      "loss": 0.1231,
      "step": 1030
    },
    {
      "epoch": 0.6800791556728232,
      "grad_norm": 5.695122718811035,
      "learning_rate": 7.735268249780124e-05,
      "loss": 0.3798,
      "step": 1031
    },
    {
      "epoch": 0.6807387862796834,
      "grad_norm": 3.055661201477051,
      "learning_rate": 7.73306948109059e-05,
      "loss": 0.2357,
      "step": 1032
    },
    {
      "epoch": 0.6813984168865436,
      "grad_norm": 4.0902886390686035,
      "learning_rate": 7.730870712401056e-05,
      "loss": 0.2369,
      "step": 1033
    },
    {
      "epoch": 0.6820580474934037,
      "grad_norm": 1.4043080806732178,
      "learning_rate": 7.728671943711522e-05,
      "loss": 0.1168,
      "step": 1034
    },
    {
      "epoch": 0.6827176781002638,
      "grad_norm": 1.651716947555542,
      "learning_rate": 7.726473175021988e-05,
      "loss": 0.1309,
      "step": 1035
    },
    {
      "epoch": 0.683377308707124,
      "grad_norm": 3.7356247901916504,
      "learning_rate": 7.724274406332455e-05,
      "loss": 0.275,
      "step": 1036
    },
    {
      "epoch": 0.6840369393139841,
      "grad_norm": 0.8962963223457336,
      "learning_rate": 7.72207563764292e-05,
      "loss": 0.1195,
      "step": 1037
    },
    {
      "epoch": 0.6846965699208444,
      "grad_norm": 8.246417999267578,
      "learning_rate": 7.719876868953386e-05,
      "loss": 0.9045,
      "step": 1038
    },
    {
      "epoch": 0.6853562005277045,
      "grad_norm": 9.135819435119629,
      "learning_rate": 7.717678100263852e-05,
      "loss": 0.6416,
      "step": 1039
    },
    {
      "epoch": 0.6860158311345647,
      "grad_norm": 1.6355764865875244,
      "learning_rate": 7.715479331574319e-05,
      "loss": 0.1209,
      "step": 1040
    },
    {
      "epoch": 0.6866754617414248,
      "grad_norm": 3.0069363117218018,
      "learning_rate": 7.713280562884785e-05,
      "loss": 0.1856,
      "step": 1041
    },
    {
      "epoch": 0.6873350923482849,
      "grad_norm": 1.3429369926452637,
      "learning_rate": 7.71108179419525e-05,
      "loss": 0.1893,
      "step": 1042
    },
    {
      "epoch": 0.6879947229551451,
      "grad_norm": 1.1850895881652832,
      "learning_rate": 7.708883025505717e-05,
      "loss": 0.0761,
      "step": 1043
    },
    {
      "epoch": 0.6886543535620053,
      "grad_norm": 1.1493849754333496,
      "learning_rate": 7.706684256816183e-05,
      "loss": 0.0859,
      "step": 1044
    },
    {
      "epoch": 0.6893139841688655,
      "grad_norm": 12.888643264770508,
      "learning_rate": 7.704485488126649e-05,
      "loss": 1.0317,
      "step": 1045
    },
    {
      "epoch": 0.6899736147757256,
      "grad_norm": 2.6974093914031982,
      "learning_rate": 7.702286719437114e-05,
      "loss": 0.1751,
      "step": 1046
    },
    {
      "epoch": 0.6906332453825857,
      "grad_norm": 9.93198013305664,
      "learning_rate": 7.700087950747582e-05,
      "loss": 0.5571,
      "step": 1047
    },
    {
      "epoch": 0.6912928759894459,
      "grad_norm": 0.8181663155555725,
      "learning_rate": 7.697889182058047e-05,
      "loss": 0.0694,
      "step": 1048
    },
    {
      "epoch": 0.691952506596306,
      "grad_norm": 7.375720500946045,
      "learning_rate": 7.695690413368514e-05,
      "loss": 0.4358,
      "step": 1049
    },
    {
      "epoch": 0.6926121372031663,
      "grad_norm": 1.3216954469680786,
      "learning_rate": 7.693491644678981e-05,
      "loss": 0.0712,
      "step": 1050
    },
    {
      "epoch": 0.6932717678100264,
      "grad_norm": 3.1906332969665527,
      "learning_rate": 7.691292875989447e-05,
      "loss": 0.1514,
      "step": 1051
    },
    {
      "epoch": 0.6939313984168866,
      "grad_norm": 14.177899360656738,
      "learning_rate": 7.689094107299913e-05,
      "loss": 1.572,
      "step": 1052
    },
    {
      "epoch": 0.6945910290237467,
      "grad_norm": 0.5329820513725281,
      "learning_rate": 7.686895338610378e-05,
      "loss": 0.082,
      "step": 1053
    },
    {
      "epoch": 0.6952506596306068,
      "grad_norm": 9.317948341369629,
      "learning_rate": 7.684696569920845e-05,
      "loss": 1.5045,
      "step": 1054
    },
    {
      "epoch": 0.695910290237467,
      "grad_norm": 6.466119289398193,
      "learning_rate": 7.682497801231311e-05,
      "loss": 0.3313,
      "step": 1055
    },
    {
      "epoch": 0.6965699208443272,
      "grad_norm": 0.9817763566970825,
      "learning_rate": 7.680299032541777e-05,
      "loss": 0.1139,
      "step": 1056
    },
    {
      "epoch": 0.6972295514511874,
      "grad_norm": 0.9545913338661194,
      "learning_rate": 7.678100263852243e-05,
      "loss": 0.0515,
      "step": 1057
    },
    {
      "epoch": 0.6978891820580475,
      "grad_norm": 1.3805409669876099,
      "learning_rate": 7.67590149516271e-05,
      "loss": 0.1231,
      "step": 1058
    },
    {
      "epoch": 0.6985488126649076,
      "grad_norm": 2.6688530445098877,
      "learning_rate": 7.673702726473175e-05,
      "loss": 0.1385,
      "step": 1059
    },
    {
      "epoch": 0.6992084432717678,
      "grad_norm": 0.6328195333480835,
      "learning_rate": 7.671503957783641e-05,
      "loss": 0.0772,
      "step": 1060
    },
    {
      "epoch": 0.6998680738786279,
      "grad_norm": 1.1651661396026611,
      "learning_rate": 7.669305189094108e-05,
      "loss": 0.1363,
      "step": 1061
    },
    {
      "epoch": 0.7005277044854882,
      "grad_norm": 1.3446882963180542,
      "learning_rate": 7.667106420404574e-05,
      "loss": 0.0946,
      "step": 1062
    },
    {
      "epoch": 0.7011873350923483,
      "grad_norm": 1.1306594610214233,
      "learning_rate": 7.66490765171504e-05,
      "loss": 0.0712,
      "step": 1063
    },
    {
      "epoch": 0.7018469656992085,
      "grad_norm": 4.598593235015869,
      "learning_rate": 7.662708883025505e-05,
      "loss": 0.1606,
      "step": 1064
    },
    {
      "epoch": 0.7025065963060686,
      "grad_norm": 5.129065036773682,
      "learning_rate": 7.660510114335972e-05,
      "loss": 0.1866,
      "step": 1065
    },
    {
      "epoch": 0.7031662269129287,
      "grad_norm": 7.5300750732421875,
      "learning_rate": 7.658311345646438e-05,
      "loss": 0.2436,
      "step": 1066
    },
    {
      "epoch": 0.7038258575197889,
      "grad_norm": 10.877668380737305,
      "learning_rate": 7.656112576956904e-05,
      "loss": 0.9879,
      "step": 1067
    },
    {
      "epoch": 0.7044854881266491,
      "grad_norm": 2.9022483825683594,
      "learning_rate": 7.65391380826737e-05,
      "loss": 0.1433,
      "step": 1068
    },
    {
      "epoch": 0.7051451187335093,
      "grad_norm": 0.7574637532234192,
      "learning_rate": 7.651715039577836e-05,
      "loss": 0.064,
      "step": 1069
    },
    {
      "epoch": 0.7058047493403694,
      "grad_norm": 1.0043786764144897,
      "learning_rate": 7.649516270888303e-05,
      "loss": 0.1071,
      "step": 1070
    },
    {
      "epoch": 0.7064643799472295,
      "grad_norm": 16.74319839477539,
      "learning_rate": 7.647317502198769e-05,
      "loss": 1.4133,
      "step": 1071
    },
    {
      "epoch": 0.7071240105540897,
      "grad_norm": 11.749259948730469,
      "learning_rate": 7.645118733509236e-05,
      "loss": 0.7526,
      "step": 1072
    },
    {
      "epoch": 0.7077836411609498,
      "grad_norm": 0.5796514749526978,
      "learning_rate": 7.642919964819702e-05,
      "loss": 0.0509,
      "step": 1073
    },
    {
      "epoch": 0.7084432717678101,
      "grad_norm": 0.8316157460212708,
      "learning_rate": 7.640721196130168e-05,
      "loss": 0.0679,
      "step": 1074
    },
    {
      "epoch": 0.7091029023746702,
      "grad_norm": 9.047161102294922,
      "learning_rate": 7.638522427440633e-05,
      "loss": 1.3636,
      "step": 1075
    },
    {
      "epoch": 0.7097625329815304,
      "grad_norm": 9.1492280960083,
      "learning_rate": 7.6363236587511e-05,
      "loss": 0.6228,
      "step": 1076
    },
    {
      "epoch": 0.7104221635883905,
      "grad_norm": 1.611647367477417,
      "learning_rate": 7.634124890061566e-05,
      "loss": 0.0716,
      "step": 1077
    },
    {
      "epoch": 0.7110817941952506,
      "grad_norm": 5.115839958190918,
      "learning_rate": 7.631926121372032e-05,
      "loss": 0.2301,
      "step": 1078
    },
    {
      "epoch": 0.7117414248021108,
      "grad_norm": 19.054431915283203,
      "learning_rate": 7.629727352682499e-05,
      "loss": 0.628,
      "step": 1079
    },
    {
      "epoch": 0.712401055408971,
      "grad_norm": 2.8728084564208984,
      "learning_rate": 7.627528583992964e-05,
      "loss": 0.1847,
      "step": 1080
    },
    {
      "epoch": 0.7130606860158312,
      "grad_norm": 9.149382591247559,
      "learning_rate": 7.62532981530343e-05,
      "loss": 0.7626,
      "step": 1081
    },
    {
      "epoch": 0.7137203166226913,
      "grad_norm": 12.843242645263672,
      "learning_rate": 7.623131046613896e-05,
      "loss": 1.125,
      "step": 1082
    },
    {
      "epoch": 0.7143799472295514,
      "grad_norm": 1.6112658977508545,
      "learning_rate": 7.620932277924363e-05,
      "loss": 0.1107,
      "step": 1083
    },
    {
      "epoch": 0.7150395778364116,
      "grad_norm": 6.347500801086426,
      "learning_rate": 7.618733509234829e-05,
      "loss": 0.6039,
      "step": 1084
    },
    {
      "epoch": 0.7156992084432717,
      "grad_norm": 0.6613885164260864,
      "learning_rate": 7.616534740545294e-05,
      "loss": 0.1114,
      "step": 1085
    },
    {
      "epoch": 0.716358839050132,
      "grad_norm": 1.3358752727508545,
      "learning_rate": 7.614335971855761e-05,
      "loss": 0.1559,
      "step": 1086
    },
    {
      "epoch": 0.7170184696569921,
      "grad_norm": 1.3077207803726196,
      "learning_rate": 7.612137203166227e-05,
      "loss": 0.1644,
      "step": 1087
    },
    {
      "epoch": 0.7176781002638523,
      "grad_norm": 1.2372804880142212,
      "learning_rate": 7.609938434476693e-05,
      "loss": 0.1143,
      "step": 1088
    },
    {
      "epoch": 0.7183377308707124,
      "grad_norm": 1.0159881114959717,
      "learning_rate": 7.60773966578716e-05,
      "loss": 0.1298,
      "step": 1089
    },
    {
      "epoch": 0.7189973614775725,
      "grad_norm": 1.2765467166900635,
      "learning_rate": 7.605540897097626e-05,
      "loss": 0.1117,
      "step": 1090
    },
    {
      "epoch": 0.7196569920844327,
      "grad_norm": 10.23522663116455,
      "learning_rate": 7.603342128408093e-05,
      "loss": 0.7527,
      "step": 1091
    },
    {
      "epoch": 0.7203166226912929,
      "grad_norm": 12.468811988830566,
      "learning_rate": 7.601143359718558e-05,
      "loss": 1.1263,
      "step": 1092
    },
    {
      "epoch": 0.7209762532981531,
      "grad_norm": 6.220778942108154,
      "learning_rate": 7.598944591029024e-05,
      "loss": 0.2623,
      "step": 1093
    },
    {
      "epoch": 0.7216358839050132,
      "grad_norm": 8.428770065307617,
      "learning_rate": 7.596745822339491e-05,
      "loss": 0.7465,
      "step": 1094
    },
    {
      "epoch": 0.7222955145118733,
      "grad_norm": 1.4703335762023926,
      "learning_rate": 7.594547053649957e-05,
      "loss": 0.1167,
      "step": 1095
    },
    {
      "epoch": 0.7229551451187335,
      "grad_norm": 11.140384674072266,
      "learning_rate": 7.592348284960422e-05,
      "loss": 0.7469,
      "step": 1096
    },
    {
      "epoch": 0.7236147757255936,
      "grad_norm": 1.9069324731826782,
      "learning_rate": 7.59014951627089e-05,
      "loss": 0.1186,
      "step": 1097
    },
    {
      "epoch": 0.7242744063324539,
      "grad_norm": 7.2934136390686035,
      "learning_rate": 7.587950747581355e-05,
      "loss": 0.4798,
      "step": 1098
    },
    {
      "epoch": 0.724934036939314,
      "grad_norm": 1.4924274682998657,
      "learning_rate": 7.585751978891821e-05,
      "loss": 0.1481,
      "step": 1099
    },
    {
      "epoch": 0.7255936675461742,
      "grad_norm": 2.9909322261810303,
      "learning_rate": 7.583553210202287e-05,
      "loss": 0.1934,
      "step": 1100
    },
    {
      "epoch": 0.7262532981530343,
      "grad_norm": 5.568381309509277,
      "learning_rate": 7.581354441512754e-05,
      "loss": 0.2316,
      "step": 1101
    },
    {
      "epoch": 0.7269129287598944,
      "grad_norm": 1.8208675384521484,
      "learning_rate": 7.579155672823219e-05,
      "loss": 0.1297,
      "step": 1102
    },
    {
      "epoch": 0.7275725593667546,
      "grad_norm": 1.7588130235671997,
      "learning_rate": 7.576956904133685e-05,
      "loss": 0.1287,
      "step": 1103
    },
    {
      "epoch": 0.7282321899736148,
      "grad_norm": 4.3070855140686035,
      "learning_rate": 7.574758135444152e-05,
      "loss": 0.2215,
      "step": 1104
    },
    {
      "epoch": 0.728891820580475,
      "grad_norm": 8.1346435546875,
      "learning_rate": 7.572559366754618e-05,
      "loss": 0.5196,
      "step": 1105
    },
    {
      "epoch": 0.7295514511873351,
      "grad_norm": 6.720842361450195,
      "learning_rate": 7.570360598065083e-05,
      "loss": 0.5592,
      "step": 1106
    },
    {
      "epoch": 0.7302110817941952,
      "grad_norm": 7.27770471572876,
      "learning_rate": 7.568161829375549e-05,
      "loss": 0.5273,
      "step": 1107
    },
    {
      "epoch": 0.7308707124010554,
      "grad_norm": 1.152502179145813,
      "learning_rate": 7.565963060686016e-05,
      "loss": 0.0975,
      "step": 1108
    },
    {
      "epoch": 0.7315303430079155,
      "grad_norm": 0.930152952671051,
      "learning_rate": 7.563764291996482e-05,
      "loss": 0.1039,
      "step": 1109
    },
    {
      "epoch": 0.7321899736147758,
      "grad_norm": 4.181188106536865,
      "learning_rate": 7.561565523306948e-05,
      "loss": 0.1865,
      "step": 1110
    },
    {
      "epoch": 0.7328496042216359,
      "grad_norm": 1.0133998394012451,
      "learning_rate": 7.559366754617415e-05,
      "loss": 0.1261,
      "step": 1111
    },
    {
      "epoch": 0.7335092348284961,
      "grad_norm": 11.247882843017578,
      "learning_rate": 7.55716798592788e-05,
      "loss": 0.6645,
      "step": 1112
    },
    {
      "epoch": 0.7341688654353562,
      "grad_norm": 2.430478572845459,
      "learning_rate": 7.554969217238347e-05,
      "loss": 0.1683,
      "step": 1113
    },
    {
      "epoch": 0.7348284960422163,
      "grad_norm": 7.247551441192627,
      "learning_rate": 7.552770448548813e-05,
      "loss": 0.5872,
      "step": 1114
    },
    {
      "epoch": 0.7354881266490765,
      "grad_norm": 3.395139217376709,
      "learning_rate": 7.55057167985928e-05,
      "loss": 0.2608,
      "step": 1115
    },
    {
      "epoch": 0.7361477572559367,
      "grad_norm": 2.8304736614227295,
      "learning_rate": 7.548372911169746e-05,
      "loss": 0.2061,
      "step": 1116
    },
    {
      "epoch": 0.7368073878627969,
      "grad_norm": 3.340054750442505,
      "learning_rate": 7.546174142480212e-05,
      "loss": 0.2516,
      "step": 1117
    },
    {
      "epoch": 0.737467018469657,
      "grad_norm": 2.055103063583374,
      "learning_rate": 7.543975373790677e-05,
      "loss": 0.222,
      "step": 1118
    },
    {
      "epoch": 0.7381266490765171,
      "grad_norm": 2.771186113357544,
      "learning_rate": 7.541776605101144e-05,
      "loss": 0.1838,
      "step": 1119
    },
    {
      "epoch": 0.7387862796833773,
      "grad_norm": 1.879564881324768,
      "learning_rate": 7.53957783641161e-05,
      "loss": 0.1312,
      "step": 1120
    },
    {
      "epoch": 0.7394459102902374,
      "grad_norm": 2.6744346618652344,
      "learning_rate": 7.537379067722076e-05,
      "loss": 0.179,
      "step": 1121
    },
    {
      "epoch": 0.7401055408970977,
      "grad_norm": 3.5923221111297607,
      "learning_rate": 7.535180299032543e-05,
      "loss": 0.2547,
      "step": 1122
    },
    {
      "epoch": 0.7407651715039578,
      "grad_norm": 0.9588581919670105,
      "learning_rate": 7.532981530343008e-05,
      "loss": 0.1036,
      "step": 1123
    },
    {
      "epoch": 0.741424802110818,
      "grad_norm": 1.923675298690796,
      "learning_rate": 7.530782761653474e-05,
      "loss": 0.1481,
      "step": 1124
    },
    {
      "epoch": 0.7420844327176781,
      "grad_norm": 2.8191044330596924,
      "learning_rate": 7.52858399296394e-05,
      "loss": 0.1565,
      "step": 1125
    },
    {
      "epoch": 0.7427440633245382,
      "grad_norm": 13.495972633361816,
      "learning_rate": 7.526385224274407e-05,
      "loss": 1.1033,
      "step": 1126
    },
    {
      "epoch": 0.7434036939313984,
      "grad_norm": 8.3764066696167,
      "learning_rate": 7.524186455584873e-05,
      "loss": 0.9432,
      "step": 1127
    },
    {
      "epoch": 0.7440633245382586,
      "grad_norm": 2.4161765575408936,
      "learning_rate": 7.521987686895338e-05,
      "loss": 0.1484,
      "step": 1128
    },
    {
      "epoch": 0.7447229551451188,
      "grad_norm": 4.615076541900635,
      "learning_rate": 7.519788918205804e-05,
      "loss": 0.1976,
      "step": 1129
    },
    {
      "epoch": 0.7453825857519789,
      "grad_norm": 2.362576723098755,
      "learning_rate": 7.517590149516271e-05,
      "loss": 0.1951,
      "step": 1130
    },
    {
      "epoch": 0.746042216358839,
      "grad_norm": 1.187734842300415,
      "learning_rate": 7.515391380826737e-05,
      "loss": 0.1531,
      "step": 1131
    },
    {
      "epoch": 0.7467018469656992,
      "grad_norm": 4.299764633178711,
      "learning_rate": 7.513192612137204e-05,
      "loss": 0.2521,
      "step": 1132
    },
    {
      "epoch": 0.7473614775725593,
      "grad_norm": 5.023017406463623,
      "learning_rate": 7.51099384344767e-05,
      "loss": 0.3807,
      "step": 1133
    },
    {
      "epoch": 0.7480211081794196,
      "grad_norm": 4.599670886993408,
      "learning_rate": 7.508795074758137e-05,
      "loss": 0.4221,
      "step": 1134
    },
    {
      "epoch": 0.7486807387862797,
      "grad_norm": 2.810206651687622,
      "learning_rate": 7.506596306068602e-05,
      "loss": 0.2313,
      "step": 1135
    },
    {
      "epoch": 0.7493403693931399,
      "grad_norm": 1.7746119499206543,
      "learning_rate": 7.504397537379068e-05,
      "loss": 0.1875,
      "step": 1136
    },
    {
      "epoch": 0.75,
      "grad_norm": 2.770570755004883,
      "learning_rate": 7.502198768689535e-05,
      "loss": 0.1689,
      "step": 1137
    },
    {
      "epoch": 0.7506596306068601,
      "grad_norm": 1.8628278970718384,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.0941,
      "step": 1138
    },
    {
      "epoch": 0.7513192612137203,
      "grad_norm": 8.61409854888916,
      "learning_rate": 7.497801231310466e-05,
      "loss": 0.3382,
      "step": 1139
    },
    {
      "epoch": 0.7519788918205804,
      "grad_norm": 8.75823974609375,
      "learning_rate": 7.495602462620933e-05,
      "loss": 0.5143,
      "step": 1140
    },
    {
      "epoch": 0.7526385224274407,
      "grad_norm": 1.5411545038223267,
      "learning_rate": 7.493403693931399e-05,
      "loss": 0.1872,
      "step": 1141
    },
    {
      "epoch": 0.7532981530343008,
      "grad_norm": 1.7158658504486084,
      "learning_rate": 7.491204925241865e-05,
      "loss": 0.16,
      "step": 1142
    },
    {
      "epoch": 0.753957783641161,
      "grad_norm": 1.2425768375396729,
      "learning_rate": 7.48900615655233e-05,
      "loss": 0.1221,
      "step": 1143
    },
    {
      "epoch": 0.7546174142480211,
      "grad_norm": 4.557229995727539,
      "learning_rate": 7.486807387862798e-05,
      "loss": 0.2223,
      "step": 1144
    },
    {
      "epoch": 0.7552770448548812,
      "grad_norm": 11.045930862426758,
      "learning_rate": 7.484608619173263e-05,
      "loss": 0.693,
      "step": 1145
    },
    {
      "epoch": 0.7559366754617414,
      "grad_norm": 2.276625871658325,
      "learning_rate": 7.482409850483729e-05,
      "loss": 0.1098,
      "step": 1146
    },
    {
      "epoch": 0.7565963060686016,
      "grad_norm": 5.6701130867004395,
      "learning_rate": 7.480211081794196e-05,
      "loss": 0.243,
      "step": 1147
    },
    {
      "epoch": 0.7572559366754618,
      "grad_norm": 8.682157516479492,
      "learning_rate": 7.478012313104662e-05,
      "loss": 0.3437,
      "step": 1148
    },
    {
      "epoch": 0.7579155672823219,
      "grad_norm": 5.736200332641602,
      "learning_rate": 7.475813544415127e-05,
      "loss": 0.3655,
      "step": 1149
    },
    {
      "epoch": 0.758575197889182,
      "grad_norm": 1.3048285245895386,
      "learning_rate": 7.473614775725593e-05,
      "loss": 0.1031,
      "step": 1150
    },
    {
      "epoch": 0.7592348284960422,
      "grad_norm": 0.9322561025619507,
      "learning_rate": 7.47141600703606e-05,
      "loss": 0.1228,
      "step": 1151
    },
    {
      "epoch": 0.7598944591029023,
      "grad_norm": 5.932257652282715,
      "learning_rate": 7.469217238346526e-05,
      "loss": 0.2727,
      "step": 1152
    },
    {
      "epoch": 0.7605540897097626,
      "grad_norm": 2.1204335689544678,
      "learning_rate": 7.467018469656992e-05,
      "loss": 0.132,
      "step": 1153
    },
    {
      "epoch": 0.7612137203166227,
      "grad_norm": 6.532655239105225,
      "learning_rate": 7.464819700967459e-05,
      "loss": 0.8242,
      "step": 1154
    },
    {
      "epoch": 0.7618733509234829,
      "grad_norm": 7.672903060913086,
      "learning_rate": 7.462620932277926e-05,
      "loss": 0.528,
      "step": 1155
    },
    {
      "epoch": 0.762532981530343,
      "grad_norm": 9.14714241027832,
      "learning_rate": 7.460422163588391e-05,
      "loss": 0.3812,
      "step": 1156
    },
    {
      "epoch": 0.7631926121372031,
      "grad_norm": 1.1271227598190308,
      "learning_rate": 7.458223394898857e-05,
      "loss": 0.118,
      "step": 1157
    },
    {
      "epoch": 0.7638522427440633,
      "grad_norm": 1.1786962747573853,
      "learning_rate": 7.456024626209324e-05,
      "loss": 0.1303,
      "step": 1158
    },
    {
      "epoch": 0.7645118733509235,
      "grad_norm": 2.248565912246704,
      "learning_rate": 7.45382585751979e-05,
      "loss": 0.1385,
      "step": 1159
    },
    {
      "epoch": 0.7651715039577837,
      "grad_norm": 9.07319164276123,
      "learning_rate": 7.451627088830256e-05,
      "loss": 0.6097,
      "step": 1160
    },
    {
      "epoch": 0.7658311345646438,
      "grad_norm": 1.7411022186279297,
      "learning_rate": 7.449428320140721e-05,
      "loss": 0.1227,
      "step": 1161
    },
    {
      "epoch": 0.7664907651715039,
      "grad_norm": 3.052371025085449,
      "learning_rate": 7.447229551451188e-05,
      "loss": 0.2174,
      "step": 1162
    },
    {
      "epoch": 0.7671503957783641,
      "grad_norm": 4.875460147857666,
      "learning_rate": 7.445030782761654e-05,
      "loss": 0.4368,
      "step": 1163
    },
    {
      "epoch": 0.7678100263852242,
      "grad_norm": 1.732859492301941,
      "learning_rate": 7.44283201407212e-05,
      "loss": 0.1659,
      "step": 1164
    },
    {
      "epoch": 0.7684696569920845,
      "grad_norm": 8.17945384979248,
      "learning_rate": 7.440633245382587e-05,
      "loss": 0.5741,
      "step": 1165
    },
    {
      "epoch": 0.7691292875989446,
      "grad_norm": 1.907379388809204,
      "learning_rate": 7.438434476693052e-05,
      "loss": 0.1003,
      "step": 1166
    },
    {
      "epoch": 0.7697889182058048,
      "grad_norm": 5.4178242683410645,
      "learning_rate": 7.436235708003518e-05,
      "loss": 0.2128,
      "step": 1167
    },
    {
      "epoch": 0.7704485488126649,
      "grad_norm": 0.7902146577835083,
      "learning_rate": 7.434036939313984e-05,
      "loss": 0.0903,
      "step": 1168
    },
    {
      "epoch": 0.771108179419525,
      "grad_norm": 3.7594783306121826,
      "learning_rate": 7.431838170624451e-05,
      "loss": 0.3173,
      "step": 1169
    },
    {
      "epoch": 0.7717678100263852,
      "grad_norm": 0.8513535261154175,
      "learning_rate": 7.429639401934917e-05,
      "loss": 0.0886,
      "step": 1170
    },
    {
      "epoch": 0.7724274406332454,
      "grad_norm": 3.035379409790039,
      "learning_rate": 7.427440633245382e-05,
      "loss": 0.1214,
      "step": 1171
    },
    {
      "epoch": 0.7730870712401056,
      "grad_norm": 9.969569206237793,
      "learning_rate": 7.425241864555848e-05,
      "loss": 0.4842,
      "step": 1172
    },
    {
      "epoch": 0.7737467018469657,
      "grad_norm": 2.0406064987182617,
      "learning_rate": 7.423043095866315e-05,
      "loss": 0.1211,
      "step": 1173
    },
    {
      "epoch": 0.7744063324538258,
      "grad_norm": 3.054861068725586,
      "learning_rate": 7.420844327176781e-05,
      "loss": 0.1852,
      "step": 1174
    },
    {
      "epoch": 0.775065963060686,
      "grad_norm": 1.097653865814209,
      "learning_rate": 7.418645558487248e-05,
      "loss": 0.0999,
      "step": 1175
    },
    {
      "epoch": 0.7757255936675461,
      "grad_norm": 2.9788050651550293,
      "learning_rate": 7.416446789797713e-05,
      "loss": 0.2333,
      "step": 1176
    },
    {
      "epoch": 0.7763852242744064,
      "grad_norm": 5.255477428436279,
      "learning_rate": 7.41424802110818e-05,
      "loss": 0.3789,
      "step": 1177
    },
    {
      "epoch": 0.7770448548812665,
      "grad_norm": 2.662280321121216,
      "learning_rate": 7.412049252418646e-05,
      "loss": 0.2276,
      "step": 1178
    },
    {
      "epoch": 0.7777044854881267,
      "grad_norm": 2.3209164142608643,
      "learning_rate": 7.409850483729112e-05,
      "loss": 0.1319,
      "step": 1179
    },
    {
      "epoch": 0.7783641160949868,
      "grad_norm": 10.789143562316895,
      "learning_rate": 7.407651715039579e-05,
      "loss": 1.2245,
      "step": 1180
    },
    {
      "epoch": 0.7790237467018469,
      "grad_norm": 3.887491226196289,
      "learning_rate": 7.405452946350045e-05,
      "loss": 0.1664,
      "step": 1181
    },
    {
      "epoch": 0.7796833773087071,
      "grad_norm": 1.6121047735214233,
      "learning_rate": 7.40325417766051e-05,
      "loss": 0.0857,
      "step": 1182
    },
    {
      "epoch": 0.7803430079155673,
      "grad_norm": 7.284989833831787,
      "learning_rate": 7.401055408970977e-05,
      "loss": 0.3815,
      "step": 1183
    },
    {
      "epoch": 0.7810026385224275,
      "grad_norm": 4.432650089263916,
      "learning_rate": 7.398856640281443e-05,
      "loss": 0.2732,
      "step": 1184
    },
    {
      "epoch": 0.7816622691292876,
      "grad_norm": 3.788809299468994,
      "learning_rate": 7.396657871591909e-05,
      "loss": 0.1873,
      "step": 1185
    },
    {
      "epoch": 0.7823218997361477,
      "grad_norm": 0.7817307114601135,
      "learning_rate": 7.394459102902375e-05,
      "loss": 0.0995,
      "step": 1186
    },
    {
      "epoch": 0.7829815303430079,
      "grad_norm": 8.454636573791504,
      "learning_rate": 7.392260334212842e-05,
      "loss": 0.5144,
      "step": 1187
    },
    {
      "epoch": 0.783641160949868,
      "grad_norm": 4.386679172515869,
      "learning_rate": 7.390061565523307e-05,
      "loss": 0.1617,
      "step": 1188
    },
    {
      "epoch": 0.7843007915567283,
      "grad_norm": 1.9116514921188354,
      "learning_rate": 7.387862796833773e-05,
      "loss": 0.1267,
      "step": 1189
    },
    {
      "epoch": 0.7849604221635884,
      "grad_norm": 10.690702438354492,
      "learning_rate": 7.385664028144239e-05,
      "loss": 0.6488,
      "step": 1190
    },
    {
      "epoch": 0.7856200527704486,
      "grad_norm": 3.145374059677124,
      "learning_rate": 7.383465259454706e-05,
      "loss": 0.201,
      "step": 1191
    },
    {
      "epoch": 0.7862796833773087,
      "grad_norm": 1.4167405366897583,
      "learning_rate": 7.381266490765171e-05,
      "loss": 0.1226,
      "step": 1192
    },
    {
      "epoch": 0.7869393139841688,
      "grad_norm": 0.6337651014328003,
      "learning_rate": 7.379067722075637e-05,
      "loss": 0.0965,
      "step": 1193
    },
    {
      "epoch": 0.787598944591029,
      "grad_norm": 2.1172056198120117,
      "learning_rate": 7.376868953386104e-05,
      "loss": 0.1413,
      "step": 1194
    },
    {
      "epoch": 0.7882585751978892,
      "grad_norm": 1.5609164237976074,
      "learning_rate": 7.37467018469657e-05,
      "loss": 0.1114,
      "step": 1195
    },
    {
      "epoch": 0.7889182058047494,
      "grad_norm": 1.2697992324829102,
      "learning_rate": 7.372471416007037e-05,
      "loss": 0.0941,
      "step": 1196
    },
    {
      "epoch": 0.7895778364116095,
      "grad_norm": 5.575024604797363,
      "learning_rate": 7.370272647317503e-05,
      "loss": 0.3626,
      "step": 1197
    },
    {
      "epoch": 0.7902374670184696,
      "grad_norm": 3.829204559326172,
      "learning_rate": 7.36807387862797e-05,
      "loss": 0.1837,
      "step": 1198
    },
    {
      "epoch": 0.7908970976253298,
      "grad_norm": 8.141222953796387,
      "learning_rate": 7.365875109938435e-05,
      "loss": 0.9849,
      "step": 1199
    },
    {
      "epoch": 0.7915567282321899,
      "grad_norm": 1.5222623348236084,
      "learning_rate": 7.363676341248901e-05,
      "loss": 0.1051,
      "step": 1200
    },
    {
      "epoch": 0.7922163588390502,
      "grad_norm": 0.8363921046257019,
      "learning_rate": 7.361477572559368e-05,
      "loss": 0.1197,
      "step": 1201
    },
    {
      "epoch": 0.7928759894459103,
      "grad_norm": 1.686952829360962,
      "learning_rate": 7.359278803869834e-05,
      "loss": 0.1508,
      "step": 1202
    },
    {
      "epoch": 0.7935356200527705,
      "grad_norm": 4.032611846923828,
      "learning_rate": 7.3570800351803e-05,
      "loss": 0.2914,
      "step": 1203
    },
    {
      "epoch": 0.7941952506596306,
      "grad_norm": 1.8421735763549805,
      "learning_rate": 7.354881266490765e-05,
      "loss": 0.1479,
      "step": 1204
    },
    {
      "epoch": 0.7948548812664907,
      "grad_norm": 2.6728782653808594,
      "learning_rate": 7.352682497801232e-05,
      "loss": 0.2263,
      "step": 1205
    },
    {
      "epoch": 0.7955145118733509,
      "grad_norm": 4.62345552444458,
      "learning_rate": 7.350483729111698e-05,
      "loss": 0.337,
      "step": 1206
    },
    {
      "epoch": 0.7961741424802111,
      "grad_norm": 1.9891915321350098,
      "learning_rate": 7.348284960422164e-05,
      "loss": 0.1414,
      "step": 1207
    },
    {
      "epoch": 0.7968337730870713,
      "grad_norm": 10.326050758361816,
      "learning_rate": 7.34608619173263e-05,
      "loss": 0.3979,
      "step": 1208
    },
    {
      "epoch": 0.7974934036939314,
      "grad_norm": 1.9226102828979492,
      "learning_rate": 7.343887423043096e-05,
      "loss": 0.2005,
      "step": 1209
    },
    {
      "epoch": 0.7981530343007915,
      "grad_norm": 1.563346028327942,
      "learning_rate": 7.341688654353562e-05,
      "loss": 0.1662,
      "step": 1210
    },
    {
      "epoch": 0.7988126649076517,
      "grad_norm": 1.2119908332824707,
      "learning_rate": 7.339489885664028e-05,
      "loss": 0.0824,
      "step": 1211
    },
    {
      "epoch": 0.7994722955145118,
      "grad_norm": 3.7893261909484863,
      "learning_rate": 7.337291116974495e-05,
      "loss": 0.2304,
      "step": 1212
    },
    {
      "epoch": 0.8001319261213721,
      "grad_norm": 2.0434305667877197,
      "learning_rate": 7.33509234828496e-05,
      "loss": 0.1301,
      "step": 1213
    },
    {
      "epoch": 0.8007915567282322,
      "grad_norm": 10.700847625732422,
      "learning_rate": 7.332893579595426e-05,
      "loss": 0.6454,
      "step": 1214
    },
    {
      "epoch": 0.8014511873350924,
      "grad_norm": 8.061247825622559,
      "learning_rate": 7.330694810905892e-05,
      "loss": 0.9974,
      "step": 1215
    },
    {
      "epoch": 0.8021108179419525,
      "grad_norm": 6.267648696899414,
      "learning_rate": 7.328496042216359e-05,
      "loss": 0.2211,
      "step": 1216
    },
    {
      "epoch": 0.8027704485488126,
      "grad_norm": 1.6094900369644165,
      "learning_rate": 7.326297273526825e-05,
      "loss": 0.1604,
      "step": 1217
    },
    {
      "epoch": 0.8034300791556728,
      "grad_norm": 2.47963809967041,
      "learning_rate": 7.324098504837292e-05,
      "loss": 0.1416,
      "step": 1218
    },
    {
      "epoch": 0.804089709762533,
      "grad_norm": 1.317921757698059,
      "learning_rate": 7.321899736147757e-05,
      "loss": 0.1011,
      "step": 1219
    },
    {
      "epoch": 0.8047493403693932,
      "grad_norm": 11.891366958618164,
      "learning_rate": 7.319700967458225e-05,
      "loss": 1.1452,
      "step": 1220
    },
    {
      "epoch": 0.8054089709762533,
      "grad_norm": 0.8147135376930237,
      "learning_rate": 7.31750219876869e-05,
      "loss": 0.0611,
      "step": 1221
    },
    {
      "epoch": 0.8060686015831134,
      "grad_norm": 13.418449401855469,
      "learning_rate": 7.315303430079156e-05,
      "loss": 0.6557,
      "step": 1222
    },
    {
      "epoch": 0.8067282321899736,
      "grad_norm": 2.441636085510254,
      "learning_rate": 7.313104661389623e-05,
      "loss": 0.1027,
      "step": 1223
    },
    {
      "epoch": 0.8073878627968337,
      "grad_norm": 0.8860911130905151,
      "learning_rate": 7.310905892700089e-05,
      "loss": 0.0844,
      "step": 1224
    },
    {
      "epoch": 0.808047493403694,
      "grad_norm": 2.434401750564575,
      "learning_rate": 7.308707124010554e-05,
      "loss": 0.1172,
      "step": 1225
    },
    {
      "epoch": 0.8087071240105541,
      "grad_norm": 11.067763328552246,
      "learning_rate": 7.30650835532102e-05,
      "loss": 0.3359,
      "step": 1226
    },
    {
      "epoch": 0.8093667546174143,
      "grad_norm": 18.672487258911133,
      "learning_rate": 7.304309586631487e-05,
      "loss": 0.9188,
      "step": 1227
    },
    {
      "epoch": 0.8100263852242744,
      "grad_norm": 6.836871147155762,
      "learning_rate": 7.302110817941953e-05,
      "loss": 0.9518,
      "step": 1228
    },
    {
      "epoch": 0.8106860158311345,
      "grad_norm": 9.7413911819458,
      "learning_rate": 7.299912049252419e-05,
      "loss": 0.447,
      "step": 1229
    },
    {
      "epoch": 0.8113456464379947,
      "grad_norm": 1.5182768106460571,
      "learning_rate": 7.297713280562886e-05,
      "loss": 0.1424,
      "step": 1230
    },
    {
      "epoch": 0.8120052770448549,
      "grad_norm": 2.1102705001831055,
      "learning_rate": 7.295514511873351e-05,
      "loss": 0.1281,
      "step": 1231
    },
    {
      "epoch": 0.8126649076517151,
      "grad_norm": 1.3725205659866333,
      "learning_rate": 7.293315743183817e-05,
      "loss": 0.1126,
      "step": 1232
    },
    {
      "epoch": 0.8133245382585752,
      "grad_norm": 1.1505591869354248,
      "learning_rate": 7.291116974494283e-05,
      "loss": 0.1138,
      "step": 1233
    },
    {
      "epoch": 0.8139841688654353,
      "grad_norm": 2.884002685546875,
      "learning_rate": 7.28891820580475e-05,
      "loss": 0.1709,
      "step": 1234
    },
    {
      "epoch": 0.8146437994722955,
      "grad_norm": 1.8248106241226196,
      "learning_rate": 7.286719437115215e-05,
      "loss": 0.1665,
      "step": 1235
    },
    {
      "epoch": 0.8153034300791556,
      "grad_norm": 1.4808790683746338,
      "learning_rate": 7.284520668425681e-05,
      "loss": 0.1387,
      "step": 1236
    },
    {
      "epoch": 0.8159630606860159,
      "grad_norm": 7.033433437347412,
      "learning_rate": 7.282321899736148e-05,
      "loss": 0.409,
      "step": 1237
    },
    {
      "epoch": 0.816622691292876,
      "grad_norm": 9.084525108337402,
      "learning_rate": 7.280123131046614e-05,
      "loss": 0.3544,
      "step": 1238
    },
    {
      "epoch": 0.8172823218997362,
      "grad_norm": 3.752854108810425,
      "learning_rate": 7.277924362357081e-05,
      "loss": 0.2256,
      "step": 1239
    },
    {
      "epoch": 0.8179419525065963,
      "grad_norm": 1.066298007965088,
      "learning_rate": 7.275725593667547e-05,
      "loss": 0.1038,
      "step": 1240
    },
    {
      "epoch": 0.8186015831134564,
      "grad_norm": 6.117469787597656,
      "learning_rate": 7.273526824978014e-05,
      "loss": 0.7175,
      "step": 1241
    },
    {
      "epoch": 0.8192612137203166,
      "grad_norm": 2.4609591960906982,
      "learning_rate": 7.27132805628848e-05,
      "loss": 0.229,
      "step": 1242
    },
    {
      "epoch": 0.8199208443271768,
      "grad_norm": 9.306368827819824,
      "learning_rate": 7.269129287598945e-05,
      "loss": 0.4868,
      "step": 1243
    },
    {
      "epoch": 0.820580474934037,
      "grad_norm": 2.5504956245422363,
      "learning_rate": 7.266930518909411e-05,
      "loss": 0.1964,
      "step": 1244
    },
    {
      "epoch": 0.8212401055408971,
      "grad_norm": 3.126267194747925,
      "learning_rate": 7.264731750219878e-05,
      "loss": 0.2432,
      "step": 1245
    },
    {
      "epoch": 0.8218997361477572,
      "grad_norm": 9.198246955871582,
      "learning_rate": 7.262532981530344e-05,
      "loss": 0.6958,
      "step": 1246
    },
    {
      "epoch": 0.8225593667546174,
      "grad_norm": 1.9426465034484863,
      "learning_rate": 7.260334212840809e-05,
      "loss": 0.135,
      "step": 1247
    },
    {
      "epoch": 0.8232189973614775,
      "grad_norm": 2.7409913539886475,
      "learning_rate": 7.258135444151276e-05,
      "loss": 0.2177,
      "step": 1248
    },
    {
      "epoch": 0.8238786279683378,
      "grad_norm": 7.184163570404053,
      "learning_rate": 7.255936675461742e-05,
      "loss": 0.2963,
      "step": 1249
    },
    {
      "epoch": 0.8245382585751979,
      "grad_norm": 4.934175491333008,
      "learning_rate": 7.253737906772208e-05,
      "loss": 0.248,
      "step": 1250
    },
    {
      "epoch": 0.825197889182058,
      "grad_norm": 1.8293273448944092,
      "learning_rate": 7.251539138082673e-05,
      "loss": 0.1229,
      "step": 1251
    },
    {
      "epoch": 0.8258575197889182,
      "grad_norm": 6.709878921508789,
      "learning_rate": 7.24934036939314e-05,
      "loss": 0.6703,
      "step": 1252
    },
    {
      "epoch": 0.8265171503957783,
      "grad_norm": 1.5328634977340698,
      "learning_rate": 7.247141600703606e-05,
      "loss": 0.1984,
      "step": 1253
    },
    {
      "epoch": 0.8271767810026385,
      "grad_norm": 1.8937844038009644,
      "learning_rate": 7.244942832014072e-05,
      "loss": 0.1263,
      "step": 1254
    },
    {
      "epoch": 0.8278364116094987,
      "grad_norm": 1.1109861135482788,
      "learning_rate": 7.242744063324539e-05,
      "loss": 0.0897,
      "step": 1255
    },
    {
      "epoch": 0.8284960422163589,
      "grad_norm": 12.431402206420898,
      "learning_rate": 7.240545294635005e-05,
      "loss": 0.7774,
      "step": 1256
    },
    {
      "epoch": 0.829155672823219,
      "grad_norm": 1.5568636655807495,
      "learning_rate": 7.23834652594547e-05,
      "loss": 0.0851,
      "step": 1257
    },
    {
      "epoch": 0.8298153034300791,
      "grad_norm": 13.243879318237305,
      "learning_rate": 7.236147757255936e-05,
      "loss": 0.61,
      "step": 1258
    },
    {
      "epoch": 0.8304749340369393,
      "grad_norm": 3.7683048248291016,
      "learning_rate": 7.233948988566403e-05,
      "loss": 0.167,
      "step": 1259
    },
    {
      "epoch": 0.8311345646437994,
      "grad_norm": 14.661237716674805,
      "learning_rate": 7.23175021987687e-05,
      "loss": 0.8642,
      "step": 1260
    },
    {
      "epoch": 0.8317941952506597,
      "grad_norm": 1.015993356704712,
      "learning_rate": 7.229551451187336e-05,
      "loss": 0.0983,
      "step": 1261
    },
    {
      "epoch": 0.8324538258575198,
      "grad_norm": 15.993131637573242,
      "learning_rate": 7.227352682497801e-05,
      "loss": 0.4326,
      "step": 1262
    },
    {
      "epoch": 0.83311345646438,
      "grad_norm": 0.7681789994239807,
      "learning_rate": 7.225153913808269e-05,
      "loss": 0.0639,
      "step": 1263
    },
    {
      "epoch": 0.8337730870712401,
      "grad_norm": 17.475637435913086,
      "learning_rate": 7.222955145118734e-05,
      "loss": 1.2777,
      "step": 1264
    },
    {
      "epoch": 0.8344327176781002,
      "grad_norm": 9.376352310180664,
      "learning_rate": 7.2207563764292e-05,
      "loss": 0.5052,
      "step": 1265
    },
    {
      "epoch": 0.8350923482849604,
      "grad_norm": 7.66080379486084,
      "learning_rate": 7.218557607739667e-05,
      "loss": 0.4109,
      "step": 1266
    },
    {
      "epoch": 0.8357519788918206,
      "grad_norm": 2.4443881511688232,
      "learning_rate": 7.216358839050133e-05,
      "loss": 0.1839,
      "step": 1267
    },
    {
      "epoch": 0.8364116094986808,
      "grad_norm": 1.6216665506362915,
      "learning_rate": 7.214160070360598e-05,
      "loss": 0.1367,
      "step": 1268
    },
    {
      "epoch": 0.8370712401055409,
      "grad_norm": 2.6289172172546387,
      "learning_rate": 7.211961301671064e-05,
      "loss": 0.2049,
      "step": 1269
    },
    {
      "epoch": 0.837730870712401,
      "grad_norm": 8.450302124023438,
      "learning_rate": 7.209762532981531e-05,
      "loss": 0.54,
      "step": 1270
    },
    {
      "epoch": 0.8383905013192612,
      "grad_norm": 1.1133506298065186,
      "learning_rate": 7.207563764291997e-05,
      "loss": 0.0803,
      "step": 1271
    },
    {
      "epoch": 0.8390501319261213,
      "grad_norm": 0.9511915445327759,
      "learning_rate": 7.205364995602463e-05,
      "loss": 0.0933,
      "step": 1272
    },
    {
      "epoch": 0.8397097625329816,
      "grad_norm": 8.724719047546387,
      "learning_rate": 7.20316622691293e-05,
      "loss": 0.8569,
      "step": 1273
    },
    {
      "epoch": 0.8403693931398417,
      "grad_norm": 8.587272644042969,
      "learning_rate": 7.200967458223395e-05,
      "loss": 0.1908,
      "step": 1274
    },
    {
      "epoch": 0.8410290237467019,
      "grad_norm": 7.499882221221924,
      "learning_rate": 7.198768689533861e-05,
      "loss": 0.3806,
      "step": 1275
    },
    {
      "epoch": 0.841688654353562,
      "grad_norm": 4.631711959838867,
      "learning_rate": 7.196569920844327e-05,
      "loss": 0.2178,
      "step": 1276
    },
    {
      "epoch": 0.8423482849604221,
      "grad_norm": 11.484779357910156,
      "learning_rate": 7.194371152154794e-05,
      "loss": 0.7807,
      "step": 1277
    },
    {
      "epoch": 0.8430079155672823,
      "grad_norm": 2.7092576026916504,
      "learning_rate": 7.19217238346526e-05,
      "loss": 0.1444,
      "step": 1278
    },
    {
      "epoch": 0.8436675461741425,
      "grad_norm": 1.6325404644012451,
      "learning_rate": 7.189973614775725e-05,
      "loss": 0.0985,
      "step": 1279
    },
    {
      "epoch": 0.8443271767810027,
      "grad_norm": 1.9197297096252441,
      "learning_rate": 7.187774846086192e-05,
      "loss": 0.1218,
      "step": 1280
    },
    {
      "epoch": 0.8449868073878628,
      "grad_norm": 2.733705520629883,
      "learning_rate": 7.185576077396658e-05,
      "loss": 0.1621,
      "step": 1281
    },
    {
      "epoch": 0.8456464379947229,
      "grad_norm": 6.912718296051025,
      "learning_rate": 7.183377308707125e-05,
      "loss": 0.3676,
      "step": 1282
    },
    {
      "epoch": 0.8463060686015831,
      "grad_norm": 2.081939220428467,
      "learning_rate": 7.18117854001759e-05,
      "loss": 0.1231,
      "step": 1283
    },
    {
      "epoch": 0.8469656992084432,
      "grad_norm": 9.149785995483398,
      "learning_rate": 7.178979771328058e-05,
      "loss": 1.1909,
      "step": 1284
    },
    {
      "epoch": 0.8476253298153035,
      "grad_norm": 1.1184277534484863,
      "learning_rate": 7.176781002638523e-05,
      "loss": 0.0866,
      "step": 1285
    },
    {
      "epoch": 0.8482849604221636,
      "grad_norm": 7.467662811279297,
      "learning_rate": 7.174582233948989e-05,
      "loss": 0.2437,
      "step": 1286
    },
    {
      "epoch": 0.8489445910290238,
      "grad_norm": 1.1963495016098022,
      "learning_rate": 7.172383465259455e-05,
      "loss": 0.0848,
      "step": 1287
    },
    {
      "epoch": 0.8496042216358839,
      "grad_norm": 1.0388990640640259,
      "learning_rate": 7.170184696569922e-05,
      "loss": 0.1268,
      "step": 1288
    },
    {
      "epoch": 0.850263852242744,
      "grad_norm": 14.592671394348145,
      "learning_rate": 7.167985927880388e-05,
      "loss": 0.5202,
      "step": 1289
    },
    {
      "epoch": 0.8509234828496042,
      "grad_norm": 6.340848922729492,
      "learning_rate": 7.165787159190853e-05,
      "loss": 0.5715,
      "step": 1290
    },
    {
      "epoch": 0.8515831134564644,
      "grad_norm": 12.66623306274414,
      "learning_rate": 7.16358839050132e-05,
      "loss": 0.7604,
      "step": 1291
    },
    {
      "epoch": 0.8522427440633246,
      "grad_norm": 1.0338391065597534,
      "learning_rate": 7.161389621811786e-05,
      "loss": 0.1071,
      "step": 1292
    },
    {
      "epoch": 0.8529023746701847,
      "grad_norm": 5.9188737869262695,
      "learning_rate": 7.159190853122252e-05,
      "loss": 0.6484,
      "step": 1293
    },
    {
      "epoch": 0.8535620052770448,
      "grad_norm": 2.757664442062378,
      "learning_rate": 7.156992084432717e-05,
      "loss": 0.1383,
      "step": 1294
    },
    {
      "epoch": 0.854221635883905,
      "grad_norm": 3.918135643005371,
      "learning_rate": 7.154793315743184e-05,
      "loss": 0.2723,
      "step": 1295
    },
    {
      "epoch": 0.8548812664907651,
      "grad_norm": 1.5906606912612915,
      "learning_rate": 7.15259454705365e-05,
      "loss": 0.1207,
      "step": 1296
    },
    {
      "epoch": 0.8555408970976254,
      "grad_norm": 6.043204307556152,
      "learning_rate": 7.150395778364116e-05,
      "loss": 0.5028,
      "step": 1297
    },
    {
      "epoch": 0.8562005277044855,
      "grad_norm": 4.359861850738525,
      "learning_rate": 7.148197009674582e-05,
      "loss": 0.3282,
      "step": 1298
    },
    {
      "epoch": 0.8568601583113457,
      "grad_norm": 4.675388336181641,
      "learning_rate": 7.145998240985049e-05,
      "loss": 0.2683,
      "step": 1299
    },
    {
      "epoch": 0.8575197889182058,
      "grad_norm": 3.1145071983337402,
      "learning_rate": 7.143799472295514e-05,
      "loss": 0.242,
      "step": 1300
    },
    {
      "epoch": 0.8581794195250659,
      "grad_norm": 2.505929946899414,
      "learning_rate": 7.141600703605981e-05,
      "loss": 0.2399,
      "step": 1301
    },
    {
      "epoch": 0.8588390501319261,
      "grad_norm": 2.696861982345581,
      "learning_rate": 7.139401934916447e-05,
      "loss": 0.187,
      "step": 1302
    },
    {
      "epoch": 0.8594986807387863,
      "grad_norm": 2.193457841873169,
      "learning_rate": 7.137203166226914e-05,
      "loss": 0.1717,
      "step": 1303
    },
    {
      "epoch": 0.8601583113456465,
      "grad_norm": 1.7806284427642822,
      "learning_rate": 7.13500439753738e-05,
      "loss": 0.1434,
      "step": 1304
    },
    {
      "epoch": 0.8608179419525066,
      "grad_norm": 1.335363745689392,
      "learning_rate": 7.132805628847845e-05,
      "loss": 0.1176,
      "step": 1305
    },
    {
      "epoch": 0.8614775725593667,
      "grad_norm": 0.6049299240112305,
      "learning_rate": 7.130606860158313e-05,
      "loss": 0.0949,
      "step": 1306
    },
    {
      "epoch": 0.8621372031662269,
      "grad_norm": 2.297654390335083,
      "learning_rate": 7.128408091468778e-05,
      "loss": 0.1329,
      "step": 1307
    },
    {
      "epoch": 0.862796833773087,
      "grad_norm": 0.9257974624633789,
      "learning_rate": 7.126209322779244e-05,
      "loss": 0.0867,
      "step": 1308
    },
    {
      "epoch": 0.8634564643799473,
      "grad_norm": 14.553083419799805,
      "learning_rate": 7.124010554089711e-05,
      "loss": 0.9599,
      "step": 1309
    },
    {
      "epoch": 0.8641160949868074,
      "grad_norm": 0.601212739944458,
      "learning_rate": 7.121811785400177e-05,
      "loss": 0.0549,
      "step": 1310
    },
    {
      "epoch": 0.8647757255936676,
      "grad_norm": 11.095919609069824,
      "learning_rate": 7.119613016710642e-05,
      "loss": 0.9927,
      "step": 1311
    },
    {
      "epoch": 0.8654353562005277,
      "grad_norm": 9.059020042419434,
      "learning_rate": 7.117414248021108e-05,
      "loss": 0.6034,
      "step": 1312
    },
    {
      "epoch": 0.8660949868073878,
      "grad_norm": 1.2691235542297363,
      "learning_rate": 7.115215479331575e-05,
      "loss": 0.0787,
      "step": 1313
    },
    {
      "epoch": 0.866754617414248,
      "grad_norm": 17.570205688476562,
      "learning_rate": 7.113016710642041e-05,
      "loss": 1.3236,
      "step": 1314
    },
    {
      "epoch": 0.8674142480211082,
      "grad_norm": 15.04443359375,
      "learning_rate": 7.110817941952507e-05,
      "loss": 2.0051,
      "step": 1315
    },
    {
      "epoch": 0.8680738786279684,
      "grad_norm": 8.1328763961792,
      "learning_rate": 7.108619173262972e-05,
      "loss": 0.4223,
      "step": 1316
    },
    {
      "epoch": 0.8687335092348285,
      "grad_norm": 1.3752127885818481,
      "learning_rate": 7.106420404573439e-05,
      "loss": 0.0682,
      "step": 1317
    },
    {
      "epoch": 0.8693931398416886,
      "grad_norm": 14.429526329040527,
      "learning_rate": 7.104221635883905e-05,
      "loss": 0.4812,
      "step": 1318
    },
    {
      "epoch": 0.8700527704485488,
      "grad_norm": 10.521676063537598,
      "learning_rate": 7.10202286719437e-05,
      "loss": 0.8688,
      "step": 1319
    },
    {
      "epoch": 0.8707124010554089,
      "grad_norm": 4.552806377410889,
      "learning_rate": 7.099824098504838e-05,
      "loss": 0.1352,
      "step": 1320
    },
    {
      "epoch": 0.8713720316622692,
      "grad_norm": 6.958561897277832,
      "learning_rate": 7.097625329815303e-05,
      "loss": 0.3895,
      "step": 1321
    },
    {
      "epoch": 0.8720316622691293,
      "grad_norm": 4.6454644203186035,
      "learning_rate": 7.095426561125769e-05,
      "loss": 0.2103,
      "step": 1322
    },
    {
      "epoch": 0.8726912928759895,
      "grad_norm": 1.5674951076507568,
      "learning_rate": 7.093227792436236e-05,
      "loss": 0.1238,
      "step": 1323
    },
    {
      "epoch": 0.8733509234828496,
      "grad_norm": 1.044966220855713,
      "learning_rate": 7.091029023746702e-05,
      "loss": 0.0967,
      "step": 1324
    },
    {
      "epoch": 0.8740105540897097,
      "grad_norm": 1.6588327884674072,
      "learning_rate": 7.088830255057169e-05,
      "loss": 0.13,
      "step": 1325
    },
    {
      "epoch": 0.8746701846965699,
      "grad_norm": 5.550680637359619,
      "learning_rate": 7.086631486367635e-05,
      "loss": 0.5382,
      "step": 1326
    },
    {
      "epoch": 0.8753298153034301,
      "grad_norm": 8.825727462768555,
      "learning_rate": 7.084432717678102e-05,
      "loss": 0.7939,
      "step": 1327
    },
    {
      "epoch": 0.8759894459102903,
      "grad_norm": 2.121044397354126,
      "learning_rate": 7.082233948988567e-05,
      "loss": 0.1944,
      "step": 1328
    },
    {
      "epoch": 0.8766490765171504,
      "grad_norm": 1.7348387241363525,
      "learning_rate": 7.080035180299033e-05,
      "loss": 0.1624,
      "step": 1329
    },
    {
      "epoch": 0.8773087071240105,
      "grad_norm": 1.6122055053710938,
      "learning_rate": 7.077836411609499e-05,
      "loss": 0.2106,
      "step": 1330
    },
    {
      "epoch": 0.8779683377308707,
      "grad_norm": 2.1397647857666016,
      "learning_rate": 7.075637642919966e-05,
      "loss": 0.2306,
      "step": 1331
    },
    {
      "epoch": 0.8786279683377308,
      "grad_norm": 1.3226624727249146,
      "learning_rate": 7.073438874230432e-05,
      "loss": 0.139,
      "step": 1332
    },
    {
      "epoch": 0.8792875989445911,
      "grad_norm": 1.488722324371338,
      "learning_rate": 7.071240105540897e-05,
      "loss": 0.16,
      "step": 1333
    },
    {
      "epoch": 0.8799472295514512,
      "grad_norm": 6.499186992645264,
      "learning_rate": 7.069041336851363e-05,
      "loss": 0.3542,
      "step": 1334
    },
    {
      "epoch": 0.8806068601583114,
      "grad_norm": 12.483899116516113,
      "learning_rate": 7.06684256816183e-05,
      "loss": 0.7534,
      "step": 1335
    },
    {
      "epoch": 0.8812664907651715,
      "grad_norm": 5.885683536529541,
      "learning_rate": 7.064643799472296e-05,
      "loss": 0.5699,
      "step": 1336
    },
    {
      "epoch": 0.8819261213720316,
      "grad_norm": 1.3030096292495728,
      "learning_rate": 7.062445030782761e-05,
      "loss": 0.1287,
      "step": 1337
    },
    {
      "epoch": 0.8825857519788918,
      "grad_norm": 1.955776572227478,
      "learning_rate": 7.060246262093228e-05,
      "loss": 0.1658,
      "step": 1338
    },
    {
      "epoch": 0.883245382585752,
      "grad_norm": 1.0527215003967285,
      "learning_rate": 7.058047493403694e-05,
      "loss": 0.1398,
      "step": 1339
    },
    {
      "epoch": 0.8839050131926122,
      "grad_norm": 1.9756780862808228,
      "learning_rate": 7.05584872471416e-05,
      "loss": 0.1559,
      "step": 1340
    },
    {
      "epoch": 0.8845646437994723,
      "grad_norm": 1.8005578517913818,
      "learning_rate": 7.053649956024626e-05,
      "loss": 0.1157,
      "step": 1341
    },
    {
      "epoch": 0.8852242744063324,
      "grad_norm": 5.034745693206787,
      "learning_rate": 7.051451187335093e-05,
      "loss": 0.1693,
      "step": 1342
    },
    {
      "epoch": 0.8858839050131926,
      "grad_norm": 5.521919250488281,
      "learning_rate": 7.049252418645558e-05,
      "loss": 0.1511,
      "step": 1343
    },
    {
      "epoch": 0.8865435356200527,
      "grad_norm": 1.5968241691589355,
      "learning_rate": 7.047053649956025e-05,
      "loss": 0.129,
      "step": 1344
    },
    {
      "epoch": 0.887203166226913,
      "grad_norm": 8.477872848510742,
      "learning_rate": 7.044854881266491e-05,
      "loss": 0.3725,
      "step": 1345
    },
    {
      "epoch": 0.8878627968337731,
      "grad_norm": 1.8259507417678833,
      "learning_rate": 7.042656112576958e-05,
      "loss": 0.0994,
      "step": 1346
    },
    {
      "epoch": 0.8885224274406333,
      "grad_norm": 1.101580262184143,
      "learning_rate": 7.040457343887424e-05,
      "loss": 0.0906,
      "step": 1347
    },
    {
      "epoch": 0.8891820580474934,
      "grad_norm": 10.306282997131348,
      "learning_rate": 7.03825857519789e-05,
      "loss": 0.8782,
      "step": 1348
    },
    {
      "epoch": 0.8898416886543535,
      "grad_norm": 15.526986122131348,
      "learning_rate": 7.036059806508357e-05,
      "loss": 0.6501,
      "step": 1349
    },
    {
      "epoch": 0.8905013192612137,
      "grad_norm": 0.883723258972168,
      "learning_rate": 7.033861037818822e-05,
      "loss": 0.0598,
      "step": 1350
    },
    {
      "epoch": 0.8911609498680739,
      "grad_norm": 8.182527542114258,
      "learning_rate": 7.031662269129288e-05,
      "loss": 0.5607,
      "step": 1351
    },
    {
      "epoch": 0.8918205804749341,
      "grad_norm": 1.6464027166366577,
      "learning_rate": 7.029463500439754e-05,
      "loss": 0.1298,
      "step": 1352
    },
    {
      "epoch": 0.8924802110817942,
      "grad_norm": 14.568886756896973,
      "learning_rate": 7.02726473175022e-05,
      "loss": 0.8604,
      "step": 1353
    },
    {
      "epoch": 0.8931398416886543,
      "grad_norm": 9.215087890625,
      "learning_rate": 7.025065963060686e-05,
      "loss": 0.8393,
      "step": 1354
    },
    {
      "epoch": 0.8937994722955145,
      "grad_norm": 7.827467918395996,
      "learning_rate": 7.022867194371152e-05,
      "loss": 0.317,
      "step": 1355
    },
    {
      "epoch": 0.8944591029023746,
      "grad_norm": 3.778693914413452,
      "learning_rate": 7.020668425681619e-05,
      "loss": 0.2063,
      "step": 1356
    },
    {
      "epoch": 0.8951187335092349,
      "grad_norm": 1.649800181388855,
      "learning_rate": 7.018469656992085e-05,
      "loss": 0.1332,
      "step": 1357
    },
    {
      "epoch": 0.895778364116095,
      "grad_norm": 5.574481010437012,
      "learning_rate": 7.01627088830255e-05,
      "loss": 0.3543,
      "step": 1358
    },
    {
      "epoch": 0.8964379947229552,
      "grad_norm": 2.9780945777893066,
      "learning_rate": 7.014072119613016e-05,
      "loss": 0.2355,
      "step": 1359
    },
    {
      "epoch": 0.8970976253298153,
      "grad_norm": 1.052825927734375,
      "learning_rate": 7.011873350923483e-05,
      "loss": 0.1349,
      "step": 1360
    },
    {
      "epoch": 0.8977572559366754,
      "grad_norm": 14.091297149658203,
      "learning_rate": 7.009674582233949e-05,
      "loss": 0.477,
      "step": 1361
    },
    {
      "epoch": 0.8984168865435356,
      "grad_norm": 1.6434860229492188,
      "learning_rate": 7.007475813544415e-05,
      "loss": 0.152,
      "step": 1362
    },
    {
      "epoch": 0.8990765171503958,
      "grad_norm": 7.155645370483398,
      "learning_rate": 7.005277044854882e-05,
      "loss": 0.279,
      "step": 1363
    },
    {
      "epoch": 0.899736147757256,
      "grad_norm": 7.262573719024658,
      "learning_rate": 7.003078276165347e-05,
      "loss": 0.6331,
      "step": 1364
    },
    {
      "epoch": 0.9003957783641161,
      "grad_norm": 3.773361921310425,
      "learning_rate": 7.000879507475813e-05,
      "loss": 0.3248,
      "step": 1365
    },
    {
      "epoch": 0.9010554089709762,
      "grad_norm": 6.343410015106201,
      "learning_rate": 6.99868073878628e-05,
      "loss": 0.3485,
      "step": 1366
    },
    {
      "epoch": 0.9017150395778364,
      "grad_norm": 1.284759283065796,
      "learning_rate": 6.996481970096747e-05,
      "loss": 0.1925,
      "step": 1367
    },
    {
      "epoch": 0.9023746701846965,
      "grad_norm": 1.0382758378982544,
      "learning_rate": 6.994283201407213e-05,
      "loss": 0.1497,
      "step": 1368
    },
    {
      "epoch": 0.9030343007915568,
      "grad_norm": 1.4643335342407227,
      "learning_rate": 6.992084432717679e-05,
      "loss": 0.187,
      "step": 1369
    },
    {
      "epoch": 0.9036939313984169,
      "grad_norm": 6.773533344268799,
      "learning_rate": 6.989885664028144e-05,
      "loss": 0.344,
      "step": 1370
    },
    {
      "epoch": 0.9043535620052771,
      "grad_norm": 10.011722564697266,
      "learning_rate": 6.987686895338611e-05,
      "loss": 0.7951,
      "step": 1371
    },
    {
      "epoch": 0.9050131926121372,
      "grad_norm": 3.88635516166687,
      "learning_rate": 6.985488126649077e-05,
      "loss": 0.2803,
      "step": 1372
    },
    {
      "epoch": 0.9056728232189973,
      "grad_norm": 1.8780384063720703,
      "learning_rate": 6.983289357959543e-05,
      "loss": 0.1475,
      "step": 1373
    },
    {
      "epoch": 0.9063324538258575,
      "grad_norm": 2.3969504833221436,
      "learning_rate": 6.98109058927001e-05,
      "loss": 0.203,
      "step": 1374
    },
    {
      "epoch": 0.9069920844327177,
      "grad_norm": 2.9268898963928223,
      "learning_rate": 6.978891820580476e-05,
      "loss": 0.2214,
      "step": 1375
    },
    {
      "epoch": 0.9076517150395779,
      "grad_norm": 2.2129805088043213,
      "learning_rate": 6.976693051890941e-05,
      "loss": 0.2683,
      "step": 1376
    },
    {
      "epoch": 0.908311345646438,
      "grad_norm": 1.8175421953201294,
      "learning_rate": 6.974494283201407e-05,
      "loss": 0.1962,
      "step": 1377
    },
    {
      "epoch": 0.9089709762532981,
      "grad_norm": 1.6354306936264038,
      "learning_rate": 6.972295514511874e-05,
      "loss": 0.1746,
      "step": 1378
    },
    {
      "epoch": 0.9096306068601583,
      "grad_norm": 1.9106920957565308,
      "learning_rate": 6.97009674582234e-05,
      "loss": 0.2046,
      "step": 1379
    },
    {
      "epoch": 0.9102902374670184,
      "grad_norm": 1.0871697664260864,
      "learning_rate": 6.967897977132805e-05,
      "loss": 0.1396,
      "step": 1380
    },
    {
      "epoch": 0.9109498680738787,
      "grad_norm": 9.243202209472656,
      "learning_rate": 6.965699208443272e-05,
      "loss": 0.4248,
      "step": 1381
    },
    {
      "epoch": 0.9116094986807388,
      "grad_norm": 9.716400146484375,
      "learning_rate": 6.963500439753738e-05,
      "loss": 0.6353,
      "step": 1382
    },
    {
      "epoch": 0.912269129287599,
      "grad_norm": 1.5269137620925903,
      "learning_rate": 6.961301671064204e-05,
      "loss": 0.1254,
      "step": 1383
    },
    {
      "epoch": 0.9129287598944591,
      "grad_norm": 11.175103187561035,
      "learning_rate": 6.95910290237467e-05,
      "loss": 0.6459,
      "step": 1384
    },
    {
      "epoch": 0.9135883905013192,
      "grad_norm": 6.61058235168457,
      "learning_rate": 6.956904133685137e-05,
      "loss": 0.3206,
      "step": 1385
    },
    {
      "epoch": 0.9142480211081794,
      "grad_norm": 12.882307052612305,
      "learning_rate": 6.954705364995602e-05,
      "loss": 0.3598,
      "step": 1386
    },
    {
      "epoch": 0.9149076517150396,
      "grad_norm": 1.3296420574188232,
      "learning_rate": 6.952506596306069e-05,
      "loss": 0.097,
      "step": 1387
    },
    {
      "epoch": 0.9155672823218998,
      "grad_norm": 7.156212329864502,
      "learning_rate": 6.950307827616535e-05,
      "loss": 0.4558,
      "step": 1388
    },
    {
      "epoch": 0.9162269129287599,
      "grad_norm": 9.493928909301758,
      "learning_rate": 6.948109058927002e-05,
      "loss": 0.7985,
      "step": 1389
    },
    {
      "epoch": 0.91688654353562,
      "grad_norm": 1.6267640590667725,
      "learning_rate": 6.945910290237468e-05,
      "loss": 0.144,
      "step": 1390
    },
    {
      "epoch": 0.9175461741424802,
      "grad_norm": 6.859350204467773,
      "learning_rate": 6.943711521547933e-05,
      "loss": 0.3093,
      "step": 1391
    },
    {
      "epoch": 0.9182058047493403,
      "grad_norm": 8.689475059509277,
      "learning_rate": 6.9415127528584e-05,
      "loss": 0.2511,
      "step": 1392
    },
    {
      "epoch": 0.9188654353562006,
      "grad_norm": 7.370567798614502,
      "learning_rate": 6.939313984168866e-05,
      "loss": 0.398,
      "step": 1393
    },
    {
      "epoch": 0.9195250659630607,
      "grad_norm": 1.915027379989624,
      "learning_rate": 6.937115215479332e-05,
      "loss": 0.1973,
      "step": 1394
    },
    {
      "epoch": 0.9201846965699209,
      "grad_norm": 2.1181905269622803,
      "learning_rate": 6.934916446789798e-05,
      "loss": 0.1678,
      "step": 1395
    },
    {
      "epoch": 0.920844327176781,
      "grad_norm": 1.4119939804077148,
      "learning_rate": 6.932717678100265e-05,
      "loss": 0.1224,
      "step": 1396
    },
    {
      "epoch": 0.9215039577836411,
      "grad_norm": 8.195084571838379,
      "learning_rate": 6.93051890941073e-05,
      "loss": 0.4396,
      "step": 1397
    },
    {
      "epoch": 0.9221635883905013,
      "grad_norm": 2.342750072479248,
      "learning_rate": 6.928320140721196e-05,
      "loss": 0.1949,
      "step": 1398
    },
    {
      "epoch": 0.9228232189973615,
      "grad_norm": 8.263040542602539,
      "learning_rate": 6.926121372031663e-05,
      "loss": 0.9118,
      "step": 1399
    },
    {
      "epoch": 0.9234828496042217,
      "grad_norm": 8.073260307312012,
      "learning_rate": 6.923922603342129e-05,
      "loss": 0.4217,
      "step": 1400
    },
    {
      "epoch": 0.9241424802110818,
      "grad_norm": 1.8547745943069458,
      "learning_rate": 6.921723834652595e-05,
      "loss": 0.2321,
      "step": 1401
    },
    {
      "epoch": 0.924802110817942,
      "grad_norm": 5.850190162658691,
      "learning_rate": 6.91952506596306e-05,
      "loss": 0.3877,
      "step": 1402
    },
    {
      "epoch": 0.9254617414248021,
      "grad_norm": 15.066916465759277,
      "learning_rate": 6.917326297273527e-05,
      "loss": 0.5446,
      "step": 1403
    },
    {
      "epoch": 0.9261213720316622,
      "grad_norm": 2.5834875106811523,
      "learning_rate": 6.915127528583993e-05,
      "loss": 0.2179,
      "step": 1404
    },
    {
      "epoch": 0.9267810026385225,
      "grad_norm": 5.149223804473877,
      "learning_rate": 6.912928759894459e-05,
      "loss": 0.3453,
      "step": 1405
    },
    {
      "epoch": 0.9274406332453826,
      "grad_norm": 4.29860782623291,
      "learning_rate": 6.910729991204926e-05,
      "loss": 0.3036,
      "step": 1406
    },
    {
      "epoch": 0.9281002638522428,
      "grad_norm": 4.426950454711914,
      "learning_rate": 6.908531222515391e-05,
      "loss": 0.3574,
      "step": 1407
    },
    {
      "epoch": 0.9287598944591029,
      "grad_norm": 1.4163566827774048,
      "learning_rate": 6.906332453825858e-05,
      "loss": 0.1432,
      "step": 1408
    },
    {
      "epoch": 0.929419525065963,
      "grad_norm": 5.042421817779541,
      "learning_rate": 6.904133685136324e-05,
      "loss": 0.5603,
      "step": 1409
    },
    {
      "epoch": 0.9300791556728232,
      "grad_norm": 0.7768383026123047,
      "learning_rate": 6.901934916446791e-05,
      "loss": 0.0969,
      "step": 1410
    },
    {
      "epoch": 0.9307387862796834,
      "grad_norm": 9.220956802368164,
      "learning_rate": 6.899736147757257e-05,
      "loss": 0.5606,
      "step": 1411
    },
    {
      "epoch": 0.9313984168865436,
      "grad_norm": 3.6776247024536133,
      "learning_rate": 6.897537379067723e-05,
      "loss": 0.2715,
      "step": 1412
    },
    {
      "epoch": 0.9320580474934037,
      "grad_norm": 0.7779847979545593,
      "learning_rate": 6.895338610378188e-05,
      "loss": 0.1043,
      "step": 1413
    },
    {
      "epoch": 0.9327176781002638,
      "grad_norm": 1.802425503730774,
      "learning_rate": 6.893139841688655e-05,
      "loss": 0.1479,
      "step": 1414
    },
    {
      "epoch": 0.933377308707124,
      "grad_norm": 6.393472194671631,
      "learning_rate": 6.890941072999121e-05,
      "loss": 0.2325,
      "step": 1415
    },
    {
      "epoch": 0.9340369393139841,
      "grad_norm": 2.3182241916656494,
      "learning_rate": 6.888742304309587e-05,
      "loss": 0.1539,
      "step": 1416
    },
    {
      "epoch": 0.9346965699208444,
      "grad_norm": 1.9984432458877563,
      "learning_rate": 6.886543535620054e-05,
      "loss": 0.1416,
      "step": 1417
    },
    {
      "epoch": 0.9353562005277045,
      "grad_norm": 2.4230189323425293,
      "learning_rate": 6.88434476693052e-05,
      "loss": 0.2161,
      "step": 1418
    },
    {
      "epoch": 0.9360158311345647,
      "grad_norm": 3.0721182823181152,
      "learning_rate": 6.882145998240985e-05,
      "loss": 0.1622,
      "step": 1419
    },
    {
      "epoch": 0.9366754617414248,
      "grad_norm": 6.485748291015625,
      "learning_rate": 6.879947229551451e-05,
      "loss": 0.3859,
      "step": 1420
    },
    {
      "epoch": 0.9373350923482849,
      "grad_norm": 8.688453674316406,
      "learning_rate": 6.877748460861918e-05,
      "loss": 0.4692,
      "step": 1421
    },
    {
      "epoch": 0.9379947229551451,
      "grad_norm": 3.686521291732788,
      "learning_rate": 6.875549692172384e-05,
      "loss": 0.1758,
      "step": 1422
    },
    {
      "epoch": 0.9386543535620053,
      "grad_norm": 0.8288975954055786,
      "learning_rate": 6.87335092348285e-05,
      "loss": 0.1061,
      "step": 1423
    },
    {
      "epoch": 0.9393139841688655,
      "grad_norm": 1.314884066581726,
      "learning_rate": 6.871152154793315e-05,
      "loss": 0.0929,
      "step": 1424
    },
    {
      "epoch": 0.9399736147757256,
      "grad_norm": 9.108161926269531,
      "learning_rate": 6.868953386103782e-05,
      "loss": 0.8861,
      "step": 1425
    },
    {
      "epoch": 0.9406332453825857,
      "grad_norm": 3.5598487854003906,
      "learning_rate": 6.866754617414248e-05,
      "loss": 0.2297,
      "step": 1426
    },
    {
      "epoch": 0.9412928759894459,
      "grad_norm": 1.4332423210144043,
      "learning_rate": 6.864555848724713e-05,
      "loss": 0.1285,
      "step": 1427
    },
    {
      "epoch": 0.941952506596306,
      "grad_norm": 6.934497356414795,
      "learning_rate": 6.86235708003518e-05,
      "loss": 0.44,
      "step": 1428
    },
    {
      "epoch": 0.9426121372031663,
      "grad_norm": 1.583203673362732,
      "learning_rate": 6.860158311345646e-05,
      "loss": 0.1115,
      "step": 1429
    },
    {
      "epoch": 0.9432717678100264,
      "grad_norm": 10.974326133728027,
      "learning_rate": 6.857959542656113e-05,
      "loss": 1.0701,
      "step": 1430
    },
    {
      "epoch": 0.9439313984168866,
      "grad_norm": 2.9200496673583984,
      "learning_rate": 6.855760773966579e-05,
      "loss": 0.2252,
      "step": 1431
    },
    {
      "epoch": 0.9445910290237467,
      "grad_norm": 1.5554180145263672,
      "learning_rate": 6.853562005277046e-05,
      "loss": 0.1215,
      "step": 1432
    },
    {
      "epoch": 0.9452506596306068,
      "grad_norm": 1.8715732097625732,
      "learning_rate": 6.851363236587512e-05,
      "loss": 0.1302,
      "step": 1433
    },
    {
      "epoch": 0.945910290237467,
      "grad_norm": 2.8289904594421387,
      "learning_rate": 6.849164467897977e-05,
      "loss": 0.1527,
      "step": 1434
    },
    {
      "epoch": 0.9465699208443272,
      "grad_norm": 0.7461877465248108,
      "learning_rate": 6.846965699208445e-05,
      "loss": 0.0836,
      "step": 1435
    },
    {
      "epoch": 0.9472295514511874,
      "grad_norm": 10.16466999053955,
      "learning_rate": 6.84476693051891e-05,
      "loss": 1.1621,
      "step": 1436
    },
    {
      "epoch": 0.9478891820580475,
      "grad_norm": 1.7888789176940918,
      "learning_rate": 6.842568161829376e-05,
      "loss": 0.0899,
      "step": 1437
    },
    {
      "epoch": 0.9485488126649076,
      "grad_norm": 17.859243392944336,
      "learning_rate": 6.840369393139842e-05,
      "loss": 0.9886,
      "step": 1438
    },
    {
      "epoch": 0.9492084432717678,
      "grad_norm": 0.4080580174922943,
      "learning_rate": 6.838170624450309e-05,
      "loss": 0.0531,
      "step": 1439
    },
    {
      "epoch": 0.9498680738786279,
      "grad_norm": 1.2268081903457642,
      "learning_rate": 6.835971855760774e-05,
      "loss": 0.1145,
      "step": 1440
    },
    {
      "epoch": 0.9505277044854882,
      "grad_norm": 12.4158935546875,
      "learning_rate": 6.83377308707124e-05,
      "loss": 0.5261,
      "step": 1441
    },
    {
      "epoch": 0.9511873350923483,
      "grad_norm": 1.244718313217163,
      "learning_rate": 6.831574318381706e-05,
      "loss": 0.0879,
      "step": 1442
    },
    {
      "epoch": 0.9518469656992085,
      "grad_norm": 1.0271930694580078,
      "learning_rate": 6.829375549692173e-05,
      "loss": 0.0946,
      "step": 1443
    },
    {
      "epoch": 0.9525065963060686,
      "grad_norm": 0.8243055939674377,
      "learning_rate": 6.827176781002638e-05,
      "loss": 0.074,
      "step": 1444
    },
    {
      "epoch": 0.9531662269129287,
      "grad_norm": 1.5701357126235962,
      "learning_rate": 6.824978012313104e-05,
      "loss": 0.1057,
      "step": 1445
    },
    {
      "epoch": 0.9538258575197889,
      "grad_norm": 3.764120101928711,
      "learning_rate": 6.822779243623571e-05,
      "loss": 0.1242,
      "step": 1446
    },
    {
      "epoch": 0.9544854881266491,
      "grad_norm": 9.869458198547363,
      "learning_rate": 6.820580474934037e-05,
      "loss": 0.7837,
      "step": 1447
    },
    {
      "epoch": 0.9551451187335093,
      "grad_norm": 14.509734153747559,
      "learning_rate": 6.818381706244503e-05,
      "loss": 0.9897,
      "step": 1448
    },
    {
      "epoch": 0.9558047493403694,
      "grad_norm": 10.812620162963867,
      "learning_rate": 6.81618293755497e-05,
      "loss": 0.3179,
      "step": 1449
    },
    {
      "epoch": 0.9564643799472295,
      "grad_norm": 17.704566955566406,
      "learning_rate": 6.813984168865435e-05,
      "loss": 0.7873,
      "step": 1450
    },
    {
      "epoch": 0.9571240105540897,
      "grad_norm": 10.876144409179688,
      "learning_rate": 6.811785400175902e-05,
      "loss": 0.3212,
      "step": 1451
    },
    {
      "epoch": 0.9577836411609498,
      "grad_norm": 1.9635539054870605,
      "learning_rate": 6.809586631486368e-05,
      "loss": 0.1162,
      "step": 1452
    },
    {
      "epoch": 0.9584432717678101,
      "grad_norm": 9.559569358825684,
      "learning_rate": 6.807387862796835e-05,
      "loss": 0.6431,
      "step": 1453
    },
    {
      "epoch": 0.9591029023746702,
      "grad_norm": 2.624593496322632,
      "learning_rate": 6.805189094107301e-05,
      "loss": 0.1664,
      "step": 1454
    },
    {
      "epoch": 0.9597625329815304,
      "grad_norm": 3.5027010440826416,
      "learning_rate": 6.802990325417767e-05,
      "loss": 0.2677,
      "step": 1455
    },
    {
      "epoch": 0.9604221635883905,
      "grad_norm": 0.5535932779312134,
      "learning_rate": 6.800791556728232e-05,
      "loss": 0.0775,
      "step": 1456
    },
    {
      "epoch": 0.9610817941952506,
      "grad_norm": 0.688014566898346,
      "learning_rate": 6.7985927880387e-05,
      "loss": 0.0641,
      "step": 1457
    },
    {
      "epoch": 0.9617414248021108,
      "grad_norm": 1.3058034181594849,
      "learning_rate": 6.796394019349165e-05,
      "loss": 0.0958,
      "step": 1458
    },
    {
      "epoch": 0.962401055408971,
      "grad_norm": 2.560743808746338,
      "learning_rate": 6.794195250659631e-05,
      "loss": 0.1772,
      "step": 1459
    },
    {
      "epoch": 0.9630606860158312,
      "grad_norm": 0.518740177154541,
      "learning_rate": 6.791996481970096e-05,
      "loss": 0.0614,
      "step": 1460
    },
    {
      "epoch": 0.9637203166226913,
      "grad_norm": 10.888813972473145,
      "learning_rate": 6.789797713280563e-05,
      "loss": 0.7751,
      "step": 1461
    },
    {
      "epoch": 0.9643799472295514,
      "grad_norm": 8.2644624710083,
      "learning_rate": 6.787598944591029e-05,
      "loss": 1.0889,
      "step": 1462
    },
    {
      "epoch": 0.9650395778364116,
      "grad_norm": 12.899700164794922,
      "learning_rate": 6.785400175901495e-05,
      "loss": 0.5416,
      "step": 1463
    },
    {
      "epoch": 0.9656992084432717,
      "grad_norm": 8.39379596710205,
      "learning_rate": 6.783201407211962e-05,
      "loss": 0.618,
      "step": 1464
    },
    {
      "epoch": 0.966358839050132,
      "grad_norm": 8.322723388671875,
      "learning_rate": 6.781002638522428e-05,
      "loss": 0.3678,
      "step": 1465
    },
    {
      "epoch": 0.9670184696569921,
      "grad_norm": 9.560200691223145,
      "learning_rate": 6.778803869832893e-05,
      "loss": 0.4369,
      "step": 1466
    },
    {
      "epoch": 0.9676781002638523,
      "grad_norm": 2.0044496059417725,
      "learning_rate": 6.776605101143359e-05,
      "loss": 0.1595,
      "step": 1467
    },
    {
      "epoch": 0.9683377308707124,
      "grad_norm": 2.792081117630005,
      "learning_rate": 6.774406332453826e-05,
      "loss": 0.1681,
      "step": 1468
    },
    {
      "epoch": 0.9689973614775725,
      "grad_norm": 0.9214077591896057,
      "learning_rate": 6.772207563764292e-05,
      "loss": 0.0994,
      "step": 1469
    },
    {
      "epoch": 0.9696569920844327,
      "grad_norm": 1.334612488746643,
      "learning_rate": 6.770008795074757e-05,
      "loss": 0.1524,
      "step": 1470
    },
    {
      "epoch": 0.9703166226912929,
      "grad_norm": 1.475476861000061,
      "learning_rate": 6.767810026385225e-05,
      "loss": 0.1815,
      "step": 1471
    },
    {
      "epoch": 0.9709762532981531,
      "grad_norm": 3.212242364883423,
      "learning_rate": 6.765611257695692e-05,
      "loss": 0.2127,
      "step": 1472
    },
    {
      "epoch": 0.9716358839050132,
      "grad_norm": 7.480788707733154,
      "learning_rate": 6.763412489006157e-05,
      "loss": 0.8529,
      "step": 1473
    },
    {
      "epoch": 0.9722955145118733,
      "grad_norm": 2.2733495235443115,
      "learning_rate": 6.761213720316623e-05,
      "loss": 0.1598,
      "step": 1474
    },
    {
      "epoch": 0.9729551451187335,
      "grad_norm": 3.984562397003174,
      "learning_rate": 6.75901495162709e-05,
      "loss": 0.2661,
      "step": 1475
    },
    {
      "epoch": 0.9736147757255936,
      "grad_norm": 1.6655969619750977,
      "learning_rate": 6.756816182937556e-05,
      "loss": 0.1472,
      "step": 1476
    },
    {
      "epoch": 0.9742744063324539,
      "grad_norm": 2.278970241546631,
      "learning_rate": 6.754617414248021e-05,
      "loss": 0.136,
      "step": 1477
    },
    {
      "epoch": 0.974934036939314,
      "grad_norm": 3.410001516342163,
      "learning_rate": 6.752418645558487e-05,
      "loss": 0.2405,
      "step": 1478
    },
    {
      "epoch": 0.9755936675461742,
      "grad_norm": 2.948889970779419,
      "learning_rate": 6.750219876868954e-05,
      "loss": 0.1313,
      "step": 1479
    },
    {
      "epoch": 0.9762532981530343,
      "grad_norm": 1.9612771272659302,
      "learning_rate": 6.74802110817942e-05,
      "loss": 0.1412,
      "step": 1480
    },
    {
      "epoch": 0.9769129287598944,
      "grad_norm": 12.548494338989258,
      "learning_rate": 6.745822339489886e-05,
      "loss": 0.631,
      "step": 1481
    },
    {
      "epoch": 0.9775725593667546,
      "grad_norm": 5.040074348449707,
      "learning_rate": 6.743623570800353e-05,
      "loss": 0.2605,
      "step": 1482
    },
    {
      "epoch": 0.9782321899736148,
      "grad_norm": 1.060186505317688,
      "learning_rate": 6.741424802110818e-05,
      "loss": 0.0922,
      "step": 1483
    },
    {
      "epoch": 0.978891820580475,
      "grad_norm": 6.864157676696777,
      "learning_rate": 6.739226033421284e-05,
      "loss": 0.311,
      "step": 1484
    },
    {
      "epoch": 0.9795514511873351,
      "grad_norm": 2.1307573318481445,
      "learning_rate": 6.73702726473175e-05,
      "loss": 0.1292,
      "step": 1485
    },
    {
      "epoch": 0.9802110817941952,
      "grad_norm": 1.1524978876113892,
      "learning_rate": 6.734828496042217e-05,
      "loss": 0.1352,
      "step": 1486
    },
    {
      "epoch": 0.9808707124010554,
      "grad_norm": 1.9336726665496826,
      "learning_rate": 6.732629727352682e-05,
      "loss": 0.2072,
      "step": 1487
    },
    {
      "epoch": 0.9815303430079155,
      "grad_norm": 1.9258157014846802,
      "learning_rate": 6.730430958663148e-05,
      "loss": 0.1504,
      "step": 1488
    },
    {
      "epoch": 0.9821899736147758,
      "grad_norm": 3.2280192375183105,
      "learning_rate": 6.728232189973615e-05,
      "loss": 0.1399,
      "step": 1489
    },
    {
      "epoch": 0.9828496042216359,
      "grad_norm": 4.712566375732422,
      "learning_rate": 6.726033421284081e-05,
      "loss": 0.293,
      "step": 1490
    },
    {
      "epoch": 0.9835092348284961,
      "grad_norm": 1.3892525434494019,
      "learning_rate": 6.723834652594547e-05,
      "loss": 0.1241,
      "step": 1491
    },
    {
      "epoch": 0.9841688654353562,
      "grad_norm": 1.191867470741272,
      "learning_rate": 6.721635883905014e-05,
      "loss": 0.0996,
      "step": 1492
    },
    {
      "epoch": 0.9848284960422163,
      "grad_norm": 3.2587718963623047,
      "learning_rate": 6.71943711521548e-05,
      "loss": 0.2404,
      "step": 1493
    },
    {
      "epoch": 0.9854881266490765,
      "grad_norm": 1.0475373268127441,
      "learning_rate": 6.717238346525946e-05,
      "loss": 0.1147,
      "step": 1494
    },
    {
      "epoch": 0.9861477572559367,
      "grad_norm": 1.3513569831848145,
      "learning_rate": 6.715039577836412e-05,
      "loss": 0.0984,
      "step": 1495
    },
    {
      "epoch": 0.9868073878627969,
      "grad_norm": 7.677353858947754,
      "learning_rate": 6.712840809146878e-05,
      "loss": 0.268,
      "step": 1496
    },
    {
      "epoch": 0.987467018469657,
      "grad_norm": 8.097137451171875,
      "learning_rate": 6.710642040457345e-05,
      "loss": 0.3119,
      "step": 1497
    },
    {
      "epoch": 0.9881266490765171,
      "grad_norm": 6.218151569366455,
      "learning_rate": 6.70844327176781e-05,
      "loss": 0.2333,
      "step": 1498
    },
    {
      "epoch": 0.9887862796833773,
      "grad_norm": 2.4671053886413574,
      "learning_rate": 6.706244503078276e-05,
      "loss": 0.125,
      "step": 1499
    },
    {
      "epoch": 0.9894459102902374,
      "grad_norm": 9.668359756469727,
      "learning_rate": 6.704045734388743e-05,
      "loss": 0.4691,
      "step": 1500
    },
    {
      "epoch": 0.9901055408970977,
      "grad_norm": 11.276871681213379,
      "learning_rate": 6.701846965699209e-05,
      "loss": 0.6571,
      "step": 1501
    },
    {
      "epoch": 0.9907651715039578,
      "grad_norm": 4.616742134094238,
      "learning_rate": 6.699648197009675e-05,
      "loss": 0.2581,
      "step": 1502
    },
    {
      "epoch": 0.991424802110818,
      "grad_norm": 16.94306755065918,
      "learning_rate": 6.69744942832014e-05,
      "loss": 1.9458,
      "step": 1503
    },
    {
      "epoch": 0.9920844327176781,
      "grad_norm": 10.752487182617188,
      "learning_rate": 6.695250659630607e-05,
      "loss": 1.395,
      "step": 1504
    },
    {
      "epoch": 0.9927440633245382,
      "grad_norm": 3.353755474090576,
      "learning_rate": 6.693051890941073e-05,
      "loss": 0.2281,
      "step": 1505
    },
    {
      "epoch": 0.9934036939313984,
      "grad_norm": 1.7550222873687744,
      "learning_rate": 6.690853122251539e-05,
      "loss": 0.1397,
      "step": 1506
    },
    {
      "epoch": 0.9940633245382586,
      "grad_norm": 2.267163038253784,
      "learning_rate": 6.688654353562006e-05,
      "loss": 0.1605,
      "step": 1507
    },
    {
      "epoch": 0.9947229551451188,
      "grad_norm": 1.2815767526626587,
      "learning_rate": 6.686455584872472e-05,
      "loss": 0.1511,
      "step": 1508
    },
    {
      "epoch": 0.9953825857519789,
      "grad_norm": 0.7788172960281372,
      "learning_rate": 6.684256816182937e-05,
      "loss": 0.0566,
      "step": 1509
    },
    {
      "epoch": 0.996042216358839,
      "grad_norm": 0.5057218670845032,
      "learning_rate": 6.682058047493403e-05,
      "loss": 0.0682,
      "step": 1510
    },
    {
      "epoch": 0.9967018469656992,
      "grad_norm": 3.1654469966888428,
      "learning_rate": 6.67985927880387e-05,
      "loss": 0.2274,
      "step": 1511
    },
    {
      "epoch": 0.9973614775725593,
      "grad_norm": 8.347930908203125,
      "learning_rate": 6.677660510114336e-05,
      "loss": 1.1304,
      "step": 1512
    },
    {
      "epoch": 0.9980211081794196,
      "grad_norm": 4.7719621658325195,
      "learning_rate": 6.675461741424803e-05,
      "loss": 0.3275,
      "step": 1513
    },
    {
      "epoch": 0.9986807387862797,
      "grad_norm": 5.818892478942871,
      "learning_rate": 6.673262972735269e-05,
      "loss": 0.3219,
      "step": 1514
    },
    {
      "epoch": 0.9993403693931399,
      "grad_norm": 8.373933792114258,
      "learning_rate": 6.671064204045736e-05,
      "loss": 0.728,
      "step": 1515
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.753194272518158,
      "learning_rate": 6.668865435356201e-05,
      "loss": 0.0812,
      "step": 1516
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8527131782945736,
      "eval_f1": 0.8395102147721321,
      "eval_keras_BCE": 0.41633373498916626,
      "eval_loss": 0.42882341146469116,
      "eval_precision": 0.8089653874727406,
      "eval_recall": 0.8295454545454546,
      "eval_runtime": 3.9185,
      "eval_samples_per_second": 65.842,
      "eval_steps_per_second": 8.422,
      "eval_weighted BCE": 27.300519943237305,
      "step": 1516
    }
  ],
  "logging_steps": 1,
  "max_steps": 4548,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3629529586257306e+17,
  "train_batch_size": 256,
  "trial_name": null,
  "trial_params": null
}
