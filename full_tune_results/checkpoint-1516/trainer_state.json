{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1516,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006596306068601583,
      "grad_norm": 35.9672966003418,
      "learning_rate": 0.0002,
      "loss": 3.1918,
      "step": 1
    },
    {
      "epoch": 0.0013192612137203166,
      "grad_norm": 18.360219955444336,
      "learning_rate": 0.00019995602462620932,
      "loss": 1.0186,
      "step": 2
    },
    {
      "epoch": 0.001978891820580475,
      "grad_norm": 3.370255947113037,
      "learning_rate": 0.00019991204925241866,
      "loss": 0.1426,
      "step": 3
    },
    {
      "epoch": 0.002638522427440633,
      "grad_norm": 0.5084718465805054,
      "learning_rate": 0.00019986807387862798,
      "loss": 0.0656,
      "step": 4
    },
    {
      "epoch": 0.0032981530343007917,
      "grad_norm": 0.1736038774251938,
      "learning_rate": 0.0001998240985048373,
      "loss": 0.0325,
      "step": 5
    },
    {
      "epoch": 0.00395778364116095,
      "grad_norm": 0.4496656060218811,
      "learning_rate": 0.0001997801231310466,
      "loss": 0.0825,
      "step": 6
    },
    {
      "epoch": 0.004617414248021108,
      "grad_norm": 0.7282611131668091,
      "learning_rate": 0.00019973614775725595,
      "loss": 0.1465,
      "step": 7
    },
    {
      "epoch": 0.005277044854881266,
      "grad_norm": 0.16678373515605927,
      "learning_rate": 0.00019969217238346526,
      "loss": 0.0369,
      "step": 8
    },
    {
      "epoch": 0.005936675461741424,
      "grad_norm": 0.3966132700443268,
      "learning_rate": 0.00019964819700967458,
      "loss": 0.0981,
      "step": 9
    },
    {
      "epoch": 0.006596306068601583,
      "grad_norm": 0.47954031825065613,
      "learning_rate": 0.00019960422163588392,
      "loss": 0.1238,
      "step": 10
    },
    {
      "epoch": 0.007255936675461741,
      "grad_norm": 0.08518762141466141,
      "learning_rate": 0.00019956024626209323,
      "loss": 0.0272,
      "step": 11
    },
    {
      "epoch": 0.0079155672823219,
      "grad_norm": 0.2607460021972656,
      "learning_rate": 0.00019951627088830257,
      "loss": 0.0702,
      "step": 12
    },
    {
      "epoch": 0.008575197889182058,
      "grad_norm": 1.52717924118042,
      "learning_rate": 0.00019947229551451189,
      "loss": 0.1037,
      "step": 13
    },
    {
      "epoch": 0.009234828496042216,
      "grad_norm": 0.12274033576250076,
      "learning_rate": 0.00019942832014072123,
      "loss": 0.0635,
      "step": 14
    },
    {
      "epoch": 0.009894459102902375,
      "grad_norm": 0.28364449739456177,
      "learning_rate": 0.00019938434476693054,
      "loss": 0.0142,
      "step": 15
    },
    {
      "epoch": 0.010554089709762533,
      "grad_norm": 0.19363075494766235,
      "learning_rate": 0.00019934036939313985,
      "loss": 0.0485,
      "step": 16
    },
    {
      "epoch": 0.011213720316622692,
      "grad_norm": 0.2935446500778198,
      "learning_rate": 0.00019929639401934917,
      "loss": 0.0489,
      "step": 17
    },
    {
      "epoch": 0.011873350923482849,
      "grad_norm": 0.09788938611745834,
      "learning_rate": 0.0001992524186455585,
      "loss": 0.0953,
      "step": 18
    },
    {
      "epoch": 0.012532981530343008,
      "grad_norm": 0.25649335980415344,
      "learning_rate": 0.00019920844327176782,
      "loss": 0.0534,
      "step": 19
    },
    {
      "epoch": 0.013192612137203167,
      "grad_norm": 0.25099486112594604,
      "learning_rate": 0.00019916446789797714,
      "loss": 0.034,
      "step": 20
    },
    {
      "epoch": 0.013852242744063324,
      "grad_norm": 0.24971282482147217,
      "learning_rate": 0.00019912049252418648,
      "loss": 0.0131,
      "step": 21
    },
    {
      "epoch": 0.014511873350923483,
      "grad_norm": 0.1269960254430771,
      "learning_rate": 0.0001990765171503958,
      "loss": 0.0774,
      "step": 22
    },
    {
      "epoch": 0.015171503957783642,
      "grad_norm": 0.26265963912010193,
      "learning_rate": 0.0001990325417766051,
      "loss": 0.1056,
      "step": 23
    },
    {
      "epoch": 0.0158311345646438,
      "grad_norm": 0.19881552457809448,
      "learning_rate": 0.00019898856640281442,
      "loss": 0.0851,
      "step": 24
    },
    {
      "epoch": 0.016490765171503958,
      "grad_norm": 0.2981855571269989,
      "learning_rate": 0.00019894459102902376,
      "loss": 0.1113,
      "step": 25
    },
    {
      "epoch": 0.017150395778364115,
      "grad_norm": 0.15118996798992157,
      "learning_rate": 0.00019890061565523308,
      "loss": 0.0078,
      "step": 26
    },
    {
      "epoch": 0.017810026385224276,
      "grad_norm": 0.09990768879652023,
      "learning_rate": 0.0001988566402814424,
      "loss": 0.0644,
      "step": 27
    },
    {
      "epoch": 0.018469656992084433,
      "grad_norm": 0.15570694208145142,
      "learning_rate": 0.00019881266490765173,
      "loss": 0.0825,
      "step": 28
    },
    {
      "epoch": 0.01912928759894459,
      "grad_norm": 0.139003187417984,
      "learning_rate": 0.00019876868953386104,
      "loss": 0.0811,
      "step": 29
    },
    {
      "epoch": 0.01978891820580475,
      "grad_norm": 0.09388700127601624,
      "learning_rate": 0.00019872471416007036,
      "loss": 0.0606,
      "step": 30
    },
    {
      "epoch": 0.020448548812664908,
      "grad_norm": 0.08489011973142624,
      "learning_rate": 0.00019868073878627967,
      "loss": 0.0649,
      "step": 31
    },
    {
      "epoch": 0.021108179419525065,
      "grad_norm": 0.14103291928768158,
      "learning_rate": 0.000198636763412489,
      "loss": 0.0496,
      "step": 32
    },
    {
      "epoch": 0.021767810026385226,
      "grad_norm": 0.17318949103355408,
      "learning_rate": 0.00019859278803869833,
      "loss": 0.0492,
      "step": 33
    },
    {
      "epoch": 0.022427440633245383,
      "grad_norm": 0.07044827938079834,
      "learning_rate": 0.00019854881266490767,
      "loss": 0.0762,
      "step": 34
    },
    {
      "epoch": 0.02308707124010554,
      "grad_norm": 0.1580686867237091,
      "learning_rate": 0.00019850483729111698,
      "loss": 0.0499,
      "step": 35
    },
    {
      "epoch": 0.023746701846965697,
      "grad_norm": 0.11848395317792892,
      "learning_rate": 0.00019846086191732632,
      "loss": 0.0473,
      "step": 36
    },
    {
      "epoch": 0.024406332453825858,
      "grad_norm": 0.06072632223367691,
      "learning_rate": 0.00019841688654353564,
      "loss": 0.063,
      "step": 37
    },
    {
      "epoch": 0.025065963060686015,
      "grad_norm": 0.06243116408586502,
      "learning_rate": 0.00019837291116974495,
      "loss": 0.0448,
      "step": 38
    },
    {
      "epoch": 0.025725593667546173,
      "grad_norm": 0.04957958310842514,
      "learning_rate": 0.0001983289357959543,
      "loss": 0.0447,
      "step": 39
    },
    {
      "epoch": 0.026385224274406333,
      "grad_norm": 0.2380913943052292,
      "learning_rate": 0.0001982849604221636,
      "loss": 0.1005,
      "step": 40
    },
    {
      "epoch": 0.02704485488126649,
      "grad_norm": 0.07386821508407593,
      "learning_rate": 0.00019824098504837292,
      "loss": 0.0255,
      "step": 41
    },
    {
      "epoch": 0.027704485488126648,
      "grad_norm": 0.07311496138572693,
      "learning_rate": 0.00019819700967458223,
      "loss": 0.0259,
      "step": 42
    },
    {
      "epoch": 0.02836411609498681,
      "grad_norm": 0.07991109788417816,
      "learning_rate": 0.00019815303430079158,
      "loss": 0.0469,
      "step": 43
    },
    {
      "epoch": 0.029023746701846966,
      "grad_norm": 0.060403767973184586,
      "learning_rate": 0.0001981090589270009,
      "loss": 0.0237,
      "step": 44
    },
    {
      "epoch": 0.029683377308707123,
      "grad_norm": 0.24045729637145996,
      "learning_rate": 0.0001980650835532102,
      "loss": 0.0998,
      "step": 45
    },
    {
      "epoch": 0.030343007915567283,
      "grad_norm": 0.13080666959285736,
      "learning_rate": 0.00019802110817941954,
      "loss": 0.0694,
      "step": 46
    },
    {
      "epoch": 0.03100263852242744,
      "grad_norm": 0.04715995490550995,
      "learning_rate": 0.00019797713280562886,
      "loss": 0.0443,
      "step": 47
    },
    {
      "epoch": 0.0316622691292876,
      "grad_norm": 0.04275837168097496,
      "learning_rate": 0.00019793315743183817,
      "loss": 0.046,
      "step": 48
    },
    {
      "epoch": 0.032321899736147755,
      "grad_norm": 0.054124340415000916,
      "learning_rate": 0.00019788918205804749,
      "loss": 0.045,
      "step": 49
    },
    {
      "epoch": 0.032981530343007916,
      "grad_norm": 0.0623500756919384,
      "learning_rate": 0.00019784520668425683,
      "loss": 0.0452,
      "step": 50
    },
    {
      "epoch": 0.033641160949868076,
      "grad_norm": 0.18199501931667328,
      "learning_rate": 0.00019780123131046614,
      "loss": 0.0102,
      "step": 51
    },
    {
      "epoch": 0.03430079155672823,
      "grad_norm": 0.06172149255871773,
      "learning_rate": 0.00019775725593667546,
      "loss": 0.0588,
      "step": 52
    },
    {
      "epoch": 0.03496042216358839,
      "grad_norm": 0.05103158578276634,
      "learning_rate": 0.0001977132805628848,
      "loss": 0.0435,
      "step": 53
    },
    {
      "epoch": 0.03562005277044855,
      "grad_norm": 0.16191720962524414,
      "learning_rate": 0.0001976693051890941,
      "loss": 0.0089,
      "step": 54
    },
    {
      "epoch": 0.036279683377308705,
      "grad_norm": 0.13151611387729645,
      "learning_rate": 0.00019762532981530345,
      "loss": 0.0072,
      "step": 55
    },
    {
      "epoch": 0.036939313984168866,
      "grad_norm": 0.12633225321769714,
      "learning_rate": 0.00019758135444151277,
      "loss": 0.0601,
      "step": 56
    },
    {
      "epoch": 0.037598944591029027,
      "grad_norm": 0.0668325275182724,
      "learning_rate": 0.0001975373790677221,
      "loss": 0.0407,
      "step": 57
    },
    {
      "epoch": 0.03825857519788918,
      "grad_norm": 0.17144960165023804,
      "learning_rate": 0.00019749340369393142,
      "loss": 0.0702,
      "step": 58
    },
    {
      "epoch": 0.03891820580474934,
      "grad_norm": 0.206908717751503,
      "learning_rate": 0.00019744942832014073,
      "loss": 0.0815,
      "step": 59
    },
    {
      "epoch": 0.0395778364116095,
      "grad_norm": 0.07280127704143524,
      "learning_rate": 0.00019740545294635005,
      "loss": 0.0408,
      "step": 60
    },
    {
      "epoch": 0.040237467018469655,
      "grad_norm": 0.07187453657388687,
      "learning_rate": 0.0001973614775725594,
      "loss": 0.0407,
      "step": 61
    },
    {
      "epoch": 0.040897097625329816,
      "grad_norm": 0.07137905061244965,
      "learning_rate": 0.0001973175021987687,
      "loss": 0.0411,
      "step": 62
    },
    {
      "epoch": 0.04155672823218998,
      "grad_norm": 0.14471158385276794,
      "learning_rate": 0.00019727352682497802,
      "loss": 0.0721,
      "step": 63
    },
    {
      "epoch": 0.04221635883905013,
      "grad_norm": 0.07668201625347137,
      "learning_rate": 0.00019722955145118736,
      "loss": 0.0438,
      "step": 64
    },
    {
      "epoch": 0.04287598944591029,
      "grad_norm": 0.1472671926021576,
      "learning_rate": 0.00019718557607739667,
      "loss": 0.0828,
      "step": 65
    },
    {
      "epoch": 0.04353562005277045,
      "grad_norm": 0.23281940817832947,
      "learning_rate": 0.00019714160070360599,
      "loss": 0.0328,
      "step": 66
    },
    {
      "epoch": 0.044195250659630606,
      "grad_norm": 0.0707450807094574,
      "learning_rate": 0.0001970976253298153,
      "loss": 0.0723,
      "step": 67
    },
    {
      "epoch": 0.044854881266490766,
      "grad_norm": 0.19699756801128387,
      "learning_rate": 0.00019705364995602464,
      "loss": 0.0439,
      "step": 68
    },
    {
      "epoch": 0.04551451187335093,
      "grad_norm": 0.14605747163295746,
      "learning_rate": 0.00019700967458223396,
      "loss": 0.0436,
      "step": 69
    },
    {
      "epoch": 0.04617414248021108,
      "grad_norm": 0.24151253700256348,
      "learning_rate": 0.00019696569920844327,
      "loss": 0.105,
      "step": 70
    },
    {
      "epoch": 0.04683377308707124,
      "grad_norm": 0.0680900365114212,
      "learning_rate": 0.00019692172383465258,
      "loss": 0.0521,
      "step": 71
    },
    {
      "epoch": 0.047493403693931395,
      "grad_norm": 0.07927345484495163,
      "learning_rate": 0.00019687774846086192,
      "loss": 0.0404,
      "step": 72
    },
    {
      "epoch": 0.048153034300791556,
      "grad_norm": 0.07084106653928757,
      "learning_rate": 0.00019683377308707124,
      "loss": 0.0519,
      "step": 73
    },
    {
      "epoch": 0.048812664907651716,
      "grad_norm": 0.09232115745544434,
      "learning_rate": 0.00019678979771328058,
      "loss": 0.0262,
      "step": 74
    },
    {
      "epoch": 0.04947229551451187,
      "grad_norm": 0.06073536351323128,
      "learning_rate": 0.0001967458223394899,
      "loss": 0.0359,
      "step": 75
    },
    {
      "epoch": 0.05013192612137203,
      "grad_norm": 0.13216532766819,
      "learning_rate": 0.00019670184696569923,
      "loss": 0.0562,
      "step": 76
    },
    {
      "epoch": 0.05079155672823219,
      "grad_norm": 0.06590700894594193,
      "learning_rate": 0.00019665787159190855,
      "loss": 0.0385,
      "step": 77
    },
    {
      "epoch": 0.051451187335092345,
      "grad_norm": 0.24434073269367218,
      "learning_rate": 0.00019661389621811786,
      "loss": 0.0834,
      "step": 78
    },
    {
      "epoch": 0.052110817941952506,
      "grad_norm": 0.056640658527612686,
      "learning_rate": 0.0001965699208443272,
      "loss": 0.0391,
      "step": 79
    },
    {
      "epoch": 0.052770448548812667,
      "grad_norm": 0.3446822762489319,
      "learning_rate": 0.00019652594547053652,
      "loss": 0.0894,
      "step": 80
    },
    {
      "epoch": 0.05343007915567282,
      "grad_norm": 0.10367447882890701,
      "learning_rate": 0.00019648197009674583,
      "loss": 0.0619,
      "step": 81
    },
    {
      "epoch": 0.05408970976253298,
      "grad_norm": 0.3536369502544403,
      "learning_rate": 0.00019643799472295517,
      "loss": 0.0512,
      "step": 82
    },
    {
      "epoch": 0.05474934036939314,
      "grad_norm": 0.2684752345085144,
      "learning_rate": 0.00019639401934916449,
      "loss": 0.0389,
      "step": 83
    },
    {
      "epoch": 0.055408970976253295,
      "grad_norm": 0.28515341877937317,
      "learning_rate": 0.0001963500439753738,
      "loss": 0.0138,
      "step": 84
    },
    {
      "epoch": 0.056068601583113456,
      "grad_norm": 0.12716767191886902,
      "learning_rate": 0.00019630606860158311,
      "loss": 0.0453,
      "step": 85
    },
    {
      "epoch": 0.05672823218997362,
      "grad_norm": 0.27634304761886597,
      "learning_rate": 0.00019626209322779246,
      "loss": 0.0635,
      "step": 86
    },
    {
      "epoch": 0.05738786279683377,
      "grad_norm": 0.13463714718818665,
      "learning_rate": 0.00019621811785400177,
      "loss": 0.0401,
      "step": 87
    },
    {
      "epoch": 0.05804749340369393,
      "grad_norm": 0.20833075046539307,
      "learning_rate": 0.00019617414248021108,
      "loss": 0.0491,
      "step": 88
    },
    {
      "epoch": 0.05870712401055409,
      "grad_norm": 0.12235816568136215,
      "learning_rate": 0.0001961301671064204,
      "loss": 0.0336,
      "step": 89
    },
    {
      "epoch": 0.059366754617414245,
      "grad_norm": 0.043249744921922684,
      "learning_rate": 0.00019608619173262974,
      "loss": 0.0224,
      "step": 90
    },
    {
      "epoch": 0.060026385224274406,
      "grad_norm": 0.1612648218870163,
      "learning_rate": 0.00019604221635883905,
      "loss": 0.0453,
      "step": 91
    },
    {
      "epoch": 0.06068601583113457,
      "grad_norm": 0.11032130569219589,
      "learning_rate": 0.00019599824098504837,
      "loss": 0.0434,
      "step": 92
    },
    {
      "epoch": 0.06134564643799472,
      "grad_norm": 0.25559529662132263,
      "learning_rate": 0.0001959542656112577,
      "loss": 0.0307,
      "step": 93
    },
    {
      "epoch": 0.06200527704485488,
      "grad_norm": 0.19734643399715424,
      "learning_rate": 0.00019591029023746702,
      "loss": 0.0396,
      "step": 94
    },
    {
      "epoch": 0.06266490765171503,
      "grad_norm": 0.1691751778125763,
      "learning_rate": 0.00019586631486367634,
      "loss": 0.0426,
      "step": 95
    },
    {
      "epoch": 0.0633245382585752,
      "grad_norm": 0.16714054346084595,
      "learning_rate": 0.00019582233948988568,
      "loss": 0.0305,
      "step": 96
    },
    {
      "epoch": 0.06398416886543536,
      "grad_norm": 0.1062399372458458,
      "learning_rate": 0.000195778364116095,
      "loss": 0.0303,
      "step": 97
    },
    {
      "epoch": 0.06464379947229551,
      "grad_norm": 0.2215929478406906,
      "learning_rate": 0.00019573438874230433,
      "loss": 0.0494,
      "step": 98
    },
    {
      "epoch": 0.06530343007915568,
      "grad_norm": 0.09904662519693375,
      "learning_rate": 0.00019569041336851365,
      "loss": 0.0262,
      "step": 99
    },
    {
      "epoch": 0.06596306068601583,
      "grad_norm": 0.21408338844776154,
      "learning_rate": 0.00019564643799472299,
      "loss": 0.0435,
      "step": 100
    },
    {
      "epoch": 0.06662269129287599,
      "grad_norm": 0.30659642815589905,
      "learning_rate": 0.0001956024626209323,
      "loss": 0.063,
      "step": 101
    },
    {
      "epoch": 0.06728232189973615,
      "grad_norm": 0.13222965598106384,
      "learning_rate": 0.00019555848724714161,
      "loss": 0.0354,
      "step": 102
    },
    {
      "epoch": 0.0679419525065963,
      "grad_norm": 0.14156505465507507,
      "learning_rate": 0.00019551451187335093,
      "loss": 0.0289,
      "step": 103
    },
    {
      "epoch": 0.06860158311345646,
      "grad_norm": 0.17455017566680908,
      "learning_rate": 0.00019547053649956027,
      "loss": 0.0495,
      "step": 104
    },
    {
      "epoch": 0.06926121372031663,
      "grad_norm": 0.2612893879413605,
      "learning_rate": 0.00019542656112576958,
      "loss": 0.0116,
      "step": 105
    },
    {
      "epoch": 0.06992084432717678,
      "grad_norm": 0.17791563272476196,
      "learning_rate": 0.0001953825857519789,
      "loss": 0.0703,
      "step": 106
    },
    {
      "epoch": 0.07058047493403694,
      "grad_norm": 0.28025487065315247,
      "learning_rate": 0.0001953386103781882,
      "loss": 0.0312,
      "step": 107
    },
    {
      "epoch": 0.0712401055408971,
      "grad_norm": 0.10029605031013489,
      "learning_rate": 0.00019529463500439755,
      "loss": 0.0392,
      "step": 108
    },
    {
      "epoch": 0.07189973614775726,
      "grad_norm": 0.3146810233592987,
      "learning_rate": 0.00019525065963060687,
      "loss": 0.0689,
      "step": 109
    },
    {
      "epoch": 0.07255936675461741,
      "grad_norm": 0.11498094350099564,
      "learning_rate": 0.00019520668425681618,
      "loss": 0.0402,
      "step": 110
    },
    {
      "epoch": 0.07321899736147758,
      "grad_norm": 0.20288977026939392,
      "learning_rate": 0.00019516270888302552,
      "loss": 0.0089,
      "step": 111
    },
    {
      "epoch": 0.07387862796833773,
      "grad_norm": 0.09606583416461945,
      "learning_rate": 0.00019511873350923484,
      "loss": 0.0422,
      "step": 112
    },
    {
      "epoch": 0.07453825857519789,
      "grad_norm": 0.128167524933815,
      "learning_rate": 0.00019507475813544415,
      "loss": 0.0055,
      "step": 113
    },
    {
      "epoch": 0.07519788918205805,
      "grad_norm": 0.13003094494342804,
      "learning_rate": 0.00019503078276165346,
      "loss": 0.048,
      "step": 114
    },
    {
      "epoch": 0.0758575197889182,
      "grad_norm": 0.3133968114852905,
      "learning_rate": 0.0001949868073878628,
      "loss": 0.0708,
      "step": 115
    },
    {
      "epoch": 0.07651715039577836,
      "grad_norm": 0.05762765184044838,
      "learning_rate": 0.00019494283201407212,
      "loss": 0.0185,
      "step": 116
    },
    {
      "epoch": 0.07717678100263853,
      "grad_norm": 0.10894982516765594,
      "learning_rate": 0.00019489885664028146,
      "loss": 0.0277,
      "step": 117
    },
    {
      "epoch": 0.07783641160949868,
      "grad_norm": 0.21002894639968872,
      "learning_rate": 0.00019485488126649077,
      "loss": 0.0464,
      "step": 118
    },
    {
      "epoch": 0.07849604221635884,
      "grad_norm": 0.2224273681640625,
      "learning_rate": 0.00019481090589270011,
      "loss": 0.0351,
      "step": 119
    },
    {
      "epoch": 0.079155672823219,
      "grad_norm": 0.1394512951374054,
      "learning_rate": 0.00019476693051890943,
      "loss": 0.0257,
      "step": 120
    },
    {
      "epoch": 0.07981530343007916,
      "grad_norm": 0.18453086912631989,
      "learning_rate": 0.00019472295514511874,
      "loss": 0.029,
      "step": 121
    },
    {
      "epoch": 0.08047493403693931,
      "grad_norm": 0.1256110519170761,
      "learning_rate": 0.00019467897977132808,
      "loss": 0.041,
      "step": 122
    },
    {
      "epoch": 0.08113456464379948,
      "grad_norm": 0.2566694915294647,
      "learning_rate": 0.0001946350043975374,
      "loss": 0.0787,
      "step": 123
    },
    {
      "epoch": 0.08179419525065963,
      "grad_norm": 0.1477961242198944,
      "learning_rate": 0.0001945910290237467,
      "loss": 0.0467,
      "step": 124
    },
    {
      "epoch": 0.08245382585751979,
      "grad_norm": 0.18613003194332123,
      "learning_rate": 0.00019454705364995603,
      "loss": 0.0083,
      "step": 125
    },
    {
      "epoch": 0.08311345646437995,
      "grad_norm": 0.30505865812301636,
      "learning_rate": 0.00019450307827616537,
      "loss": 0.0669,
      "step": 126
    },
    {
      "epoch": 0.08377308707124011,
      "grad_norm": 0.12369900941848755,
      "learning_rate": 0.00019445910290237468,
      "loss": 0.0264,
      "step": 127
    },
    {
      "epoch": 0.08443271767810026,
      "grad_norm": 0.09146395325660706,
      "learning_rate": 0.000194415127528584,
      "loss": 0.0357,
      "step": 128
    },
    {
      "epoch": 0.08509234828496043,
      "grad_norm": 0.23533299565315247,
      "learning_rate": 0.00019437115215479334,
      "loss": 0.0593,
      "step": 129
    },
    {
      "epoch": 0.08575197889182058,
      "grad_norm": 0.11334734410047531,
      "learning_rate": 0.00019432717678100265,
      "loss": 0.0419,
      "step": 130
    },
    {
      "epoch": 0.08641160949868074,
      "grad_norm": 0.2419799119234085,
      "learning_rate": 0.00019428320140721196,
      "loss": 0.034,
      "step": 131
    },
    {
      "epoch": 0.0870712401055409,
      "grad_norm": 0.2006964385509491,
      "learning_rate": 0.00019423922603342128,
      "loss": 0.0325,
      "step": 132
    },
    {
      "epoch": 0.08773087071240106,
      "grad_norm": 0.1248619481921196,
      "learning_rate": 0.00019419525065963062,
      "loss": 0.0396,
      "step": 133
    },
    {
      "epoch": 0.08839050131926121,
      "grad_norm": 0.08222834020853043,
      "learning_rate": 0.00019415127528583993,
      "loss": 0.0424,
      "step": 134
    },
    {
      "epoch": 0.08905013192612138,
      "grad_norm": 0.24916771054267883,
      "learning_rate": 0.00019410729991204925,
      "loss": 0.0501,
      "step": 135
    },
    {
      "epoch": 0.08970976253298153,
      "grad_norm": 0.14414404332637787,
      "learning_rate": 0.0001940633245382586,
      "loss": 0.0513,
      "step": 136
    },
    {
      "epoch": 0.09036939313984169,
      "grad_norm": 0.26187941431999207,
      "learning_rate": 0.0001940193491644679,
      "loss": 0.0572,
      "step": 137
    },
    {
      "epoch": 0.09102902374670185,
      "grad_norm": 0.19284501671791077,
      "learning_rate": 0.00019397537379067722,
      "loss": 0.0086,
      "step": 138
    },
    {
      "epoch": 0.09168865435356201,
      "grad_norm": 0.11635082960128784,
      "learning_rate": 0.00019393139841688656,
      "loss": 0.0169,
      "step": 139
    },
    {
      "epoch": 0.09234828496042216,
      "grad_norm": 0.13965992629528046,
      "learning_rate": 0.0001938874230430959,
      "loss": 0.0175,
      "step": 140
    },
    {
      "epoch": 0.09300791556728233,
      "grad_norm": 0.07542465627193451,
      "learning_rate": 0.0001938434476693052,
      "loss": 0.0198,
      "step": 141
    },
    {
      "epoch": 0.09366754617414248,
      "grad_norm": 0.05143353343009949,
      "learning_rate": 0.00019379947229551453,
      "loss": 0.0183,
      "step": 142
    },
    {
      "epoch": 0.09432717678100264,
      "grad_norm": 0.07734987884759903,
      "learning_rate": 0.00019375549692172384,
      "loss": 0.0228,
      "step": 143
    },
    {
      "epoch": 0.09498680738786279,
      "grad_norm": 0.15355023741722107,
      "learning_rate": 0.00019371152154793318,
      "loss": 0.0319,
      "step": 144
    },
    {
      "epoch": 0.09564643799472296,
      "grad_norm": 0.2588460147380829,
      "learning_rate": 0.0001936675461741425,
      "loss": 0.0572,
      "step": 145
    },
    {
      "epoch": 0.09630606860158311,
      "grad_norm": 0.15116266906261444,
      "learning_rate": 0.0001936235708003518,
      "loss": 0.0373,
      "step": 146
    },
    {
      "epoch": 0.09696569920844327,
      "grad_norm": 0.12183251976966858,
      "learning_rate": 0.00019357959542656115,
      "loss": 0.0159,
      "step": 147
    },
    {
      "epoch": 0.09762532981530343,
      "grad_norm": 0.1263515055179596,
      "learning_rate": 0.00019353562005277046,
      "loss": 0.0051,
      "step": 148
    },
    {
      "epoch": 0.09828496042216359,
      "grad_norm": 0.10101950913667679,
      "learning_rate": 0.00019349164467897978,
      "loss": 0.0382,
      "step": 149
    },
    {
      "epoch": 0.09894459102902374,
      "grad_norm": 0.16407382488250732,
      "learning_rate": 0.0001934476693051891,
      "loss": 0.0072,
      "step": 150
    },
    {
      "epoch": 0.09960422163588391,
      "grad_norm": 0.284460186958313,
      "learning_rate": 0.00019340369393139843,
      "loss": 0.0582,
      "step": 151
    },
    {
      "epoch": 0.10026385224274406,
      "grad_norm": 0.14486350119113922,
      "learning_rate": 0.00019335971855760775,
      "loss": 0.0278,
      "step": 152
    },
    {
      "epoch": 0.10092348284960422,
      "grad_norm": 0.12790547311306,
      "learning_rate": 0.00019331574318381706,
      "loss": 0.0257,
      "step": 153
    },
    {
      "epoch": 0.10158311345646438,
      "grad_norm": 0.13060295581817627,
      "learning_rate": 0.0001932717678100264,
      "loss": 0.0053,
      "step": 154
    },
    {
      "epoch": 0.10224274406332454,
      "grad_norm": 0.0999758392572403,
      "learning_rate": 0.00019322779243623572,
      "loss": 0.0225,
      "step": 155
    },
    {
      "epoch": 0.10290237467018469,
      "grad_norm": 0.07807367295026779,
      "learning_rate": 0.00019318381706244503,
      "loss": 0.0208,
      "step": 156
    },
    {
      "epoch": 0.10356200527704486,
      "grad_norm": 0.16557645797729492,
      "learning_rate": 0.00019313984168865434,
      "loss": 0.0394,
      "step": 157
    },
    {
      "epoch": 0.10422163588390501,
      "grad_norm": 0.061547163873910904,
      "learning_rate": 0.00019309586631486368,
      "loss": 0.0024,
      "step": 158
    },
    {
      "epoch": 0.10488126649076517,
      "grad_norm": 0.2129908800125122,
      "learning_rate": 0.000193051890941073,
      "loss": 0.03,
      "step": 159
    },
    {
      "epoch": 0.10554089709762533,
      "grad_norm": 0.1917123943567276,
      "learning_rate": 0.00019300791556728234,
      "loss": 0.0387,
      "step": 160
    },
    {
      "epoch": 0.10620052770448549,
      "grad_norm": 0.12426037341356277,
      "learning_rate": 0.00019296394019349165,
      "loss": 0.0206,
      "step": 161
    },
    {
      "epoch": 0.10686015831134564,
      "grad_norm": 0.15173159539699554,
      "learning_rate": 0.000192919964819701,
      "loss": 0.036,
      "step": 162
    },
    {
      "epoch": 0.10751978891820581,
      "grad_norm": 0.10376128554344177,
      "learning_rate": 0.0001928759894459103,
      "loss": 0.0313,
      "step": 163
    },
    {
      "epoch": 0.10817941952506596,
      "grad_norm": 0.18105846643447876,
      "learning_rate": 0.00019283201407211962,
      "loss": 0.0534,
      "step": 164
    },
    {
      "epoch": 0.10883905013192612,
      "grad_norm": 0.3053407371044159,
      "learning_rate": 0.00019278803869832896,
      "loss": 0.0351,
      "step": 165
    },
    {
      "epoch": 0.10949868073878628,
      "grad_norm": 0.15922747552394867,
      "learning_rate": 0.00019274406332453828,
      "loss": 0.0312,
      "step": 166
    },
    {
      "epoch": 0.11015831134564644,
      "grad_norm": 0.10466444492340088,
      "learning_rate": 0.0001927000879507476,
      "loss": 0.0298,
      "step": 167
    },
    {
      "epoch": 0.11081794195250659,
      "grad_norm": 0.1910073608160019,
      "learning_rate": 0.0001926561125769569,
      "loss": 0.0087,
      "step": 168
    },
    {
      "epoch": 0.11147757255936676,
      "grad_norm": 0.19217872619628906,
      "learning_rate": 0.00019261213720316625,
      "loss": 0.0297,
      "step": 169
    },
    {
      "epoch": 0.11213720316622691,
      "grad_norm": 0.09727902710437775,
      "learning_rate": 0.00019256816182937556,
      "loss": 0.0256,
      "step": 170
    },
    {
      "epoch": 0.11279683377308707,
      "grad_norm": 0.28956523537635803,
      "learning_rate": 0.00019252418645558487,
      "loss": 0.0546,
      "step": 171
    },
    {
      "epoch": 0.11345646437994723,
      "grad_norm": 0.10394792258739471,
      "learning_rate": 0.00019248021108179422,
      "loss": 0.023,
      "step": 172
    },
    {
      "epoch": 0.11411609498680739,
      "grad_norm": 0.06451471894979477,
      "learning_rate": 0.00019243623570800353,
      "loss": 0.0087,
      "step": 173
    },
    {
      "epoch": 0.11477572559366754,
      "grad_norm": 0.243513286113739,
      "learning_rate": 0.00019239226033421284,
      "loss": 0.0601,
      "step": 174
    },
    {
      "epoch": 0.11543535620052771,
      "grad_norm": 0.2900483012199402,
      "learning_rate": 0.00019234828496042216,
      "loss": 0.0448,
      "step": 175
    },
    {
      "epoch": 0.11609498680738786,
      "grad_norm": 0.10734273493289948,
      "learning_rate": 0.0001923043095866315,
      "loss": 0.0222,
      "step": 176
    },
    {
      "epoch": 0.11675461741424802,
      "grad_norm": 0.14786824584007263,
      "learning_rate": 0.0001922603342128408,
      "loss": 0.029,
      "step": 177
    },
    {
      "epoch": 0.11741424802110818,
      "grad_norm": 0.16831807792186737,
      "learning_rate": 0.00019221635883905013,
      "loss": 0.0377,
      "step": 178
    },
    {
      "epoch": 0.11807387862796834,
      "grad_norm": 0.387617290019989,
      "learning_rate": 0.00019217238346525944,
      "loss": 0.0366,
      "step": 179
    },
    {
      "epoch": 0.11873350923482849,
      "grad_norm": 0.12525829672813416,
      "learning_rate": 0.00019212840809146878,
      "loss": 0.0445,
      "step": 180
    },
    {
      "epoch": 0.11939313984168866,
      "grad_norm": 0.14774373173713684,
      "learning_rate": 0.00019208443271767812,
      "loss": 0.0462,
      "step": 181
    },
    {
      "epoch": 0.12005277044854881,
      "grad_norm": 0.11644310504198074,
      "learning_rate": 0.00019204045734388744,
      "loss": 0.0382,
      "step": 182
    },
    {
      "epoch": 0.12071240105540897,
      "grad_norm": 0.31700724363327026,
      "learning_rate": 0.00019199648197009678,
      "loss": 0.0779,
      "step": 183
    },
    {
      "epoch": 0.12137203166226913,
      "grad_norm": 0.12285193055868149,
      "learning_rate": 0.0001919525065963061,
      "loss": 0.0324,
      "step": 184
    },
    {
      "epoch": 0.12203166226912929,
      "grad_norm": 0.18638621270656586,
      "learning_rate": 0.0001919085312225154,
      "loss": 0.0209,
      "step": 185
    },
    {
      "epoch": 0.12269129287598944,
      "grad_norm": 0.12487637251615524,
      "learning_rate": 0.00019186455584872472,
      "loss": 0.0284,
      "step": 186
    },
    {
      "epoch": 0.12335092348284961,
      "grad_norm": 0.09531760215759277,
      "learning_rate": 0.00019182058047493406,
      "loss": 0.0272,
      "step": 187
    },
    {
      "epoch": 0.12401055408970976,
      "grad_norm": 0.1428140252828598,
      "learning_rate": 0.00019177660510114337,
      "loss": 0.0313,
      "step": 188
    },
    {
      "epoch": 0.12467018469656992,
      "grad_norm": 0.12039037048816681,
      "learning_rate": 0.0001917326297273527,
      "loss": 0.027,
      "step": 189
    },
    {
      "epoch": 0.12532981530343007,
      "grad_norm": 0.26814985275268555,
      "learning_rate": 0.00019168865435356203,
      "loss": 0.0574,
      "step": 190
    },
    {
      "epoch": 0.12598944591029024,
      "grad_norm": 0.15960603952407837,
      "learning_rate": 0.00019164467897977134,
      "loss": 0.0425,
      "step": 191
    },
    {
      "epoch": 0.1266490765171504,
      "grad_norm": 0.19683250784873962,
      "learning_rate": 0.00019160070360598066,
      "loss": 0.0134,
      "step": 192
    },
    {
      "epoch": 0.12730870712401055,
      "grad_norm": 0.1749458611011505,
      "learning_rate": 0.00019155672823218997,
      "loss": 0.0479,
      "step": 193
    },
    {
      "epoch": 0.1279683377308707,
      "grad_norm": 0.17689643800258636,
      "learning_rate": 0.0001915127528583993,
      "loss": 0.0257,
      "step": 194
    },
    {
      "epoch": 0.12862796833773088,
      "grad_norm": 0.1794043332338333,
      "learning_rate": 0.00019146877748460863,
      "loss": 0.0457,
      "step": 195
    },
    {
      "epoch": 0.12928759894459102,
      "grad_norm": 0.17437396943569183,
      "learning_rate": 0.00019142480211081794,
      "loss": 0.0483,
      "step": 196
    },
    {
      "epoch": 0.1299472295514512,
      "grad_norm": 0.12864597141742706,
      "learning_rate": 0.00019138082673702725,
      "loss": 0.0215,
      "step": 197
    },
    {
      "epoch": 0.13060686015831136,
      "grad_norm": 0.35680100321769714,
      "learning_rate": 0.0001913368513632366,
      "loss": 0.0136,
      "step": 198
    },
    {
      "epoch": 0.1312664907651715,
      "grad_norm": 0.23740121722221375,
      "learning_rate": 0.0001912928759894459,
      "loss": 0.0085,
      "step": 199
    },
    {
      "epoch": 0.13192612137203166,
      "grad_norm": 0.34183815121650696,
      "learning_rate": 0.00019124890061565522,
      "loss": 0.0604,
      "step": 200
    },
    {
      "epoch": 0.13258575197889183,
      "grad_norm": 0.29078608751296997,
      "learning_rate": 0.00019120492524186456,
      "loss": 0.0729,
      "step": 201
    },
    {
      "epoch": 0.13324538258575197,
      "grad_norm": 0.1477326899766922,
      "learning_rate": 0.00019116094986807388,
      "loss": 0.0438,
      "step": 202
    },
    {
      "epoch": 0.13390501319261214,
      "grad_norm": 0.09249796718358994,
      "learning_rate": 0.00019111697449428322,
      "loss": 0.0184,
      "step": 203
    },
    {
      "epoch": 0.1345646437994723,
      "grad_norm": 0.18043112754821777,
      "learning_rate": 0.00019107299912049253,
      "loss": 0.0463,
      "step": 204
    },
    {
      "epoch": 0.13522427440633245,
      "grad_norm": 0.09831412881612778,
      "learning_rate": 0.00019102902374670187,
      "loss": 0.0375,
      "step": 205
    },
    {
      "epoch": 0.1358839050131926,
      "grad_norm": 0.19680404663085938,
      "learning_rate": 0.0001909850483729112,
      "loss": 0.0091,
      "step": 206
    },
    {
      "epoch": 0.13654353562005278,
      "grad_norm": 0.2062271386384964,
      "learning_rate": 0.0001909410729991205,
      "loss": 0.0484,
      "step": 207
    },
    {
      "epoch": 0.13720316622691292,
      "grad_norm": 0.09508629143238068,
      "learning_rate": 0.00019089709762532984,
      "loss": 0.0322,
      "step": 208
    },
    {
      "epoch": 0.1378627968337731,
      "grad_norm": 0.13301989436149597,
      "learning_rate": 0.00019085312225153916,
      "loss": 0.0211,
      "step": 209
    },
    {
      "epoch": 0.13852242744063326,
      "grad_norm": 0.09851998090744019,
      "learning_rate": 0.00019080914687774847,
      "loss": 0.0455,
      "step": 210
    },
    {
      "epoch": 0.1391820580474934,
      "grad_norm": 0.1096734032034874,
      "learning_rate": 0.00019076517150395778,
      "loss": 0.0305,
      "step": 211
    },
    {
      "epoch": 0.13984168865435356,
      "grad_norm": 0.14769691228866577,
      "learning_rate": 0.00019072119613016713,
      "loss": 0.0416,
      "step": 212
    },
    {
      "epoch": 0.14050131926121373,
      "grad_norm": 0.09760124236345291,
      "learning_rate": 0.00019067722075637644,
      "loss": 0.0248,
      "step": 213
    },
    {
      "epoch": 0.14116094986807387,
      "grad_norm": 0.31745147705078125,
      "learning_rate": 0.00019063324538258575,
      "loss": 0.0621,
      "step": 214
    },
    {
      "epoch": 0.14182058047493404,
      "grad_norm": 0.1543177217245102,
      "learning_rate": 0.00019058927000879507,
      "loss": 0.0073,
      "step": 215
    },
    {
      "epoch": 0.1424802110817942,
      "grad_norm": 0.20271125435829163,
      "learning_rate": 0.0001905452946350044,
      "loss": 0.0529,
      "step": 216
    },
    {
      "epoch": 0.14313984168865435,
      "grad_norm": 0.10752251744270325,
      "learning_rate": 0.00019050131926121372,
      "loss": 0.0287,
      "step": 217
    },
    {
      "epoch": 0.1437994722955145,
      "grad_norm": 0.21865250170230865,
      "learning_rate": 0.00019045734388742304,
      "loss": 0.0273,
      "step": 218
    },
    {
      "epoch": 0.14445910290237468,
      "grad_norm": 0.14582155644893646,
      "learning_rate": 0.00019041336851363238,
      "loss": 0.0285,
      "step": 219
    },
    {
      "epoch": 0.14511873350923482,
      "grad_norm": 0.16580945253372192,
      "learning_rate": 0.0001903693931398417,
      "loss": 0.0106,
      "step": 220
    },
    {
      "epoch": 0.145778364116095,
      "grad_norm": 0.09719638526439667,
      "learning_rate": 0.000190325417766051,
      "loss": 0.028,
      "step": 221
    },
    {
      "epoch": 0.14643799472295516,
      "grad_norm": 0.3280295729637146,
      "learning_rate": 0.00019028144239226035,
      "loss": 0.0766,
      "step": 222
    },
    {
      "epoch": 0.1470976253298153,
      "grad_norm": 0.13194382190704346,
      "learning_rate": 0.00019023746701846966,
      "loss": 0.0483,
      "step": 223
    },
    {
      "epoch": 0.14775725593667546,
      "grad_norm": 0.10151534527540207,
      "learning_rate": 0.000190193491644679,
      "loss": 0.0236,
      "step": 224
    },
    {
      "epoch": 0.14841688654353563,
      "grad_norm": 0.14077937602996826,
      "learning_rate": 0.00019014951627088832,
      "loss": 0.0394,
      "step": 225
    },
    {
      "epoch": 0.14907651715039577,
      "grad_norm": 0.20061689615249634,
      "learning_rate": 0.00019010554089709766,
      "loss": 0.0085,
      "step": 226
    },
    {
      "epoch": 0.14973614775725594,
      "grad_norm": 0.2101844847202301,
      "learning_rate": 0.00019006156552330697,
      "loss": 0.0559,
      "step": 227
    },
    {
      "epoch": 0.1503957783641161,
      "grad_norm": 0.121984101831913,
      "learning_rate": 0.00019001759014951628,
      "loss": 0.037,
      "step": 228
    },
    {
      "epoch": 0.15105540897097625,
      "grad_norm": 0.2454075813293457,
      "learning_rate": 0.0001899736147757256,
      "loss": 0.0657,
      "step": 229
    },
    {
      "epoch": 0.1517150395778364,
      "grad_norm": 0.12167758494615555,
      "learning_rate": 0.00018992963940193494,
      "loss": 0.024,
      "step": 230
    },
    {
      "epoch": 0.15237467018469658,
      "grad_norm": 0.1468701809644699,
      "learning_rate": 0.00018988566402814425,
      "loss": 0.0609,
      "step": 231
    },
    {
      "epoch": 0.15303430079155672,
      "grad_norm": 0.2747187316417694,
      "learning_rate": 0.00018984168865435357,
      "loss": 0.0234,
      "step": 232
    },
    {
      "epoch": 0.1536939313984169,
      "grad_norm": 0.21161307394504547,
      "learning_rate": 0.00018979771328056288,
      "loss": 0.0535,
      "step": 233
    },
    {
      "epoch": 0.15435356200527706,
      "grad_norm": 0.11065617203712463,
      "learning_rate": 0.00018975373790677222,
      "loss": 0.027,
      "step": 234
    },
    {
      "epoch": 0.1550131926121372,
      "grad_norm": 0.2823605537414551,
      "learning_rate": 0.00018970976253298154,
      "loss": 0.0609,
      "step": 235
    },
    {
      "epoch": 0.15567282321899736,
      "grad_norm": 0.1574711799621582,
      "learning_rate": 0.00018966578715919085,
      "loss": 0.0218,
      "step": 236
    },
    {
      "epoch": 0.15633245382585753,
      "grad_norm": 0.1133563444018364,
      "learning_rate": 0.0001896218117854002,
      "loss": 0.0303,
      "step": 237
    },
    {
      "epoch": 0.15699208443271767,
      "grad_norm": 0.23183485865592957,
      "learning_rate": 0.0001895778364116095,
      "loss": 0.0196,
      "step": 238
    },
    {
      "epoch": 0.15765171503957784,
      "grad_norm": 0.11320910602807999,
      "learning_rate": 0.00018953386103781882,
      "loss": 0.0217,
      "step": 239
    },
    {
      "epoch": 0.158311345646438,
      "grad_norm": 0.10426139831542969,
      "learning_rate": 0.00018948988566402813,
      "loss": 0.0302,
      "step": 240
    },
    {
      "epoch": 0.15897097625329815,
      "grad_norm": 0.19469720125198364,
      "learning_rate": 0.00018944591029023747,
      "loss": 0.0309,
      "step": 241
    },
    {
      "epoch": 0.15963060686015831,
      "grad_norm": 0.29449549317359924,
      "learning_rate": 0.0001894019349164468,
      "loss": 0.0587,
      "step": 242
    },
    {
      "epoch": 0.16029023746701848,
      "grad_norm": 0.1358005553483963,
      "learning_rate": 0.0001893579595426561,
      "loss": 0.0315,
      "step": 243
    },
    {
      "epoch": 0.16094986807387862,
      "grad_norm": 0.14565633237361908,
      "learning_rate": 0.00018931398416886544,
      "loss": 0.0246,
      "step": 244
    },
    {
      "epoch": 0.1616094986807388,
      "grad_norm": 0.4777294397354126,
      "learning_rate": 0.00018927000879507478,
      "loss": 0.0238,
      "step": 245
    },
    {
      "epoch": 0.16226912928759896,
      "grad_norm": 0.3011987507343292,
      "learning_rate": 0.0001892260334212841,
      "loss": 0.0636,
      "step": 246
    },
    {
      "epoch": 0.1629287598944591,
      "grad_norm": 0.17785322666168213,
      "learning_rate": 0.0001891820580474934,
      "loss": 0.0512,
      "step": 247
    },
    {
      "epoch": 0.16358839050131926,
      "grad_norm": 0.12346488237380981,
      "learning_rate": 0.00018913808267370275,
      "loss": 0.0305,
      "step": 248
    },
    {
      "epoch": 0.16424802110817943,
      "grad_norm": 0.1267511397600174,
      "learning_rate": 0.00018909410729991207,
      "loss": 0.0399,
      "step": 249
    },
    {
      "epoch": 0.16490765171503957,
      "grad_norm": 0.23225951194763184,
      "learning_rate": 0.00018905013192612138,
      "loss": 0.0443,
      "step": 250
    },
    {
      "epoch": 0.16556728232189974,
      "grad_norm": 0.17008565366268158,
      "learning_rate": 0.0001890061565523307,
      "loss": 0.0261,
      "step": 251
    },
    {
      "epoch": 0.1662269129287599,
      "grad_norm": 0.16774429380893707,
      "learning_rate": 0.00018896218117854004,
      "loss": 0.0411,
      "step": 252
    },
    {
      "epoch": 0.16688654353562005,
      "grad_norm": 0.14850173890590668,
      "learning_rate": 0.00018891820580474935,
      "loss": 0.0406,
      "step": 253
    },
    {
      "epoch": 0.16754617414248021,
      "grad_norm": 0.17768721282482147,
      "learning_rate": 0.00018887423043095866,
      "loss": 0.0074,
      "step": 254
    },
    {
      "epoch": 0.16820580474934038,
      "grad_norm": 0.09923968464136124,
      "learning_rate": 0.000188830255057168,
      "loss": 0.004,
      "step": 255
    },
    {
      "epoch": 0.16886543535620052,
      "grad_norm": 0.16113226115703583,
      "learning_rate": 0.00018878627968337732,
      "loss": 0.036,
      "step": 256
    },
    {
      "epoch": 0.1695250659630607,
      "grad_norm": 0.3584884703159332,
      "learning_rate": 0.00018874230430958663,
      "loss": 0.0683,
      "step": 257
    },
    {
      "epoch": 0.17018469656992086,
      "grad_norm": 0.13502253592014313,
      "learning_rate": 0.00018869832893579595,
      "loss": 0.0449,
      "step": 258
    },
    {
      "epoch": 0.170844327176781,
      "grad_norm": 0.31017008423805237,
      "learning_rate": 0.0001886543535620053,
      "loss": 0.075,
      "step": 259
    },
    {
      "epoch": 0.17150395778364116,
      "grad_norm": 0.20371004939079285,
      "learning_rate": 0.0001886103781882146,
      "loss": 0.0543,
      "step": 260
    },
    {
      "epoch": 0.17216358839050133,
      "grad_norm": 0.20408768951892853,
      "learning_rate": 0.00018856640281442392,
      "loss": 0.0396,
      "step": 261
    },
    {
      "epoch": 0.17282321899736147,
      "grad_norm": 0.31677430868148804,
      "learning_rate": 0.00018852242744063326,
      "loss": 0.0403,
      "step": 262
    },
    {
      "epoch": 0.17348284960422164,
      "grad_norm": 0.2106190025806427,
      "learning_rate": 0.00018847845206684257,
      "loss": 0.0446,
      "step": 263
    },
    {
      "epoch": 0.1741424802110818,
      "grad_norm": 0.37700438499450684,
      "learning_rate": 0.00018843447669305189,
      "loss": 0.0197,
      "step": 264
    },
    {
      "epoch": 0.17480211081794195,
      "grad_norm": 0.17693425714969635,
      "learning_rate": 0.00018839050131926123,
      "loss": 0.0603,
      "step": 265
    },
    {
      "epoch": 0.17546174142480211,
      "grad_norm": 0.14948789775371552,
      "learning_rate": 0.00018834652594547054,
      "loss": 0.0081,
      "step": 266
    },
    {
      "epoch": 0.17612137203166228,
      "grad_norm": 0.45674338936805725,
      "learning_rate": 0.00018830255057167988,
      "loss": 0.0299,
      "step": 267
    },
    {
      "epoch": 0.17678100263852242,
      "grad_norm": 0.07432827353477478,
      "learning_rate": 0.0001882585751978892,
      "loss": 0.0035,
      "step": 268
    },
    {
      "epoch": 0.1774406332453826,
      "grad_norm": 0.2359493225812912,
      "learning_rate": 0.0001882145998240985,
      "loss": 0.0613,
      "step": 269
    },
    {
      "epoch": 0.17810026385224276,
      "grad_norm": 0.4908621907234192,
      "learning_rate": 0.00018817062445030785,
      "loss": 0.0579,
      "step": 270
    },
    {
      "epoch": 0.1787598944591029,
      "grad_norm": 0.30601686239242554,
      "learning_rate": 0.00018812664907651716,
      "loss": 0.0594,
      "step": 271
    },
    {
      "epoch": 0.17941952506596306,
      "grad_norm": 0.12472175061702728,
      "learning_rate": 0.00018808267370272648,
      "loss": 0.0253,
      "step": 272
    },
    {
      "epoch": 0.18007915567282323,
      "grad_norm": 0.15892568230628967,
      "learning_rate": 0.00018803869832893582,
      "loss": 0.0443,
      "step": 273
    },
    {
      "epoch": 0.18073878627968337,
      "grad_norm": 0.0834997296333313,
      "learning_rate": 0.00018799472295514513,
      "loss": 0.0384,
      "step": 274
    },
    {
      "epoch": 0.18139841688654354,
      "grad_norm": 0.14860966801643372,
      "learning_rate": 0.00018795074758135445,
      "loss": 0.0077,
      "step": 275
    },
    {
      "epoch": 0.1820580474934037,
      "grad_norm": 0.15168800950050354,
      "learning_rate": 0.00018790677220756376,
      "loss": 0.0193,
      "step": 276
    },
    {
      "epoch": 0.18271767810026385,
      "grad_norm": 0.2295731008052826,
      "learning_rate": 0.0001878627968337731,
      "loss": 0.0197,
      "step": 277
    },
    {
      "epoch": 0.18337730870712401,
      "grad_norm": 0.15327389538288116,
      "learning_rate": 0.00018781882145998242,
      "loss": 0.0303,
      "step": 278
    },
    {
      "epoch": 0.18403693931398418,
      "grad_norm": 0.15900860726833344,
      "learning_rate": 0.00018777484608619173,
      "loss": 0.032,
      "step": 279
    },
    {
      "epoch": 0.18469656992084432,
      "grad_norm": 0.179190993309021,
      "learning_rate": 0.00018773087071240107,
      "loss": 0.0523,
      "step": 280
    },
    {
      "epoch": 0.1853562005277045,
      "grad_norm": 0.15199297666549683,
      "learning_rate": 0.00018768689533861039,
      "loss": 0.053,
      "step": 281
    },
    {
      "epoch": 0.18601583113456466,
      "grad_norm": 0.05995456501841545,
      "learning_rate": 0.0001876429199648197,
      "loss": 0.0238,
      "step": 282
    },
    {
      "epoch": 0.1866754617414248,
      "grad_norm": 0.06031165271997452,
      "learning_rate": 0.00018759894459102901,
      "loss": 0.0138,
      "step": 283
    },
    {
      "epoch": 0.18733509234828497,
      "grad_norm": 0.05265991389751434,
      "learning_rate": 0.00018755496921723835,
      "loss": 0.0197,
      "step": 284
    },
    {
      "epoch": 0.1879947229551451,
      "grad_norm": 0.12140937149524689,
      "learning_rate": 0.00018751099384344767,
      "loss": 0.0414,
      "step": 285
    },
    {
      "epoch": 0.18865435356200527,
      "grad_norm": 0.10338076949119568,
      "learning_rate": 0.000187467018469657,
      "loss": 0.0245,
      "step": 286
    },
    {
      "epoch": 0.18931398416886544,
      "grad_norm": 0.14068648219108582,
      "learning_rate": 0.00018742304309586632,
      "loss": 0.0436,
      "step": 287
    },
    {
      "epoch": 0.18997361477572558,
      "grad_norm": 0.15604950487613678,
      "learning_rate": 0.00018737906772207566,
      "loss": 0.055,
      "step": 288
    },
    {
      "epoch": 0.19063324538258575,
      "grad_norm": 0.12546131014823914,
      "learning_rate": 0.00018733509234828498,
      "loss": 0.0063,
      "step": 289
    },
    {
      "epoch": 0.19129287598944592,
      "grad_norm": 0.13727931678295135,
      "learning_rate": 0.0001872911169744943,
      "loss": 0.0341,
      "step": 290
    },
    {
      "epoch": 0.19195250659630606,
      "grad_norm": 0.11068456619977951,
      "learning_rate": 0.00018724714160070363,
      "loss": 0.0114,
      "step": 291
    },
    {
      "epoch": 0.19261213720316622,
      "grad_norm": 0.13765092194080353,
      "learning_rate": 0.00018720316622691295,
      "loss": 0.0199,
      "step": 292
    },
    {
      "epoch": 0.1932717678100264,
      "grad_norm": 0.0722384825348854,
      "learning_rate": 0.00018715919085312226,
      "loss": 0.0254,
      "step": 293
    },
    {
      "epoch": 0.19393139841688653,
      "grad_norm": 0.10770145803689957,
      "learning_rate": 0.00018711521547933158,
      "loss": 0.0438,
      "step": 294
    },
    {
      "epoch": 0.1945910290237467,
      "grad_norm": 0.1323946714401245,
      "learning_rate": 0.00018707124010554092,
      "loss": 0.0444,
      "step": 295
    },
    {
      "epoch": 0.19525065963060687,
      "grad_norm": 0.13147789239883423,
      "learning_rate": 0.00018702726473175023,
      "loss": 0.0315,
      "step": 296
    },
    {
      "epoch": 0.195910290237467,
      "grad_norm": 0.0838061198592186,
      "learning_rate": 0.00018698328935795954,
      "loss": 0.0196,
      "step": 297
    },
    {
      "epoch": 0.19656992084432717,
      "grad_norm": 0.11696106940507889,
      "learning_rate": 0.00018693931398416889,
      "loss": 0.0056,
      "step": 298
    },
    {
      "epoch": 0.19722955145118734,
      "grad_norm": 0.28537291288375854,
      "learning_rate": 0.0001868953386103782,
      "loss": 0.071,
      "step": 299
    },
    {
      "epoch": 0.19788918205804748,
      "grad_norm": 0.08896739035844803,
      "learning_rate": 0.00018685136323658751,
      "loss": 0.033,
      "step": 300
    },
    {
      "epoch": 0.19854881266490765,
      "grad_norm": 0.1671205759048462,
      "learning_rate": 0.00018680738786279683,
      "loss": 0.0173,
      "step": 301
    },
    {
      "epoch": 0.19920844327176782,
      "grad_norm": 0.25466111302375793,
      "learning_rate": 0.00018676341248900617,
      "loss": 0.0595,
      "step": 302
    },
    {
      "epoch": 0.19986807387862796,
      "grad_norm": 0.234773188829422,
      "learning_rate": 0.00018671943711521548,
      "loss": 0.0118,
      "step": 303
    },
    {
      "epoch": 0.20052770448548812,
      "grad_norm": 0.16967210173606873,
      "learning_rate": 0.0001866754617414248,
      "loss": 0.022,
      "step": 304
    },
    {
      "epoch": 0.2011873350923483,
      "grad_norm": 0.06990941613912582,
      "learning_rate": 0.0001866314863676341,
      "loss": 0.0168,
      "step": 305
    },
    {
      "epoch": 0.20184696569920843,
      "grad_norm": 0.08854915201663971,
      "learning_rate": 0.00018658751099384345,
      "loss": 0.0127,
      "step": 306
    },
    {
      "epoch": 0.2025065963060686,
      "grad_norm": 0.15157340466976166,
      "learning_rate": 0.00018654353562005277,
      "loss": 0.0243,
      "step": 307
    },
    {
      "epoch": 0.20316622691292877,
      "grad_norm": 0.15561407804489136,
      "learning_rate": 0.0001864995602462621,
      "loss": 0.0273,
      "step": 308
    },
    {
      "epoch": 0.2038258575197889,
      "grad_norm": 0.06243356317281723,
      "learning_rate": 0.00018645558487247142,
      "loss": 0.0117,
      "step": 309
    },
    {
      "epoch": 0.20448548812664907,
      "grad_norm": 0.09370484203100204,
      "learning_rate": 0.00018641160949868076,
      "loss": 0.0223,
      "step": 310
    },
    {
      "epoch": 0.20514511873350924,
      "grad_norm": 0.4050877094268799,
      "learning_rate": 0.00018636763412489008,
      "loss": 0.1088,
      "step": 311
    },
    {
      "epoch": 0.20580474934036938,
      "grad_norm": 0.13015133142471313,
      "learning_rate": 0.0001863236587510994,
      "loss": 0.0054,
      "step": 312
    },
    {
      "epoch": 0.20646437994722955,
      "grad_norm": 0.25691625475883484,
      "learning_rate": 0.00018627968337730873,
      "loss": 0.0123,
      "step": 313
    },
    {
      "epoch": 0.20712401055408972,
      "grad_norm": 0.307771772146225,
      "learning_rate": 0.00018623570800351804,
      "loss": 0.0441,
      "step": 314
    },
    {
      "epoch": 0.20778364116094986,
      "grad_norm": 0.13112516701221466,
      "learning_rate": 0.00018619173262972736,
      "loss": 0.0287,
      "step": 315
    },
    {
      "epoch": 0.20844327176781002,
      "grad_norm": 0.16946454346179962,
      "learning_rate": 0.0001861477572559367,
      "loss": 0.044,
      "step": 316
    },
    {
      "epoch": 0.2091029023746702,
      "grad_norm": 0.1800411343574524,
      "learning_rate": 0.00018610378188214601,
      "loss": 0.0286,
      "step": 317
    },
    {
      "epoch": 0.20976253298153033,
      "grad_norm": 0.18840743601322174,
      "learning_rate": 0.00018605980650835533,
      "loss": 0.0457,
      "step": 318
    },
    {
      "epoch": 0.2104221635883905,
      "grad_norm": 0.42243489623069763,
      "learning_rate": 0.00018601583113456464,
      "loss": 0.0226,
      "step": 319
    },
    {
      "epoch": 0.21108179419525067,
      "grad_norm": 0.281595915555954,
      "learning_rate": 0.00018597185576077398,
      "loss": 0.0122,
      "step": 320
    },
    {
      "epoch": 0.2117414248021108,
      "grad_norm": 0.1960383504629135,
      "learning_rate": 0.0001859278803869833,
      "loss": 0.0363,
      "step": 321
    },
    {
      "epoch": 0.21240105540897097,
      "grad_norm": 0.06646469980478287,
      "learning_rate": 0.0001858839050131926,
      "loss": 0.0121,
      "step": 322
    },
    {
      "epoch": 0.21306068601583114,
      "grad_norm": 0.4216509163379669,
      "learning_rate": 0.00018583992963940192,
      "loss": 0.0802,
      "step": 323
    },
    {
      "epoch": 0.21372031662269128,
      "grad_norm": 0.15882079303264618,
      "learning_rate": 0.00018579595426561127,
      "loss": 0.0278,
      "step": 324
    },
    {
      "epoch": 0.21437994722955145,
      "grad_norm": 0.23440319299697876,
      "learning_rate": 0.00018575197889182058,
      "loss": 0.0642,
      "step": 325
    },
    {
      "epoch": 0.21503957783641162,
      "grad_norm": 0.051791660487651825,
      "learning_rate": 0.0001857080035180299,
      "loss": 0.0073,
      "step": 326
    },
    {
      "epoch": 0.21569920844327176,
      "grad_norm": 0.11632749438285828,
      "learning_rate": 0.00018566402814423923,
      "loss": 0.0052,
      "step": 327
    },
    {
      "epoch": 0.21635883905013192,
      "grad_norm": 0.08130896091461182,
      "learning_rate": 0.00018562005277044855,
      "loss": 0.0255,
      "step": 328
    },
    {
      "epoch": 0.2170184696569921,
      "grad_norm": 0.10192210227251053,
      "learning_rate": 0.0001855760773966579,
      "loss": 0.0193,
      "step": 329
    },
    {
      "epoch": 0.21767810026385223,
      "grad_norm": 0.11850395053625107,
      "learning_rate": 0.0001855321020228672,
      "loss": 0.0425,
      "step": 330
    },
    {
      "epoch": 0.2183377308707124,
      "grad_norm": 0.16518525779247284,
      "learning_rate": 0.00018548812664907654,
      "loss": 0.0086,
      "step": 331
    },
    {
      "epoch": 0.21899736147757257,
      "grad_norm": 0.10314742475748062,
      "learning_rate": 0.00018544415127528586,
      "loss": 0.0051,
      "step": 332
    },
    {
      "epoch": 0.2196569920844327,
      "grad_norm": 0.16333219408988953,
      "learning_rate": 0.00018540017590149517,
      "loss": 0.0086,
      "step": 333
    },
    {
      "epoch": 0.22031662269129287,
      "grad_norm": 0.07367107272148132,
      "learning_rate": 0.00018535620052770451,
      "loss": 0.0036,
      "step": 334
    },
    {
      "epoch": 0.22097625329815304,
      "grad_norm": 0.1596449464559555,
      "learning_rate": 0.00018531222515391383,
      "loss": 0.0411,
      "step": 335
    },
    {
      "epoch": 0.22163588390501318,
      "grad_norm": 0.4504897892475128,
      "learning_rate": 0.00018526824978012314,
      "loss": 0.0423,
      "step": 336
    },
    {
      "epoch": 0.22229551451187335,
      "grad_norm": 0.33370983600616455,
      "learning_rate": 0.00018522427440633246,
      "loss": 0.0589,
      "step": 337
    },
    {
      "epoch": 0.22295514511873352,
      "grad_norm": 0.12324994802474976,
      "learning_rate": 0.0001851802990325418,
      "loss": 0.0288,
      "step": 338
    },
    {
      "epoch": 0.22361477572559366,
      "grad_norm": 0.26627981662750244,
      "learning_rate": 0.0001851363236587511,
      "loss": 0.0631,
      "step": 339
    },
    {
      "epoch": 0.22427440633245382,
      "grad_norm": 0.12521807849407196,
      "learning_rate": 0.00018509234828496042,
      "loss": 0.026,
      "step": 340
    },
    {
      "epoch": 0.224934036939314,
      "grad_norm": 0.14315369725227356,
      "learning_rate": 0.00018504837291116974,
      "loss": 0.0305,
      "step": 341
    },
    {
      "epoch": 0.22559366754617413,
      "grad_norm": 0.10920839011669159,
      "learning_rate": 0.00018500439753737908,
      "loss": 0.0247,
      "step": 342
    },
    {
      "epoch": 0.2262532981530343,
      "grad_norm": 0.11852335184812546,
      "learning_rate": 0.0001849604221635884,
      "loss": 0.0413,
      "step": 343
    },
    {
      "epoch": 0.22691292875989447,
      "grad_norm": 0.11384479701519012,
      "learning_rate": 0.0001849164467897977,
      "loss": 0.0603,
      "step": 344
    },
    {
      "epoch": 0.2275725593667546,
      "grad_norm": 0.18584024906158447,
      "learning_rate": 0.00018487247141600705,
      "loss": 0.0392,
      "step": 345
    },
    {
      "epoch": 0.22823218997361477,
      "grad_norm": 0.19693401455879211,
      "learning_rate": 0.00018482849604221636,
      "loss": 0.0713,
      "step": 346
    },
    {
      "epoch": 0.22889182058047494,
      "grad_norm": 0.11212527751922607,
      "learning_rate": 0.00018478452066842568,
      "loss": 0.0241,
      "step": 347
    },
    {
      "epoch": 0.22955145118733508,
      "grad_norm": 0.083701491355896,
      "learning_rate": 0.000184740545294635,
      "loss": 0.0311,
      "step": 348
    },
    {
      "epoch": 0.23021108179419525,
      "grad_norm": 0.11869709938764572,
      "learning_rate": 0.00018469656992084433,
      "loss": 0.0543,
      "step": 349
    },
    {
      "epoch": 0.23087071240105542,
      "grad_norm": 0.25627174973487854,
      "learning_rate": 0.00018465259454705365,
      "loss": 0.063,
      "step": 350
    },
    {
      "epoch": 0.23153034300791556,
      "grad_norm": 0.3457930386066437,
      "learning_rate": 0.000184608619173263,
      "loss": 0.0952,
      "step": 351
    },
    {
      "epoch": 0.23218997361477572,
      "grad_norm": 0.14892838895320892,
      "learning_rate": 0.00018456464379947233,
      "loss": 0.0723,
      "step": 352
    },
    {
      "epoch": 0.2328496042216359,
      "grad_norm": 0.17938075959682465,
      "learning_rate": 0.00018452066842568164,
      "loss": 0.032,
      "step": 353
    },
    {
      "epoch": 0.23350923482849603,
      "grad_norm": 0.38997694849967957,
      "learning_rate": 0.00018447669305189096,
      "loss": 0.0421,
      "step": 354
    },
    {
      "epoch": 0.2341688654353562,
      "grad_norm": 0.3800145983695984,
      "learning_rate": 0.00018443271767810027,
      "loss": 0.032,
      "step": 355
    },
    {
      "epoch": 0.23482849604221637,
      "grad_norm": 0.22173549234867096,
      "learning_rate": 0.0001843887423043096,
      "loss": 0.0388,
      "step": 356
    },
    {
      "epoch": 0.2354881266490765,
      "grad_norm": 0.11998254805803299,
      "learning_rate": 0.00018434476693051892,
      "loss": 0.0358,
      "step": 357
    },
    {
      "epoch": 0.23614775725593667,
      "grad_norm": 0.1403476595878601,
      "learning_rate": 0.00018430079155672824,
      "loss": 0.044,
      "step": 358
    },
    {
      "epoch": 0.23680738786279684,
      "grad_norm": 0.11741739511489868,
      "learning_rate": 0.00018425681618293755,
      "loss": 0.0459,
      "step": 359
    },
    {
      "epoch": 0.23746701846965698,
      "grad_norm": 0.20706066489219666,
      "learning_rate": 0.0001842128408091469,
      "loss": 0.0465,
      "step": 360
    },
    {
      "epoch": 0.23812664907651715,
      "grad_norm": 0.13768641650676727,
      "learning_rate": 0.0001841688654353562,
      "loss": 0.0313,
      "step": 361
    },
    {
      "epoch": 0.23878627968337732,
      "grad_norm": 0.10456262528896332,
      "learning_rate": 0.00018412489006156552,
      "loss": 0.0326,
      "step": 362
    },
    {
      "epoch": 0.23944591029023746,
      "grad_norm": 0.06702134013175964,
      "learning_rate": 0.00018408091468777486,
      "loss": 0.0196,
      "step": 363
    },
    {
      "epoch": 0.24010554089709762,
      "grad_norm": 0.1867019683122635,
      "learning_rate": 0.00018403693931398418,
      "loss": 0.0451,
      "step": 364
    },
    {
      "epoch": 0.2407651715039578,
      "grad_norm": 0.1328548640012741,
      "learning_rate": 0.0001839929639401935,
      "loss": 0.0346,
      "step": 365
    },
    {
      "epoch": 0.24142480211081793,
      "grad_norm": 0.17640767991542816,
      "learning_rate": 0.0001839489885664028,
      "loss": 0.0573,
      "step": 366
    },
    {
      "epoch": 0.2420844327176781,
      "grad_norm": 0.08763616532087326,
      "learning_rate": 0.00018390501319261215,
      "loss": 0.0294,
      "step": 367
    },
    {
      "epoch": 0.24274406332453827,
      "grad_norm": 0.14130359888076782,
      "learning_rate": 0.00018386103781882146,
      "loss": 0.0604,
      "step": 368
    },
    {
      "epoch": 0.2434036939313984,
      "grad_norm": 0.13940328359603882,
      "learning_rate": 0.00018381706244503077,
      "loss": 0.0508,
      "step": 369
    },
    {
      "epoch": 0.24406332453825857,
      "grad_norm": 0.2298174947500229,
      "learning_rate": 0.00018377308707124011,
      "loss": 0.0238,
      "step": 370
    },
    {
      "epoch": 0.24472295514511874,
      "grad_norm": 0.25829386711120605,
      "learning_rate": 0.00018372911169744943,
      "loss": 0.0421,
      "step": 371
    },
    {
      "epoch": 0.24538258575197888,
      "grad_norm": 0.18463067710399628,
      "learning_rate": 0.00018368513632365877,
      "loss": 0.0375,
      "step": 372
    },
    {
      "epoch": 0.24604221635883905,
      "grad_norm": 0.12453096359968185,
      "learning_rate": 0.00018364116094986808,
      "loss": 0.0568,
      "step": 373
    },
    {
      "epoch": 0.24670184696569922,
      "grad_norm": 0.23674164712429047,
      "learning_rate": 0.00018359718557607742,
      "loss": 0.0561,
      "step": 374
    },
    {
      "epoch": 0.24736147757255936,
      "grad_norm": 0.09498414397239685,
      "learning_rate": 0.00018355321020228674,
      "loss": 0.0416,
      "step": 375
    },
    {
      "epoch": 0.24802110817941952,
      "grad_norm": 0.10955262929201126,
      "learning_rate": 0.00018350923482849605,
      "loss": 0.0461,
      "step": 376
    },
    {
      "epoch": 0.2486807387862797,
      "grad_norm": 0.10013560205698013,
      "learning_rate": 0.00018346525945470537,
      "loss": 0.0316,
      "step": 377
    },
    {
      "epoch": 0.24934036939313983,
      "grad_norm": 0.19489656388759613,
      "learning_rate": 0.0001834212840809147,
      "loss": 0.0108,
      "step": 378
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.15793240070343018,
      "learning_rate": 0.00018337730870712402,
      "loss": 0.0283,
      "step": 379
    },
    {
      "epoch": 0.25065963060686014,
      "grad_norm": 0.12917138636112213,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.035,
      "step": 380
    },
    {
      "epoch": 0.25131926121372034,
      "grad_norm": 0.19483838975429535,
      "learning_rate": 0.00018328935795954268,
      "loss": 0.0577,
      "step": 381
    },
    {
      "epoch": 0.2519788918205805,
      "grad_norm": 0.1283104568719864,
      "learning_rate": 0.000183245382585752,
      "loss": 0.0415,
      "step": 382
    },
    {
      "epoch": 0.2526385224274406,
      "grad_norm": 0.1305508315563202,
      "learning_rate": 0.0001832014072119613,
      "loss": 0.0348,
      "step": 383
    },
    {
      "epoch": 0.2532981530343008,
      "grad_norm": 0.056548215448856354,
      "learning_rate": 0.00018315743183817062,
      "loss": 0.0103,
      "step": 384
    },
    {
      "epoch": 0.25395778364116095,
      "grad_norm": 0.17468403279781342,
      "learning_rate": 0.00018311345646437996,
      "loss": 0.0323,
      "step": 385
    },
    {
      "epoch": 0.2546174142480211,
      "grad_norm": 0.20664536952972412,
      "learning_rate": 0.00018306948109058927,
      "loss": 0.0399,
      "step": 386
    },
    {
      "epoch": 0.2552770448548813,
      "grad_norm": 0.38402506709098816,
      "learning_rate": 0.0001830255057167986,
      "loss": 0.0811,
      "step": 387
    },
    {
      "epoch": 0.2559366754617414,
      "grad_norm": 0.3310551047325134,
      "learning_rate": 0.00018298153034300793,
      "loss": 0.0484,
      "step": 388
    },
    {
      "epoch": 0.25659630606860157,
      "grad_norm": 0.12981268763542175,
      "learning_rate": 0.00018293755496921724,
      "loss": 0.0246,
      "step": 389
    },
    {
      "epoch": 0.25725593667546176,
      "grad_norm": 0.2896742820739746,
      "learning_rate": 0.00018289357959542656,
      "loss": 0.0268,
      "step": 390
    },
    {
      "epoch": 0.2579155672823219,
      "grad_norm": 0.4253804385662079,
      "learning_rate": 0.0001828496042216359,
      "loss": 0.0385,
      "step": 391
    },
    {
      "epoch": 0.25857519788918204,
      "grad_norm": 0.37423262000083923,
      "learning_rate": 0.0001828056288478452,
      "loss": 0.039,
      "step": 392
    },
    {
      "epoch": 0.25923482849604224,
      "grad_norm": 0.12024305015802383,
      "learning_rate": 0.00018276165347405455,
      "loss": 0.0325,
      "step": 393
    },
    {
      "epoch": 0.2598944591029024,
      "grad_norm": 0.0844896212220192,
      "learning_rate": 0.00018271767810026387,
      "loss": 0.0201,
      "step": 394
    },
    {
      "epoch": 0.2605540897097625,
      "grad_norm": 0.1823677122592926,
      "learning_rate": 0.0001826737027264732,
      "loss": 0.0407,
      "step": 395
    },
    {
      "epoch": 0.2612137203166227,
      "grad_norm": 0.11427386105060577,
      "learning_rate": 0.00018262972735268252,
      "loss": 0.0212,
      "step": 396
    },
    {
      "epoch": 0.26187335092348285,
      "grad_norm": 0.04018102213740349,
      "learning_rate": 0.00018258575197889184,
      "loss": 0.002,
      "step": 397
    },
    {
      "epoch": 0.262532981530343,
      "grad_norm": 0.07761936634778976,
      "learning_rate": 0.00018254177660510115,
      "loss": 0.0227,
      "step": 398
    },
    {
      "epoch": 0.2631926121372032,
      "grad_norm": 0.20973189175128937,
      "learning_rate": 0.0001824978012313105,
      "loss": 0.0513,
      "step": 399
    },
    {
      "epoch": 0.2638522427440633,
      "grad_norm": 0.07394681870937347,
      "learning_rate": 0.0001824538258575198,
      "loss": 0.0119,
      "step": 400
    },
    {
      "epoch": 0.26451187335092347,
      "grad_norm": 0.35939738154411316,
      "learning_rate": 0.00018240985048372912,
      "loss": 0.0913,
      "step": 401
    },
    {
      "epoch": 0.26517150395778366,
      "grad_norm": 0.0686757043004036,
      "learning_rate": 0.00018236587510993843,
      "loss": 0.0164,
      "step": 402
    },
    {
      "epoch": 0.2658311345646438,
      "grad_norm": 0.03519360348582268,
      "learning_rate": 0.00018232189973614777,
      "loss": 0.0019,
      "step": 403
    },
    {
      "epoch": 0.26649076517150394,
      "grad_norm": 0.13784541189670563,
      "learning_rate": 0.0001822779243623571,
      "loss": 0.0432,
      "step": 404
    },
    {
      "epoch": 0.26715039577836414,
      "grad_norm": 0.20169013738632202,
      "learning_rate": 0.0001822339489885664,
      "loss": 0.0486,
      "step": 405
    },
    {
      "epoch": 0.2678100263852243,
      "grad_norm": 0.08839768171310425,
      "learning_rate": 0.00018218997361477574,
      "loss": 0.0051,
      "step": 406
    },
    {
      "epoch": 0.2684696569920844,
      "grad_norm": 0.1112482026219368,
      "learning_rate": 0.00018214599824098506,
      "loss": 0.0228,
      "step": 407
    },
    {
      "epoch": 0.2691292875989446,
      "grad_norm": 0.12330775707960129,
      "learning_rate": 0.00018210202286719437,
      "loss": 0.0428,
      "step": 408
    },
    {
      "epoch": 0.26978891820580475,
      "grad_norm": 0.1506921499967575,
      "learning_rate": 0.00018205804749340368,
      "loss": 0.0176,
      "step": 409
    },
    {
      "epoch": 0.2704485488126649,
      "grad_norm": 0.1967235952615738,
      "learning_rate": 0.00018201407211961303,
      "loss": 0.0241,
      "step": 410
    },
    {
      "epoch": 0.2711081794195251,
      "grad_norm": 0.10750004649162292,
      "learning_rate": 0.00018197009674582234,
      "loss": 0.0166,
      "step": 411
    },
    {
      "epoch": 0.2717678100263852,
      "grad_norm": 0.10917887836694717,
      "learning_rate": 0.00018192612137203165,
      "loss": 0.0318,
      "step": 412
    },
    {
      "epoch": 0.27242744063324537,
      "grad_norm": 0.13071107864379883,
      "learning_rate": 0.000181882145998241,
      "loss": 0.0611,
      "step": 413
    },
    {
      "epoch": 0.27308707124010556,
      "grad_norm": 0.1297711580991745,
      "learning_rate": 0.0001818381706244503,
      "loss": 0.0453,
      "step": 414
    },
    {
      "epoch": 0.2737467018469657,
      "grad_norm": 0.12107842415571213,
      "learning_rate": 0.00018179419525065965,
      "loss": 0.0069,
      "step": 415
    },
    {
      "epoch": 0.27440633245382584,
      "grad_norm": 0.09917732328176498,
      "learning_rate": 0.00018175021987686896,
      "loss": 0.0072,
      "step": 416
    },
    {
      "epoch": 0.27506596306068604,
      "grad_norm": 0.12330090254545212,
      "learning_rate": 0.0001817062445030783,
      "loss": 0.0223,
      "step": 417
    },
    {
      "epoch": 0.2757255936675462,
      "grad_norm": 0.08666965365409851,
      "learning_rate": 0.00018166226912928762,
      "loss": 0.0201,
      "step": 418
    },
    {
      "epoch": 0.2763852242744063,
      "grad_norm": 0.2583370804786682,
      "learning_rate": 0.00018161829375549693,
      "loss": 0.0525,
      "step": 419
    },
    {
      "epoch": 0.2770448548812665,
      "grad_norm": 0.28555062413215637,
      "learning_rate": 0.00018157431838170625,
      "loss": 0.06,
      "step": 420
    },
    {
      "epoch": 0.27770448548812665,
      "grad_norm": 0.2656480669975281,
      "learning_rate": 0.0001815303430079156,
      "loss": 0.0764,
      "step": 421
    },
    {
      "epoch": 0.2783641160949868,
      "grad_norm": 0.15808185935020447,
      "learning_rate": 0.0001814863676341249,
      "loss": 0.0446,
      "step": 422
    },
    {
      "epoch": 0.279023746701847,
      "grad_norm": 0.08544658124446869,
      "learning_rate": 0.00018144239226033422,
      "loss": 0.0265,
      "step": 423
    },
    {
      "epoch": 0.2796833773087071,
      "grad_norm": 0.10644915699958801,
      "learning_rate": 0.00018139841688654356,
      "loss": 0.0384,
      "step": 424
    },
    {
      "epoch": 0.28034300791556727,
      "grad_norm": 0.14895306527614594,
      "learning_rate": 0.00018135444151275287,
      "loss": 0.0252,
      "step": 425
    },
    {
      "epoch": 0.28100263852242746,
      "grad_norm": 0.13604417443275452,
      "learning_rate": 0.00018131046613896218,
      "loss": 0.0525,
      "step": 426
    },
    {
      "epoch": 0.2816622691292876,
      "grad_norm": 0.172395259141922,
      "learning_rate": 0.0001812664907651715,
      "loss": 0.0418,
      "step": 427
    },
    {
      "epoch": 0.28232189973614774,
      "grad_norm": 0.09892599284648895,
      "learning_rate": 0.00018122251539138084,
      "loss": 0.0255,
      "step": 428
    },
    {
      "epoch": 0.28298153034300794,
      "grad_norm": 0.16506482660770416,
      "learning_rate": 0.00018117854001759015,
      "loss": 0.0659,
      "step": 429
    },
    {
      "epoch": 0.2836411609498681,
      "grad_norm": 0.2031330168247223,
      "learning_rate": 0.00018113456464379947,
      "loss": 0.0661,
      "step": 430
    },
    {
      "epoch": 0.2843007915567282,
      "grad_norm": 0.129147008061409,
      "learning_rate": 0.0001810905892700088,
      "loss": 0.0214,
      "step": 431
    },
    {
      "epoch": 0.2849604221635884,
      "grad_norm": 0.12830795347690582,
      "learning_rate": 0.00018104661389621812,
      "loss": 0.034,
      "step": 432
    },
    {
      "epoch": 0.28562005277044855,
      "grad_norm": 0.11248774081468582,
      "learning_rate": 0.00018100263852242744,
      "loss": 0.0248,
      "step": 433
    },
    {
      "epoch": 0.2862796833773087,
      "grad_norm": 0.07769862562417984,
      "learning_rate": 0.00018095866314863678,
      "loss": 0.0281,
      "step": 434
    },
    {
      "epoch": 0.2869393139841689,
      "grad_norm": 0.17599773406982422,
      "learning_rate": 0.0001809146877748461,
      "loss": 0.0665,
      "step": 435
    },
    {
      "epoch": 0.287598944591029,
      "grad_norm": 0.11136054247617722,
      "learning_rate": 0.00018087071240105543,
      "loss": 0.0317,
      "step": 436
    },
    {
      "epoch": 0.28825857519788917,
      "grad_norm": 0.11924675107002258,
      "learning_rate": 0.00018082673702726475,
      "loss": 0.0267,
      "step": 437
    },
    {
      "epoch": 0.28891820580474936,
      "grad_norm": 0.12109523266553879,
      "learning_rate": 0.00018078276165347406,
      "loss": 0.0424,
      "step": 438
    },
    {
      "epoch": 0.2895778364116095,
      "grad_norm": 0.3063255846500397,
      "learning_rate": 0.0001807387862796834,
      "loss": 0.0503,
      "step": 439
    },
    {
      "epoch": 0.29023746701846964,
      "grad_norm": 0.2281317263841629,
      "learning_rate": 0.00018069481090589272,
      "loss": 0.0584,
      "step": 440
    },
    {
      "epoch": 0.29089709762532984,
      "grad_norm": 0.11766545474529266,
      "learning_rate": 0.00018065083553210203,
      "loss": 0.0254,
      "step": 441
    },
    {
      "epoch": 0.29155672823219,
      "grad_norm": 0.16301755607128143,
      "learning_rate": 0.00018060686015831137,
      "loss": 0.0642,
      "step": 442
    },
    {
      "epoch": 0.2922163588390501,
      "grad_norm": 0.22418160736560822,
      "learning_rate": 0.00018056288478452068,
      "loss": 0.0258,
      "step": 443
    },
    {
      "epoch": 0.2928759894459103,
      "grad_norm": 0.24746108055114746,
      "learning_rate": 0.00018051890941073,
      "loss": 0.0333,
      "step": 444
    },
    {
      "epoch": 0.29353562005277045,
      "grad_norm": 0.19385595619678497,
      "learning_rate": 0.0001804749340369393,
      "loss": 0.0587,
      "step": 445
    },
    {
      "epoch": 0.2941952506596306,
      "grad_norm": 0.22688081860542297,
      "learning_rate": 0.00018043095866314865,
      "loss": 0.0227,
      "step": 446
    },
    {
      "epoch": 0.2948548812664908,
      "grad_norm": 0.08501362800598145,
      "learning_rate": 0.00018038698328935797,
      "loss": 0.0238,
      "step": 447
    },
    {
      "epoch": 0.2955145118733509,
      "grad_norm": 0.21320873498916626,
      "learning_rate": 0.00018034300791556728,
      "loss": 0.0137,
      "step": 448
    },
    {
      "epoch": 0.29617414248021107,
      "grad_norm": 0.23259323835372925,
      "learning_rate": 0.00018029903254177662,
      "loss": 0.0531,
      "step": 449
    },
    {
      "epoch": 0.29683377308707126,
      "grad_norm": 0.08675897866487503,
      "learning_rate": 0.00018025505716798594,
      "loss": 0.0212,
      "step": 450
    },
    {
      "epoch": 0.2974934036939314,
      "grad_norm": 0.15247920155525208,
      "learning_rate": 0.00018021108179419525,
      "loss": 0.0367,
      "step": 451
    },
    {
      "epoch": 0.29815303430079154,
      "grad_norm": 0.13746310770511627,
      "learning_rate": 0.00018016710642040456,
      "loss": 0.0254,
      "step": 452
    },
    {
      "epoch": 0.29881266490765174,
      "grad_norm": 0.13185934722423553,
      "learning_rate": 0.0001801231310466139,
      "loss": 0.0284,
      "step": 453
    },
    {
      "epoch": 0.2994722955145119,
      "grad_norm": 0.15614476799964905,
      "learning_rate": 0.00018007915567282322,
      "loss": 0.0368,
      "step": 454
    },
    {
      "epoch": 0.300131926121372,
      "grad_norm": 0.32293975353240967,
      "learning_rate": 0.00018003518029903253,
      "loss": 0.0667,
      "step": 455
    },
    {
      "epoch": 0.3007915567282322,
      "grad_norm": 0.14336560666561127,
      "learning_rate": 0.00017999120492524187,
      "loss": 0.0554,
      "step": 456
    },
    {
      "epoch": 0.30145118733509235,
      "grad_norm": 0.1289646178483963,
      "learning_rate": 0.00017994722955145122,
      "loss": 0.0472,
      "step": 457
    },
    {
      "epoch": 0.3021108179419525,
      "grad_norm": 0.13255007565021515,
      "learning_rate": 0.00017990325417766053,
      "loss": 0.0334,
      "step": 458
    },
    {
      "epoch": 0.3027704485488127,
      "grad_norm": 0.2818635106086731,
      "learning_rate": 0.00017985927880386984,
      "loss": 0.0283,
      "step": 459
    },
    {
      "epoch": 0.3034300791556728,
      "grad_norm": 0.5482266545295715,
      "learning_rate": 0.00017981530343007918,
      "loss": 0.0328,
      "step": 460
    },
    {
      "epoch": 0.30408970976253297,
      "grad_norm": 0.15948981046676636,
      "learning_rate": 0.0001797713280562885,
      "loss": 0.0465,
      "step": 461
    },
    {
      "epoch": 0.30474934036939316,
      "grad_norm": 0.08475402742624283,
      "learning_rate": 0.0001797273526824978,
      "loss": 0.0253,
      "step": 462
    },
    {
      "epoch": 0.3054089709762533,
      "grad_norm": 0.1172502189874649,
      "learning_rate": 0.00017968337730870713,
      "loss": 0.0274,
      "step": 463
    },
    {
      "epoch": 0.30606860158311344,
      "grad_norm": 0.13966919481754303,
      "learning_rate": 0.00017963940193491647,
      "loss": 0.0308,
      "step": 464
    },
    {
      "epoch": 0.30672823218997364,
      "grad_norm": 0.061836469918489456,
      "learning_rate": 0.00017959542656112578,
      "loss": 0.0178,
      "step": 465
    },
    {
      "epoch": 0.3073878627968338,
      "grad_norm": 0.03612888231873512,
      "learning_rate": 0.0001795514511873351,
      "loss": 0.0021,
      "step": 466
    },
    {
      "epoch": 0.3080474934036939,
      "grad_norm": 0.23663577437400818,
      "learning_rate": 0.00017950747581354444,
      "loss": 0.051,
      "step": 467
    },
    {
      "epoch": 0.3087071240105541,
      "grad_norm": 0.05052686855196953,
      "learning_rate": 0.00017946350043975375,
      "loss": 0.003,
      "step": 468
    },
    {
      "epoch": 0.30936675461741425,
      "grad_norm": 0.3238077163696289,
      "learning_rate": 0.00017941952506596306,
      "loss": 0.0819,
      "step": 469
    },
    {
      "epoch": 0.3100263852242744,
      "grad_norm": 0.05849732831120491,
      "learning_rate": 0.00017937554969217238,
      "loss": 0.0143,
      "step": 470
    },
    {
      "epoch": 0.3106860158311346,
      "grad_norm": 0.07412716001272202,
      "learning_rate": 0.00017933157431838172,
      "loss": 0.0276,
      "step": 471
    },
    {
      "epoch": 0.3113456464379947,
      "grad_norm": 0.12468355149030685,
      "learning_rate": 0.00017928759894459103,
      "loss": 0.0475,
      "step": 472
    },
    {
      "epoch": 0.31200527704485487,
      "grad_norm": 0.08822087943553925,
      "learning_rate": 0.00017924362357080035,
      "loss": 0.0283,
      "step": 473
    },
    {
      "epoch": 0.31266490765171506,
      "grad_norm": 0.10951769351959229,
      "learning_rate": 0.00017919964819700966,
      "loss": 0.0255,
      "step": 474
    },
    {
      "epoch": 0.3133245382585752,
      "grad_norm": 0.16345462203025818,
      "learning_rate": 0.000179155672823219,
      "loss": 0.0545,
      "step": 475
    },
    {
      "epoch": 0.31398416886543534,
      "grad_norm": 0.12999388575553894,
      "learning_rate": 0.00017911169744942832,
      "loss": 0.0496,
      "step": 476
    },
    {
      "epoch": 0.31464379947229554,
      "grad_norm": 0.12021619826555252,
      "learning_rate": 0.00017906772207563766,
      "loss": 0.0189,
      "step": 477
    },
    {
      "epoch": 0.3153034300791557,
      "grad_norm": 0.13588258624076843,
      "learning_rate": 0.00017902374670184697,
      "loss": 0.0486,
      "step": 478
    },
    {
      "epoch": 0.3159630606860158,
      "grad_norm": 0.13996922969818115,
      "learning_rate": 0.0001789797713280563,
      "loss": 0.0221,
      "step": 479
    },
    {
      "epoch": 0.316622691292876,
      "grad_norm": 0.14807280898094177,
      "learning_rate": 0.00017893579595426563,
      "loss": 0.0185,
      "step": 480
    },
    {
      "epoch": 0.31728232189973615,
      "grad_norm": 0.11143828183412552,
      "learning_rate": 0.00017889182058047494,
      "loss": 0.0512,
      "step": 481
    },
    {
      "epoch": 0.3179419525065963,
      "grad_norm": 0.08998066931962967,
      "learning_rate": 0.00017884784520668428,
      "loss": 0.0293,
      "step": 482
    },
    {
      "epoch": 0.3186015831134565,
      "grad_norm": 0.13245800137519836,
      "learning_rate": 0.0001788038698328936,
      "loss": 0.0441,
      "step": 483
    },
    {
      "epoch": 0.31926121372031663,
      "grad_norm": 0.14057987928390503,
      "learning_rate": 0.0001787598944591029,
      "loss": 0.0123,
      "step": 484
    },
    {
      "epoch": 0.31992084432717677,
      "grad_norm": 0.13197457790374756,
      "learning_rate": 0.00017871591908531225,
      "loss": 0.0222,
      "step": 485
    },
    {
      "epoch": 0.32058047493403696,
      "grad_norm": 0.10755467414855957,
      "learning_rate": 0.00017867194371152156,
      "loss": 0.0067,
      "step": 486
    },
    {
      "epoch": 0.3212401055408971,
      "grad_norm": 0.14179617166519165,
      "learning_rate": 0.00017862796833773088,
      "loss": 0.0204,
      "step": 487
    },
    {
      "epoch": 0.32189973614775724,
      "grad_norm": 0.18936894834041595,
      "learning_rate": 0.0001785839929639402,
      "loss": 0.0256,
      "step": 488
    },
    {
      "epoch": 0.32255936675461744,
      "grad_norm": 0.27906423807144165,
      "learning_rate": 0.00017854001759014953,
      "loss": 0.0648,
      "step": 489
    },
    {
      "epoch": 0.3232189973614776,
      "grad_norm": 0.051390640437603,
      "learning_rate": 0.00017849604221635885,
      "loss": 0.0028,
      "step": 490
    },
    {
      "epoch": 0.3238786279683377,
      "grad_norm": 0.17598767578601837,
      "learning_rate": 0.00017845206684256816,
      "loss": 0.0414,
      "step": 491
    },
    {
      "epoch": 0.3245382585751979,
      "grad_norm": 0.21808840334415436,
      "learning_rate": 0.00017840809146877747,
      "loss": 0.0541,
      "step": 492
    },
    {
      "epoch": 0.32519788918205805,
      "grad_norm": 0.16083478927612305,
      "learning_rate": 0.00017836411609498682,
      "loss": 0.0426,
      "step": 493
    },
    {
      "epoch": 0.3258575197889182,
      "grad_norm": 0.12460670620203018,
      "learning_rate": 0.00017832014072119613,
      "loss": 0.0168,
      "step": 494
    },
    {
      "epoch": 0.3265171503957784,
      "grad_norm": 0.15590853989124298,
      "learning_rate": 0.00017827616534740544,
      "loss": 0.0296,
      "step": 495
    },
    {
      "epoch": 0.32717678100263853,
      "grad_norm": 0.17553310096263885,
      "learning_rate": 0.00017823218997361478,
      "loss": 0.0382,
      "step": 496
    },
    {
      "epoch": 0.32783641160949867,
      "grad_norm": 0.20945999026298523,
      "learning_rate": 0.0001781882145998241,
      "loss": 0.0384,
      "step": 497
    },
    {
      "epoch": 0.32849604221635886,
      "grad_norm": 0.2520866096019745,
      "learning_rate": 0.00017814423922603344,
      "loss": 0.0341,
      "step": 498
    },
    {
      "epoch": 0.329155672823219,
      "grad_norm": 0.17950865626335144,
      "learning_rate": 0.00017810026385224275,
      "loss": 0.0429,
      "step": 499
    },
    {
      "epoch": 0.32981530343007914,
      "grad_norm": 0.1381823718547821,
      "learning_rate": 0.0001780562884784521,
      "loss": 0.0315,
      "step": 500
    },
    {
      "epoch": 0.33047493403693934,
      "grad_norm": 0.12731702625751495,
      "learning_rate": 0.0001780123131046614,
      "loss": 0.0068,
      "step": 501
    },
    {
      "epoch": 0.3311345646437995,
      "grad_norm": 0.14003118872642517,
      "learning_rate": 0.00017796833773087072,
      "loss": 0.0162,
      "step": 502
    },
    {
      "epoch": 0.3317941952506596,
      "grad_norm": 0.09797767549753189,
      "learning_rate": 0.00017792436235708006,
      "loss": 0.0312,
      "step": 503
    },
    {
      "epoch": 0.3324538258575198,
      "grad_norm": 0.11008993536233902,
      "learning_rate": 0.00017788038698328938,
      "loss": 0.0107,
      "step": 504
    },
    {
      "epoch": 0.33311345646437995,
      "grad_norm": 0.41574472188949585,
      "learning_rate": 0.0001778364116094987,
      "loss": 0.0911,
      "step": 505
    },
    {
      "epoch": 0.3337730870712401,
      "grad_norm": 0.1508847177028656,
      "learning_rate": 0.000177792436235708,
      "loss": 0.0504,
      "step": 506
    },
    {
      "epoch": 0.3344327176781003,
      "grad_norm": 0.2990347743034363,
      "learning_rate": 0.00017774846086191735,
      "loss": 0.061,
      "step": 507
    },
    {
      "epoch": 0.33509234828496043,
      "grad_norm": 0.22952520847320557,
      "learning_rate": 0.00017770448548812666,
      "loss": 0.0588,
      "step": 508
    },
    {
      "epoch": 0.33575197889182057,
      "grad_norm": 0.3681064546108246,
      "learning_rate": 0.00017766051011433597,
      "loss": 0.0944,
      "step": 509
    },
    {
      "epoch": 0.33641160949868076,
      "grad_norm": 0.1399623304605484,
      "learning_rate": 0.0001776165347405453,
      "loss": 0.03,
      "step": 510
    },
    {
      "epoch": 0.3370712401055409,
      "grad_norm": 0.25820067524909973,
      "learning_rate": 0.00017757255936675463,
      "loss": 0.0674,
      "step": 511
    },
    {
      "epoch": 0.33773087071240104,
      "grad_norm": 0.32180172204971313,
      "learning_rate": 0.00017752858399296394,
      "loss": 0.0424,
      "step": 512
    },
    {
      "epoch": 0.33839050131926124,
      "grad_norm": 0.22695939242839813,
      "learning_rate": 0.00017748460861917326,
      "loss": 0.0393,
      "step": 513
    },
    {
      "epoch": 0.3390501319261214,
      "grad_norm": 0.2922760546207428,
      "learning_rate": 0.0001774406332453826,
      "loss": 0.0505,
      "step": 514
    },
    {
      "epoch": 0.3397097625329815,
      "grad_norm": 0.20311327278614044,
      "learning_rate": 0.0001773966578715919,
      "loss": 0.0252,
      "step": 515
    },
    {
      "epoch": 0.3403693931398417,
      "grad_norm": 0.15219224989414215,
      "learning_rate": 0.00017735268249780123,
      "loss": 0.0102,
      "step": 516
    },
    {
      "epoch": 0.34102902374670185,
      "grad_norm": 0.11788099259138107,
      "learning_rate": 0.00017730870712401054,
      "loss": 0.0469,
      "step": 517
    },
    {
      "epoch": 0.341688654353562,
      "grad_norm": 0.1571049839258194,
      "learning_rate": 0.00017726473175021988,
      "loss": 0.0323,
      "step": 518
    },
    {
      "epoch": 0.3423482849604222,
      "grad_norm": 0.1453552544116974,
      "learning_rate": 0.0001772207563764292,
      "loss": 0.0434,
      "step": 519
    },
    {
      "epoch": 0.34300791556728233,
      "grad_norm": 0.08905339986085892,
      "learning_rate": 0.00017717678100263854,
      "loss": 0.0175,
      "step": 520
    },
    {
      "epoch": 0.34366754617414247,
      "grad_norm": 0.33609575033187866,
      "learning_rate": 0.00017713280562884785,
      "loss": 0.0956,
      "step": 521
    },
    {
      "epoch": 0.34432717678100266,
      "grad_norm": 0.08100952953100204,
      "learning_rate": 0.0001770888302550572,
      "loss": 0.0143,
      "step": 522
    },
    {
      "epoch": 0.3449868073878628,
      "grad_norm": 0.07588719576597214,
      "learning_rate": 0.0001770448548812665,
      "loss": 0.0155,
      "step": 523
    },
    {
      "epoch": 0.34564643799472294,
      "grad_norm": 0.16540296375751495,
      "learning_rate": 0.00017700087950747582,
      "loss": 0.0559,
      "step": 524
    },
    {
      "epoch": 0.34630606860158314,
      "grad_norm": 0.060502782464027405,
      "learning_rate": 0.00017695690413368516,
      "loss": 0.017,
      "step": 525
    },
    {
      "epoch": 0.3469656992084433,
      "grad_norm": 0.2910462021827698,
      "learning_rate": 0.00017691292875989447,
      "loss": 0.0967,
      "step": 526
    },
    {
      "epoch": 0.3476253298153034,
      "grad_norm": 0.203109472990036,
      "learning_rate": 0.0001768689533861038,
      "loss": 0.0667,
      "step": 527
    },
    {
      "epoch": 0.3482849604221636,
      "grad_norm": 0.11342716217041016,
      "learning_rate": 0.0001768249780123131,
      "loss": 0.0342,
      "step": 528
    },
    {
      "epoch": 0.34894459102902375,
      "grad_norm": 0.09255530685186386,
      "learning_rate": 0.00017678100263852244,
      "loss": 0.0304,
      "step": 529
    },
    {
      "epoch": 0.3496042216358839,
      "grad_norm": 0.1405492126941681,
      "learning_rate": 0.00017673702726473176,
      "loss": 0.0598,
      "step": 530
    },
    {
      "epoch": 0.3502638522427441,
      "grad_norm": 0.2468791902065277,
      "learning_rate": 0.00017669305189094107,
      "loss": 0.0291,
      "step": 531
    },
    {
      "epoch": 0.35092348284960423,
      "grad_norm": 0.2715703547000885,
      "learning_rate": 0.0001766490765171504,
      "loss": 0.0629,
      "step": 532
    },
    {
      "epoch": 0.35158311345646437,
      "grad_norm": 0.27965304255485535,
      "learning_rate": 0.00017660510114335973,
      "loss": 0.0498,
      "step": 533
    },
    {
      "epoch": 0.35224274406332456,
      "grad_norm": 0.11333087831735611,
      "learning_rate": 0.00017656112576956904,
      "loss": 0.0634,
      "step": 534
    },
    {
      "epoch": 0.3529023746701847,
      "grad_norm": 0.09166604280471802,
      "learning_rate": 0.00017651715039577835,
      "loss": 0.0514,
      "step": 535
    },
    {
      "epoch": 0.35356200527704484,
      "grad_norm": 0.16664843261241913,
      "learning_rate": 0.0001764731750219877,
      "loss": 0.0131,
      "step": 536
    },
    {
      "epoch": 0.35422163588390504,
      "grad_norm": 0.06952808052301407,
      "learning_rate": 0.000176429199648197,
      "loss": 0.0145,
      "step": 537
    },
    {
      "epoch": 0.3548812664907652,
      "grad_norm": 0.10104518383741379,
      "learning_rate": 0.00017638522427440632,
      "loss": 0.0209,
      "step": 538
    },
    {
      "epoch": 0.3555408970976253,
      "grad_norm": 0.16494765877723694,
      "learning_rate": 0.00017634124890061566,
      "loss": 0.0565,
      "step": 539
    },
    {
      "epoch": 0.3562005277044855,
      "grad_norm": 0.153669536113739,
      "learning_rate": 0.00017629727352682498,
      "loss": 0.0383,
      "step": 540
    },
    {
      "epoch": 0.35686015831134565,
      "grad_norm": 0.09522706270217896,
      "learning_rate": 0.00017625329815303432,
      "loss": 0.0306,
      "step": 541
    },
    {
      "epoch": 0.3575197889182058,
      "grad_norm": 0.12431294471025467,
      "learning_rate": 0.00017620932277924363,
      "loss": 0.0405,
      "step": 542
    },
    {
      "epoch": 0.358179419525066,
      "grad_norm": 0.2440674901008606,
      "learning_rate": 0.00017616534740545297,
      "loss": 0.0704,
      "step": 543
    },
    {
      "epoch": 0.35883905013192613,
      "grad_norm": 0.08420421928167343,
      "learning_rate": 0.0001761213720316623,
      "loss": 0.0396,
      "step": 544
    },
    {
      "epoch": 0.35949868073878627,
      "grad_norm": 0.05449569225311279,
      "learning_rate": 0.0001760773966578716,
      "loss": 0.0166,
      "step": 545
    },
    {
      "epoch": 0.36015831134564646,
      "grad_norm": 0.1502639204263687,
      "learning_rate": 0.00017603342128408092,
      "loss": 0.0381,
      "step": 546
    },
    {
      "epoch": 0.3608179419525066,
      "grad_norm": 0.09017304331064224,
      "learning_rate": 0.00017598944591029026,
      "loss": 0.0193,
      "step": 547
    },
    {
      "epoch": 0.36147757255936674,
      "grad_norm": 0.07766924798488617,
      "learning_rate": 0.00017594547053649957,
      "loss": 0.0141,
      "step": 548
    },
    {
      "epoch": 0.36213720316622694,
      "grad_norm": 0.6279740333557129,
      "learning_rate": 0.00017590149516270889,
      "loss": 0.058,
      "step": 549
    },
    {
      "epoch": 0.3627968337730871,
      "grad_norm": 0.15317709743976593,
      "learning_rate": 0.00017585751978891823,
      "loss": 0.0108,
      "step": 550
    },
    {
      "epoch": 0.3634564643799472,
      "grad_norm": 0.15768329799175262,
      "learning_rate": 0.00017581354441512754,
      "loss": 0.0111,
      "step": 551
    },
    {
      "epoch": 0.3641160949868074,
      "grad_norm": 0.15749453008174896,
      "learning_rate": 0.00017576956904133685,
      "loss": 0.0114,
      "step": 552
    },
    {
      "epoch": 0.36477572559366755,
      "grad_norm": 0.08472646772861481,
      "learning_rate": 0.00017572559366754617,
      "loss": 0.0194,
      "step": 553
    },
    {
      "epoch": 0.3654353562005277,
      "grad_norm": 0.08876559138298035,
      "learning_rate": 0.0001756816182937555,
      "loss": 0.0108,
      "step": 554
    },
    {
      "epoch": 0.3660949868073879,
      "grad_norm": 0.1103358343243599,
      "learning_rate": 0.00017563764291996482,
      "loss": 0.0186,
      "step": 555
    },
    {
      "epoch": 0.36675461741424803,
      "grad_norm": 0.053025148808956146,
      "learning_rate": 0.00017559366754617414,
      "loss": 0.009,
      "step": 556
    },
    {
      "epoch": 0.36741424802110817,
      "grad_norm": 0.24557147920131683,
      "learning_rate": 0.00017554969217238348,
      "loss": 0.0402,
      "step": 557
    },
    {
      "epoch": 0.36807387862796836,
      "grad_norm": 0.24285441637039185,
      "learning_rate": 0.0001755057167985928,
      "loss": 0.0444,
      "step": 558
    },
    {
      "epoch": 0.3687335092348285,
      "grad_norm": 0.11464741080999374,
      "learning_rate": 0.0001754617414248021,
      "loss": 0.0217,
      "step": 559
    },
    {
      "epoch": 0.36939313984168864,
      "grad_norm": 0.1534205973148346,
      "learning_rate": 0.00017541776605101142,
      "loss": 0.0434,
      "step": 560
    },
    {
      "epoch": 0.37005277044854884,
      "grad_norm": 0.1622535139322281,
      "learning_rate": 0.00017537379067722076,
      "loss": 0.0205,
      "step": 561
    },
    {
      "epoch": 0.370712401055409,
      "grad_norm": 0.09242672473192215,
      "learning_rate": 0.0001753298153034301,
      "loss": 0.0072,
      "step": 562
    },
    {
      "epoch": 0.3713720316622691,
      "grad_norm": 0.1214994490146637,
      "learning_rate": 0.00017528583992963942,
      "loss": 0.0102,
      "step": 563
    },
    {
      "epoch": 0.3720316622691293,
      "grad_norm": 0.18668735027313232,
      "learning_rate": 0.00017524186455584873,
      "loss": 0.0318,
      "step": 564
    },
    {
      "epoch": 0.37269129287598945,
      "grad_norm": 0.3094939887523651,
      "learning_rate": 0.00017519788918205807,
      "loss": 0.0854,
      "step": 565
    },
    {
      "epoch": 0.3733509234828496,
      "grad_norm": 0.11820654571056366,
      "learning_rate": 0.00017515391380826739,
      "loss": 0.0071,
      "step": 566
    },
    {
      "epoch": 0.3740105540897098,
      "grad_norm": 0.08634255826473236,
      "learning_rate": 0.0001751099384344767,
      "loss": 0.0181,
      "step": 567
    },
    {
      "epoch": 0.37467018469656993,
      "grad_norm": 0.11999626457691193,
      "learning_rate": 0.00017506596306068604,
      "loss": 0.0099,
      "step": 568
    },
    {
      "epoch": 0.37532981530343007,
      "grad_norm": 0.12005443125963211,
      "learning_rate": 0.00017502198768689535,
      "loss": 0.0186,
      "step": 569
    },
    {
      "epoch": 0.3759894459102902,
      "grad_norm": 0.29930341243743896,
      "learning_rate": 0.00017497801231310467,
      "loss": 0.063,
      "step": 570
    },
    {
      "epoch": 0.3766490765171504,
      "grad_norm": 0.13971397280693054,
      "learning_rate": 0.00017493403693931398,
      "loss": 0.0493,
      "step": 571
    },
    {
      "epoch": 0.37730870712401055,
      "grad_norm": 0.05365758016705513,
      "learning_rate": 0.00017489006156552332,
      "loss": 0.009,
      "step": 572
    },
    {
      "epoch": 0.3779683377308707,
      "grad_norm": 0.11153816431760788,
      "learning_rate": 0.00017484608619173264,
      "loss": 0.0342,
      "step": 573
    },
    {
      "epoch": 0.3786279683377309,
      "grad_norm": 0.1781376749277115,
      "learning_rate": 0.00017480211081794195,
      "loss": 0.0602,
      "step": 574
    },
    {
      "epoch": 0.379287598944591,
      "grad_norm": 0.12304244935512543,
      "learning_rate": 0.0001747581354441513,
      "loss": 0.017,
      "step": 575
    },
    {
      "epoch": 0.37994722955145116,
      "grad_norm": 0.13509978353977203,
      "learning_rate": 0.0001747141600703606,
      "loss": 0.0254,
      "step": 576
    },
    {
      "epoch": 0.38060686015831136,
      "grad_norm": 0.09458523243665695,
      "learning_rate": 0.00017467018469656992,
      "loss": 0.0293,
      "step": 577
    },
    {
      "epoch": 0.3812664907651715,
      "grad_norm": 0.12338078767061234,
      "learning_rate": 0.00017462620932277923,
      "loss": 0.0308,
      "step": 578
    },
    {
      "epoch": 0.38192612137203164,
      "grad_norm": 0.1271258145570755,
      "learning_rate": 0.00017458223394898858,
      "loss": 0.0249,
      "step": 579
    },
    {
      "epoch": 0.38258575197889183,
      "grad_norm": 0.13460206985473633,
      "learning_rate": 0.0001745382585751979,
      "loss": 0.0436,
      "step": 580
    },
    {
      "epoch": 0.38324538258575197,
      "grad_norm": 0.10893967747688293,
      "learning_rate": 0.0001744942832014072,
      "loss": 0.0276,
      "step": 581
    },
    {
      "epoch": 0.3839050131926121,
      "grad_norm": 0.14189521968364716,
      "learning_rate": 0.00017445030782761654,
      "loss": 0.0313,
      "step": 582
    },
    {
      "epoch": 0.3845646437994723,
      "grad_norm": 0.13459742069244385,
      "learning_rate": 0.00017440633245382586,
      "loss": 0.0196,
      "step": 583
    },
    {
      "epoch": 0.38522427440633245,
      "grad_norm": 0.12679463624954224,
      "learning_rate": 0.0001743623570800352,
      "loss": 0.0424,
      "step": 584
    },
    {
      "epoch": 0.3858839050131926,
      "grad_norm": 0.059193387627601624,
      "learning_rate": 0.00017431838170624451,
      "loss": 0.0063,
      "step": 585
    },
    {
      "epoch": 0.3865435356200528,
      "grad_norm": 0.1217634454369545,
      "learning_rate": 0.00017427440633245385,
      "loss": 0.047,
      "step": 586
    },
    {
      "epoch": 0.3872031662269129,
      "grad_norm": 0.05458416789770126,
      "learning_rate": 0.00017423043095866317,
      "loss": 0.0114,
      "step": 587
    },
    {
      "epoch": 0.38786279683377306,
      "grad_norm": 0.0850694552063942,
      "learning_rate": 0.00017418645558487248,
      "loss": 0.022,
      "step": 588
    },
    {
      "epoch": 0.38852242744063326,
      "grad_norm": 0.26102539896965027,
      "learning_rate": 0.0001741424802110818,
      "loss": 0.0728,
      "step": 589
    },
    {
      "epoch": 0.3891820580474934,
      "grad_norm": 0.23015743494033813,
      "learning_rate": 0.00017409850483729114,
      "loss": 0.034,
      "step": 590
    },
    {
      "epoch": 0.38984168865435354,
      "grad_norm": 0.05954989045858383,
      "learning_rate": 0.00017405452946350045,
      "loss": 0.0034,
      "step": 591
    },
    {
      "epoch": 0.39050131926121373,
      "grad_norm": 0.11615171283483505,
      "learning_rate": 0.00017401055408970977,
      "loss": 0.0445,
      "step": 592
    },
    {
      "epoch": 0.39116094986807387,
      "grad_norm": 0.39702269434928894,
      "learning_rate": 0.0001739665787159191,
      "loss": 0.0864,
      "step": 593
    },
    {
      "epoch": 0.391820580474934,
      "grad_norm": 0.1561075896024704,
      "learning_rate": 0.00017392260334212842,
      "loss": 0.0431,
      "step": 594
    },
    {
      "epoch": 0.3924802110817942,
      "grad_norm": 0.16138938069343567,
      "learning_rate": 0.00017387862796833773,
      "loss": 0.0366,
      "step": 595
    },
    {
      "epoch": 0.39313984168865435,
      "grad_norm": 0.16959801316261292,
      "learning_rate": 0.00017383465259454705,
      "loss": 0.0396,
      "step": 596
    },
    {
      "epoch": 0.3937994722955145,
      "grad_norm": 0.19846434891223907,
      "learning_rate": 0.0001737906772207564,
      "loss": 0.0249,
      "step": 597
    },
    {
      "epoch": 0.3944591029023747,
      "grad_norm": 0.27855753898620605,
      "learning_rate": 0.0001737467018469657,
      "loss": 0.041,
      "step": 598
    },
    {
      "epoch": 0.3951187335092348,
      "grad_norm": 0.2733895778656006,
      "learning_rate": 0.00017370272647317502,
      "loss": 0.0342,
      "step": 599
    },
    {
      "epoch": 0.39577836411609496,
      "grad_norm": 0.19528038799762726,
      "learning_rate": 0.00017365875109938433,
      "loss": 0.0654,
      "step": 600
    },
    {
      "epoch": 0.39643799472295516,
      "grad_norm": 0.12669987976551056,
      "learning_rate": 0.00017361477572559367,
      "loss": 0.025,
      "step": 601
    },
    {
      "epoch": 0.3970976253298153,
      "grad_norm": 0.16816918551921844,
      "learning_rate": 0.000173570800351803,
      "loss": 0.0268,
      "step": 602
    },
    {
      "epoch": 0.39775725593667544,
      "grad_norm": 0.14748910069465637,
      "learning_rate": 0.00017352682497801233,
      "loss": 0.0374,
      "step": 603
    },
    {
      "epoch": 0.39841688654353563,
      "grad_norm": 0.11140201985836029,
      "learning_rate": 0.00017348284960422164,
      "loss": 0.02,
      "step": 604
    },
    {
      "epoch": 0.39907651715039577,
      "grad_norm": 0.18530671298503876,
      "learning_rate": 0.00017343887423043098,
      "loss": 0.0437,
      "step": 605
    },
    {
      "epoch": 0.3997361477572559,
      "grad_norm": 0.09564288705587387,
      "learning_rate": 0.0001733948988566403,
      "loss": 0.0264,
      "step": 606
    },
    {
      "epoch": 0.4003957783641161,
      "grad_norm": 0.11248395591974258,
      "learning_rate": 0.0001733509234828496,
      "loss": 0.0372,
      "step": 607
    },
    {
      "epoch": 0.40105540897097625,
      "grad_norm": 0.26821836829185486,
      "learning_rate": 0.00017330694810905895,
      "loss": 0.0685,
      "step": 608
    },
    {
      "epoch": 0.4017150395778364,
      "grad_norm": 0.1726316213607788,
      "learning_rate": 0.00017326297273526827,
      "loss": 0.058,
      "step": 609
    },
    {
      "epoch": 0.4023746701846966,
      "grad_norm": 0.14271146059036255,
      "learning_rate": 0.00017321899736147758,
      "loss": 0.0431,
      "step": 610
    },
    {
      "epoch": 0.4030343007915567,
      "grad_norm": 0.12576010823249817,
      "learning_rate": 0.00017317502198768692,
      "loss": 0.0493,
      "step": 611
    },
    {
      "epoch": 0.40369393139841686,
      "grad_norm": 0.10740651935338974,
      "learning_rate": 0.00017313104661389623,
      "loss": 0.029,
      "step": 612
    },
    {
      "epoch": 0.40435356200527706,
      "grad_norm": 0.10534904152154922,
      "learning_rate": 0.00017308707124010555,
      "loss": 0.0501,
      "step": 613
    },
    {
      "epoch": 0.4050131926121372,
      "grad_norm": 0.14784583449363708,
      "learning_rate": 0.00017304309586631486,
      "loss": 0.0159,
      "step": 614
    },
    {
      "epoch": 0.40567282321899734,
      "grad_norm": 0.1654161810874939,
      "learning_rate": 0.0001729991204925242,
      "loss": 0.0274,
      "step": 615
    },
    {
      "epoch": 0.40633245382585753,
      "grad_norm": 0.0964420959353447,
      "learning_rate": 0.00017295514511873352,
      "loss": 0.0306,
      "step": 616
    },
    {
      "epoch": 0.40699208443271767,
      "grad_norm": 0.10248211771249771,
      "learning_rate": 0.00017291116974494283,
      "loss": 0.0156,
      "step": 617
    },
    {
      "epoch": 0.4076517150395778,
      "grad_norm": 0.12284287810325623,
      "learning_rate": 0.00017286719437115215,
      "loss": 0.0201,
      "step": 618
    },
    {
      "epoch": 0.408311345646438,
      "grad_norm": 0.2426195740699768,
      "learning_rate": 0.0001728232189973615,
      "loss": 0.0824,
      "step": 619
    },
    {
      "epoch": 0.40897097625329815,
      "grad_norm": 0.09909763187170029,
      "learning_rate": 0.0001727792436235708,
      "loss": 0.0066,
      "step": 620
    },
    {
      "epoch": 0.4096306068601583,
      "grad_norm": 0.0986630916595459,
      "learning_rate": 0.00017273526824978011,
      "loss": 0.0231,
      "step": 621
    },
    {
      "epoch": 0.4102902374670185,
      "grad_norm": 0.09607844054698944,
      "learning_rate": 0.00017269129287598946,
      "loss": 0.0254,
      "step": 622
    },
    {
      "epoch": 0.4109498680738786,
      "grad_norm": 0.13871046900749207,
      "learning_rate": 0.00017264731750219877,
      "loss": 0.0355,
      "step": 623
    },
    {
      "epoch": 0.41160949868073876,
      "grad_norm": 0.3096542954444885,
      "learning_rate": 0.00017260334212840808,
      "loss": 0.0903,
      "step": 624
    },
    {
      "epoch": 0.41226912928759896,
      "grad_norm": 0.16437563300132751,
      "learning_rate": 0.00017255936675461742,
      "loss": 0.0239,
      "step": 625
    },
    {
      "epoch": 0.4129287598944591,
      "grad_norm": 0.14544640481472015,
      "learning_rate": 0.00017251539138082674,
      "loss": 0.0498,
      "step": 626
    },
    {
      "epoch": 0.41358839050131924,
      "grad_norm": 0.10620558261871338,
      "learning_rate": 0.00017247141600703608,
      "loss": 0.0281,
      "step": 627
    },
    {
      "epoch": 0.41424802110817943,
      "grad_norm": 0.13954584300518036,
      "learning_rate": 0.0001724274406332454,
      "loss": 0.01,
      "step": 628
    },
    {
      "epoch": 0.41490765171503957,
      "grad_norm": 0.12443288415670395,
      "learning_rate": 0.00017238346525945473,
      "loss": 0.0082,
      "step": 629
    },
    {
      "epoch": 0.4155672823218997,
      "grad_norm": 0.08447328209877014,
      "learning_rate": 0.00017233948988566405,
      "loss": 0.0269,
      "step": 630
    },
    {
      "epoch": 0.4162269129287599,
      "grad_norm": 0.05555072799324989,
      "learning_rate": 0.00017229551451187336,
      "loss": 0.0126,
      "step": 631
    },
    {
      "epoch": 0.41688654353562005,
      "grad_norm": 0.12880447506904602,
      "learning_rate": 0.00017225153913808268,
      "loss": 0.0211,
      "step": 632
    },
    {
      "epoch": 0.4175461741424802,
      "grad_norm": 0.157676562666893,
      "learning_rate": 0.00017220756376429202,
      "loss": 0.0425,
      "step": 633
    },
    {
      "epoch": 0.4182058047493404,
      "grad_norm": 0.08693722635507584,
      "learning_rate": 0.00017216358839050133,
      "loss": 0.0057,
      "step": 634
    },
    {
      "epoch": 0.4188654353562005,
      "grad_norm": 0.10858321189880371,
      "learning_rate": 0.00017211961301671065,
      "loss": 0.0308,
      "step": 635
    },
    {
      "epoch": 0.41952506596306066,
      "grad_norm": 0.1768357753753662,
      "learning_rate": 0.00017207563764291996,
      "loss": 0.0503,
      "step": 636
    },
    {
      "epoch": 0.42018469656992086,
      "grad_norm": 0.09593243151903152,
      "learning_rate": 0.0001720316622691293,
      "loss": 0.0293,
      "step": 637
    },
    {
      "epoch": 0.420844327176781,
      "grad_norm": 0.06336937844753265,
      "learning_rate": 0.00017198768689533861,
      "loss": 0.0149,
      "step": 638
    },
    {
      "epoch": 0.42150395778364114,
      "grad_norm": 0.0789070725440979,
      "learning_rate": 0.00017194371152154793,
      "loss": 0.0128,
      "step": 639
    },
    {
      "epoch": 0.42216358839050133,
      "grad_norm": 0.07617644220590591,
      "learning_rate": 0.00017189973614775727,
      "loss": 0.0216,
      "step": 640
    },
    {
      "epoch": 0.42282321899736147,
      "grad_norm": 0.17923715710639954,
      "learning_rate": 0.00017185576077396658,
      "loss": 0.0266,
      "step": 641
    },
    {
      "epoch": 0.4234828496042216,
      "grad_norm": 0.29969608783721924,
      "learning_rate": 0.0001718117854001759,
      "loss": 0.0748,
      "step": 642
    },
    {
      "epoch": 0.4241424802110818,
      "grad_norm": 0.13071812689304352,
      "learning_rate": 0.0001717678100263852,
      "loss": 0.0253,
      "step": 643
    },
    {
      "epoch": 0.42480211081794195,
      "grad_norm": 0.15491260588169098,
      "learning_rate": 0.00017172383465259455,
      "loss": 0.0421,
      "step": 644
    },
    {
      "epoch": 0.4254617414248021,
      "grad_norm": 0.24334624409675598,
      "learning_rate": 0.00017167985927880387,
      "loss": 0.0756,
      "step": 645
    },
    {
      "epoch": 0.4261213720316623,
      "grad_norm": 0.22025108337402344,
      "learning_rate": 0.0001716358839050132,
      "loss": 0.0234,
      "step": 646
    },
    {
      "epoch": 0.4267810026385224,
      "grad_norm": 0.18972614407539368,
      "learning_rate": 0.00017159190853122252,
      "loss": 0.0399,
      "step": 647
    },
    {
      "epoch": 0.42744063324538256,
      "grad_norm": 0.2991578280925751,
      "learning_rate": 0.00017154793315743186,
      "loss": 0.0214,
      "step": 648
    },
    {
      "epoch": 0.42810026385224276,
      "grad_norm": 0.1477070450782776,
      "learning_rate": 0.00017150395778364118,
      "loss": 0.0623,
      "step": 649
    },
    {
      "epoch": 0.4287598944591029,
      "grad_norm": 0.11711940914392471,
      "learning_rate": 0.0001714599824098505,
      "loss": 0.0389,
      "step": 650
    },
    {
      "epoch": 0.42941952506596304,
      "grad_norm": 0.1010143905878067,
      "learning_rate": 0.00017141600703605983,
      "loss": 0.0292,
      "step": 651
    },
    {
      "epoch": 0.43007915567282323,
      "grad_norm": 0.27926313877105713,
      "learning_rate": 0.00017137203166226915,
      "loss": 0.068,
      "step": 652
    },
    {
      "epoch": 0.43073878627968337,
      "grad_norm": 0.08412474393844604,
      "learning_rate": 0.00017132805628847846,
      "loss": 0.0052,
      "step": 653
    },
    {
      "epoch": 0.4313984168865435,
      "grad_norm": 0.14838393032550812,
      "learning_rate": 0.00017128408091468777,
      "loss": 0.0495,
      "step": 654
    },
    {
      "epoch": 0.4320580474934037,
      "grad_norm": 0.10675439983606339,
      "learning_rate": 0.00017124010554089711,
      "loss": 0.0183,
      "step": 655
    },
    {
      "epoch": 0.43271767810026385,
      "grad_norm": 0.11925056576728821,
      "learning_rate": 0.00017119613016710643,
      "loss": 0.037,
      "step": 656
    },
    {
      "epoch": 0.433377308707124,
      "grad_norm": 0.23897388577461243,
      "learning_rate": 0.00017115215479331574,
      "loss": 0.0716,
      "step": 657
    },
    {
      "epoch": 0.4340369393139842,
      "grad_norm": 0.1437949538230896,
      "learning_rate": 0.00017110817941952508,
      "loss": 0.0426,
      "step": 658
    },
    {
      "epoch": 0.4346965699208443,
      "grad_norm": 0.1784209907054901,
      "learning_rate": 0.0001710642040457344,
      "loss": 0.0343,
      "step": 659
    },
    {
      "epoch": 0.43535620052770446,
      "grad_norm": 0.19162200391292572,
      "learning_rate": 0.0001710202286719437,
      "loss": 0.0375,
      "step": 660
    },
    {
      "epoch": 0.43601583113456466,
      "grad_norm": 0.11864830553531647,
      "learning_rate": 0.00017097625329815303,
      "loss": 0.0417,
      "step": 661
    },
    {
      "epoch": 0.4366754617414248,
      "grad_norm": 0.19719339907169342,
      "learning_rate": 0.00017093227792436237,
      "loss": 0.0141,
      "step": 662
    },
    {
      "epoch": 0.43733509234828494,
      "grad_norm": 0.3453345000743866,
      "learning_rate": 0.00017088830255057168,
      "loss": 0.0771,
      "step": 663
    },
    {
      "epoch": 0.43799472295514513,
      "grad_norm": 0.16988348960876465,
      "learning_rate": 0.000170844327176781,
      "loss": 0.0476,
      "step": 664
    },
    {
      "epoch": 0.4386543535620053,
      "grad_norm": 0.1897146999835968,
      "learning_rate": 0.00017080035180299034,
      "loss": 0.0137,
      "step": 665
    },
    {
      "epoch": 0.4393139841688654,
      "grad_norm": 0.19516395032405853,
      "learning_rate": 0.00017075637642919965,
      "loss": 0.0192,
      "step": 666
    },
    {
      "epoch": 0.4399736147757256,
      "grad_norm": 0.12646600604057312,
      "learning_rate": 0.000170712401055409,
      "loss": 0.0337,
      "step": 667
    },
    {
      "epoch": 0.44063324538258575,
      "grad_norm": 0.13872069120407104,
      "learning_rate": 0.0001706684256816183,
      "loss": 0.0425,
      "step": 668
    },
    {
      "epoch": 0.4412928759894459,
      "grad_norm": 0.07439431548118591,
      "learning_rate": 0.00017062445030782765,
      "loss": 0.0214,
      "step": 669
    },
    {
      "epoch": 0.4419525065963061,
      "grad_norm": 0.09757578372955322,
      "learning_rate": 0.00017058047493403696,
      "loss": 0.0279,
      "step": 670
    },
    {
      "epoch": 0.4426121372031662,
      "grad_norm": 0.11254796385765076,
      "learning_rate": 0.00017053649956024627,
      "loss": 0.0307,
      "step": 671
    },
    {
      "epoch": 0.44327176781002636,
      "grad_norm": 0.3169785141944885,
      "learning_rate": 0.0001704925241864556,
      "loss": 0.0751,
      "step": 672
    },
    {
      "epoch": 0.44393139841688656,
      "grad_norm": 0.17285670340061188,
      "learning_rate": 0.00017044854881266493,
      "loss": 0.0412,
      "step": 673
    },
    {
      "epoch": 0.4445910290237467,
      "grad_norm": 0.10227113217115402,
      "learning_rate": 0.00017040457343887424,
      "loss": 0.0347,
      "step": 674
    },
    {
      "epoch": 0.44525065963060684,
      "grad_norm": 0.20437723398208618,
      "learning_rate": 0.00017036059806508356,
      "loss": 0.0132,
      "step": 675
    },
    {
      "epoch": 0.44591029023746703,
      "grad_norm": 0.20661160349845886,
      "learning_rate": 0.0001703166226912929,
      "loss": 0.0709,
      "step": 676
    },
    {
      "epoch": 0.4465699208443272,
      "grad_norm": 0.10978713631629944,
      "learning_rate": 0.0001702726473175022,
      "loss": 0.031,
      "step": 677
    },
    {
      "epoch": 0.4472295514511873,
      "grad_norm": 0.13026802241802216,
      "learning_rate": 0.00017022867194371153,
      "loss": 0.0199,
      "step": 678
    },
    {
      "epoch": 0.4478891820580475,
      "grad_norm": 0.17573313415050507,
      "learning_rate": 0.00017018469656992084,
      "loss": 0.0307,
      "step": 679
    },
    {
      "epoch": 0.44854881266490765,
      "grad_norm": 0.1286337673664093,
      "learning_rate": 0.00017014072119613018,
      "loss": 0.0288,
      "step": 680
    },
    {
      "epoch": 0.4492084432717678,
      "grad_norm": 0.13432836532592773,
      "learning_rate": 0.0001700967458223395,
      "loss": 0.0253,
      "step": 681
    },
    {
      "epoch": 0.449868073878628,
      "grad_norm": 0.137332946062088,
      "learning_rate": 0.0001700527704485488,
      "loss": 0.018,
      "step": 682
    },
    {
      "epoch": 0.4505277044854881,
      "grad_norm": 0.1680961549282074,
      "learning_rate": 0.00017000879507475815,
      "loss": 0.0252,
      "step": 683
    },
    {
      "epoch": 0.45118733509234826,
      "grad_norm": 0.1503438800573349,
      "learning_rate": 0.00016996481970096746,
      "loss": 0.0285,
      "step": 684
    },
    {
      "epoch": 0.45184696569920846,
      "grad_norm": 0.20819221436977386,
      "learning_rate": 0.00016992084432717678,
      "loss": 0.048,
      "step": 685
    },
    {
      "epoch": 0.4525065963060686,
      "grad_norm": 0.0681789219379425,
      "learning_rate": 0.0001698768689533861,
      "loss": 0.0144,
      "step": 686
    },
    {
      "epoch": 0.45316622691292874,
      "grad_norm": 0.11056903004646301,
      "learning_rate": 0.00016983289357959543,
      "loss": 0.025,
      "step": 687
    },
    {
      "epoch": 0.45382585751978893,
      "grad_norm": 0.06471849232912064,
      "learning_rate": 0.00016978891820580475,
      "loss": 0.0036,
      "step": 688
    },
    {
      "epoch": 0.4544854881266491,
      "grad_norm": 0.13875260949134827,
      "learning_rate": 0.0001697449428320141,
      "loss": 0.0234,
      "step": 689
    },
    {
      "epoch": 0.4551451187335092,
      "grad_norm": 0.249802827835083,
      "learning_rate": 0.0001697009674582234,
      "loss": 0.0601,
      "step": 690
    },
    {
      "epoch": 0.4558047493403694,
      "grad_norm": 0.239874005317688,
      "learning_rate": 0.00016965699208443274,
      "loss": 0.0472,
      "step": 691
    },
    {
      "epoch": 0.45646437994722955,
      "grad_norm": 0.32739171385765076,
      "learning_rate": 0.00016961301671064206,
      "loss": 0.0611,
      "step": 692
    },
    {
      "epoch": 0.4571240105540897,
      "grad_norm": 0.13187430799007416,
      "learning_rate": 0.00016956904133685137,
      "loss": 0.0287,
      "step": 693
    },
    {
      "epoch": 0.4577836411609499,
      "grad_norm": 0.15504947304725647,
      "learning_rate": 0.0001695250659630607,
      "loss": 0.0093,
      "step": 694
    },
    {
      "epoch": 0.45844327176781,
      "grad_norm": 0.16062529385089874,
      "learning_rate": 0.00016948109058927003,
      "loss": 0.0449,
      "step": 695
    },
    {
      "epoch": 0.45910290237467016,
      "grad_norm": 0.26844239234924316,
      "learning_rate": 0.00016943711521547934,
      "loss": 0.0184,
      "step": 696
    },
    {
      "epoch": 0.45976253298153036,
      "grad_norm": 0.16579926013946533,
      "learning_rate": 0.00016939313984168865,
      "loss": 0.0175,
      "step": 697
    },
    {
      "epoch": 0.4604221635883905,
      "grad_norm": 0.1426556408405304,
      "learning_rate": 0.000169349164467898,
      "loss": 0.0397,
      "step": 698
    },
    {
      "epoch": 0.46108179419525064,
      "grad_norm": 0.10163968056440353,
      "learning_rate": 0.0001693051890941073,
      "loss": 0.0205,
      "step": 699
    },
    {
      "epoch": 0.46174142480211083,
      "grad_norm": 0.09195016324520111,
      "learning_rate": 0.00016926121372031662,
      "loss": 0.0061,
      "step": 700
    },
    {
      "epoch": 0.462401055408971,
      "grad_norm": 0.18080900609493256,
      "learning_rate": 0.00016921723834652596,
      "loss": 0.0464,
      "step": 701
    },
    {
      "epoch": 0.4630606860158311,
      "grad_norm": 0.18538020551204681,
      "learning_rate": 0.00016917326297273528,
      "loss": 0.0396,
      "step": 702
    },
    {
      "epoch": 0.4637203166226913,
      "grad_norm": 0.0946730226278305,
      "learning_rate": 0.0001691292875989446,
      "loss": 0.0266,
      "step": 703
    },
    {
      "epoch": 0.46437994722955145,
      "grad_norm": 0.04454510658979416,
      "learning_rate": 0.0001690853122251539,
      "loss": 0.0027,
      "step": 704
    },
    {
      "epoch": 0.4650395778364116,
      "grad_norm": 0.29219353199005127,
      "learning_rate": 0.00016904133685136325,
      "loss": 0.0948,
      "step": 705
    },
    {
      "epoch": 0.4656992084432718,
      "grad_norm": 0.3077755570411682,
      "learning_rate": 0.00016899736147757256,
      "loss": 0.0846,
      "step": 706
    },
    {
      "epoch": 0.4663588390501319,
      "grad_norm": 0.1003488227725029,
      "learning_rate": 0.00016895338610378187,
      "loss": 0.0268,
      "step": 707
    },
    {
      "epoch": 0.46701846965699206,
      "grad_norm": 0.07244012504816055,
      "learning_rate": 0.00016890941072999122,
      "loss": 0.022,
      "step": 708
    },
    {
      "epoch": 0.46767810026385226,
      "grad_norm": 0.09854458272457123,
      "learning_rate": 0.00016886543535620053,
      "loss": 0.0218,
      "step": 709
    },
    {
      "epoch": 0.4683377308707124,
      "grad_norm": 0.15189984440803528,
      "learning_rate": 0.00016882145998240987,
      "loss": 0.023,
      "step": 710
    },
    {
      "epoch": 0.46899736147757254,
      "grad_norm": 0.16658373177051544,
      "learning_rate": 0.00016877748460861918,
      "loss": 0.0633,
      "step": 711
    },
    {
      "epoch": 0.46965699208443273,
      "grad_norm": 0.11164552718400955,
      "learning_rate": 0.00016873350923482853,
      "loss": 0.0515,
      "step": 712
    },
    {
      "epoch": 0.4703166226912929,
      "grad_norm": 0.1381760984659195,
      "learning_rate": 0.00016868953386103784,
      "loss": 0.0501,
      "step": 713
    },
    {
      "epoch": 0.470976253298153,
      "grad_norm": 0.180783212184906,
      "learning_rate": 0.00016864555848724715,
      "loss": 0.0383,
      "step": 714
    },
    {
      "epoch": 0.4716358839050132,
      "grad_norm": 0.15020851790905,
      "learning_rate": 0.00016860158311345647,
      "loss": 0.0347,
      "step": 715
    },
    {
      "epoch": 0.47229551451187335,
      "grad_norm": 0.21067741513252258,
      "learning_rate": 0.0001685576077396658,
      "loss": 0.0188,
      "step": 716
    },
    {
      "epoch": 0.4729551451187335,
      "grad_norm": 0.18821308016777039,
      "learning_rate": 0.00016851363236587512,
      "loss": 0.0257,
      "step": 717
    },
    {
      "epoch": 0.4736147757255937,
      "grad_norm": 0.10825753957033157,
      "learning_rate": 0.00016846965699208444,
      "loss": 0.034,
      "step": 718
    },
    {
      "epoch": 0.4742744063324538,
      "grad_norm": 0.1268332302570343,
      "learning_rate": 0.00016842568161829378,
      "loss": 0.0156,
      "step": 719
    },
    {
      "epoch": 0.47493403693931396,
      "grad_norm": 0.19828951358795166,
      "learning_rate": 0.0001683817062445031,
      "loss": 0.0275,
      "step": 720
    },
    {
      "epoch": 0.47559366754617416,
      "grad_norm": 0.1330440640449524,
      "learning_rate": 0.0001683377308707124,
      "loss": 0.0253,
      "step": 721
    },
    {
      "epoch": 0.4762532981530343,
      "grad_norm": 0.24470682442188263,
      "learning_rate": 0.00016829375549692172,
      "loss": 0.0353,
      "step": 722
    },
    {
      "epoch": 0.47691292875989444,
      "grad_norm": 0.07278455048799515,
      "learning_rate": 0.00016824978012313106,
      "loss": 0.0136,
      "step": 723
    },
    {
      "epoch": 0.47757255936675463,
      "grad_norm": 0.0699383020401001,
      "learning_rate": 0.00016820580474934037,
      "loss": 0.0043,
      "step": 724
    },
    {
      "epoch": 0.4782321899736148,
      "grad_norm": 0.16378137469291687,
      "learning_rate": 0.0001681618293755497,
      "loss": 0.0328,
      "step": 725
    },
    {
      "epoch": 0.4788918205804749,
      "grad_norm": 0.11612925678491592,
      "learning_rate": 0.000168117854001759,
      "loss": 0.0202,
      "step": 726
    },
    {
      "epoch": 0.4795514511873351,
      "grad_norm": 0.2824101448059082,
      "learning_rate": 0.00016807387862796834,
      "loss": 0.0521,
      "step": 727
    },
    {
      "epoch": 0.48021108179419525,
      "grad_norm": 0.13710707426071167,
      "learning_rate": 0.00016802990325417766,
      "loss": 0.0228,
      "step": 728
    },
    {
      "epoch": 0.4808707124010554,
      "grad_norm": 0.14824001491069794,
      "learning_rate": 0.00016798592788038697,
      "loss": 0.0288,
      "step": 729
    },
    {
      "epoch": 0.4815303430079156,
      "grad_norm": 0.13887721300125122,
      "learning_rate": 0.0001679419525065963,
      "loss": 0.031,
      "step": 730
    },
    {
      "epoch": 0.4821899736147757,
      "grad_norm": 0.15214954316616058,
      "learning_rate": 0.00016789797713280563,
      "loss": 0.0236,
      "step": 731
    },
    {
      "epoch": 0.48284960422163586,
      "grad_norm": 0.1885674148797989,
      "learning_rate": 0.00016785400175901497,
      "loss": 0.0428,
      "step": 732
    },
    {
      "epoch": 0.48350923482849606,
      "grad_norm": 0.13549697399139404,
      "learning_rate": 0.00016781002638522428,
      "loss": 0.0084,
      "step": 733
    },
    {
      "epoch": 0.4841688654353562,
      "grad_norm": 0.17942377924919128,
      "learning_rate": 0.00016776605101143362,
      "loss": 0.0294,
      "step": 734
    },
    {
      "epoch": 0.48482849604221634,
      "grad_norm": 0.2669023275375366,
      "learning_rate": 0.00016772207563764294,
      "loss": 0.0635,
      "step": 735
    },
    {
      "epoch": 0.48548812664907653,
      "grad_norm": 0.21253515779972076,
      "learning_rate": 0.00016767810026385225,
      "loss": 0.0547,
      "step": 736
    },
    {
      "epoch": 0.4861477572559367,
      "grad_norm": 0.1683473140001297,
      "learning_rate": 0.0001676341248900616,
      "loss": 0.0271,
      "step": 737
    },
    {
      "epoch": 0.4868073878627968,
      "grad_norm": 0.17070162296295166,
      "learning_rate": 0.0001675901495162709,
      "loss": 0.0152,
      "step": 738
    },
    {
      "epoch": 0.487467018469657,
      "grad_norm": 0.3062136173248291,
      "learning_rate": 0.00016754617414248022,
      "loss": 0.1041,
      "step": 739
    },
    {
      "epoch": 0.48812664907651715,
      "grad_norm": 0.24565961956977844,
      "learning_rate": 0.00016750219876868953,
      "loss": 0.0486,
      "step": 740
    },
    {
      "epoch": 0.4887862796833773,
      "grad_norm": 0.09109683334827423,
      "learning_rate": 0.00016745822339489887,
      "loss": 0.006,
      "step": 741
    },
    {
      "epoch": 0.4894459102902375,
      "grad_norm": 0.17389611899852753,
      "learning_rate": 0.0001674142480211082,
      "loss": 0.0565,
      "step": 742
    },
    {
      "epoch": 0.4901055408970976,
      "grad_norm": 0.0947922021150589,
      "learning_rate": 0.0001673702726473175,
      "loss": 0.0184,
      "step": 743
    },
    {
      "epoch": 0.49076517150395776,
      "grad_norm": 0.08004336059093475,
      "learning_rate": 0.00016732629727352682,
      "loss": 0.0059,
      "step": 744
    },
    {
      "epoch": 0.49142480211081796,
      "grad_norm": 0.09362511336803436,
      "learning_rate": 0.00016728232189973616,
      "loss": 0.0067,
      "step": 745
    },
    {
      "epoch": 0.4920844327176781,
      "grad_norm": 0.09706075489521027,
      "learning_rate": 0.00016723834652594547,
      "loss": 0.0304,
      "step": 746
    },
    {
      "epoch": 0.49274406332453824,
      "grad_norm": 0.12087903171777725,
      "learning_rate": 0.00016719437115215479,
      "loss": 0.0254,
      "step": 747
    },
    {
      "epoch": 0.49340369393139843,
      "grad_norm": 0.18939432501792908,
      "learning_rate": 0.00016715039577836413,
      "loss": 0.042,
      "step": 748
    },
    {
      "epoch": 0.4940633245382586,
      "grad_norm": 0.18836718797683716,
      "learning_rate": 0.00016710642040457344,
      "loss": 0.0442,
      "step": 749
    },
    {
      "epoch": 0.4947229551451187,
      "grad_norm": 0.08189081400632858,
      "learning_rate": 0.00016706244503078275,
      "loss": 0.006,
      "step": 750
    },
    {
      "epoch": 0.4953825857519789,
      "grad_norm": 0.1043143942952156,
      "learning_rate": 0.0001670184696569921,
      "loss": 0.0088,
      "step": 751
    },
    {
      "epoch": 0.49604221635883905,
      "grad_norm": 0.11912861466407776,
      "learning_rate": 0.0001669744942832014,
      "loss": 0.0332,
      "step": 752
    },
    {
      "epoch": 0.4967018469656992,
      "grad_norm": 0.12317201495170593,
      "learning_rate": 0.00016693051890941075,
      "loss": 0.0324,
      "step": 753
    },
    {
      "epoch": 0.4973614775725594,
      "grad_norm": 0.08862116932868958,
      "learning_rate": 0.00016688654353562006,
      "loss": 0.0241,
      "step": 754
    },
    {
      "epoch": 0.4980211081794195,
      "grad_norm": 0.11721250414848328,
      "learning_rate": 0.0001668425681618294,
      "loss": 0.0421,
      "step": 755
    },
    {
      "epoch": 0.49868073878627966,
      "grad_norm": 0.24449339509010315,
      "learning_rate": 0.00016679859278803872,
      "loss": 0.068,
      "step": 756
    },
    {
      "epoch": 0.49934036939313986,
      "grad_norm": 0.1834089308977127,
      "learning_rate": 0.00016675461741424803,
      "loss": 0.048,
      "step": 757
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.13484910130500793,
      "learning_rate": 0.00016671064204045735,
      "loss": 0.0457,
      "step": 758
    },
    {
      "epoch": 0.5006596306068601,
      "grad_norm": 0.15829245746135712,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.0152,
      "step": 759
    },
    {
      "epoch": 0.5013192612137203,
      "grad_norm": 0.16184395551681519,
      "learning_rate": 0.000166622691292876,
      "loss": 0.024,
      "step": 760
    },
    {
      "epoch": 0.5019788918205804,
      "grad_norm": 0.19768235087394714,
      "learning_rate": 0.00016657871591908532,
      "loss": 0.035,
      "step": 761
    },
    {
      "epoch": 0.5026385224274407,
      "grad_norm": 0.11875247955322266,
      "learning_rate": 0.00016653474054529466,
      "loss": 0.0206,
      "step": 762
    },
    {
      "epoch": 0.5032981530343008,
      "grad_norm": 0.1290995329618454,
      "learning_rate": 0.00016649076517150397,
      "loss": 0.0188,
      "step": 763
    },
    {
      "epoch": 0.503957783641161,
      "grad_norm": 0.09279179573059082,
      "learning_rate": 0.00016644678979771329,
      "loss": 0.0206,
      "step": 764
    },
    {
      "epoch": 0.5046174142480211,
      "grad_norm": 0.15528860688209534,
      "learning_rate": 0.0001664028144239226,
      "loss": 0.0463,
      "step": 765
    },
    {
      "epoch": 0.5052770448548812,
      "grad_norm": 0.10247810184955597,
      "learning_rate": 0.00016635883905013194,
      "loss": 0.0341,
      "step": 766
    },
    {
      "epoch": 0.5059366754617414,
      "grad_norm": 0.26573941111564636,
      "learning_rate": 0.00016631486367634125,
      "loss": 0.053,
      "step": 767
    },
    {
      "epoch": 0.5065963060686016,
      "grad_norm": 0.09183336794376373,
      "learning_rate": 0.00016627088830255057,
      "loss": 0.0203,
      "step": 768
    },
    {
      "epoch": 0.5072559366754618,
      "grad_norm": 0.2392536848783493,
      "learning_rate": 0.00016622691292875988,
      "loss": 0.0472,
      "step": 769
    },
    {
      "epoch": 0.5079155672823219,
      "grad_norm": 0.15285541117191315,
      "learning_rate": 0.00016618293755496922,
      "loss": 0.0284,
      "step": 770
    },
    {
      "epoch": 0.508575197889182,
      "grad_norm": 0.1571388840675354,
      "learning_rate": 0.00016613896218117854,
      "loss": 0.0384,
      "step": 771
    },
    {
      "epoch": 0.5092348284960422,
      "grad_norm": 0.13082726299762726,
      "learning_rate": 0.00016609498680738785,
      "loss": 0.0112,
      "step": 772
    },
    {
      "epoch": 0.5098944591029023,
      "grad_norm": 0.12676246464252472,
      "learning_rate": 0.0001660510114335972,
      "loss": 0.0395,
      "step": 773
    },
    {
      "epoch": 0.5105540897097626,
      "grad_norm": 0.15130093693733215,
      "learning_rate": 0.00016600703605980653,
      "loss": 0.0379,
      "step": 774
    },
    {
      "epoch": 0.5112137203166227,
      "grad_norm": 0.24176988005638123,
      "learning_rate": 0.00016596306068601585,
      "loss": 0.0188,
      "step": 775
    },
    {
      "epoch": 0.5118733509234829,
      "grad_norm": 0.10990148782730103,
      "learning_rate": 0.00016591908531222516,
      "loss": 0.017,
      "step": 776
    },
    {
      "epoch": 0.512532981530343,
      "grad_norm": 0.11432012170553207,
      "learning_rate": 0.0001658751099384345,
      "loss": 0.0273,
      "step": 777
    },
    {
      "epoch": 0.5131926121372031,
      "grad_norm": 0.17256824672222137,
      "learning_rate": 0.00016583113456464382,
      "loss": 0.0288,
      "step": 778
    },
    {
      "epoch": 0.5138522427440633,
      "grad_norm": 0.2287413328886032,
      "learning_rate": 0.00016578715919085313,
      "loss": 0.0509,
      "step": 779
    },
    {
      "epoch": 0.5145118733509235,
      "grad_norm": 0.21687175333499908,
      "learning_rate": 0.00016574318381706247,
      "loss": 0.0291,
      "step": 780
    },
    {
      "epoch": 0.5151715039577837,
      "grad_norm": 0.20919537544250488,
      "learning_rate": 0.00016569920844327179,
      "loss": 0.0269,
      "step": 781
    },
    {
      "epoch": 0.5158311345646438,
      "grad_norm": 0.07335364818572998,
      "learning_rate": 0.0001656552330694811,
      "loss": 0.008,
      "step": 782
    },
    {
      "epoch": 0.5164907651715039,
      "grad_norm": 0.24343697726726532,
      "learning_rate": 0.0001656112576956904,
      "loss": 0.0329,
      "step": 783
    },
    {
      "epoch": 0.5171503957783641,
      "grad_norm": 0.1775093376636505,
      "learning_rate": 0.00016556728232189975,
      "loss": 0.0339,
      "step": 784
    },
    {
      "epoch": 0.5178100263852242,
      "grad_norm": 0.15460079908370972,
      "learning_rate": 0.00016552330694810907,
      "loss": 0.0214,
      "step": 785
    },
    {
      "epoch": 0.5184696569920845,
      "grad_norm": 0.10496518015861511,
      "learning_rate": 0.00016547933157431838,
      "loss": 0.0064,
      "step": 786
    },
    {
      "epoch": 0.5191292875989446,
      "grad_norm": 0.1062120795249939,
      "learning_rate": 0.0001654353562005277,
      "loss": 0.0206,
      "step": 787
    },
    {
      "epoch": 0.5197889182058048,
      "grad_norm": 0.0770968496799469,
      "learning_rate": 0.00016539138082673704,
      "loss": 0.0057,
      "step": 788
    },
    {
      "epoch": 0.5204485488126649,
      "grad_norm": 0.20542390644550323,
      "learning_rate": 0.00016534740545294635,
      "loss": 0.0309,
      "step": 789
    },
    {
      "epoch": 0.521108179419525,
      "grad_norm": 0.16939209401607513,
      "learning_rate": 0.00016530343007915566,
      "loss": 0.0295,
      "step": 790
    },
    {
      "epoch": 0.5217678100263852,
      "grad_norm": 0.13170386850833893,
      "learning_rate": 0.000165259454705365,
      "loss": 0.0298,
      "step": 791
    },
    {
      "epoch": 0.5224274406332454,
      "grad_norm": 0.07829774916172028,
      "learning_rate": 0.00016521547933157432,
      "loss": 0.0097,
      "step": 792
    },
    {
      "epoch": 0.5230870712401056,
      "grad_norm": 0.13823619484901428,
      "learning_rate": 0.00016517150395778363,
      "loss": 0.017,
      "step": 793
    },
    {
      "epoch": 0.5237467018469657,
      "grad_norm": 0.15901830792427063,
      "learning_rate": 0.00016512752858399298,
      "loss": 0.0118,
      "step": 794
    },
    {
      "epoch": 0.5244063324538258,
      "grad_norm": 0.12909024953842163,
      "learning_rate": 0.0001650835532102023,
      "loss": 0.0112,
      "step": 795
    },
    {
      "epoch": 0.525065963060686,
      "grad_norm": 0.07755231112241745,
      "learning_rate": 0.00016503957783641163,
      "loss": 0.0128,
      "step": 796
    },
    {
      "epoch": 0.5257255936675461,
      "grad_norm": 0.2716083526611328,
      "learning_rate": 0.00016499560246262094,
      "loss": 0.0654,
      "step": 797
    },
    {
      "epoch": 0.5263852242744064,
      "grad_norm": 0.06811270117759705,
      "learning_rate": 0.00016495162708883029,
      "loss": 0.0121,
      "step": 798
    },
    {
      "epoch": 0.5270448548812665,
      "grad_norm": 0.08879679441452026,
      "learning_rate": 0.0001649076517150396,
      "loss": 0.0166,
      "step": 799
    },
    {
      "epoch": 0.5277044854881267,
      "grad_norm": 0.09889304637908936,
      "learning_rate": 0.0001648636763412489,
      "loss": 0.0167,
      "step": 800
    },
    {
      "epoch": 0.5283641160949868,
      "grad_norm": 0.12466204911470413,
      "learning_rate": 0.00016481970096745823,
      "loss": 0.0119,
      "step": 801
    },
    {
      "epoch": 0.5290237467018469,
      "grad_norm": 0.17922843992710114,
      "learning_rate": 0.00016477572559366757,
      "loss": 0.0313,
      "step": 802
    },
    {
      "epoch": 0.5296833773087071,
      "grad_norm": 0.17628532648086548,
      "learning_rate": 0.00016473175021987688,
      "loss": 0.048,
      "step": 803
    },
    {
      "epoch": 0.5303430079155673,
      "grad_norm": 0.09787875413894653,
      "learning_rate": 0.0001646877748460862,
      "loss": 0.0242,
      "step": 804
    },
    {
      "epoch": 0.5310026385224275,
      "grad_norm": 0.1326446384191513,
      "learning_rate": 0.0001646437994722955,
      "loss": 0.0255,
      "step": 805
    },
    {
      "epoch": 0.5316622691292876,
      "grad_norm": 0.12446925044059753,
      "learning_rate": 0.00016459982409850485,
      "loss": 0.0234,
      "step": 806
    },
    {
      "epoch": 0.5323218997361477,
      "grad_norm": 0.11294776946306229,
      "learning_rate": 0.00016455584872471416,
      "loss": 0.0238,
      "step": 807
    },
    {
      "epoch": 0.5329815303430079,
      "grad_norm": 0.1418045461177826,
      "learning_rate": 0.00016451187335092348,
      "loss": 0.0298,
      "step": 808
    },
    {
      "epoch": 0.533641160949868,
      "grad_norm": 0.12874659895896912,
      "learning_rate": 0.00016446789797713282,
      "loss": 0.0339,
      "step": 809
    },
    {
      "epoch": 0.5343007915567283,
      "grad_norm": 0.18263046443462372,
      "learning_rate": 0.00016442392260334213,
      "loss": 0.0528,
      "step": 810
    },
    {
      "epoch": 0.5349604221635884,
      "grad_norm": 0.08413567394018173,
      "learning_rate": 0.00016437994722955145,
      "loss": 0.0196,
      "step": 811
    },
    {
      "epoch": 0.5356200527704486,
      "grad_norm": 0.0979427620768547,
      "learning_rate": 0.00016433597185576076,
      "loss": 0.0058,
      "step": 812
    },
    {
      "epoch": 0.5362796833773087,
      "grad_norm": 0.1473580002784729,
      "learning_rate": 0.0001642919964819701,
      "loss": 0.0365,
      "step": 813
    },
    {
      "epoch": 0.5369393139841688,
      "grad_norm": 0.2155044823884964,
      "learning_rate": 0.00016424802110817942,
      "loss": 0.0775,
      "step": 814
    },
    {
      "epoch": 0.537598944591029,
      "grad_norm": 0.1093115359544754,
      "learning_rate": 0.00016420404573438876,
      "loss": 0.0068,
      "step": 815
    },
    {
      "epoch": 0.5382585751978892,
      "grad_norm": 0.10064983367919922,
      "learning_rate": 0.00016416007036059807,
      "loss": 0.0342,
      "step": 816
    },
    {
      "epoch": 0.5389182058047494,
      "grad_norm": 0.11435982584953308,
      "learning_rate": 0.0001641160949868074,
      "loss": 0.0108,
      "step": 817
    },
    {
      "epoch": 0.5395778364116095,
      "grad_norm": 0.05945746973156929,
      "learning_rate": 0.00016407211961301673,
      "loss": 0.0124,
      "step": 818
    },
    {
      "epoch": 0.5402374670184696,
      "grad_norm": 0.0685262531042099,
      "learning_rate": 0.00016402814423922604,
      "loss": 0.0138,
      "step": 819
    },
    {
      "epoch": 0.5408970976253298,
      "grad_norm": 0.19727307558059692,
      "learning_rate": 0.00016398416886543538,
      "loss": 0.0544,
      "step": 820
    },
    {
      "epoch": 0.5415567282321899,
      "grad_norm": 0.17107714712619781,
      "learning_rate": 0.0001639401934916447,
      "loss": 0.0641,
      "step": 821
    },
    {
      "epoch": 0.5422163588390502,
      "grad_norm": 0.07629573345184326,
      "learning_rate": 0.000163896218117854,
      "loss": 0.02,
      "step": 822
    },
    {
      "epoch": 0.5428759894459103,
      "grad_norm": 0.11815351992845535,
      "learning_rate": 0.00016385224274406332,
      "loss": 0.0086,
      "step": 823
    },
    {
      "epoch": 0.5435356200527705,
      "grad_norm": 0.11437845975160599,
      "learning_rate": 0.00016380826737027266,
      "loss": 0.0315,
      "step": 824
    },
    {
      "epoch": 0.5441952506596306,
      "grad_norm": 0.08114766329526901,
      "learning_rate": 0.00016376429199648198,
      "loss": 0.0165,
      "step": 825
    },
    {
      "epoch": 0.5448548812664907,
      "grad_norm": 0.20141105353832245,
      "learning_rate": 0.0001637203166226913,
      "loss": 0.0575,
      "step": 826
    },
    {
      "epoch": 0.5455145118733509,
      "grad_norm": 0.09921807795763016,
      "learning_rate": 0.00016367634124890063,
      "loss": 0.0073,
      "step": 827
    },
    {
      "epoch": 0.5461741424802111,
      "grad_norm": 0.0695984810590744,
      "learning_rate": 0.00016363236587510995,
      "loss": 0.0256,
      "step": 828
    },
    {
      "epoch": 0.5468337730870713,
      "grad_norm": 0.06928963214159012,
      "learning_rate": 0.00016358839050131926,
      "loss": 0.0123,
      "step": 829
    },
    {
      "epoch": 0.5474934036939314,
      "grad_norm": 0.07559728622436523,
      "learning_rate": 0.00016354441512752858,
      "loss": 0.0257,
      "step": 830
    },
    {
      "epoch": 0.5481530343007915,
      "grad_norm": 0.12516073882579803,
      "learning_rate": 0.00016350043975373792,
      "loss": 0.0439,
      "step": 831
    },
    {
      "epoch": 0.5488126649076517,
      "grad_norm": 0.1136423870921135,
      "learning_rate": 0.00016345646437994723,
      "loss": 0.0308,
      "step": 832
    },
    {
      "epoch": 0.5494722955145118,
      "grad_norm": 0.06928764283657074,
      "learning_rate": 0.00016341248900615654,
      "loss": 0.0045,
      "step": 833
    },
    {
      "epoch": 0.5501319261213721,
      "grad_norm": 0.07665978372097015,
      "learning_rate": 0.00016336851363236589,
      "loss": 0.0101,
      "step": 834
    },
    {
      "epoch": 0.5507915567282322,
      "grad_norm": 0.06622395664453506,
      "learning_rate": 0.0001633245382585752,
      "loss": 0.004,
      "step": 835
    },
    {
      "epoch": 0.5514511873350924,
      "grad_norm": 0.07649010419845581,
      "learning_rate": 0.00016328056288478451,
      "loss": 0.028,
      "step": 836
    },
    {
      "epoch": 0.5521108179419525,
      "grad_norm": 0.21328362822532654,
      "learning_rate": 0.00016323658751099385,
      "loss": 0.0622,
      "step": 837
    },
    {
      "epoch": 0.5527704485488126,
      "grad_norm": 0.10636536777019501,
      "learning_rate": 0.00016319261213720317,
      "loss": 0.0245,
      "step": 838
    },
    {
      "epoch": 0.5534300791556728,
      "grad_norm": 0.06261996179819107,
      "learning_rate": 0.0001631486367634125,
      "loss": 0.0231,
      "step": 839
    },
    {
      "epoch": 0.554089709762533,
      "grad_norm": 0.08158326148986816,
      "learning_rate": 0.00016310466138962182,
      "loss": 0.0123,
      "step": 840
    },
    {
      "epoch": 0.5547493403693932,
      "grad_norm": 0.28988441824913025,
      "learning_rate": 0.00016306068601583114,
      "loss": 0.0745,
      "step": 841
    },
    {
      "epoch": 0.5554089709762533,
      "grad_norm": 0.06478836387395859,
      "learning_rate": 0.00016301671064204048,
      "loss": 0.0171,
      "step": 842
    },
    {
      "epoch": 0.5560686015831134,
      "grad_norm": 0.10714208334684372,
      "learning_rate": 0.0001629727352682498,
      "loss": 0.0265,
      "step": 843
    },
    {
      "epoch": 0.5567282321899736,
      "grad_norm": 0.1045900508761406,
      "learning_rate": 0.0001629287598944591,
      "loss": 0.0071,
      "step": 844
    },
    {
      "epoch": 0.5573878627968337,
      "grad_norm": 0.11891687661409378,
      "learning_rate": 0.00016288478452066845,
      "loss": 0.021,
      "step": 845
    },
    {
      "epoch": 0.558047493403694,
      "grad_norm": 0.2228630632162094,
      "learning_rate": 0.00016284080914687776,
      "loss": 0.0473,
      "step": 846
    },
    {
      "epoch": 0.5587071240105541,
      "grad_norm": 0.18387839198112488,
      "learning_rate": 0.00016279683377308708,
      "loss": 0.0571,
      "step": 847
    },
    {
      "epoch": 0.5593667546174143,
      "grad_norm": 0.1295257806777954,
      "learning_rate": 0.0001627528583992964,
      "loss": 0.0402,
      "step": 848
    },
    {
      "epoch": 0.5600263852242744,
      "grad_norm": 0.1746588945388794,
      "learning_rate": 0.00016270888302550573,
      "loss": 0.0163,
      "step": 849
    },
    {
      "epoch": 0.5606860158311345,
      "grad_norm": 0.1574990600347519,
      "learning_rate": 0.00016266490765171504,
      "loss": 0.0214,
      "step": 850
    },
    {
      "epoch": 0.5613456464379947,
      "grad_norm": 0.1672859787940979,
      "learning_rate": 0.00016262093227792436,
      "loss": 0.0532,
      "step": 851
    },
    {
      "epoch": 0.5620052770448549,
      "grad_norm": 0.08692740648984909,
      "learning_rate": 0.0001625769569041337,
      "loss": 0.0127,
      "step": 852
    },
    {
      "epoch": 0.5626649076517151,
      "grad_norm": 0.14617761969566345,
      "learning_rate": 0.00016253298153034301,
      "loss": 0.0281,
      "step": 853
    },
    {
      "epoch": 0.5633245382585752,
      "grad_norm": 0.19108228385448456,
      "learning_rate": 0.00016248900615655233,
      "loss": 0.0397,
      "step": 854
    },
    {
      "epoch": 0.5639841688654353,
      "grad_norm": 0.15531682968139648,
      "learning_rate": 0.00016244503078276164,
      "loss": 0.0386,
      "step": 855
    },
    {
      "epoch": 0.5646437994722955,
      "grad_norm": 0.09716622531414032,
      "learning_rate": 0.00016240105540897098,
      "loss": 0.0064,
      "step": 856
    },
    {
      "epoch": 0.5653034300791556,
      "grad_norm": 0.16020314395427704,
      "learning_rate": 0.0001623570800351803,
      "loss": 0.0434,
      "step": 857
    },
    {
      "epoch": 0.5659630606860159,
      "grad_norm": 0.1453268826007843,
      "learning_rate": 0.00016231310466138964,
      "loss": 0.0274,
      "step": 858
    },
    {
      "epoch": 0.566622691292876,
      "grad_norm": 0.10597624629735947,
      "learning_rate": 0.00016226912928759895,
      "loss": 0.0261,
      "step": 859
    },
    {
      "epoch": 0.5672823218997362,
      "grad_norm": 0.10441892594099045,
      "learning_rate": 0.0001622251539138083,
      "loss": 0.016,
      "step": 860
    },
    {
      "epoch": 0.5679419525065963,
      "grad_norm": 0.10103638470172882,
      "learning_rate": 0.0001621811785400176,
      "loss": 0.0202,
      "step": 861
    },
    {
      "epoch": 0.5686015831134564,
      "grad_norm": 0.11365985870361328,
      "learning_rate": 0.00016213720316622692,
      "loss": 0.0152,
      "step": 862
    },
    {
      "epoch": 0.5692612137203166,
      "grad_norm": 0.09640052914619446,
      "learning_rate": 0.00016209322779243626,
      "loss": 0.0283,
      "step": 863
    },
    {
      "epoch": 0.5699208443271768,
      "grad_norm": 0.07413340359926224,
      "learning_rate": 0.00016204925241864558,
      "loss": 0.0146,
      "step": 864
    },
    {
      "epoch": 0.570580474934037,
      "grad_norm": 0.12051556259393692,
      "learning_rate": 0.0001620052770448549,
      "loss": 0.038,
      "step": 865
    },
    {
      "epoch": 0.5712401055408971,
      "grad_norm": 0.23685699701309204,
      "learning_rate": 0.0001619613016710642,
      "loss": 0.057,
      "step": 866
    },
    {
      "epoch": 0.5718997361477572,
      "grad_norm": 0.11726243048906326,
      "learning_rate": 0.00016191732629727354,
      "loss": 0.0214,
      "step": 867
    },
    {
      "epoch": 0.5725593667546174,
      "grad_norm": 0.10750575363636017,
      "learning_rate": 0.00016187335092348286,
      "loss": 0.024,
      "step": 868
    },
    {
      "epoch": 0.5732189973614775,
      "grad_norm": 0.0985398218035698,
      "learning_rate": 0.00016182937554969217,
      "loss": 0.0222,
      "step": 869
    },
    {
      "epoch": 0.5738786279683378,
      "grad_norm": 0.08084539324045181,
      "learning_rate": 0.00016178540017590151,
      "loss": 0.0144,
      "step": 870
    },
    {
      "epoch": 0.5745382585751979,
      "grad_norm": 0.17210683226585388,
      "learning_rate": 0.00016174142480211083,
      "loss": 0.0439,
      "step": 871
    },
    {
      "epoch": 0.575197889182058,
      "grad_norm": 0.13360704481601715,
      "learning_rate": 0.00016169744942832014,
      "loss": 0.0229,
      "step": 872
    },
    {
      "epoch": 0.5758575197889182,
      "grad_norm": 0.2410958707332611,
      "learning_rate": 0.00016165347405452946,
      "loss": 0.0572,
      "step": 873
    },
    {
      "epoch": 0.5765171503957783,
      "grad_norm": 0.1375456154346466,
      "learning_rate": 0.0001616094986807388,
      "loss": 0.0089,
      "step": 874
    },
    {
      "epoch": 0.5771767810026385,
      "grad_norm": 0.2247239053249359,
      "learning_rate": 0.0001615655233069481,
      "loss": 0.0649,
      "step": 875
    },
    {
      "epoch": 0.5778364116094987,
      "grad_norm": 0.13344936072826385,
      "learning_rate": 0.00016152154793315742,
      "loss": 0.0351,
      "step": 876
    },
    {
      "epoch": 0.5784960422163589,
      "grad_norm": 0.19980163872241974,
      "learning_rate": 0.00016147757255936674,
      "loss": 0.049,
      "step": 877
    },
    {
      "epoch": 0.579155672823219,
      "grad_norm": 0.13127416372299194,
      "learning_rate": 0.00016143359718557608,
      "loss": 0.0388,
      "step": 878
    },
    {
      "epoch": 0.5798153034300791,
      "grad_norm": 0.1394331157207489,
      "learning_rate": 0.00016138962181178542,
      "loss": 0.0423,
      "step": 879
    },
    {
      "epoch": 0.5804749340369393,
      "grad_norm": 0.23271070420742035,
      "learning_rate": 0.00016134564643799473,
      "loss": 0.067,
      "step": 880
    },
    {
      "epoch": 0.5811345646437994,
      "grad_norm": 0.1587025672197342,
      "learning_rate": 0.00016130167106420408,
      "loss": 0.0295,
      "step": 881
    },
    {
      "epoch": 0.5817941952506597,
      "grad_norm": 0.15456527471542358,
      "learning_rate": 0.0001612576956904134,
      "loss": 0.0379,
      "step": 882
    },
    {
      "epoch": 0.5824538258575198,
      "grad_norm": 0.150905042886734,
      "learning_rate": 0.0001612137203166227,
      "loss": 0.0405,
      "step": 883
    },
    {
      "epoch": 0.58311345646438,
      "grad_norm": 0.16252924501895905,
      "learning_rate": 0.00016116974494283202,
      "loss": 0.0472,
      "step": 884
    },
    {
      "epoch": 0.5837730870712401,
      "grad_norm": 0.17589141428470612,
      "learning_rate": 0.00016112576956904136,
      "loss": 0.0281,
      "step": 885
    },
    {
      "epoch": 0.5844327176781002,
      "grad_norm": 0.16443116962909698,
      "learning_rate": 0.00016108179419525067,
      "loss": 0.0205,
      "step": 886
    },
    {
      "epoch": 0.5850923482849604,
      "grad_norm": 0.13737007975578308,
      "learning_rate": 0.00016103781882146,
      "loss": 0.0205,
      "step": 887
    },
    {
      "epoch": 0.5857519788918206,
      "grad_norm": 0.08024121820926666,
      "learning_rate": 0.00016099384344766933,
      "loss": 0.023,
      "step": 888
    },
    {
      "epoch": 0.5864116094986808,
      "grad_norm": 0.10417483001947403,
      "learning_rate": 0.00016094986807387864,
      "loss": 0.0267,
      "step": 889
    },
    {
      "epoch": 0.5870712401055409,
      "grad_norm": 0.13819114863872528,
      "learning_rate": 0.00016090589270008796,
      "loss": 0.0217,
      "step": 890
    },
    {
      "epoch": 0.587730870712401,
      "grad_norm": 0.1462433636188507,
      "learning_rate": 0.00016086191732629727,
      "loss": 0.0321,
      "step": 891
    },
    {
      "epoch": 0.5883905013192612,
      "grad_norm": 0.1836545765399933,
      "learning_rate": 0.0001608179419525066,
      "loss": 0.0467,
      "step": 892
    },
    {
      "epoch": 0.5890501319261213,
      "grad_norm": 0.06258735060691833,
      "learning_rate": 0.00016077396657871592,
      "loss": 0.0138,
      "step": 893
    },
    {
      "epoch": 0.5897097625329816,
      "grad_norm": 0.19848643243312836,
      "learning_rate": 0.00016072999120492524,
      "loss": 0.0485,
      "step": 894
    },
    {
      "epoch": 0.5903693931398417,
      "grad_norm": 0.05641037970781326,
      "learning_rate": 0.00016068601583113455,
      "loss": 0.0038,
      "step": 895
    },
    {
      "epoch": 0.5910290237467019,
      "grad_norm": 0.1724139302968979,
      "learning_rate": 0.0001606420404573439,
      "loss": 0.0432,
      "step": 896
    },
    {
      "epoch": 0.591688654353562,
      "grad_norm": 0.08092499524354935,
      "learning_rate": 0.0001605980650835532,
      "loss": 0.0056,
      "step": 897
    },
    {
      "epoch": 0.5923482849604221,
      "grad_norm": 0.14691054821014404,
      "learning_rate": 0.00016055408970976252,
      "loss": 0.0307,
      "step": 898
    },
    {
      "epoch": 0.5930079155672823,
      "grad_norm": 0.11350515484809875,
      "learning_rate": 0.00016051011433597186,
      "loss": 0.0356,
      "step": 899
    },
    {
      "epoch": 0.5936675461741425,
      "grad_norm": 0.0784933939576149,
      "learning_rate": 0.00016046613896218118,
      "loss": 0.0053,
      "step": 900
    },
    {
      "epoch": 0.5943271767810027,
      "grad_norm": 0.07258125394582748,
      "learning_rate": 0.00016042216358839052,
      "loss": 0.0144,
      "step": 901
    },
    {
      "epoch": 0.5949868073878628,
      "grad_norm": 0.08221647143363953,
      "learning_rate": 0.00016037818821459983,
      "loss": 0.0054,
      "step": 902
    },
    {
      "epoch": 0.5956464379947229,
      "grad_norm": 0.25402069091796875,
      "learning_rate": 0.00016033421284080917,
      "loss": 0.0488,
      "step": 903
    },
    {
      "epoch": 0.5963060686015831,
      "grad_norm": 0.1378733366727829,
      "learning_rate": 0.0001602902374670185,
      "loss": 0.0314,
      "step": 904
    },
    {
      "epoch": 0.5969656992084432,
      "grad_norm": 0.11056813597679138,
      "learning_rate": 0.0001602462620932278,
      "loss": 0.0252,
      "step": 905
    },
    {
      "epoch": 0.5976253298153035,
      "grad_norm": 0.2209276407957077,
      "learning_rate": 0.00016020228671943714,
      "loss": 0.0513,
      "step": 906
    },
    {
      "epoch": 0.5982849604221636,
      "grad_norm": 0.11637742817401886,
      "learning_rate": 0.00016015831134564646,
      "loss": 0.0234,
      "step": 907
    },
    {
      "epoch": 0.5989445910290238,
      "grad_norm": 0.09707394242286682,
      "learning_rate": 0.00016011433597185577,
      "loss": 0.0111,
      "step": 908
    },
    {
      "epoch": 0.5996042216358839,
      "grad_norm": 0.08183937519788742,
      "learning_rate": 0.00016007036059806508,
      "loss": 0.0202,
      "step": 909
    },
    {
      "epoch": 0.600263852242744,
      "grad_norm": 0.12094836682081223,
      "learning_rate": 0.00016002638522427442,
      "loss": 0.0284,
      "step": 910
    },
    {
      "epoch": 0.6009234828496042,
      "grad_norm": 0.09794480353593826,
      "learning_rate": 0.00015998240985048374,
      "loss": 0.022,
      "step": 911
    },
    {
      "epoch": 0.6015831134564644,
      "grad_norm": 0.1491338014602661,
      "learning_rate": 0.00015993843447669305,
      "loss": 0.0348,
      "step": 912
    },
    {
      "epoch": 0.6022427440633246,
      "grad_norm": 0.14360669255256653,
      "learning_rate": 0.00015989445910290237,
      "loss": 0.038,
      "step": 913
    },
    {
      "epoch": 0.6029023746701847,
      "grad_norm": 0.1561790555715561,
      "learning_rate": 0.0001598504837291117,
      "loss": 0.0209,
      "step": 914
    },
    {
      "epoch": 0.6035620052770448,
      "grad_norm": 0.08833194524049759,
      "learning_rate": 0.00015980650835532102,
      "loss": 0.0132,
      "step": 915
    },
    {
      "epoch": 0.604221635883905,
      "grad_norm": 0.1346270591020584,
      "learning_rate": 0.00015976253298153034,
      "loss": 0.0267,
      "step": 916
    },
    {
      "epoch": 0.6048812664907651,
      "grad_norm": 0.3076432943344116,
      "learning_rate": 0.00015971855760773968,
      "loss": 0.0696,
      "step": 917
    },
    {
      "epoch": 0.6055408970976254,
      "grad_norm": 0.13718098402023315,
      "learning_rate": 0.000159674582233949,
      "loss": 0.0361,
      "step": 918
    },
    {
      "epoch": 0.6062005277044855,
      "grad_norm": 0.12586481869220734,
      "learning_rate": 0.0001596306068601583,
      "loss": 0.0165,
      "step": 919
    },
    {
      "epoch": 0.6068601583113457,
      "grad_norm": 0.14634259045124054,
      "learning_rate": 0.00015958663148636765,
      "loss": 0.0292,
      "step": 920
    },
    {
      "epoch": 0.6075197889182058,
      "grad_norm": 0.2459302395582199,
      "learning_rate": 0.00015954265611257696,
      "loss": 0.0479,
      "step": 921
    },
    {
      "epoch": 0.6081794195250659,
      "grad_norm": 0.26423442363739014,
      "learning_rate": 0.0001594986807387863,
      "loss": 0.0428,
      "step": 922
    },
    {
      "epoch": 0.6088390501319261,
      "grad_norm": 0.12176792323589325,
      "learning_rate": 0.00015945470536499561,
      "loss": 0.0111,
      "step": 923
    },
    {
      "epoch": 0.6094986807387863,
      "grad_norm": 0.15606682002544403,
      "learning_rate": 0.00015941072999120496,
      "loss": 0.0361,
      "step": 924
    },
    {
      "epoch": 0.6101583113456465,
      "grad_norm": 0.1871809959411621,
      "learning_rate": 0.00015936675461741427,
      "loss": 0.0126,
      "step": 925
    },
    {
      "epoch": 0.6108179419525066,
      "grad_norm": 0.14851655066013336,
      "learning_rate": 0.00015932277924362358,
      "loss": 0.0511,
      "step": 926
    },
    {
      "epoch": 0.6114775725593667,
      "grad_norm": 0.1062006801366806,
      "learning_rate": 0.0001592788038698329,
      "loss": 0.0291,
      "step": 927
    },
    {
      "epoch": 0.6121372031662269,
      "grad_norm": 0.14740480482578278,
      "learning_rate": 0.00015923482849604224,
      "loss": 0.0304,
      "step": 928
    },
    {
      "epoch": 0.612796833773087,
      "grad_norm": 0.07930178940296173,
      "learning_rate": 0.00015919085312225155,
      "loss": 0.0049,
      "step": 929
    },
    {
      "epoch": 0.6134564643799473,
      "grad_norm": 0.10649827122688293,
      "learning_rate": 0.00015914687774846087,
      "loss": 0.0142,
      "step": 930
    },
    {
      "epoch": 0.6141160949868074,
      "grad_norm": 0.09115011245012283,
      "learning_rate": 0.00015910290237467018,
      "loss": 0.0128,
      "step": 931
    },
    {
      "epoch": 0.6147757255936676,
      "grad_norm": 0.06659293919801712,
      "learning_rate": 0.00015905892700087952,
      "loss": 0.0145,
      "step": 932
    },
    {
      "epoch": 0.6154353562005277,
      "grad_norm": 0.19745437800884247,
      "learning_rate": 0.00015901495162708884,
      "loss": 0.0317,
      "step": 933
    },
    {
      "epoch": 0.6160949868073878,
      "grad_norm": 0.054526008665561676,
      "learning_rate": 0.00015897097625329815,
      "loss": 0.0032,
      "step": 934
    },
    {
      "epoch": 0.616754617414248,
      "grad_norm": 0.24129463732242584,
      "learning_rate": 0.0001589270008795075,
      "loss": 0.0348,
      "step": 935
    },
    {
      "epoch": 0.6174142480211082,
      "grad_norm": 0.04278230294585228,
      "learning_rate": 0.0001588830255057168,
      "loss": 0.0025,
      "step": 936
    },
    {
      "epoch": 0.6180738786279684,
      "grad_norm": 0.13796864449977875,
      "learning_rate": 0.00015883905013192612,
      "loss": 0.0159,
      "step": 937
    },
    {
      "epoch": 0.6187335092348285,
      "grad_norm": 0.06337767094373703,
      "learning_rate": 0.00015879507475813543,
      "loss": 0.0083,
      "step": 938
    },
    {
      "epoch": 0.6193931398416886,
      "grad_norm": 0.07446835935115814,
      "learning_rate": 0.00015875109938434477,
      "loss": 0.0066,
      "step": 939
    },
    {
      "epoch": 0.6200527704485488,
      "grad_norm": 0.21451464295387268,
      "learning_rate": 0.0001587071240105541,
      "loss": 0.0353,
      "step": 940
    },
    {
      "epoch": 0.6207124010554089,
      "grad_norm": 0.1440437138080597,
      "learning_rate": 0.0001586631486367634,
      "loss": 0.0198,
      "step": 941
    },
    {
      "epoch": 0.6213720316622692,
      "grad_norm": 0.10582882910966873,
      "learning_rate": 0.00015861917326297274,
      "loss": 0.0157,
      "step": 942
    },
    {
      "epoch": 0.6220316622691293,
      "grad_norm": 0.11347091943025589,
      "learning_rate": 0.00015857519788918206,
      "loss": 0.0229,
      "step": 943
    },
    {
      "epoch": 0.6226912928759895,
      "grad_norm": 0.22886668145656586,
      "learning_rate": 0.0001585312225153914,
      "loss": 0.022,
      "step": 944
    },
    {
      "epoch": 0.6233509234828496,
      "grad_norm": 0.12520091235637665,
      "learning_rate": 0.0001584872471416007,
      "loss": 0.0237,
      "step": 945
    },
    {
      "epoch": 0.6240105540897097,
      "grad_norm": 0.26471760869026184,
      "learning_rate": 0.00015844327176781005,
      "loss": 0.0609,
      "step": 946
    },
    {
      "epoch": 0.6246701846965699,
      "grad_norm": 0.11332923918962479,
      "learning_rate": 0.00015839929639401937,
      "loss": 0.0097,
      "step": 947
    },
    {
      "epoch": 0.6253298153034301,
      "grad_norm": 0.1581287980079651,
      "learning_rate": 0.00015835532102022868,
      "loss": 0.025,
      "step": 948
    },
    {
      "epoch": 0.6259894459102903,
      "grad_norm": 0.46915915608406067,
      "learning_rate": 0.000158311345646438,
      "loss": 0.1089,
      "step": 949
    },
    {
      "epoch": 0.6266490765171504,
      "grad_norm": 0.21147440373897552,
      "learning_rate": 0.00015826737027264734,
      "loss": 0.0225,
      "step": 950
    },
    {
      "epoch": 0.6273087071240105,
      "grad_norm": 0.20106656849384308,
      "learning_rate": 0.00015822339489885665,
      "loss": 0.0431,
      "step": 951
    },
    {
      "epoch": 0.6279683377308707,
      "grad_norm": 0.15368051826953888,
      "learning_rate": 0.00015817941952506596,
      "loss": 0.0266,
      "step": 952
    },
    {
      "epoch": 0.6286279683377308,
      "grad_norm": 0.17008207738399506,
      "learning_rate": 0.0001581354441512753,
      "loss": 0.0242,
      "step": 953
    },
    {
      "epoch": 0.6292875989445911,
      "grad_norm": 0.14930576086044312,
      "learning_rate": 0.00015809146877748462,
      "loss": 0.0427,
      "step": 954
    },
    {
      "epoch": 0.6299472295514512,
      "grad_norm": 0.4089459180831909,
      "learning_rate": 0.00015804749340369393,
      "loss": 0.112,
      "step": 955
    },
    {
      "epoch": 0.6306068601583114,
      "grad_norm": 0.10803564637899399,
      "learning_rate": 0.00015800351802990325,
      "loss": 0.0081,
      "step": 956
    },
    {
      "epoch": 0.6312664907651715,
      "grad_norm": 0.12273850291967392,
      "learning_rate": 0.0001579595426561126,
      "loss": 0.0327,
      "step": 957
    },
    {
      "epoch": 0.6319261213720316,
      "grad_norm": 0.0818730816245079,
      "learning_rate": 0.0001579155672823219,
      "loss": 0.0323,
      "step": 958
    },
    {
      "epoch": 0.6325857519788918,
      "grad_norm": 0.13661757111549377,
      "learning_rate": 0.00015787159190853122,
      "loss": 0.0106,
      "step": 959
    },
    {
      "epoch": 0.633245382585752,
      "grad_norm": 0.19437672197818756,
      "learning_rate": 0.00015782761653474056,
      "loss": 0.0479,
      "step": 960
    },
    {
      "epoch": 0.6339050131926122,
      "grad_norm": 0.11600769311189651,
      "learning_rate": 0.00015778364116094987,
      "loss": 0.0323,
      "step": 961
    },
    {
      "epoch": 0.6345646437994723,
      "grad_norm": 0.08743426948785782,
      "learning_rate": 0.00015773966578715918,
      "loss": 0.0401,
      "step": 962
    },
    {
      "epoch": 0.6352242744063324,
      "grad_norm": 0.09629420191049576,
      "learning_rate": 0.00015769569041336853,
      "loss": 0.0298,
      "step": 963
    },
    {
      "epoch": 0.6358839050131926,
      "grad_norm": 0.09650755673646927,
      "learning_rate": 0.00015765171503957784,
      "loss": 0.0461,
      "step": 964
    },
    {
      "epoch": 0.6365435356200527,
      "grad_norm": 0.11365342140197754,
      "learning_rate": 0.00015760773966578718,
      "loss": 0.0331,
      "step": 965
    },
    {
      "epoch": 0.637203166226913,
      "grad_norm": 0.1234249547123909,
      "learning_rate": 0.0001575637642919965,
      "loss": 0.0493,
      "step": 966
    },
    {
      "epoch": 0.6378627968337731,
      "grad_norm": 0.17661775648593903,
      "learning_rate": 0.0001575197889182058,
      "loss": 0.0648,
      "step": 967
    },
    {
      "epoch": 0.6385224274406333,
      "grad_norm": 0.14137084782123566,
      "learning_rate": 0.00015747581354441515,
      "loss": 0.0111,
      "step": 968
    },
    {
      "epoch": 0.6391820580474934,
      "grad_norm": 0.135686993598938,
      "learning_rate": 0.00015743183817062446,
      "loss": 0.0209,
      "step": 969
    },
    {
      "epoch": 0.6398416886543535,
      "grad_norm": 0.11412093788385391,
      "learning_rate": 0.00015738786279683378,
      "loss": 0.0382,
      "step": 970
    },
    {
      "epoch": 0.6405013192612137,
      "grad_norm": 0.07197537273168564,
      "learning_rate": 0.00015734388742304312,
      "loss": 0.0187,
      "step": 971
    },
    {
      "epoch": 0.6411609498680739,
      "grad_norm": 0.0985872745513916,
      "learning_rate": 0.00015729991204925243,
      "loss": 0.0068,
      "step": 972
    },
    {
      "epoch": 0.6418205804749341,
      "grad_norm": 0.1737734079360962,
      "learning_rate": 0.00015725593667546175,
      "loss": 0.0502,
      "step": 973
    },
    {
      "epoch": 0.6424802110817942,
      "grad_norm": 0.10803570598363876,
      "learning_rate": 0.00015721196130167106,
      "loss": 0.0372,
      "step": 974
    },
    {
      "epoch": 0.6431398416886543,
      "grad_norm": 0.18142466247081757,
      "learning_rate": 0.0001571679859278804,
      "loss": 0.035,
      "step": 975
    },
    {
      "epoch": 0.6437994722955145,
      "grad_norm": 0.11020297557115555,
      "learning_rate": 0.00015712401055408972,
      "loss": 0.0312,
      "step": 976
    },
    {
      "epoch": 0.6444591029023746,
      "grad_norm": 0.2217065542936325,
      "learning_rate": 0.00015708003518029903,
      "loss": 0.0335,
      "step": 977
    },
    {
      "epoch": 0.6451187335092349,
      "grad_norm": 0.14916415512561798,
      "learning_rate": 0.00015703605980650837,
      "loss": 0.039,
      "step": 978
    },
    {
      "epoch": 0.645778364116095,
      "grad_norm": 0.07461486011743546,
      "learning_rate": 0.00015699208443271768,
      "loss": 0.0109,
      "step": 979
    },
    {
      "epoch": 0.6464379947229552,
      "grad_norm": 0.08587154000997543,
      "learning_rate": 0.000156948109058927,
      "loss": 0.0193,
      "step": 980
    },
    {
      "epoch": 0.6470976253298153,
      "grad_norm": 0.08579342067241669,
      "learning_rate": 0.0001569041336851363,
      "loss": 0.0111,
      "step": 981
    },
    {
      "epoch": 0.6477572559366754,
      "grad_norm": 0.11165007948875427,
      "learning_rate": 0.00015686015831134565,
      "loss": 0.0292,
      "step": 982
    },
    {
      "epoch": 0.6484168865435356,
      "grad_norm": 0.10112418234348297,
      "learning_rate": 0.00015681618293755497,
      "loss": 0.0293,
      "step": 983
    },
    {
      "epoch": 0.6490765171503958,
      "grad_norm": 0.11943784356117249,
      "learning_rate": 0.0001567722075637643,
      "loss": 0.0264,
      "step": 984
    },
    {
      "epoch": 0.649736147757256,
      "grad_norm": 0.12381291389465332,
      "learning_rate": 0.00015672823218997362,
      "loss": 0.0082,
      "step": 985
    },
    {
      "epoch": 0.6503957783641161,
      "grad_norm": 0.269265741109848,
      "learning_rate": 0.00015668425681618296,
      "loss": 0.065,
      "step": 986
    },
    {
      "epoch": 0.6510554089709762,
      "grad_norm": 0.14796003699302673,
      "learning_rate": 0.00015664028144239228,
      "loss": 0.0371,
      "step": 987
    },
    {
      "epoch": 0.6517150395778364,
      "grad_norm": 0.5739474296569824,
      "learning_rate": 0.0001565963060686016,
      "loss": 0.0686,
      "step": 988
    },
    {
      "epoch": 0.6523746701846965,
      "grad_norm": 0.17836593091487885,
      "learning_rate": 0.00015655233069481093,
      "loss": 0.0108,
      "step": 989
    },
    {
      "epoch": 0.6530343007915568,
      "grad_norm": 0.18586622178554535,
      "learning_rate": 0.00015650835532102025,
      "loss": 0.0167,
      "step": 990
    },
    {
      "epoch": 0.6536939313984169,
      "grad_norm": 0.20984943211078644,
      "learning_rate": 0.00015646437994722956,
      "loss": 0.0536,
      "step": 991
    },
    {
      "epoch": 0.6543535620052771,
      "grad_norm": 0.14788976311683655,
      "learning_rate": 0.00015642040457343887,
      "loss": 0.0421,
      "step": 992
    },
    {
      "epoch": 0.6550131926121372,
      "grad_norm": 0.15083199739456177,
      "learning_rate": 0.00015637642919964822,
      "loss": 0.0469,
      "step": 993
    },
    {
      "epoch": 0.6556728232189973,
      "grad_norm": 0.29549872875213623,
      "learning_rate": 0.00015633245382585753,
      "loss": 0.0739,
      "step": 994
    },
    {
      "epoch": 0.6563324538258575,
      "grad_norm": 0.1467006802558899,
      "learning_rate": 0.00015628847845206684,
      "loss": 0.0423,
      "step": 995
    },
    {
      "epoch": 0.6569920844327177,
      "grad_norm": 0.0846908688545227,
      "learning_rate": 0.00015624450307827618,
      "loss": 0.0146,
      "step": 996
    },
    {
      "epoch": 0.6576517150395779,
      "grad_norm": 0.11508435755968094,
      "learning_rate": 0.0001562005277044855,
      "loss": 0.0325,
      "step": 997
    },
    {
      "epoch": 0.658311345646438,
      "grad_norm": 0.13174307346343994,
      "learning_rate": 0.0001561565523306948,
      "loss": 0.036,
      "step": 998
    },
    {
      "epoch": 0.6589709762532981,
      "grad_norm": 0.2578682005405426,
      "learning_rate": 0.00015611257695690413,
      "loss": 0.0631,
      "step": 999
    },
    {
      "epoch": 0.6596306068601583,
      "grad_norm": 0.1076539009809494,
      "learning_rate": 0.00015606860158311347,
      "loss": 0.0278,
      "step": 1000
    },
    {
      "epoch": 0.6602902374670184,
      "grad_norm": 0.13605403900146484,
      "learning_rate": 0.00015602462620932278,
      "loss": 0.0258,
      "step": 1001
    },
    {
      "epoch": 0.6609498680738787,
      "grad_norm": 0.48534172773361206,
      "learning_rate": 0.0001559806508355321,
      "loss": 0.0192,
      "step": 1002
    },
    {
      "epoch": 0.6616094986807388,
      "grad_norm": 0.15487313270568848,
      "learning_rate": 0.0001559366754617414,
      "loss": 0.0108,
      "step": 1003
    },
    {
      "epoch": 0.662269129287599,
      "grad_norm": 0.15927378833293915,
      "learning_rate": 0.00015589270008795075,
      "loss": 0.0399,
      "step": 1004
    },
    {
      "epoch": 0.6629287598944591,
      "grad_norm": 0.19018100202083588,
      "learning_rate": 0.00015584872471416006,
      "loss": 0.0399,
      "step": 1005
    },
    {
      "epoch": 0.6635883905013192,
      "grad_norm": 0.10342439264059067,
      "learning_rate": 0.0001558047493403694,
      "loss": 0.0234,
      "step": 1006
    },
    {
      "epoch": 0.6642480211081794,
      "grad_norm": 0.07611986249685287,
      "learning_rate": 0.00015576077396657872,
      "loss": 0.0129,
      "step": 1007
    },
    {
      "epoch": 0.6649076517150396,
      "grad_norm": 0.1673540621995926,
      "learning_rate": 0.00015571679859278806,
      "loss": 0.0428,
      "step": 1008
    },
    {
      "epoch": 0.6655672823218998,
      "grad_norm": 0.1906452476978302,
      "learning_rate": 0.00015567282321899737,
      "loss": 0.0409,
      "step": 1009
    },
    {
      "epoch": 0.6662269129287599,
      "grad_norm": 0.3691564202308655,
      "learning_rate": 0.0001556288478452067,
      "loss": 0.0747,
      "step": 1010
    },
    {
      "epoch": 0.66688654353562,
      "grad_norm": 0.1758706122636795,
      "learning_rate": 0.00015558487247141603,
      "loss": 0.055,
      "step": 1011
    },
    {
      "epoch": 0.6675461741424802,
      "grad_norm": 0.22103512287139893,
      "learning_rate": 0.00015554089709762534,
      "loss": 0.0621,
      "step": 1012
    },
    {
      "epoch": 0.6682058047493403,
      "grad_norm": 0.15778981149196625,
      "learning_rate": 0.00015549692172383466,
      "loss": 0.0444,
      "step": 1013
    },
    {
      "epoch": 0.6688654353562006,
      "grad_norm": 0.2332177609205246,
      "learning_rate": 0.000155452946350044,
      "loss": 0.033,
      "step": 1014
    },
    {
      "epoch": 0.6695250659630607,
      "grad_norm": 0.21355974674224854,
      "learning_rate": 0.0001554089709762533,
      "loss": 0.0433,
      "step": 1015
    },
    {
      "epoch": 0.6701846965699209,
      "grad_norm": 0.32643255591392517,
      "learning_rate": 0.00015536499560246263,
      "loss": 0.0308,
      "step": 1016
    },
    {
      "epoch": 0.670844327176781,
      "grad_norm": 0.18646511435508728,
      "learning_rate": 0.00015532102022867194,
      "loss": 0.0677,
      "step": 1017
    },
    {
      "epoch": 0.6715039577836411,
      "grad_norm": 0.2442457526922226,
      "learning_rate": 0.00015527704485488128,
      "loss": 0.0426,
      "step": 1018
    },
    {
      "epoch": 0.6721635883905013,
      "grad_norm": 0.16497278213500977,
      "learning_rate": 0.0001552330694810906,
      "loss": 0.02,
      "step": 1019
    },
    {
      "epoch": 0.6728232189973615,
      "grad_norm": 0.10226771235466003,
      "learning_rate": 0.0001551890941072999,
      "loss": 0.0307,
      "step": 1020
    },
    {
      "epoch": 0.6734828496042217,
      "grad_norm": 0.09462656080722809,
      "learning_rate": 0.00015514511873350922,
      "loss": 0.0164,
      "step": 1021
    },
    {
      "epoch": 0.6741424802110818,
      "grad_norm": 0.07883922010660172,
      "learning_rate": 0.00015510114335971856,
      "loss": 0.019,
      "step": 1022
    },
    {
      "epoch": 0.674802110817942,
      "grad_norm": 0.21317118406295776,
      "learning_rate": 0.00015505716798592788,
      "loss": 0.0277,
      "step": 1023
    },
    {
      "epoch": 0.6754617414248021,
      "grad_norm": 0.2575826644897461,
      "learning_rate": 0.0001550131926121372,
      "loss": 0.0198,
      "step": 1024
    },
    {
      "epoch": 0.6761213720316622,
      "grad_norm": 0.19067777693271637,
      "learning_rate": 0.00015496921723834653,
      "loss": 0.0453,
      "step": 1025
    },
    {
      "epoch": 0.6767810026385225,
      "grad_norm": 0.3654399514198303,
      "learning_rate": 0.00015492524186455585,
      "loss": 0.0741,
      "step": 1026
    },
    {
      "epoch": 0.6774406332453826,
      "grad_norm": 0.06156393513083458,
      "learning_rate": 0.0001548812664907652,
      "loss": 0.0104,
      "step": 1027
    },
    {
      "epoch": 0.6781002638522428,
      "grad_norm": 0.23557965457439423,
      "learning_rate": 0.0001548372911169745,
      "loss": 0.0524,
      "step": 1028
    },
    {
      "epoch": 0.6787598944591029,
      "grad_norm": 0.06950674206018448,
      "learning_rate": 0.00015479331574318384,
      "loss": 0.0185,
      "step": 1029
    },
    {
      "epoch": 0.679419525065963,
      "grad_norm": 0.1712658554315567,
      "learning_rate": 0.00015474934036939316,
      "loss": 0.0317,
      "step": 1030
    },
    {
      "epoch": 0.6800791556728232,
      "grad_norm": 0.15258488059043884,
      "learning_rate": 0.00015470536499560247,
      "loss": 0.0572,
      "step": 1031
    },
    {
      "epoch": 0.6807387862796834,
      "grad_norm": 0.1701010763645172,
      "learning_rate": 0.0001546613896218118,
      "loss": 0.0352,
      "step": 1032
    },
    {
      "epoch": 0.6813984168865436,
      "grad_norm": 0.1473444253206253,
      "learning_rate": 0.00015461741424802113,
      "loss": 0.0403,
      "step": 1033
    },
    {
      "epoch": 0.6820580474934037,
      "grad_norm": 0.1350085586309433,
      "learning_rate": 0.00015457343887423044,
      "loss": 0.0242,
      "step": 1034
    },
    {
      "epoch": 0.6827176781002638,
      "grad_norm": 0.16764503717422485,
      "learning_rate": 0.00015452946350043975,
      "loss": 0.0182,
      "step": 1035
    },
    {
      "epoch": 0.683377308707124,
      "grad_norm": 0.31583908200263977,
      "learning_rate": 0.0001544854881266491,
      "loss": 0.0269,
      "step": 1036
    },
    {
      "epoch": 0.6840369393139841,
      "grad_norm": 0.10506128519773483,
      "learning_rate": 0.0001544415127528584,
      "loss": 0.0315,
      "step": 1037
    },
    {
      "epoch": 0.6846965699208444,
      "grad_norm": 0.47811004519462585,
      "learning_rate": 0.00015439753737906772,
      "loss": 0.0416,
      "step": 1038
    },
    {
      "epoch": 0.6853562005277045,
      "grad_norm": 0.15494322776794434,
      "learning_rate": 0.00015435356200527704,
      "loss": 0.0226,
      "step": 1039
    },
    {
      "epoch": 0.6860158311345647,
      "grad_norm": 0.08502530306577682,
      "learning_rate": 0.00015430958663148638,
      "loss": 0.0164,
      "step": 1040
    },
    {
      "epoch": 0.6866754617414248,
      "grad_norm": 0.09847616404294968,
      "learning_rate": 0.0001542656112576957,
      "loss": 0.0069,
      "step": 1041
    },
    {
      "epoch": 0.6873350923482849,
      "grad_norm": 0.11679835617542267,
      "learning_rate": 0.000154221635883905,
      "loss": 0.0242,
      "step": 1042
    },
    {
      "epoch": 0.6879947229551451,
      "grad_norm": 0.0547482967376709,
      "learning_rate": 0.00015417766051011435,
      "loss": 0.0102,
      "step": 1043
    },
    {
      "epoch": 0.6886543535620053,
      "grad_norm": 0.06231018900871277,
      "learning_rate": 0.00015413368513632366,
      "loss": 0.0065,
      "step": 1044
    },
    {
      "epoch": 0.6893139841688655,
      "grad_norm": 0.19784489274024963,
      "learning_rate": 0.00015408970976253298,
      "loss": 0.0478,
      "step": 1045
    },
    {
      "epoch": 0.6899736147757256,
      "grad_norm": 0.09717239439487457,
      "learning_rate": 0.0001540457343887423,
      "loss": 0.0167,
      "step": 1046
    },
    {
      "epoch": 0.6906332453825857,
      "grad_norm": 0.4859819710254669,
      "learning_rate": 0.00015400175901495163,
      "loss": 0.1096,
      "step": 1047
    },
    {
      "epoch": 0.6912928759894459,
      "grad_norm": 0.04957457631826401,
      "learning_rate": 0.00015395778364116094,
      "loss": 0.0069,
      "step": 1048
    },
    {
      "epoch": 0.691952506596306,
      "grad_norm": 0.2587793469429016,
      "learning_rate": 0.00015391380826737029,
      "loss": 0.0605,
      "step": 1049
    },
    {
      "epoch": 0.6926121372031663,
      "grad_norm": 0.04869779199361801,
      "learning_rate": 0.00015386983289357963,
      "loss": 0.0116,
      "step": 1050
    },
    {
      "epoch": 0.6932717678100264,
      "grad_norm": 0.14741291105747223,
      "learning_rate": 0.00015382585751978894,
      "loss": 0.0304,
      "step": 1051
    },
    {
      "epoch": 0.6939313984168866,
      "grad_norm": 0.279996395111084,
      "learning_rate": 0.00015378188214599825,
      "loss": 0.0771,
      "step": 1052
    },
    {
      "epoch": 0.6945910290237467,
      "grad_norm": 0.12454206496477127,
      "learning_rate": 0.00015373790677220757,
      "loss": 0.0203,
      "step": 1053
    },
    {
      "epoch": 0.6952506596306068,
      "grad_norm": 0.1441766619682312,
      "learning_rate": 0.0001536939313984169,
      "loss": 0.0463,
      "step": 1054
    },
    {
      "epoch": 0.695910290237467,
      "grad_norm": 0.13774041831493378,
      "learning_rate": 0.00015364995602462622,
      "loss": 0.0361,
      "step": 1055
    },
    {
      "epoch": 0.6965699208443272,
      "grad_norm": 0.1935005635023117,
      "learning_rate": 0.00015360598065083554,
      "loss": 0.0319,
      "step": 1056
    },
    {
      "epoch": 0.6972295514511874,
      "grad_norm": 0.1527407169342041,
      "learning_rate": 0.00015356200527704485,
      "loss": 0.0144,
      "step": 1057
    },
    {
      "epoch": 0.6978891820580475,
      "grad_norm": 0.25606435537338257,
      "learning_rate": 0.0001535180299032542,
      "loss": 0.0253,
      "step": 1058
    },
    {
      "epoch": 0.6985488126649076,
      "grad_norm": 0.18142572045326233,
      "learning_rate": 0.0001534740545294635,
      "loss": 0.0233,
      "step": 1059
    },
    {
      "epoch": 0.6992084432717678,
      "grad_norm": 0.11222656071186066,
      "learning_rate": 0.00015343007915567282,
      "loss": 0.0277,
      "step": 1060
    },
    {
      "epoch": 0.6998680738786279,
      "grad_norm": 0.11700859665870667,
      "learning_rate": 0.00015338610378188216,
      "loss": 0.0205,
      "step": 1061
    },
    {
      "epoch": 0.7005277044854882,
      "grad_norm": 0.07980934530496597,
      "learning_rate": 0.00015334212840809148,
      "loss": 0.0119,
      "step": 1062
    },
    {
      "epoch": 0.7011873350923483,
      "grad_norm": 0.04784706234931946,
      "learning_rate": 0.0001532981530343008,
      "loss": 0.009,
      "step": 1063
    },
    {
      "epoch": 0.7018469656992085,
      "grad_norm": 0.09032504260540009,
      "learning_rate": 0.0001532541776605101,
      "loss": 0.0171,
      "step": 1064
    },
    {
      "epoch": 0.7025065963060686,
      "grad_norm": 0.27711087465286255,
      "learning_rate": 0.00015321020228671944,
      "loss": 0.0611,
      "step": 1065
    },
    {
      "epoch": 0.7031662269129287,
      "grad_norm": 0.2623445987701416,
      "learning_rate": 0.00015316622691292876,
      "loss": 0.0597,
      "step": 1066
    },
    {
      "epoch": 0.7038258575197889,
      "grad_norm": 0.19574065506458282,
      "learning_rate": 0.00015312225153913807,
      "loss": 0.0502,
      "step": 1067
    },
    {
      "epoch": 0.7044854881266491,
      "grad_norm": 0.10838811844587326,
      "learning_rate": 0.0001530782761653474,
      "loss": 0.0255,
      "step": 1068
    },
    {
      "epoch": 0.7051451187335093,
      "grad_norm": 0.12475432455539703,
      "learning_rate": 0.00015303430079155673,
      "loss": 0.02,
      "step": 1069
    },
    {
      "epoch": 0.7058047493403694,
      "grad_norm": 0.07454540580511093,
      "learning_rate": 0.00015299032541776607,
      "loss": 0.0167,
      "step": 1070
    },
    {
      "epoch": 0.7064643799472295,
      "grad_norm": 0.38088977336883545,
      "learning_rate": 0.00015294635004397538,
      "loss": 0.1182,
      "step": 1071
    },
    {
      "epoch": 0.7071240105540897,
      "grad_norm": 0.10724731534719467,
      "learning_rate": 0.00015290237467018472,
      "loss": 0.0291,
      "step": 1072
    },
    {
      "epoch": 0.7077836411609498,
      "grad_norm": 0.09993357211351395,
      "learning_rate": 0.00015285839929639404,
      "loss": 0.024,
      "step": 1073
    },
    {
      "epoch": 0.7084432717678101,
      "grad_norm": 0.12846559286117554,
      "learning_rate": 0.00015281442392260335,
      "loss": 0.0187,
      "step": 1074
    },
    {
      "epoch": 0.7091029023746702,
      "grad_norm": 0.12419798225164413,
      "learning_rate": 0.00015277044854881267,
      "loss": 0.0344,
      "step": 1075
    },
    {
      "epoch": 0.7097625329815304,
      "grad_norm": 0.22292475402355194,
      "learning_rate": 0.000152726473175022,
      "loss": 0.0436,
      "step": 1076
    },
    {
      "epoch": 0.7104221635883905,
      "grad_norm": 0.18017816543579102,
      "learning_rate": 0.00015268249780123132,
      "loss": 0.0143,
      "step": 1077
    },
    {
      "epoch": 0.7110817941952506,
      "grad_norm": 0.09987972676753998,
      "learning_rate": 0.00015263852242744063,
      "loss": 0.0305,
      "step": 1078
    },
    {
      "epoch": 0.7117414248021108,
      "grad_norm": 0.10090066492557526,
      "learning_rate": 0.00015259454705364998,
      "loss": 0.0209,
      "step": 1079
    },
    {
      "epoch": 0.712401055408971,
      "grad_norm": 0.11489244550466537,
      "learning_rate": 0.0001525505716798593,
      "loss": 0.0409,
      "step": 1080
    },
    {
      "epoch": 0.7130606860158312,
      "grad_norm": 0.8420856595039368,
      "learning_rate": 0.0001525065963060686,
      "loss": 0.0843,
      "step": 1081
    },
    {
      "epoch": 0.7137203166226913,
      "grad_norm": 0.2139727771282196,
      "learning_rate": 0.00015246262093227792,
      "loss": 0.0709,
      "step": 1082
    },
    {
      "epoch": 0.7143799472295514,
      "grad_norm": 0.1132567971944809,
      "learning_rate": 0.00015241864555848726,
      "loss": 0.019,
      "step": 1083
    },
    {
      "epoch": 0.7150395778364116,
      "grad_norm": 0.12938563525676727,
      "learning_rate": 0.00015237467018469657,
      "loss": 0.0408,
      "step": 1084
    },
    {
      "epoch": 0.7156992084432717,
      "grad_norm": 0.10310547798871994,
      "learning_rate": 0.00015233069481090589,
      "loss": 0.0317,
      "step": 1085
    },
    {
      "epoch": 0.716358839050132,
      "grad_norm": 0.06401032954454422,
      "learning_rate": 0.00015228671943711523,
      "loss": 0.0168,
      "step": 1086
    },
    {
      "epoch": 0.7170184696569921,
      "grad_norm": 0.09109515696763992,
      "learning_rate": 0.00015224274406332454,
      "loss": 0.022,
      "step": 1087
    },
    {
      "epoch": 0.7176781002638523,
      "grad_norm": 0.07810726016759872,
      "learning_rate": 0.00015219876868953386,
      "loss": 0.0194,
      "step": 1088
    },
    {
      "epoch": 0.7183377308707124,
      "grad_norm": 0.13174159824848175,
      "learning_rate": 0.0001521547933157432,
      "loss": 0.0338,
      "step": 1089
    },
    {
      "epoch": 0.7189973614775725,
      "grad_norm": 0.07021994888782501,
      "learning_rate": 0.0001521108179419525,
      "loss": 0.0146,
      "step": 1090
    },
    {
      "epoch": 0.7196569920844327,
      "grad_norm": 0.1110735610127449,
      "learning_rate": 0.00015206684256816185,
      "loss": 0.0328,
      "step": 1091
    },
    {
      "epoch": 0.7203166226912929,
      "grad_norm": 0.7329087853431702,
      "learning_rate": 0.00015202286719437117,
      "loss": 0.0613,
      "step": 1092
    },
    {
      "epoch": 0.7209762532981531,
      "grad_norm": 0.18205001950263977,
      "learning_rate": 0.00015197889182058048,
      "loss": 0.0535,
      "step": 1093
    },
    {
      "epoch": 0.7216358839050132,
      "grad_norm": 0.1592993438243866,
      "learning_rate": 0.00015193491644678982,
      "loss": 0.0403,
      "step": 1094
    },
    {
      "epoch": 0.7222955145118733,
      "grad_norm": 0.09413556754589081,
      "learning_rate": 0.00015189094107299913,
      "loss": 0.018,
      "step": 1095
    },
    {
      "epoch": 0.7229551451187335,
      "grad_norm": 0.23609623312950134,
      "learning_rate": 0.00015184696569920845,
      "loss": 0.0415,
      "step": 1096
    },
    {
      "epoch": 0.7236147757255936,
      "grad_norm": 0.09255460649728775,
      "learning_rate": 0.0001518029903254178,
      "loss": 0.0074,
      "step": 1097
    },
    {
      "epoch": 0.7242744063324539,
      "grad_norm": 0.1699853539466858,
      "learning_rate": 0.0001517590149516271,
      "loss": 0.0499,
      "step": 1098
    },
    {
      "epoch": 0.724934036939314,
      "grad_norm": 0.09596843272447586,
      "learning_rate": 0.00015171503957783642,
      "loss": 0.0191,
      "step": 1099
    },
    {
      "epoch": 0.7255936675461742,
      "grad_norm": 0.09993493556976318,
      "learning_rate": 0.00015167106420404573,
      "loss": 0.0296,
      "step": 1100
    },
    {
      "epoch": 0.7262532981530343,
      "grad_norm": 0.10386785119771957,
      "learning_rate": 0.00015162708883025507,
      "loss": 0.0179,
      "step": 1101
    },
    {
      "epoch": 0.7269129287598944,
      "grad_norm": 0.08137441426515579,
      "learning_rate": 0.00015158311345646439,
      "loss": 0.0065,
      "step": 1102
    },
    {
      "epoch": 0.7275725593667546,
      "grad_norm": 0.12285342067480087,
      "learning_rate": 0.0001515391380826737,
      "loss": 0.0321,
      "step": 1103
    },
    {
      "epoch": 0.7282321899736148,
      "grad_norm": 0.12240971624851227,
      "learning_rate": 0.00015149516270888304,
      "loss": 0.0283,
      "step": 1104
    },
    {
      "epoch": 0.728891820580475,
      "grad_norm": 0.14870119094848633,
      "learning_rate": 0.00015145118733509236,
      "loss": 0.0361,
      "step": 1105
    },
    {
      "epoch": 0.7295514511873351,
      "grad_norm": 0.2626568078994751,
      "learning_rate": 0.00015140721196130167,
      "loss": 0.0774,
      "step": 1106
    },
    {
      "epoch": 0.7302110817941952,
      "grad_norm": 0.16298460960388184,
      "learning_rate": 0.00015136323658751098,
      "loss": 0.0352,
      "step": 1107
    },
    {
      "epoch": 0.7308707124010554,
      "grad_norm": 0.07011725753545761,
      "learning_rate": 0.00015131926121372032,
      "loss": 0.0158,
      "step": 1108
    },
    {
      "epoch": 0.7315303430079155,
      "grad_norm": 0.09363522380590439,
      "learning_rate": 0.00015127528583992964,
      "loss": 0.0288,
      "step": 1109
    },
    {
      "epoch": 0.7321899736147758,
      "grad_norm": 0.11241000145673752,
      "learning_rate": 0.00015123131046613895,
      "loss": 0.0331,
      "step": 1110
    },
    {
      "epoch": 0.7328496042216359,
      "grad_norm": 0.08750134706497192,
      "learning_rate": 0.0001511873350923483,
      "loss": 0.0282,
      "step": 1111
    },
    {
      "epoch": 0.7335092348284961,
      "grad_norm": 0.16730190813541412,
      "learning_rate": 0.0001511433597185576,
      "loss": 0.0493,
      "step": 1112
    },
    {
      "epoch": 0.7341688654353562,
      "grad_norm": 0.1700805425643921,
      "learning_rate": 0.00015109938434476695,
      "loss": 0.0441,
      "step": 1113
    },
    {
      "epoch": 0.7348284960422163,
      "grad_norm": 0.1306026130914688,
      "learning_rate": 0.00015105540897097626,
      "loss": 0.0277,
      "step": 1114
    },
    {
      "epoch": 0.7354881266490765,
      "grad_norm": 0.22116461396217346,
      "learning_rate": 0.0001510114335971856,
      "loss": 0.0368,
      "step": 1115
    },
    {
      "epoch": 0.7361477572559367,
      "grad_norm": 0.15216684341430664,
      "learning_rate": 0.00015096745822339492,
      "loss": 0.0154,
      "step": 1116
    },
    {
      "epoch": 0.7368073878627969,
      "grad_norm": 0.18610331416130066,
      "learning_rate": 0.00015092348284960423,
      "loss": 0.0166,
      "step": 1117
    },
    {
      "epoch": 0.737467018469657,
      "grad_norm": 0.167336106300354,
      "learning_rate": 0.00015087950747581354,
      "loss": 0.0403,
      "step": 1118
    },
    {
      "epoch": 0.7381266490765171,
      "grad_norm": 0.0707329586148262,
      "learning_rate": 0.00015083553210202289,
      "loss": 0.0082,
      "step": 1119
    },
    {
      "epoch": 0.7387862796833773,
      "grad_norm": 0.09035850316286087,
      "learning_rate": 0.0001507915567282322,
      "loss": 0.0184,
      "step": 1120
    },
    {
      "epoch": 0.7394459102902374,
      "grad_norm": 0.07237391918897629,
      "learning_rate": 0.00015074758135444151,
      "loss": 0.0133,
      "step": 1121
    },
    {
      "epoch": 0.7401055408970977,
      "grad_norm": 0.2025974690914154,
      "learning_rate": 0.00015070360598065086,
      "loss": 0.046,
      "step": 1122
    },
    {
      "epoch": 0.7407651715039578,
      "grad_norm": 0.12884214520454407,
      "learning_rate": 0.00015065963060686017,
      "loss": 0.0315,
      "step": 1123
    },
    {
      "epoch": 0.741424802110818,
      "grad_norm": 0.11418743431568146,
      "learning_rate": 0.00015061565523306948,
      "loss": 0.0227,
      "step": 1124
    },
    {
      "epoch": 0.7420844327176781,
      "grad_norm": 0.2529642581939697,
      "learning_rate": 0.0001505716798592788,
      "loss": 0.0461,
      "step": 1125
    },
    {
      "epoch": 0.7427440633245382,
      "grad_norm": 0.24269109964370728,
      "learning_rate": 0.00015052770448548814,
      "loss": 0.0586,
      "step": 1126
    },
    {
      "epoch": 0.7434036939313984,
      "grad_norm": 0.24893821775913239,
      "learning_rate": 0.00015048372911169745,
      "loss": 0.0458,
      "step": 1127
    },
    {
      "epoch": 0.7440633245382586,
      "grad_norm": 0.09815414994955063,
      "learning_rate": 0.00015043975373790677,
      "loss": 0.0071,
      "step": 1128
    },
    {
      "epoch": 0.7447229551451188,
      "grad_norm": 0.16130994260311127,
      "learning_rate": 0.00015039577836411608,
      "loss": 0.0373,
      "step": 1129
    },
    {
      "epoch": 0.7453825857519789,
      "grad_norm": 0.1290937066078186,
      "learning_rate": 0.00015035180299032542,
      "loss": 0.0247,
      "step": 1130
    },
    {
      "epoch": 0.746042216358839,
      "grad_norm": 0.1623949557542801,
      "learning_rate": 0.00015030782761653473,
      "loss": 0.0305,
      "step": 1131
    },
    {
      "epoch": 0.7467018469656992,
      "grad_norm": 0.1786722093820572,
      "learning_rate": 0.00015026385224274408,
      "loss": 0.0494,
      "step": 1132
    },
    {
      "epoch": 0.7473614775725593,
      "grad_norm": 0.255016028881073,
      "learning_rate": 0.0001502198768689534,
      "loss": 0.0633,
      "step": 1133
    },
    {
      "epoch": 0.7480211081794196,
      "grad_norm": 0.27951112389564514,
      "learning_rate": 0.00015017590149516273,
      "loss": 0.0436,
      "step": 1134
    },
    {
      "epoch": 0.7486807387862797,
      "grad_norm": 0.27387458086013794,
      "learning_rate": 0.00015013192612137204,
      "loss": 0.0257,
      "step": 1135
    },
    {
      "epoch": 0.7493403693931399,
      "grad_norm": 0.08721756935119629,
      "learning_rate": 0.00015008795074758136,
      "loss": 0.014,
      "step": 1136
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.07264388352632523,
      "learning_rate": 0.0001500439753737907,
      "loss": 0.0052,
      "step": 1137
    },
    {
      "epoch": 0.7506596306068601,
      "grad_norm": 0.029796576127409935,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.002,
      "step": 1138
    },
    {
      "epoch": 0.7513192612137203,
      "grad_norm": 0.21070197224617004,
      "learning_rate": 0.00014995602462620933,
      "loss": 0.0516,
      "step": 1139
    },
    {
      "epoch": 0.7519788918205804,
      "grad_norm": 0.13773980736732483,
      "learning_rate": 0.00014991204925241867,
      "loss": 0.0303,
      "step": 1140
    },
    {
      "epoch": 0.7526385224274407,
      "grad_norm": 0.054930493235588074,
      "learning_rate": 0.00014986807387862798,
      "loss": 0.0166,
      "step": 1141
    },
    {
      "epoch": 0.7532981530343008,
      "grad_norm": 0.05685633420944214,
      "learning_rate": 0.0001498240985048373,
      "loss": 0.015,
      "step": 1142
    },
    {
      "epoch": 0.753957783641161,
      "grad_norm": 0.05582136660814285,
      "learning_rate": 0.0001497801231310466,
      "loss": 0.0135,
      "step": 1143
    },
    {
      "epoch": 0.7546174142480211,
      "grad_norm": 0.09212943911552429,
      "learning_rate": 0.00014973614775725595,
      "loss": 0.0268,
      "step": 1144
    },
    {
      "epoch": 0.7552770448548812,
      "grad_norm": 0.21025791764259338,
      "learning_rate": 0.00014969217238346527,
      "loss": 0.0533,
      "step": 1145
    },
    {
      "epoch": 0.7559366754617414,
      "grad_norm": 0.14615651965141296,
      "learning_rate": 0.00014964819700967458,
      "loss": 0.0364,
      "step": 1146
    },
    {
      "epoch": 0.7565963060686016,
      "grad_norm": 0.10061509162187576,
      "learning_rate": 0.00014960422163588392,
      "loss": 0.0246,
      "step": 1147
    },
    {
      "epoch": 0.7572559366754618,
      "grad_norm": 0.20791466534137726,
      "learning_rate": 0.00014956024626209323,
      "loss": 0.0513,
      "step": 1148
    },
    {
      "epoch": 0.7579155672823219,
      "grad_norm": 0.14291352033615112,
      "learning_rate": 0.00014951627088830255,
      "loss": 0.0463,
      "step": 1149
    },
    {
      "epoch": 0.758575197889182,
      "grad_norm": 0.06363843381404877,
      "learning_rate": 0.00014947229551451186,
      "loss": 0.0204,
      "step": 1150
    },
    {
      "epoch": 0.7592348284960422,
      "grad_norm": 0.06722096353769302,
      "learning_rate": 0.0001494283201407212,
      "loss": 0.0228,
      "step": 1151
    },
    {
      "epoch": 0.7598944591029023,
      "grad_norm": 0.16037046909332275,
      "learning_rate": 0.00014938434476693052,
      "loss": 0.0372,
      "step": 1152
    },
    {
      "epoch": 0.7605540897097626,
      "grad_norm": 0.08683419972658157,
      "learning_rate": 0.00014934036939313983,
      "loss": 0.0067,
      "step": 1153
    },
    {
      "epoch": 0.7612137203166227,
      "grad_norm": 0.12339337915182114,
      "learning_rate": 0.00014929639401934917,
      "loss": 0.0378,
      "step": 1154
    },
    {
      "epoch": 0.7618733509234829,
      "grad_norm": 0.14152918756008148,
      "learning_rate": 0.00014925241864555851,
      "loss": 0.0292,
      "step": 1155
    },
    {
      "epoch": 0.762532981530343,
      "grad_norm": 0.13841411471366882,
      "learning_rate": 0.00014920844327176783,
      "loss": 0.0394,
      "step": 1156
    },
    {
      "epoch": 0.7631926121372031,
      "grad_norm": 0.14338889718055725,
      "learning_rate": 0.00014916446789797714,
      "loss": 0.0333,
      "step": 1157
    },
    {
      "epoch": 0.7638522427440633,
      "grad_norm": 0.12224128097295761,
      "learning_rate": 0.00014912049252418648,
      "loss": 0.0303,
      "step": 1158
    },
    {
      "epoch": 0.7645118733509235,
      "grad_norm": 0.12962348759174347,
      "learning_rate": 0.0001490765171503958,
      "loss": 0.0125,
      "step": 1159
    },
    {
      "epoch": 0.7651715039577837,
      "grad_norm": 0.17277273535728455,
      "learning_rate": 0.0001490325417766051,
      "loss": 0.0554,
      "step": 1160
    },
    {
      "epoch": 0.7658311345646438,
      "grad_norm": 0.08710812032222748,
      "learning_rate": 0.00014898856640281442,
      "loss": 0.0176,
      "step": 1161
    },
    {
      "epoch": 0.7664907651715039,
      "grad_norm": 0.14732180535793304,
      "learning_rate": 0.00014894459102902377,
      "loss": 0.0171,
      "step": 1162
    },
    {
      "epoch": 0.7671503957783641,
      "grad_norm": 0.16634072363376617,
      "learning_rate": 0.00014890061565523308,
      "loss": 0.046,
      "step": 1163
    },
    {
      "epoch": 0.7678100263852242,
      "grad_norm": 0.09661095589399338,
      "learning_rate": 0.0001488566402814424,
      "loss": 0.0164,
      "step": 1164
    },
    {
      "epoch": 0.7684696569920845,
      "grad_norm": 0.3226813077926636,
      "learning_rate": 0.00014881266490765173,
      "loss": 0.0608,
      "step": 1165
    },
    {
      "epoch": 0.7691292875989446,
      "grad_norm": 0.03305474668741226,
      "learning_rate": 0.00014876868953386105,
      "loss": 0.0017,
      "step": 1166
    },
    {
      "epoch": 0.7697889182058048,
      "grad_norm": 0.21746405959129333,
      "learning_rate": 0.00014872471416007036,
      "loss": 0.0329,
      "step": 1167
    },
    {
      "epoch": 0.7704485488126649,
      "grad_norm": 0.12151490151882172,
      "learning_rate": 0.00014868073878627968,
      "loss": 0.0272,
      "step": 1168
    },
    {
      "epoch": 0.771108179419525,
      "grad_norm": 0.17121681571006775,
      "learning_rate": 0.00014863676341248902,
      "loss": 0.0366,
      "step": 1169
    },
    {
      "epoch": 0.7717678100263852,
      "grad_norm": 0.08975103497505188,
      "learning_rate": 0.00014859278803869833,
      "loss": 0.018,
      "step": 1170
    },
    {
      "epoch": 0.7724274406332454,
      "grad_norm": 0.1666780710220337,
      "learning_rate": 0.00014854881266490765,
      "loss": 0.0264,
      "step": 1171
    },
    {
      "epoch": 0.7730870712401056,
      "grad_norm": 0.4702862501144409,
      "learning_rate": 0.00014850483729111696,
      "loss": 0.103,
      "step": 1172
    },
    {
      "epoch": 0.7737467018469657,
      "grad_norm": 0.08853144943714142,
      "learning_rate": 0.0001484608619173263,
      "loss": 0.0054,
      "step": 1173
    },
    {
      "epoch": 0.7744063324538258,
      "grad_norm": 0.23726508021354675,
      "learning_rate": 0.00014841688654353561,
      "loss": 0.0415,
      "step": 1174
    },
    {
      "epoch": 0.775065963060686,
      "grad_norm": 0.18575792014598846,
      "learning_rate": 0.00014837291116974496,
      "loss": 0.0267,
      "step": 1175
    },
    {
      "epoch": 0.7757255936675461,
      "grad_norm": 0.2986917197704315,
      "learning_rate": 0.00014832893579595427,
      "loss": 0.0262,
      "step": 1176
    },
    {
      "epoch": 0.7763852242744064,
      "grad_norm": 0.24924983084201813,
      "learning_rate": 0.0001482849604221636,
      "loss": 0.0444,
      "step": 1177
    },
    {
      "epoch": 0.7770448548812665,
      "grad_norm": 0.258334755897522,
      "learning_rate": 0.00014824098504837292,
      "loss": 0.0209,
      "step": 1178
    },
    {
      "epoch": 0.7777044854881267,
      "grad_norm": 0.1042453944683075,
      "learning_rate": 0.00014819700967458224,
      "loss": 0.007,
      "step": 1179
    },
    {
      "epoch": 0.7783641160949868,
      "grad_norm": 0.2734003961086273,
      "learning_rate": 0.00014815303430079158,
      "loss": 0.0261,
      "step": 1180
    },
    {
      "epoch": 0.7790237467018469,
      "grad_norm": 0.12018900364637375,
      "learning_rate": 0.0001481090589270009,
      "loss": 0.0184,
      "step": 1181
    },
    {
      "epoch": 0.7796833773087071,
      "grad_norm": 0.03656395152211189,
      "learning_rate": 0.0001480650835532102,
      "loss": 0.0022,
      "step": 1182
    },
    {
      "epoch": 0.7803430079155673,
      "grad_norm": 0.1691662222146988,
      "learning_rate": 0.00014802110817941955,
      "loss": 0.0384,
      "step": 1183
    },
    {
      "epoch": 0.7810026385224275,
      "grad_norm": 0.19660137593746185,
      "learning_rate": 0.00014797713280562886,
      "loss": 0.0432,
      "step": 1184
    },
    {
      "epoch": 0.7816622691292876,
      "grad_norm": 0.10865235328674316,
      "learning_rate": 0.00014793315743183818,
      "loss": 0.024,
      "step": 1185
    },
    {
      "epoch": 0.7823218997361477,
      "grad_norm": 0.12858985364437103,
      "learning_rate": 0.0001478891820580475,
      "loss": 0.0264,
      "step": 1186
    },
    {
      "epoch": 0.7829815303430079,
      "grad_norm": 0.1542041152715683,
      "learning_rate": 0.00014784520668425683,
      "loss": 0.0304,
      "step": 1187
    },
    {
      "epoch": 0.783641160949868,
      "grad_norm": 0.13013005256652832,
      "learning_rate": 0.00014780123131046615,
      "loss": 0.0251,
      "step": 1188
    },
    {
      "epoch": 0.7843007915567283,
      "grad_norm": 0.06414933502674103,
      "learning_rate": 0.00014775725593667546,
      "loss": 0.0043,
      "step": 1189
    },
    {
      "epoch": 0.7849604221635884,
      "grad_norm": 0.2554457485675812,
      "learning_rate": 0.00014771328056288477,
      "loss": 0.0389,
      "step": 1190
    },
    {
      "epoch": 0.7856200527704486,
      "grad_norm": 0.08315733820199966,
      "learning_rate": 0.00014766930518909411,
      "loss": 0.0177,
      "step": 1191
    },
    {
      "epoch": 0.7862796833773087,
      "grad_norm": 0.1026112362742424,
      "learning_rate": 0.00014762532981530343,
      "loss": 0.0252,
      "step": 1192
    },
    {
      "epoch": 0.7869393139841688,
      "grad_norm": 0.17099954187870026,
      "learning_rate": 0.00014758135444151274,
      "loss": 0.0367,
      "step": 1193
    },
    {
      "epoch": 0.787598944591029,
      "grad_norm": 0.13773764669895172,
      "learning_rate": 0.00014753737906772208,
      "loss": 0.0418,
      "step": 1194
    },
    {
      "epoch": 0.7882585751978892,
      "grad_norm": 0.10139429569244385,
      "learning_rate": 0.0001474934036939314,
      "loss": 0.034,
      "step": 1195
    },
    {
      "epoch": 0.7889182058047494,
      "grad_norm": 0.0975533127784729,
      "learning_rate": 0.00014744942832014074,
      "loss": 0.0176,
      "step": 1196
    },
    {
      "epoch": 0.7895778364116095,
      "grad_norm": 0.14308832585811615,
      "learning_rate": 0.00014740545294635005,
      "loss": 0.0452,
      "step": 1197
    },
    {
      "epoch": 0.7902374670184696,
      "grad_norm": 0.09715338796377182,
      "learning_rate": 0.0001473614775725594,
      "loss": 0.0266,
      "step": 1198
    },
    {
      "epoch": 0.7908970976253298,
      "grad_norm": 0.22028984129428864,
      "learning_rate": 0.0001473175021987687,
      "loss": 0.0619,
      "step": 1199
    },
    {
      "epoch": 0.7915567282321899,
      "grad_norm": 0.14724934101104736,
      "learning_rate": 0.00014727352682497802,
      "loss": 0.0188,
      "step": 1200
    },
    {
      "epoch": 0.7922163588390502,
      "grad_norm": 0.14176255464553833,
      "learning_rate": 0.00014722955145118736,
      "loss": 0.0371,
      "step": 1201
    },
    {
      "epoch": 0.7928759894459103,
      "grad_norm": 0.2141813188791275,
      "learning_rate": 0.00014718557607739668,
      "loss": 0.0614,
      "step": 1202
    },
    {
      "epoch": 0.7935356200527705,
      "grad_norm": 0.22877326607704163,
      "learning_rate": 0.000147141600703606,
      "loss": 0.0364,
      "step": 1203
    },
    {
      "epoch": 0.7941952506596306,
      "grad_norm": 0.1719219833612442,
      "learning_rate": 0.0001470976253298153,
      "loss": 0.0239,
      "step": 1204
    },
    {
      "epoch": 0.7948548812664907,
      "grad_norm": 0.17348599433898926,
      "learning_rate": 0.00014705364995602465,
      "loss": 0.0223,
      "step": 1205
    },
    {
      "epoch": 0.7955145118733509,
      "grad_norm": 0.1077476441860199,
      "learning_rate": 0.00014700967458223396,
      "loss": 0.0263,
      "step": 1206
    },
    {
      "epoch": 0.7961741424802111,
      "grad_norm": 0.07950446009635925,
      "learning_rate": 0.00014696569920844327,
      "loss": 0.0195,
      "step": 1207
    },
    {
      "epoch": 0.7968337730870713,
      "grad_norm": 0.2656054198741913,
      "learning_rate": 0.0001469217238346526,
      "loss": 0.0552,
      "step": 1208
    },
    {
      "epoch": 0.7974934036939314,
      "grad_norm": 0.18435898423194885,
      "learning_rate": 0.00014687774846086193,
      "loss": 0.0459,
      "step": 1209
    },
    {
      "epoch": 0.7981530343007915,
      "grad_norm": 0.10144588351249695,
      "learning_rate": 0.00014683377308707124,
      "loss": 0.0248,
      "step": 1210
    },
    {
      "epoch": 0.7988126649076517,
      "grad_norm": 0.12228976935148239,
      "learning_rate": 0.00014678979771328056,
      "loss": 0.024,
      "step": 1211
    },
    {
      "epoch": 0.7994722955145118,
      "grad_norm": 0.14885996282100677,
      "learning_rate": 0.0001467458223394899,
      "loss": 0.0366,
      "step": 1212
    },
    {
      "epoch": 0.8001319261213721,
      "grad_norm": 0.08718258142471313,
      "learning_rate": 0.0001467018469656992,
      "loss": 0.0125,
      "step": 1213
    },
    {
      "epoch": 0.8007915567282322,
      "grad_norm": 0.150014728307724,
      "learning_rate": 0.00014665787159190853,
      "loss": 0.026,
      "step": 1214
    },
    {
      "epoch": 0.8014511873350924,
      "grad_norm": 0.2707614600658417,
      "learning_rate": 0.00014661389621811784,
      "loss": 0.0616,
      "step": 1215
    },
    {
      "epoch": 0.8021108179419525,
      "grad_norm": 0.12168911099433899,
      "learning_rate": 0.00014656992084432718,
      "loss": 0.019,
      "step": 1216
    },
    {
      "epoch": 0.8027704485488126,
      "grad_norm": 0.12318751215934753,
      "learning_rate": 0.0001465259454705365,
      "loss": 0.0343,
      "step": 1217
    },
    {
      "epoch": 0.8034300791556728,
      "grad_norm": 0.10471601784229279,
      "learning_rate": 0.00014648197009674584,
      "loss": 0.0073,
      "step": 1218
    },
    {
      "epoch": 0.804089709762533,
      "grad_norm": 0.10562022030353546,
      "learning_rate": 0.00014643799472295515,
      "loss": 0.0138,
      "step": 1219
    },
    {
      "epoch": 0.8047493403693932,
      "grad_norm": 0.32828986644744873,
      "learning_rate": 0.0001463940193491645,
      "loss": 0.094,
      "step": 1220
    },
    {
      "epoch": 0.8054089709762533,
      "grad_norm": 0.07620206475257874,
      "learning_rate": 0.0001463500439753738,
      "loss": 0.0119,
      "step": 1221
    },
    {
      "epoch": 0.8060686015831134,
      "grad_norm": 0.22666795551776886,
      "learning_rate": 0.00014630606860158312,
      "loss": 0.0616,
      "step": 1222
    },
    {
      "epoch": 0.8067282321899736,
      "grad_norm": 0.0935964286327362,
      "learning_rate": 0.00014626209322779246,
      "loss": 0.0202,
      "step": 1223
    },
    {
      "epoch": 0.8073878627968337,
      "grad_norm": 0.10287509858608246,
      "learning_rate": 0.00014621811785400177,
      "loss": 0.0216,
      "step": 1224
    },
    {
      "epoch": 0.808047493403694,
      "grad_norm": 0.10089585930109024,
      "learning_rate": 0.0001461741424802111,
      "loss": 0.0272,
      "step": 1225
    },
    {
      "epoch": 0.8087071240105541,
      "grad_norm": 0.14899571239948273,
      "learning_rate": 0.0001461301671064204,
      "loss": 0.0391,
      "step": 1226
    },
    {
      "epoch": 0.8093667546174143,
      "grad_norm": 0.13873064517974854,
      "learning_rate": 0.00014608619173262974,
      "loss": 0.0342,
      "step": 1227
    },
    {
      "epoch": 0.8100263852242744,
      "grad_norm": 0.1265122890472412,
      "learning_rate": 0.00014604221635883906,
      "loss": 0.0532,
      "step": 1228
    },
    {
      "epoch": 0.8106860158311345,
      "grad_norm": 0.13974390923976898,
      "learning_rate": 0.00014599824098504837,
      "loss": 0.037,
      "step": 1229
    },
    {
      "epoch": 0.8113456464379947,
      "grad_norm": 0.09109117090702057,
      "learning_rate": 0.0001459542656112577,
      "loss": 0.018,
      "step": 1230
    },
    {
      "epoch": 0.8120052770448549,
      "grad_norm": 0.12234988063573837,
      "learning_rate": 0.00014591029023746703,
      "loss": 0.0235,
      "step": 1231
    },
    {
      "epoch": 0.8126649076517151,
      "grad_norm": 0.0939435288310051,
      "learning_rate": 0.00014586631486367634,
      "loss": 0.0198,
      "step": 1232
    },
    {
      "epoch": 0.8133245382585752,
      "grad_norm": 0.18718048930168152,
      "learning_rate": 0.00014582233948988565,
      "loss": 0.0492,
      "step": 1233
    },
    {
      "epoch": 0.8139841688654353,
      "grad_norm": 0.17891019582748413,
      "learning_rate": 0.000145778364116095,
      "loss": 0.0254,
      "step": 1234
    },
    {
      "epoch": 0.8146437994722955,
      "grad_norm": 0.13952727615833282,
      "learning_rate": 0.0001457343887423043,
      "loss": 0.0263,
      "step": 1235
    },
    {
      "epoch": 0.8153034300791556,
      "grad_norm": 0.14642462134361267,
      "learning_rate": 0.00014569041336851362,
      "loss": 0.0365,
      "step": 1236
    },
    {
      "epoch": 0.8159630606860159,
      "grad_norm": 0.1176135465502739,
      "learning_rate": 0.00014564643799472296,
      "loss": 0.031,
      "step": 1237
    },
    {
      "epoch": 0.816622691292876,
      "grad_norm": 0.08808088302612305,
      "learning_rate": 0.00014560246262093228,
      "loss": 0.0191,
      "step": 1238
    },
    {
      "epoch": 0.8172823218997362,
      "grad_norm": 0.18374067544937134,
      "learning_rate": 0.00014555848724714162,
      "loss": 0.0323,
      "step": 1239
    },
    {
      "epoch": 0.8179419525065963,
      "grad_norm": 0.1290590614080429,
      "learning_rate": 0.00014551451187335093,
      "loss": 0.0334,
      "step": 1240
    },
    {
      "epoch": 0.8186015831134564,
      "grad_norm": 0.1516120582818985,
      "learning_rate": 0.00014547053649956027,
      "loss": 0.0537,
      "step": 1241
    },
    {
      "epoch": 0.8192612137203166,
      "grad_norm": 0.11060772836208344,
      "learning_rate": 0.0001454265611257696,
      "loss": 0.0236,
      "step": 1242
    },
    {
      "epoch": 0.8199208443271768,
      "grad_norm": 0.2270449846982956,
      "learning_rate": 0.0001453825857519789,
      "loss": 0.0516,
      "step": 1243
    },
    {
      "epoch": 0.820580474934037,
      "grad_norm": 0.10795996338129044,
      "learning_rate": 0.00014533861037818822,
      "loss": 0.0269,
      "step": 1244
    },
    {
      "epoch": 0.8212401055408971,
      "grad_norm": 0.12526631355285645,
      "learning_rate": 0.00014529463500439756,
      "loss": 0.0162,
      "step": 1245
    },
    {
      "epoch": 0.8218997361477572,
      "grad_norm": 0.21156255900859833,
      "learning_rate": 0.00014525065963060687,
      "loss": 0.0429,
      "step": 1246
    },
    {
      "epoch": 0.8225593667546174,
      "grad_norm": 0.07933206111192703,
      "learning_rate": 0.00014520668425681618,
      "loss": 0.0077,
      "step": 1247
    },
    {
      "epoch": 0.8232189973614775,
      "grad_norm": 0.1327560693025589,
      "learning_rate": 0.00014516270888302553,
      "loss": 0.0323,
      "step": 1248
    },
    {
      "epoch": 0.8238786279683378,
      "grad_norm": 0.18084308505058289,
      "learning_rate": 0.00014511873350923484,
      "loss": 0.0564,
      "step": 1249
    },
    {
      "epoch": 0.8245382585751979,
      "grad_norm": 0.23599852621555328,
      "learning_rate": 0.00014507475813544415,
      "loss": 0.0532,
      "step": 1250
    },
    {
      "epoch": 0.825197889182058,
      "grad_norm": 0.07861629873514175,
      "learning_rate": 0.00014503078276165347,
      "loss": 0.0117,
      "step": 1251
    },
    {
      "epoch": 0.8258575197889182,
      "grad_norm": 0.26975560188293457,
      "learning_rate": 0.0001449868073878628,
      "loss": 0.0765,
      "step": 1252
    },
    {
      "epoch": 0.8265171503957783,
      "grad_norm": 0.11733822524547577,
      "learning_rate": 0.00014494283201407212,
      "loss": 0.0234,
      "step": 1253
    },
    {
      "epoch": 0.8271767810026385,
      "grad_norm": 0.1287408322095871,
      "learning_rate": 0.00014489885664028144,
      "loss": 0.0099,
      "step": 1254
    },
    {
      "epoch": 0.8278364116094987,
      "grad_norm": 0.11043570190668106,
      "learning_rate": 0.00014485488126649078,
      "loss": 0.0202,
      "step": 1255
    },
    {
      "epoch": 0.8284960422163589,
      "grad_norm": 0.15035487711429596,
      "learning_rate": 0.0001448109058927001,
      "loss": 0.0296,
      "step": 1256
    },
    {
      "epoch": 0.829155672823219,
      "grad_norm": 0.09346380829811096,
      "learning_rate": 0.0001447669305189094,
      "loss": 0.0061,
      "step": 1257
    },
    {
      "epoch": 0.8298153034300791,
      "grad_norm": 0.20183919370174408,
      "learning_rate": 0.00014472295514511872,
      "loss": 0.0498,
      "step": 1258
    },
    {
      "epoch": 0.8304749340369393,
      "grad_norm": 0.266988068819046,
      "learning_rate": 0.00014467897977132806,
      "loss": 0.037,
      "step": 1259
    },
    {
      "epoch": 0.8311345646437994,
      "grad_norm": 0.13506411015987396,
      "learning_rate": 0.0001446350043975374,
      "loss": 0.0305,
      "step": 1260
    },
    {
      "epoch": 0.8317941952506597,
      "grad_norm": 0.10283072292804718,
      "learning_rate": 0.00014459102902374672,
      "loss": 0.0179,
      "step": 1261
    },
    {
      "epoch": 0.8324538258575198,
      "grad_norm": 0.13362711668014526,
      "learning_rate": 0.00014454705364995603,
      "loss": 0.0226,
      "step": 1262
    },
    {
      "epoch": 0.83311345646438,
      "grad_norm": 0.08259125798940659,
      "learning_rate": 0.00014450307827616537,
      "loss": 0.0093,
      "step": 1263
    },
    {
      "epoch": 0.8337730870712401,
      "grad_norm": 0.18642807006835938,
      "learning_rate": 0.00014445910290237468,
      "loss": 0.0526,
      "step": 1264
    },
    {
      "epoch": 0.8344327176781002,
      "grad_norm": 0.07984864711761475,
      "learning_rate": 0.000144415127528584,
      "loss": 0.0177,
      "step": 1265
    },
    {
      "epoch": 0.8350923482849604,
      "grad_norm": 0.14551129937171936,
      "learning_rate": 0.00014437115215479334,
      "loss": 0.0393,
      "step": 1266
    },
    {
      "epoch": 0.8357519788918206,
      "grad_norm": 0.13752523064613342,
      "learning_rate": 0.00014432717678100265,
      "loss": 0.0097,
      "step": 1267
    },
    {
      "epoch": 0.8364116094986808,
      "grad_norm": 0.07517698407173157,
      "learning_rate": 0.00014428320140721197,
      "loss": 0.0068,
      "step": 1268
    },
    {
      "epoch": 0.8370712401055409,
      "grad_norm": 0.10392218828201294,
      "learning_rate": 0.00014423922603342128,
      "loss": 0.0068,
      "step": 1269
    },
    {
      "epoch": 0.837730870712401,
      "grad_norm": 0.20159578323364258,
      "learning_rate": 0.00014419525065963062,
      "loss": 0.0511,
      "step": 1270
    },
    {
      "epoch": 0.8383905013192612,
      "grad_norm": 0.05239509418606758,
      "learning_rate": 0.00014415127528583994,
      "loss": 0.0111,
      "step": 1271
    },
    {
      "epoch": 0.8390501319261213,
      "grad_norm": 0.07003647834062576,
      "learning_rate": 0.00014410729991204925,
      "loss": 0.0101,
      "step": 1272
    },
    {
      "epoch": 0.8397097625329816,
      "grad_norm": 0.1692083179950714,
      "learning_rate": 0.0001440633245382586,
      "loss": 0.0355,
      "step": 1273
    },
    {
      "epoch": 0.8403693931398417,
      "grad_norm": 0.08963880687952042,
      "learning_rate": 0.0001440193491644679,
      "loss": 0.0093,
      "step": 1274
    },
    {
      "epoch": 0.8410290237467019,
      "grad_norm": 0.18244701623916626,
      "learning_rate": 0.00014397537379067722,
      "loss": 0.0466,
      "step": 1275
    },
    {
      "epoch": 0.841688654353562,
      "grad_norm": 0.13303536176681519,
      "learning_rate": 0.00014393139841688653,
      "loss": 0.0299,
      "step": 1276
    },
    {
      "epoch": 0.8423482849604221,
      "grad_norm": 0.09780425578355789,
      "learning_rate": 0.00014388742304309587,
      "loss": 0.0225,
      "step": 1277
    },
    {
      "epoch": 0.8430079155672823,
      "grad_norm": 0.09323029220104218,
      "learning_rate": 0.0001438434476693052,
      "loss": 0.0211,
      "step": 1278
    },
    {
      "epoch": 0.8436675461741425,
      "grad_norm": 0.06148659437894821,
      "learning_rate": 0.0001437994722955145,
      "loss": 0.0038,
      "step": 1279
    },
    {
      "epoch": 0.8443271767810027,
      "grad_norm": 0.05174132436513901,
      "learning_rate": 0.00014375549692172384,
      "loss": 0.0031,
      "step": 1280
    },
    {
      "epoch": 0.8449868073878628,
      "grad_norm": 0.09371868520975113,
      "learning_rate": 0.00014371152154793316,
      "loss": 0.0142,
      "step": 1281
    },
    {
      "epoch": 0.8456464379947229,
      "grad_norm": 0.12266460806131363,
      "learning_rate": 0.0001436675461741425,
      "loss": 0.0343,
      "step": 1282
    },
    {
      "epoch": 0.8463060686015831,
      "grad_norm": 0.07019917666912079,
      "learning_rate": 0.0001436235708003518,
      "loss": 0.0048,
      "step": 1283
    },
    {
      "epoch": 0.8469656992084432,
      "grad_norm": 0.29698646068573,
      "learning_rate": 0.00014357959542656115,
      "loss": 0.0862,
      "step": 1284
    },
    {
      "epoch": 0.8476253298153035,
      "grad_norm": 0.07559879869222641,
      "learning_rate": 0.00014353562005277047,
      "loss": 0.0152,
      "step": 1285
    },
    {
      "epoch": 0.8482849604221636,
      "grad_norm": 0.1548224687576294,
      "learning_rate": 0.00014349164467897978,
      "loss": 0.0239,
      "step": 1286
    },
    {
      "epoch": 0.8489445910290238,
      "grad_norm": 0.08191896975040436,
      "learning_rate": 0.0001434476693051891,
      "loss": 0.0166,
      "step": 1287
    },
    {
      "epoch": 0.8496042216358839,
      "grad_norm": 0.09388023614883423,
      "learning_rate": 0.00014340369393139844,
      "loss": 0.0185,
      "step": 1288
    },
    {
      "epoch": 0.850263852242744,
      "grad_norm": 0.15337370336055756,
      "learning_rate": 0.00014335971855760775,
      "loss": 0.0228,
      "step": 1289
    },
    {
      "epoch": 0.8509234828496042,
      "grad_norm": 0.3032304346561432,
      "learning_rate": 0.00014331574318381706,
      "loss": 0.0326,
      "step": 1290
    },
    {
      "epoch": 0.8515831134564644,
      "grad_norm": 0.1170523390173912,
      "learning_rate": 0.0001432717678100264,
      "loss": 0.0392,
      "step": 1291
    },
    {
      "epoch": 0.8522427440633246,
      "grad_norm": 0.1462438702583313,
      "learning_rate": 0.00014322779243623572,
      "loss": 0.0209,
      "step": 1292
    },
    {
      "epoch": 0.8529023746701847,
      "grad_norm": 0.281231552362442,
      "learning_rate": 0.00014318381706244503,
      "loss": 0.0825,
      "step": 1293
    },
    {
      "epoch": 0.8535620052770448,
      "grad_norm": 0.1950969249010086,
      "learning_rate": 0.00014313984168865435,
      "loss": 0.0495,
      "step": 1294
    },
    {
      "epoch": 0.854221635883905,
      "grad_norm": 0.14262135326862335,
      "learning_rate": 0.0001430958663148637,
      "loss": 0.0299,
      "step": 1295
    },
    {
      "epoch": 0.8548812664907651,
      "grad_norm": 0.08876535296440125,
      "learning_rate": 0.000143051890941073,
      "loss": 0.0122,
      "step": 1296
    },
    {
      "epoch": 0.8555408970976254,
      "grad_norm": 0.11767338216304779,
      "learning_rate": 0.00014300791556728232,
      "loss": 0.0199,
      "step": 1297
    },
    {
      "epoch": 0.8562005277044855,
      "grad_norm": 0.2595147490501404,
      "learning_rate": 0.00014296394019349163,
      "loss": 0.0213,
      "step": 1298
    },
    {
      "epoch": 0.8568601583113457,
      "grad_norm": 0.1563408374786377,
      "learning_rate": 0.00014291996481970097,
      "loss": 0.05,
      "step": 1299
    },
    {
      "epoch": 0.8575197889182058,
      "grad_norm": 0.16566471755504608,
      "learning_rate": 0.00014287598944591029,
      "loss": 0.0155,
      "step": 1300
    },
    {
      "epoch": 0.8581794195250659,
      "grad_norm": 0.0943496897816658,
      "learning_rate": 0.00014283201407211963,
      "loss": 0.0172,
      "step": 1301
    },
    {
      "epoch": 0.8588390501319261,
      "grad_norm": 0.07889246940612793,
      "learning_rate": 0.00014278803869832894,
      "loss": 0.0139,
      "step": 1302
    },
    {
      "epoch": 0.8594986807387863,
      "grad_norm": 0.08166167140007019,
      "learning_rate": 0.00014274406332453828,
      "loss": 0.0138,
      "step": 1303
    },
    {
      "epoch": 0.8601583113456465,
      "grad_norm": 0.09045393764972687,
      "learning_rate": 0.0001427000879507476,
      "loss": 0.0137,
      "step": 1304
    },
    {
      "epoch": 0.8608179419525066,
      "grad_norm": 0.07021123915910721,
      "learning_rate": 0.0001426561125769569,
      "loss": 0.0132,
      "step": 1305
    },
    {
      "epoch": 0.8614775725593667,
      "grad_norm": 0.1563224047422409,
      "learning_rate": 0.00014261213720316625,
      "loss": 0.0346,
      "step": 1306
    },
    {
      "epoch": 0.8621372031662269,
      "grad_norm": 0.17002050578594208,
      "learning_rate": 0.00014256816182937556,
      "loss": 0.0324,
      "step": 1307
    },
    {
      "epoch": 0.862796833773087,
      "grad_norm": 0.06564031541347504,
      "learning_rate": 0.00014252418645558488,
      "loss": 0.0121,
      "step": 1308
    },
    {
      "epoch": 0.8634564643799473,
      "grad_norm": 0.22095881402492523,
      "learning_rate": 0.00014248021108179422,
      "loss": 0.0323,
      "step": 1309
    },
    {
      "epoch": 0.8641160949868074,
      "grad_norm": 0.045915208756923676,
      "learning_rate": 0.00014243623570800353,
      "loss": 0.006,
      "step": 1310
    },
    {
      "epoch": 0.8647757255936676,
      "grad_norm": 0.1573653370141983,
      "learning_rate": 0.00014239226033421285,
      "loss": 0.0257,
      "step": 1311
    },
    {
      "epoch": 0.8654353562005277,
      "grad_norm": 0.15514235198497772,
      "learning_rate": 0.00014234828496042216,
      "loss": 0.0346,
      "step": 1312
    },
    {
      "epoch": 0.8660949868073878,
      "grad_norm": 0.044752173125743866,
      "learning_rate": 0.0001423043095866315,
      "loss": 0.0086,
      "step": 1313
    },
    {
      "epoch": 0.866754617414248,
      "grad_norm": 0.293486088514328,
      "learning_rate": 0.00014226033421284082,
      "loss": 0.0563,
      "step": 1314
    },
    {
      "epoch": 0.8674142480211082,
      "grad_norm": 0.22442936897277832,
      "learning_rate": 0.00014221635883905013,
      "loss": 0.0659,
      "step": 1315
    },
    {
      "epoch": 0.8680738786279684,
      "grad_norm": 0.12776990234851837,
      "learning_rate": 0.00014217238346525944,
      "loss": 0.0259,
      "step": 1316
    },
    {
      "epoch": 0.8687335092348285,
      "grad_norm": 0.1338663399219513,
      "learning_rate": 0.00014212840809146879,
      "loss": 0.0098,
      "step": 1317
    },
    {
      "epoch": 0.8693931398416886,
      "grad_norm": 0.3490079343318939,
      "learning_rate": 0.0001420844327176781,
      "loss": 0.0587,
      "step": 1318
    },
    {
      "epoch": 0.8700527704485488,
      "grad_norm": 0.1996694654226303,
      "learning_rate": 0.0001420404573438874,
      "loss": 0.0516,
      "step": 1319
    },
    {
      "epoch": 0.8707124010554089,
      "grad_norm": 0.15276199579238892,
      "learning_rate": 0.00014199648197009675,
      "loss": 0.0162,
      "step": 1320
    },
    {
      "epoch": 0.8713720316622692,
      "grad_norm": 0.1346679925918579,
      "learning_rate": 0.00014195250659630607,
      "loss": 0.0369,
      "step": 1321
    },
    {
      "epoch": 0.8720316622691293,
      "grad_norm": 0.20684269070625305,
      "learning_rate": 0.00014190853122251538,
      "loss": 0.0279,
      "step": 1322
    },
    {
      "epoch": 0.8726912928759895,
      "grad_norm": 0.20856115221977234,
      "learning_rate": 0.00014186455584872472,
      "loss": 0.0338,
      "step": 1323
    },
    {
      "epoch": 0.8733509234828496,
      "grad_norm": 0.16655179858207703,
      "learning_rate": 0.00014182058047493404,
      "loss": 0.0312,
      "step": 1324
    },
    {
      "epoch": 0.8740105540897097,
      "grad_norm": 0.11420699954032898,
      "learning_rate": 0.00014177660510114338,
      "loss": 0.0136,
      "step": 1325
    },
    {
      "epoch": 0.8746701846965699,
      "grad_norm": 0.124714195728302,
      "learning_rate": 0.0001417326297273527,
      "loss": 0.0328,
      "step": 1326
    },
    {
      "epoch": 0.8753298153034301,
      "grad_norm": 0.3159627914428711,
      "learning_rate": 0.00014168865435356203,
      "loss": 0.1004,
      "step": 1327
    },
    {
      "epoch": 0.8759894459102903,
      "grad_norm": 0.1728256642818451,
      "learning_rate": 0.00014164467897977135,
      "loss": 0.0237,
      "step": 1328
    },
    {
      "epoch": 0.8766490765171504,
      "grad_norm": 0.13123871386051178,
      "learning_rate": 0.00014160070360598066,
      "loss": 0.0281,
      "step": 1329
    },
    {
      "epoch": 0.8773087071240105,
      "grad_norm": 0.21968139708042145,
      "learning_rate": 0.00014155672823218998,
      "loss": 0.0412,
      "step": 1330
    },
    {
      "epoch": 0.8779683377308707,
      "grad_norm": 0.09749391674995422,
      "learning_rate": 0.00014151275285839932,
      "loss": 0.0202,
      "step": 1331
    },
    {
      "epoch": 0.8786279683377308,
      "grad_norm": 0.1240428239107132,
      "learning_rate": 0.00014146877748460863,
      "loss": 0.0297,
      "step": 1332
    },
    {
      "epoch": 0.8792875989445911,
      "grad_norm": 0.1886092573404312,
      "learning_rate": 0.00014142480211081794,
      "loss": 0.0536,
      "step": 1333
    },
    {
      "epoch": 0.8799472295514512,
      "grad_norm": 0.1138390377163887,
      "learning_rate": 0.00014138082673702726,
      "loss": 0.032,
      "step": 1334
    },
    {
      "epoch": 0.8806068601583114,
      "grad_norm": 0.18579861521720886,
      "learning_rate": 0.0001413368513632366,
      "loss": 0.0547,
      "step": 1335
    },
    {
      "epoch": 0.8812664907651715,
      "grad_norm": 0.21592891216278076,
      "learning_rate": 0.0001412928759894459,
      "loss": 0.0727,
      "step": 1336
    },
    {
      "epoch": 0.8819261213720316,
      "grad_norm": 0.08176259696483612,
      "learning_rate": 0.00014124890061565523,
      "loss": 0.0209,
      "step": 1337
    },
    {
      "epoch": 0.8825857519788918,
      "grad_norm": 0.06549037247896194,
      "learning_rate": 0.00014120492524186457,
      "loss": 0.0152,
      "step": 1338
    },
    {
      "epoch": 0.883245382585752,
      "grad_norm": 0.08579954504966736,
      "learning_rate": 0.00014116094986807388,
      "loss": 0.0293,
      "step": 1339
    },
    {
      "epoch": 0.8839050131926122,
      "grad_norm": 0.10942251235246658,
      "learning_rate": 0.0001411169744942832,
      "loss": 0.0204,
      "step": 1340
    },
    {
      "epoch": 0.8845646437994723,
      "grad_norm": 0.09873675554990768,
      "learning_rate": 0.0001410729991204925,
      "loss": 0.0157,
      "step": 1341
    },
    {
      "epoch": 0.8852242744063324,
      "grad_norm": 0.09376165270805359,
      "learning_rate": 0.00014102902374670185,
      "loss": 0.0186,
      "step": 1342
    },
    {
      "epoch": 0.8858839050131926,
      "grad_norm": 0.08669188618659973,
      "learning_rate": 0.00014098504837291117,
      "loss": 0.0168,
      "step": 1343
    },
    {
      "epoch": 0.8865435356200527,
      "grad_norm": 0.08325769752264023,
      "learning_rate": 0.0001409410729991205,
      "loss": 0.0163,
      "step": 1344
    },
    {
      "epoch": 0.887203166226913,
      "grad_norm": 0.10199886560440063,
      "learning_rate": 0.00014089709762532982,
      "loss": 0.0313,
      "step": 1345
    },
    {
      "epoch": 0.8878627968337731,
      "grad_norm": 0.08334241062402725,
      "learning_rate": 0.00014085312225153916,
      "loss": 0.0215,
      "step": 1346
    },
    {
      "epoch": 0.8885224274406333,
      "grad_norm": 0.08929333090782166,
      "learning_rate": 0.00014080914687774848,
      "loss": 0.0146,
      "step": 1347
    },
    {
      "epoch": 0.8891820580474934,
      "grad_norm": 0.33808550238609314,
      "learning_rate": 0.0001407651715039578,
      "loss": 0.0884,
      "step": 1348
    },
    {
      "epoch": 0.8898416886543535,
      "grad_norm": 0.16011489927768707,
      "learning_rate": 0.00014072119613016713,
      "loss": 0.037,
      "step": 1349
    },
    {
      "epoch": 0.8905013192612137,
      "grad_norm": 0.06436391174793243,
      "learning_rate": 0.00014067722075637644,
      "loss": 0.0123,
      "step": 1350
    },
    {
      "epoch": 0.8911609498680739,
      "grad_norm": 0.1120365560054779,
      "learning_rate": 0.00014063324538258576,
      "loss": 0.0367,
      "step": 1351
    },
    {
      "epoch": 0.8918205804749341,
      "grad_norm": 0.08241221308708191,
      "learning_rate": 0.00014058927000879507,
      "loss": 0.021,
      "step": 1352
    },
    {
      "epoch": 0.8924802110817942,
      "grad_norm": 0.16637222468852997,
      "learning_rate": 0.0001405452946350044,
      "loss": 0.0395,
      "step": 1353
    },
    {
      "epoch": 0.8931398416886543,
      "grad_norm": 0.17845404148101807,
      "learning_rate": 0.00014050131926121373,
      "loss": 0.048,
      "step": 1354
    },
    {
      "epoch": 0.8937994722955145,
      "grad_norm": 0.1448507159948349,
      "learning_rate": 0.00014045734388742304,
      "loss": 0.0404,
      "step": 1355
    },
    {
      "epoch": 0.8944591029023746,
      "grad_norm": 0.21283674240112305,
      "learning_rate": 0.00014041336851363238,
      "loss": 0.0573,
      "step": 1356
    },
    {
      "epoch": 0.8951187335092349,
      "grad_norm": 0.1098385602235794,
      "learning_rate": 0.0001403693931398417,
      "loss": 0.0161,
      "step": 1357
    },
    {
      "epoch": 0.895778364116095,
      "grad_norm": 0.13666486740112305,
      "learning_rate": 0.000140325417766051,
      "loss": 0.0408,
      "step": 1358
    },
    {
      "epoch": 0.8964379947229552,
      "grad_norm": 0.22571367025375366,
      "learning_rate": 0.00014028144239226032,
      "loss": 0.0227,
      "step": 1359
    },
    {
      "epoch": 0.8970976253298153,
      "grad_norm": 0.13301482796669006,
      "learning_rate": 0.00014023746701846967,
      "loss": 0.043,
      "step": 1360
    },
    {
      "epoch": 0.8977572559366754,
      "grad_norm": 0.14945071935653687,
      "learning_rate": 0.00014019349164467898,
      "loss": 0.0325,
      "step": 1361
    },
    {
      "epoch": 0.8984168865435356,
      "grad_norm": 0.11216725409030914,
      "learning_rate": 0.0001401495162708883,
      "loss": 0.0279,
      "step": 1362
    },
    {
      "epoch": 0.8990765171503958,
      "grad_norm": 0.19254687428474426,
      "learning_rate": 0.00014010554089709763,
      "loss": 0.0412,
      "step": 1363
    },
    {
      "epoch": 0.899736147757256,
      "grad_norm": 0.17095187306404114,
      "learning_rate": 0.00014006156552330695,
      "loss": 0.071,
      "step": 1364
    },
    {
      "epoch": 0.9003957783641161,
      "grad_norm": 0.14502452313899994,
      "learning_rate": 0.00014001759014951626,
      "loss": 0.0109,
      "step": 1365
    },
    {
      "epoch": 0.9010554089709762,
      "grad_norm": 0.16917142271995544,
      "learning_rate": 0.0001399736147757256,
      "loss": 0.0414,
      "step": 1366
    },
    {
      "epoch": 0.9017150395778364,
      "grad_norm": 0.13072580099105835,
      "learning_rate": 0.00013992963940193494,
      "loss": 0.0383,
      "step": 1367
    },
    {
      "epoch": 0.9023746701846965,
      "grad_norm": 0.16975946724414825,
      "learning_rate": 0.00013988566402814426,
      "loss": 0.044,
      "step": 1368
    },
    {
      "epoch": 0.9030343007915568,
      "grad_norm": 0.1344100534915924,
      "learning_rate": 0.00013984168865435357,
      "loss": 0.0379,
      "step": 1369
    },
    {
      "epoch": 0.9036939313984169,
      "grad_norm": 0.1509508192539215,
      "learning_rate": 0.00013979771328056289,
      "loss": 0.0374,
      "step": 1370
    },
    {
      "epoch": 0.9043535620052771,
      "grad_norm": 0.13903911411762238,
      "learning_rate": 0.00013975373790677223,
      "loss": 0.0376,
      "step": 1371
    },
    {
      "epoch": 0.9050131926121372,
      "grad_norm": 0.11929338425397873,
      "learning_rate": 0.00013970976253298154,
      "loss": 0.0411,
      "step": 1372
    },
    {
      "epoch": 0.9056728232189973,
      "grad_norm": 0.07643572241067886,
      "learning_rate": 0.00013966578715919086,
      "loss": 0.0147,
      "step": 1373
    },
    {
      "epoch": 0.9063324538258575,
      "grad_norm": 0.13500478863716125,
      "learning_rate": 0.0001396218117854002,
      "loss": 0.026,
      "step": 1374
    },
    {
      "epoch": 0.9069920844327177,
      "grad_norm": 0.1231357753276825,
      "learning_rate": 0.0001395778364116095,
      "loss": 0.01,
      "step": 1375
    },
    {
      "epoch": 0.9076517150395779,
      "grad_norm": 0.14031130075454712,
      "learning_rate": 0.00013953386103781882,
      "loss": 0.0415,
      "step": 1376
    },
    {
      "epoch": 0.908311345646438,
      "grad_norm": 0.0862111747264862,
      "learning_rate": 0.00013948988566402814,
      "loss": 0.0163,
      "step": 1377
    },
    {
      "epoch": 0.9089709762532981,
      "grad_norm": 0.10504557937383652,
      "learning_rate": 0.00013944591029023748,
      "loss": 0.0208,
      "step": 1378
    },
    {
      "epoch": 0.9096306068601583,
      "grad_norm": 0.07509136945009232,
      "learning_rate": 0.0001394019349164468,
      "loss": 0.0152,
      "step": 1379
    },
    {
      "epoch": 0.9102902374670184,
      "grad_norm": 0.06189476698637009,
      "learning_rate": 0.0001393579595426561,
      "loss": 0.0153,
      "step": 1380
    },
    {
      "epoch": 0.9109498680738787,
      "grad_norm": 0.11700501292943954,
      "learning_rate": 0.00013931398416886545,
      "loss": 0.0357,
      "step": 1381
    },
    {
      "epoch": 0.9116094986807388,
      "grad_norm": 0.12965664267539978,
      "learning_rate": 0.00013927000879507476,
      "loss": 0.03,
      "step": 1382
    },
    {
      "epoch": 0.912269129287599,
      "grad_norm": 0.0972810760140419,
      "learning_rate": 0.00013922603342128408,
      "loss": 0.0126,
      "step": 1383
    },
    {
      "epoch": 0.9129287598944591,
      "grad_norm": 0.1544565111398697,
      "learning_rate": 0.0001391820580474934,
      "loss": 0.032,
      "step": 1384
    },
    {
      "epoch": 0.9135883905013192,
      "grad_norm": 0.18777737021446228,
      "learning_rate": 0.00013913808267370273,
      "loss": 0.0396,
      "step": 1385
    },
    {
      "epoch": 0.9142480211081794,
      "grad_norm": 0.16748462617397308,
      "learning_rate": 0.00013909410729991205,
      "loss": 0.0298,
      "step": 1386
    },
    {
      "epoch": 0.9149076517150396,
      "grad_norm": 0.09444555640220642,
      "learning_rate": 0.00013905013192612139,
      "loss": 0.0189,
      "step": 1387
    },
    {
      "epoch": 0.9155672823218998,
      "grad_norm": 0.174606055021286,
      "learning_rate": 0.0001390061565523307,
      "loss": 0.0451,
      "step": 1388
    },
    {
      "epoch": 0.9162269129287599,
      "grad_norm": 0.2531820833683014,
      "learning_rate": 0.00013896218117854004,
      "loss": 0.0702,
      "step": 1389
    },
    {
      "epoch": 0.91688654353562,
      "grad_norm": 0.11356265097856522,
      "learning_rate": 0.00013891820580474936,
      "loss": 0.0126,
      "step": 1390
    },
    {
      "epoch": 0.9175461741424802,
      "grad_norm": 0.10554396361112595,
      "learning_rate": 0.00013887423043095867,
      "loss": 0.03,
      "step": 1391
    },
    {
      "epoch": 0.9182058047493403,
      "grad_norm": 0.22750453650951385,
      "learning_rate": 0.000138830255057168,
      "loss": 0.0458,
      "step": 1392
    },
    {
      "epoch": 0.9188654353562006,
      "grad_norm": 0.1572130024433136,
      "learning_rate": 0.00013878627968337732,
      "loss": 0.0309,
      "step": 1393
    },
    {
      "epoch": 0.9195250659630607,
      "grad_norm": 0.17781557142734528,
      "learning_rate": 0.00013874230430958664,
      "loss": 0.0236,
      "step": 1394
    },
    {
      "epoch": 0.9201846965699209,
      "grad_norm": 0.10994111746549606,
      "learning_rate": 0.00013869832893579595,
      "loss": 0.016,
      "step": 1395
    },
    {
      "epoch": 0.920844327176781,
      "grad_norm": 0.1530650556087494,
      "learning_rate": 0.0001386543535620053,
      "loss": 0.0467,
      "step": 1396
    },
    {
      "epoch": 0.9215039577836411,
      "grad_norm": 0.1340877115726471,
      "learning_rate": 0.0001386103781882146,
      "loss": 0.0429,
      "step": 1397
    },
    {
      "epoch": 0.9221635883905013,
      "grad_norm": 0.14443635940551758,
      "learning_rate": 0.00013856640281442392,
      "loss": 0.0175,
      "step": 1398
    },
    {
      "epoch": 0.9228232189973615,
      "grad_norm": 0.1688997745513916,
      "learning_rate": 0.00013852242744063326,
      "loss": 0.0612,
      "step": 1399
    },
    {
      "epoch": 0.9234828496042217,
      "grad_norm": 0.20491701364517212,
      "learning_rate": 0.00013847845206684258,
      "loss": 0.0559,
      "step": 1400
    },
    {
      "epoch": 0.9241424802110818,
      "grad_norm": 0.13896392285823822,
      "learning_rate": 0.0001384344766930519,
      "loss": 0.0336,
      "step": 1401
    },
    {
      "epoch": 0.924802110817942,
      "grad_norm": 0.19579200446605682,
      "learning_rate": 0.0001383905013192612,
      "loss": 0.0607,
      "step": 1402
    },
    {
      "epoch": 0.9254617414248021,
      "grad_norm": 0.11532715708017349,
      "learning_rate": 0.00013834652594547055,
      "loss": 0.0309,
      "step": 1403
    },
    {
      "epoch": 0.9261213720316622,
      "grad_norm": 0.0862959697842598,
      "learning_rate": 0.00013830255057167986,
      "loss": 0.0153,
      "step": 1404
    },
    {
      "epoch": 0.9267810026385225,
      "grad_norm": 0.10423921793699265,
      "learning_rate": 0.00013825857519788917,
      "loss": 0.0271,
      "step": 1405
    },
    {
      "epoch": 0.9274406332453826,
      "grad_norm": 0.08387498557567596,
      "learning_rate": 0.00013821459982409851,
      "loss": 0.0062,
      "step": 1406
    },
    {
      "epoch": 0.9281002638522428,
      "grad_norm": 0.11579256504774094,
      "learning_rate": 0.00013817062445030783,
      "loss": 0.009,
      "step": 1407
    },
    {
      "epoch": 0.9287598944591029,
      "grad_norm": 0.09461778402328491,
      "learning_rate": 0.00013812664907651717,
      "loss": 0.0202,
      "step": 1408
    },
    {
      "epoch": 0.929419525065963,
      "grad_norm": 0.2067088782787323,
      "learning_rate": 0.00013808267370272648,
      "loss": 0.0588,
      "step": 1409
    },
    {
      "epoch": 0.9300791556728232,
      "grad_norm": 0.1987171471118927,
      "learning_rate": 0.00013803869832893582,
      "loss": 0.0505,
      "step": 1410
    },
    {
      "epoch": 0.9307387862796834,
      "grad_norm": 0.19550925493240356,
      "learning_rate": 0.00013799472295514514,
      "loss": 0.0649,
      "step": 1411
    },
    {
      "epoch": 0.9313984168865436,
      "grad_norm": 0.12751424312591553,
      "learning_rate": 0.00013795074758135445,
      "loss": 0.0276,
      "step": 1412
    },
    {
      "epoch": 0.9320580474934037,
      "grad_norm": 0.11977527290582657,
      "learning_rate": 0.00013790677220756377,
      "loss": 0.0401,
      "step": 1413
    },
    {
      "epoch": 0.9327176781002638,
      "grad_norm": 0.1224345862865448,
      "learning_rate": 0.0001378627968337731,
      "loss": 0.0309,
      "step": 1414
    },
    {
      "epoch": 0.933377308707124,
      "grad_norm": 0.08867436647415161,
      "learning_rate": 0.00013781882145998242,
      "loss": 0.017,
      "step": 1415
    },
    {
      "epoch": 0.9340369393139841,
      "grad_norm": 0.08907410502433777,
      "learning_rate": 0.00013777484608619174,
      "loss": 0.0186,
      "step": 1416
    },
    {
      "epoch": 0.9346965699208444,
      "grad_norm": 0.12770384550094604,
      "learning_rate": 0.00013773087071240108,
      "loss": 0.0156,
      "step": 1417
    },
    {
      "epoch": 0.9353562005277045,
      "grad_norm": 0.10536859929561615,
      "learning_rate": 0.0001376868953386104,
      "loss": 0.0194,
      "step": 1418
    },
    {
      "epoch": 0.9360158311345647,
      "grad_norm": 0.10853305459022522,
      "learning_rate": 0.0001376429199648197,
      "loss": 0.0329,
      "step": 1419
    },
    {
      "epoch": 0.9366754617414248,
      "grad_norm": 0.1748216599225998,
      "learning_rate": 0.00013759894459102902,
      "loss": 0.048,
      "step": 1420
    },
    {
      "epoch": 0.9373350923482849,
      "grad_norm": 0.15145626664161682,
      "learning_rate": 0.00013755496921723836,
      "loss": 0.0359,
      "step": 1421
    },
    {
      "epoch": 0.9379947229551451,
      "grad_norm": 0.1458299458026886,
      "learning_rate": 0.00013751099384344767,
      "loss": 0.0264,
      "step": 1422
    },
    {
      "epoch": 0.9386543535620053,
      "grad_norm": 0.1351323127746582,
      "learning_rate": 0.000137467018469657,
      "loss": 0.0342,
      "step": 1423
    },
    {
      "epoch": 0.9393139841688655,
      "grad_norm": 0.1268400102853775,
      "learning_rate": 0.0001374230430958663,
      "loss": 0.0273,
      "step": 1424
    },
    {
      "epoch": 0.9399736147757256,
      "grad_norm": 0.17962677776813507,
      "learning_rate": 0.00013737906772207564,
      "loss": 0.0589,
      "step": 1425
    },
    {
      "epoch": 0.9406332453825857,
      "grad_norm": 0.1407070904970169,
      "learning_rate": 0.00013733509234828496,
      "loss": 0.0231,
      "step": 1426
    },
    {
      "epoch": 0.9412928759894459,
      "grad_norm": 0.07474631816148758,
      "learning_rate": 0.00013729111697449427,
      "loss": 0.0132,
      "step": 1427
    },
    {
      "epoch": 0.941952506596306,
      "grad_norm": 0.1300322562456131,
      "learning_rate": 0.0001372471416007036,
      "loss": 0.0231,
      "step": 1428
    },
    {
      "epoch": 0.9426121372031663,
      "grad_norm": 0.1802651584148407,
      "learning_rate": 0.00013720316622691292,
      "loss": 0.0251,
      "step": 1429
    },
    {
      "epoch": 0.9432717678100264,
      "grad_norm": 0.2056036740541458,
      "learning_rate": 0.00013715919085312227,
      "loss": 0.0305,
      "step": 1430
    },
    {
      "epoch": 0.9439313984168866,
      "grad_norm": 0.14322225749492645,
      "learning_rate": 0.00013711521547933158,
      "loss": 0.0095,
      "step": 1431
    },
    {
      "epoch": 0.9445910290237467,
      "grad_norm": 0.08642763644456863,
      "learning_rate": 0.00013707124010554092,
      "loss": 0.0104,
      "step": 1432
    },
    {
      "epoch": 0.9452506596306068,
      "grad_norm": 0.07629302144050598,
      "learning_rate": 0.00013702726473175024,
      "loss": 0.0096,
      "step": 1433
    },
    {
      "epoch": 0.945910290237467,
      "grad_norm": 0.2438475787639618,
      "learning_rate": 0.00013698328935795955,
      "loss": 0.0407,
      "step": 1434
    },
    {
      "epoch": 0.9465699208443272,
      "grad_norm": 0.14563535153865814,
      "learning_rate": 0.0001369393139841689,
      "loss": 0.0273,
      "step": 1435
    },
    {
      "epoch": 0.9472295514511874,
      "grad_norm": 0.16298136115074158,
      "learning_rate": 0.0001368953386103782,
      "loss": 0.0328,
      "step": 1436
    },
    {
      "epoch": 0.9478891820580475,
      "grad_norm": 0.19234518706798553,
      "learning_rate": 0.00013685136323658752,
      "loss": 0.0304,
      "step": 1437
    },
    {
      "epoch": 0.9485488126649076,
      "grad_norm": 0.24605469405651093,
      "learning_rate": 0.00013680738786279683,
      "loss": 0.0471,
      "step": 1438
    },
    {
      "epoch": 0.9492084432717678,
      "grad_norm": 0.11248517036437988,
      "learning_rate": 0.00013676341248900617,
      "loss": 0.0222,
      "step": 1439
    },
    {
      "epoch": 0.9498680738786279,
      "grad_norm": 0.09104634821414948,
      "learning_rate": 0.0001367194371152155,
      "loss": 0.0122,
      "step": 1440
    },
    {
      "epoch": 0.9505277044854882,
      "grad_norm": 0.24818916618824005,
      "learning_rate": 0.0001366754617414248,
      "loss": 0.0479,
      "step": 1441
    },
    {
      "epoch": 0.9511873350923483,
      "grad_norm": 0.13978122174739838,
      "learning_rate": 0.00013663148636763411,
      "loss": 0.0134,
      "step": 1442
    },
    {
      "epoch": 0.9518469656992085,
      "grad_norm": 0.1366599053144455,
      "learning_rate": 0.00013658751099384346,
      "loss": 0.0182,
      "step": 1443
    },
    {
      "epoch": 0.9525065963060686,
      "grad_norm": 0.09141973406076431,
      "learning_rate": 0.00013654353562005277,
      "loss": 0.0091,
      "step": 1444
    },
    {
      "epoch": 0.9531662269129287,
      "grad_norm": 0.15576261281967163,
      "learning_rate": 0.00013649956024626208,
      "loss": 0.0342,
      "step": 1445
    },
    {
      "epoch": 0.9538258575197889,
      "grad_norm": 0.18730528652668,
      "learning_rate": 0.00013645558487247142,
      "loss": 0.0208,
      "step": 1446
    },
    {
      "epoch": 0.9544854881266491,
      "grad_norm": 0.13546380400657654,
      "learning_rate": 0.00013641160949868074,
      "loss": 0.033,
      "step": 1447
    },
    {
      "epoch": 0.9551451187335093,
      "grad_norm": 0.33218470215797424,
      "learning_rate": 0.00013636763412489005,
      "loss": 0.0652,
      "step": 1448
    },
    {
      "epoch": 0.9558047493403694,
      "grad_norm": 0.1990104466676712,
      "learning_rate": 0.0001363236587510994,
      "loss": 0.0281,
      "step": 1449
    },
    {
      "epoch": 0.9564643799472295,
      "grad_norm": 0.18450461328029633,
      "learning_rate": 0.0001362796833773087,
      "loss": 0.043,
      "step": 1450
    },
    {
      "epoch": 0.9571240105540897,
      "grad_norm": 0.1633075475692749,
      "learning_rate": 0.00013623570800351805,
      "loss": 0.0242,
      "step": 1451
    },
    {
      "epoch": 0.9577836411609498,
      "grad_norm": 0.09079620242118835,
      "learning_rate": 0.00013619173262972736,
      "loss": 0.0053,
      "step": 1452
    },
    {
      "epoch": 0.9584432717678101,
      "grad_norm": 0.21772818267345428,
      "learning_rate": 0.0001361477572559367,
      "loss": 0.0671,
      "step": 1453
    },
    {
      "epoch": 0.9591029023746702,
      "grad_norm": 0.11405884474515915,
      "learning_rate": 0.00013610378188214602,
      "loss": 0.0286,
      "step": 1454
    },
    {
      "epoch": 0.9597625329815304,
      "grad_norm": 0.21953897178173065,
      "learning_rate": 0.00013605980650835533,
      "loss": 0.0171,
      "step": 1455
    },
    {
      "epoch": 0.9604221635883905,
      "grad_norm": 0.12651553750038147,
      "learning_rate": 0.00013601583113456465,
      "loss": 0.0366,
      "step": 1456
    },
    {
      "epoch": 0.9610817941952506,
      "grad_norm": 0.14760233461856842,
      "learning_rate": 0.000135971855760774,
      "loss": 0.0328,
      "step": 1457
    },
    {
      "epoch": 0.9617414248021108,
      "grad_norm": 0.0651514008641243,
      "learning_rate": 0.0001359278803869833,
      "loss": 0.0121,
      "step": 1458
    },
    {
      "epoch": 0.962401055408971,
      "grad_norm": 0.07470578700304031,
      "learning_rate": 0.00013588390501319261,
      "loss": 0.0047,
      "step": 1459
    },
    {
      "epoch": 0.9630606860158312,
      "grad_norm": 0.22181552648544312,
      "learning_rate": 0.00013583992963940193,
      "loss": 0.042,
      "step": 1460
    },
    {
      "epoch": 0.9637203166226913,
      "grad_norm": 0.19369708001613617,
      "learning_rate": 0.00013579595426561127,
      "loss": 0.0483,
      "step": 1461
    },
    {
      "epoch": 0.9643799472295514,
      "grad_norm": 0.192474365234375,
      "learning_rate": 0.00013575197889182058,
      "loss": 0.0544,
      "step": 1462
    },
    {
      "epoch": 0.9650395778364116,
      "grad_norm": 0.13801313936710358,
      "learning_rate": 0.0001357080035180299,
      "loss": 0.0296,
      "step": 1463
    },
    {
      "epoch": 0.9656992084432717,
      "grad_norm": 0.14092716574668884,
      "learning_rate": 0.00013566402814423924,
      "loss": 0.0363,
      "step": 1464
    },
    {
      "epoch": 0.966358839050132,
      "grad_norm": 0.1475318968296051,
      "learning_rate": 0.00013562005277044855,
      "loss": 0.0474,
      "step": 1465
    },
    {
      "epoch": 0.9670184696569921,
      "grad_norm": 0.1308039426803589,
      "learning_rate": 0.00013557607739665787,
      "loss": 0.0338,
      "step": 1466
    },
    {
      "epoch": 0.9676781002638523,
      "grad_norm": 0.11696742475032806,
      "learning_rate": 0.00013553210202286718,
      "loss": 0.0321,
      "step": 1467
    },
    {
      "epoch": 0.9683377308707124,
      "grad_norm": 0.11275412887334824,
      "learning_rate": 0.00013548812664907652,
      "loss": 0.0072,
      "step": 1468
    },
    {
      "epoch": 0.9689973614775725,
      "grad_norm": 0.11244922876358032,
      "learning_rate": 0.00013544415127528584,
      "loss": 0.0286,
      "step": 1469
    },
    {
      "epoch": 0.9696569920844327,
      "grad_norm": 0.10941194742918015,
      "learning_rate": 0.00013540017590149515,
      "loss": 0.0306,
      "step": 1470
    },
    {
      "epoch": 0.9703166226912929,
      "grad_norm": 0.13597334921360016,
      "learning_rate": 0.0001353562005277045,
      "loss": 0.0301,
      "step": 1471
    },
    {
      "epoch": 0.9709762532981531,
      "grad_norm": 0.1315174251794815,
      "learning_rate": 0.00013531222515391383,
      "loss": 0.0089,
      "step": 1472
    },
    {
      "epoch": 0.9716358839050132,
      "grad_norm": 0.2824278175830841,
      "learning_rate": 0.00013526824978012315,
      "loss": 0.0836,
      "step": 1473
    },
    {
      "epoch": 0.9722955145118733,
      "grad_norm": 0.13090477883815765,
      "learning_rate": 0.00013522427440633246,
      "loss": 0.0176,
      "step": 1474
    },
    {
      "epoch": 0.9729551451187335,
      "grad_norm": 0.14466793835163116,
      "learning_rate": 0.0001351802990325418,
      "loss": 0.0309,
      "step": 1475
    },
    {
      "epoch": 0.9736147757255936,
      "grad_norm": 0.09564338624477386,
      "learning_rate": 0.00013513632365875111,
      "loss": 0.0153,
      "step": 1476
    },
    {
      "epoch": 0.9742744063324539,
      "grad_norm": 0.0979272723197937,
      "learning_rate": 0.00013509234828496043,
      "loss": 0.0061,
      "step": 1477
    },
    {
      "epoch": 0.974934036939314,
      "grad_norm": 0.16389422118663788,
      "learning_rate": 0.00013504837291116974,
      "loss": 0.042,
      "step": 1478
    },
    {
      "epoch": 0.9755936675461742,
      "grad_norm": 0.2719620168209076,
      "learning_rate": 0.00013500439753737908,
      "loss": 0.0557,
      "step": 1479
    },
    {
      "epoch": 0.9762532981530343,
      "grad_norm": 0.10780271142721176,
      "learning_rate": 0.0001349604221635884,
      "loss": 0.0164,
      "step": 1480
    },
    {
      "epoch": 0.9769129287598944,
      "grad_norm": 0.12940935790538788,
      "learning_rate": 0.0001349164467897977,
      "loss": 0.0325,
      "step": 1481
    },
    {
      "epoch": 0.9775725593667546,
      "grad_norm": 0.11078586429357529,
      "learning_rate": 0.00013487247141600705,
      "loss": 0.0293,
      "step": 1482
    },
    {
      "epoch": 0.9782321899736148,
      "grad_norm": 0.22676342725753784,
      "learning_rate": 0.00013482849604221637,
      "loss": 0.0462,
      "step": 1483
    },
    {
      "epoch": 0.978891820580475,
      "grad_norm": 0.14810596406459808,
      "learning_rate": 0.00013478452066842568,
      "loss": 0.0242,
      "step": 1484
    },
    {
      "epoch": 0.9795514511873351,
      "grad_norm": 0.1205986961722374,
      "learning_rate": 0.000134740545294635,
      "loss": 0.0118,
      "step": 1485
    },
    {
      "epoch": 0.9802110817941952,
      "grad_norm": 0.13070084154605865,
      "learning_rate": 0.00013469656992084434,
      "loss": 0.0355,
      "step": 1486
    },
    {
      "epoch": 0.9808707124010554,
      "grad_norm": 0.15466423332691193,
      "learning_rate": 0.00013465259454705365,
      "loss": 0.025,
      "step": 1487
    },
    {
      "epoch": 0.9815303430079155,
      "grad_norm": 0.11172927170991898,
      "learning_rate": 0.00013460861917326296,
      "loss": 0.0157,
      "step": 1488
    },
    {
      "epoch": 0.9821899736147758,
      "grad_norm": 0.20864300429821014,
      "learning_rate": 0.0001345646437994723,
      "loss": 0.0258,
      "step": 1489
    },
    {
      "epoch": 0.9828496042216359,
      "grad_norm": 0.25203242897987366,
      "learning_rate": 0.00013452066842568162,
      "loss": 0.0405,
      "step": 1490
    },
    {
      "epoch": 0.9835092348284961,
      "grad_norm": 0.0959254652261734,
      "learning_rate": 0.00013447669305189093,
      "loss": 0.0122,
      "step": 1491
    },
    {
      "epoch": 0.9841688654353562,
      "grad_norm": 0.099158376455307,
      "learning_rate": 0.00013443271767810027,
      "loss": 0.0216,
      "step": 1492
    },
    {
      "epoch": 0.9848284960422163,
      "grad_norm": 0.19597625732421875,
      "learning_rate": 0.0001343887423043096,
      "loss": 0.0135,
      "step": 1493
    },
    {
      "epoch": 0.9854881266490765,
      "grad_norm": 0.1351017951965332,
      "learning_rate": 0.00013434476693051893,
      "loss": 0.0237,
      "step": 1494
    },
    {
      "epoch": 0.9861477572559367,
      "grad_norm": 0.08852732181549072,
      "learning_rate": 0.00013430079155672824,
      "loss": 0.0107,
      "step": 1495
    },
    {
      "epoch": 0.9868073878627969,
      "grad_norm": 0.308567613363266,
      "learning_rate": 0.00013425681618293756,
      "loss": 0.0625,
      "step": 1496
    },
    {
      "epoch": 0.987467018469657,
      "grad_norm": 0.14887158572673798,
      "learning_rate": 0.0001342128408091469,
      "loss": 0.0313,
      "step": 1497
    },
    {
      "epoch": 0.9881266490765171,
      "grad_norm": 0.21746350824832916,
      "learning_rate": 0.0001341688654353562,
      "loss": 0.038,
      "step": 1498
    },
    {
      "epoch": 0.9887862796833773,
      "grad_norm": 0.13009917736053467,
      "learning_rate": 0.00013412489006156553,
      "loss": 0.0201,
      "step": 1499
    },
    {
      "epoch": 0.9894459102902374,
      "grad_norm": 0.2146957963705063,
      "learning_rate": 0.00013408091468777487,
      "loss": 0.0441,
      "step": 1500
    },
    {
      "epoch": 0.9901055408970977,
      "grad_norm": 0.17439879477024078,
      "learning_rate": 0.00013403693931398418,
      "loss": 0.0332,
      "step": 1501
    },
    {
      "epoch": 0.9907651715039578,
      "grad_norm": 0.1243106797337532,
      "learning_rate": 0.0001339929639401935,
      "loss": 0.0251,
      "step": 1502
    },
    {
      "epoch": 0.991424802110818,
      "grad_norm": 0.41834357380867004,
      "learning_rate": 0.0001339489885664028,
      "loss": 0.1038,
      "step": 1503
    },
    {
      "epoch": 0.9920844327176781,
      "grad_norm": 0.15863096714019775,
      "learning_rate": 0.00013390501319261215,
      "loss": 0.0303,
      "step": 1504
    },
    {
      "epoch": 0.9927440633245382,
      "grad_norm": 0.20410676300525665,
      "learning_rate": 0.00013386103781882146,
      "loss": 0.0146,
      "step": 1505
    },
    {
      "epoch": 0.9934036939313984,
      "grad_norm": 0.16025516390800476,
      "learning_rate": 0.00013381706244503078,
      "loss": 0.0148,
      "step": 1506
    },
    {
      "epoch": 0.9940633245382586,
      "grad_norm": 0.117454893887043,
      "learning_rate": 0.00013377308707124012,
      "loss": 0.0259,
      "step": 1507
    },
    {
      "epoch": 0.9947229551451188,
      "grad_norm": 0.188646137714386,
      "learning_rate": 0.00013372911169744943,
      "loss": 0.0368,
      "step": 1508
    },
    {
      "epoch": 0.9953825857519789,
      "grad_norm": 0.14104565978050232,
      "learning_rate": 0.00013368513632365875,
      "loss": 0.0257,
      "step": 1509
    },
    {
      "epoch": 0.996042216358839,
      "grad_norm": 0.20529550313949585,
      "learning_rate": 0.00013364116094986806,
      "loss": 0.0487,
      "step": 1510
    },
    {
      "epoch": 0.9967018469656992,
      "grad_norm": 0.1505880355834961,
      "learning_rate": 0.0001335971855760774,
      "loss": 0.0116,
      "step": 1511
    },
    {
      "epoch": 0.9973614775725593,
      "grad_norm": 0.14477118849754333,
      "learning_rate": 0.00013355321020228672,
      "loss": 0.0433,
      "step": 1512
    },
    {
      "epoch": 0.9980211081794196,
      "grad_norm": 0.12461160123348236,
      "learning_rate": 0.00013350923482849606,
      "loss": 0.039,
      "step": 1513
    },
    {
      "epoch": 0.9986807387862797,
      "grad_norm": 0.09497349709272385,
      "learning_rate": 0.00013346525945470537,
      "loss": 0.0243,
      "step": 1514
    },
    {
      "epoch": 0.9993403693931399,
      "grad_norm": 0.12263771146535873,
      "learning_rate": 0.0001334212840809147,
      "loss": 0.0416,
      "step": 1515
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.1482406109571457,
      "learning_rate": 0.00013337730870712403,
      "loss": 0.0312,
      "step": 1516
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6954022988505747,
      "eval_f1": 0.4732392757182841,
      "eval_keras_BCE": 0.8198239803314209,
      "eval_loss": 0.8196753263473511,
      "eval_precision": 0.7567053869557355,
      "eval_runtime": 5.2197,
      "eval_samples_per_second": 33.336,
      "eval_steps_per_second": 4.215,
      "eval_weigthed BCE": 0.5484339594841003,
      "step": 1516
    }
  ],
  "logging_steps": 1,
  "max_steps": 4548,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.043510858947825e+17,
  "train_batch_size": 256,
  "trial_name": null,
  "trial_params": null
}
