{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3032,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006596306068601583,
      "grad_norm": 35.9672966003418,
      "learning_rate": 0.0002,
      "loss": 3.1918,
      "step": 1
    },
    {
      "epoch": 0.0013192612137203166,
      "grad_norm": 18.360219955444336,
      "learning_rate": 0.00019995602462620932,
      "loss": 1.0186,
      "step": 2
    },
    {
      "epoch": 0.001978891820580475,
      "grad_norm": 3.370255947113037,
      "learning_rate": 0.00019991204925241866,
      "loss": 0.1426,
      "step": 3
    },
    {
      "epoch": 0.002638522427440633,
      "grad_norm": 0.5084718465805054,
      "learning_rate": 0.00019986807387862798,
      "loss": 0.0656,
      "step": 4
    },
    {
      "epoch": 0.0032981530343007917,
      "grad_norm": 0.1736038774251938,
      "learning_rate": 0.0001998240985048373,
      "loss": 0.0325,
      "step": 5
    },
    {
      "epoch": 0.00395778364116095,
      "grad_norm": 0.4496656060218811,
      "learning_rate": 0.0001997801231310466,
      "loss": 0.0825,
      "step": 6
    },
    {
      "epoch": 0.004617414248021108,
      "grad_norm": 0.7282611131668091,
      "learning_rate": 0.00019973614775725595,
      "loss": 0.1465,
      "step": 7
    },
    {
      "epoch": 0.005277044854881266,
      "grad_norm": 0.16678373515605927,
      "learning_rate": 0.00019969217238346526,
      "loss": 0.0369,
      "step": 8
    },
    {
      "epoch": 0.005936675461741424,
      "grad_norm": 0.3966132700443268,
      "learning_rate": 0.00019964819700967458,
      "loss": 0.0981,
      "step": 9
    },
    {
      "epoch": 0.006596306068601583,
      "grad_norm": 0.47954031825065613,
      "learning_rate": 0.00019960422163588392,
      "loss": 0.1238,
      "step": 10
    },
    {
      "epoch": 0.007255936675461741,
      "grad_norm": 0.08518762141466141,
      "learning_rate": 0.00019956024626209323,
      "loss": 0.0272,
      "step": 11
    },
    {
      "epoch": 0.0079155672823219,
      "grad_norm": 0.2607460021972656,
      "learning_rate": 0.00019951627088830257,
      "loss": 0.0702,
      "step": 12
    },
    {
      "epoch": 0.008575197889182058,
      "grad_norm": 1.52717924118042,
      "learning_rate": 0.00019947229551451189,
      "loss": 0.1037,
      "step": 13
    },
    {
      "epoch": 0.009234828496042216,
      "grad_norm": 0.12274033576250076,
      "learning_rate": 0.00019942832014072123,
      "loss": 0.0635,
      "step": 14
    },
    {
      "epoch": 0.009894459102902375,
      "grad_norm": 0.28364449739456177,
      "learning_rate": 0.00019938434476693054,
      "loss": 0.0142,
      "step": 15
    },
    {
      "epoch": 0.010554089709762533,
      "grad_norm": 0.19363075494766235,
      "learning_rate": 0.00019934036939313985,
      "loss": 0.0485,
      "step": 16
    },
    {
      "epoch": 0.011213720316622692,
      "grad_norm": 0.2935446500778198,
      "learning_rate": 0.00019929639401934917,
      "loss": 0.0489,
      "step": 17
    },
    {
      "epoch": 0.011873350923482849,
      "grad_norm": 0.09788938611745834,
      "learning_rate": 0.0001992524186455585,
      "loss": 0.0953,
      "step": 18
    },
    {
      "epoch": 0.012532981530343008,
      "grad_norm": 0.25649335980415344,
      "learning_rate": 0.00019920844327176782,
      "loss": 0.0534,
      "step": 19
    },
    {
      "epoch": 0.013192612137203167,
      "grad_norm": 0.25099486112594604,
      "learning_rate": 0.00019916446789797714,
      "loss": 0.034,
      "step": 20
    },
    {
      "epoch": 0.013852242744063324,
      "grad_norm": 0.24971282482147217,
      "learning_rate": 0.00019912049252418648,
      "loss": 0.0131,
      "step": 21
    },
    {
      "epoch": 0.014511873350923483,
      "grad_norm": 0.1269960254430771,
      "learning_rate": 0.0001990765171503958,
      "loss": 0.0774,
      "step": 22
    },
    {
      "epoch": 0.015171503957783642,
      "grad_norm": 0.26265963912010193,
      "learning_rate": 0.0001990325417766051,
      "loss": 0.1056,
      "step": 23
    },
    {
      "epoch": 0.0158311345646438,
      "grad_norm": 0.19881552457809448,
      "learning_rate": 0.00019898856640281442,
      "loss": 0.0851,
      "step": 24
    },
    {
      "epoch": 0.016490765171503958,
      "grad_norm": 0.2981855571269989,
      "learning_rate": 0.00019894459102902376,
      "loss": 0.1113,
      "step": 25
    },
    {
      "epoch": 0.017150395778364115,
      "grad_norm": 0.15118996798992157,
      "learning_rate": 0.00019890061565523308,
      "loss": 0.0078,
      "step": 26
    },
    {
      "epoch": 0.017810026385224276,
      "grad_norm": 0.09990768879652023,
      "learning_rate": 0.0001988566402814424,
      "loss": 0.0644,
      "step": 27
    },
    {
      "epoch": 0.018469656992084433,
      "grad_norm": 0.15570694208145142,
      "learning_rate": 0.00019881266490765173,
      "loss": 0.0825,
      "step": 28
    },
    {
      "epoch": 0.01912928759894459,
      "grad_norm": 0.139003187417984,
      "learning_rate": 0.00019876868953386104,
      "loss": 0.0811,
      "step": 29
    },
    {
      "epoch": 0.01978891820580475,
      "grad_norm": 0.09388700127601624,
      "learning_rate": 0.00019872471416007036,
      "loss": 0.0606,
      "step": 30
    },
    {
      "epoch": 0.020448548812664908,
      "grad_norm": 0.08489011973142624,
      "learning_rate": 0.00019868073878627967,
      "loss": 0.0649,
      "step": 31
    },
    {
      "epoch": 0.021108179419525065,
      "grad_norm": 0.14103291928768158,
      "learning_rate": 0.000198636763412489,
      "loss": 0.0496,
      "step": 32
    },
    {
      "epoch": 0.021767810026385226,
      "grad_norm": 0.17318949103355408,
      "learning_rate": 0.00019859278803869833,
      "loss": 0.0492,
      "step": 33
    },
    {
      "epoch": 0.022427440633245383,
      "grad_norm": 0.07044827938079834,
      "learning_rate": 0.00019854881266490767,
      "loss": 0.0762,
      "step": 34
    },
    {
      "epoch": 0.02308707124010554,
      "grad_norm": 0.1580686867237091,
      "learning_rate": 0.00019850483729111698,
      "loss": 0.0499,
      "step": 35
    },
    {
      "epoch": 0.023746701846965697,
      "grad_norm": 0.11848395317792892,
      "learning_rate": 0.00019846086191732632,
      "loss": 0.0473,
      "step": 36
    },
    {
      "epoch": 0.024406332453825858,
      "grad_norm": 0.06072632223367691,
      "learning_rate": 0.00019841688654353564,
      "loss": 0.063,
      "step": 37
    },
    {
      "epoch": 0.025065963060686015,
      "grad_norm": 0.06243116408586502,
      "learning_rate": 0.00019837291116974495,
      "loss": 0.0448,
      "step": 38
    },
    {
      "epoch": 0.025725593667546173,
      "grad_norm": 0.04957958310842514,
      "learning_rate": 0.0001983289357959543,
      "loss": 0.0447,
      "step": 39
    },
    {
      "epoch": 0.026385224274406333,
      "grad_norm": 0.2380913943052292,
      "learning_rate": 0.0001982849604221636,
      "loss": 0.1005,
      "step": 40
    },
    {
      "epoch": 0.02704485488126649,
      "grad_norm": 0.07386821508407593,
      "learning_rate": 0.00019824098504837292,
      "loss": 0.0255,
      "step": 41
    },
    {
      "epoch": 0.027704485488126648,
      "grad_norm": 0.07311496138572693,
      "learning_rate": 0.00019819700967458223,
      "loss": 0.0259,
      "step": 42
    },
    {
      "epoch": 0.02836411609498681,
      "grad_norm": 0.07991109788417816,
      "learning_rate": 0.00019815303430079158,
      "loss": 0.0469,
      "step": 43
    },
    {
      "epoch": 0.029023746701846966,
      "grad_norm": 0.060403767973184586,
      "learning_rate": 0.0001981090589270009,
      "loss": 0.0237,
      "step": 44
    },
    {
      "epoch": 0.029683377308707123,
      "grad_norm": 0.24045729637145996,
      "learning_rate": 0.0001980650835532102,
      "loss": 0.0998,
      "step": 45
    },
    {
      "epoch": 0.030343007915567283,
      "grad_norm": 0.13080666959285736,
      "learning_rate": 0.00019802110817941954,
      "loss": 0.0694,
      "step": 46
    },
    {
      "epoch": 0.03100263852242744,
      "grad_norm": 0.04715995490550995,
      "learning_rate": 0.00019797713280562886,
      "loss": 0.0443,
      "step": 47
    },
    {
      "epoch": 0.0316622691292876,
      "grad_norm": 0.04275837168097496,
      "learning_rate": 0.00019793315743183817,
      "loss": 0.046,
      "step": 48
    },
    {
      "epoch": 0.032321899736147755,
      "grad_norm": 0.054124340415000916,
      "learning_rate": 0.00019788918205804749,
      "loss": 0.045,
      "step": 49
    },
    {
      "epoch": 0.032981530343007916,
      "grad_norm": 0.0623500756919384,
      "learning_rate": 0.00019784520668425683,
      "loss": 0.0452,
      "step": 50
    },
    {
      "epoch": 0.033641160949868076,
      "grad_norm": 0.18199501931667328,
      "learning_rate": 0.00019780123131046614,
      "loss": 0.0102,
      "step": 51
    },
    {
      "epoch": 0.03430079155672823,
      "grad_norm": 0.06172149255871773,
      "learning_rate": 0.00019775725593667546,
      "loss": 0.0588,
      "step": 52
    },
    {
      "epoch": 0.03496042216358839,
      "grad_norm": 0.05103158578276634,
      "learning_rate": 0.0001977132805628848,
      "loss": 0.0435,
      "step": 53
    },
    {
      "epoch": 0.03562005277044855,
      "grad_norm": 0.16191720962524414,
      "learning_rate": 0.0001976693051890941,
      "loss": 0.0089,
      "step": 54
    },
    {
      "epoch": 0.036279683377308705,
      "grad_norm": 0.13151611387729645,
      "learning_rate": 0.00019762532981530345,
      "loss": 0.0072,
      "step": 55
    },
    {
      "epoch": 0.036939313984168866,
      "grad_norm": 0.12633225321769714,
      "learning_rate": 0.00019758135444151277,
      "loss": 0.0601,
      "step": 56
    },
    {
      "epoch": 0.037598944591029027,
      "grad_norm": 0.0668325275182724,
      "learning_rate": 0.0001975373790677221,
      "loss": 0.0407,
      "step": 57
    },
    {
      "epoch": 0.03825857519788918,
      "grad_norm": 0.17144960165023804,
      "learning_rate": 0.00019749340369393142,
      "loss": 0.0702,
      "step": 58
    },
    {
      "epoch": 0.03891820580474934,
      "grad_norm": 0.206908717751503,
      "learning_rate": 0.00019744942832014073,
      "loss": 0.0815,
      "step": 59
    },
    {
      "epoch": 0.0395778364116095,
      "grad_norm": 0.07280127704143524,
      "learning_rate": 0.00019740545294635005,
      "loss": 0.0408,
      "step": 60
    },
    {
      "epoch": 0.040237467018469655,
      "grad_norm": 0.07187453657388687,
      "learning_rate": 0.0001973614775725594,
      "loss": 0.0407,
      "step": 61
    },
    {
      "epoch": 0.040897097625329816,
      "grad_norm": 0.07137905061244965,
      "learning_rate": 0.0001973175021987687,
      "loss": 0.0411,
      "step": 62
    },
    {
      "epoch": 0.04155672823218998,
      "grad_norm": 0.14471158385276794,
      "learning_rate": 0.00019727352682497802,
      "loss": 0.0721,
      "step": 63
    },
    {
      "epoch": 0.04221635883905013,
      "grad_norm": 0.07668201625347137,
      "learning_rate": 0.00019722955145118736,
      "loss": 0.0438,
      "step": 64
    },
    {
      "epoch": 0.04287598944591029,
      "grad_norm": 0.1472671926021576,
      "learning_rate": 0.00019718557607739667,
      "loss": 0.0828,
      "step": 65
    },
    {
      "epoch": 0.04353562005277045,
      "grad_norm": 0.23281940817832947,
      "learning_rate": 0.00019714160070360599,
      "loss": 0.0328,
      "step": 66
    },
    {
      "epoch": 0.044195250659630606,
      "grad_norm": 0.0707450807094574,
      "learning_rate": 0.0001970976253298153,
      "loss": 0.0723,
      "step": 67
    },
    {
      "epoch": 0.044854881266490766,
      "grad_norm": 0.19699756801128387,
      "learning_rate": 0.00019705364995602464,
      "loss": 0.0439,
      "step": 68
    },
    {
      "epoch": 0.04551451187335093,
      "grad_norm": 0.14605747163295746,
      "learning_rate": 0.00019700967458223396,
      "loss": 0.0436,
      "step": 69
    },
    {
      "epoch": 0.04617414248021108,
      "grad_norm": 0.24151253700256348,
      "learning_rate": 0.00019696569920844327,
      "loss": 0.105,
      "step": 70
    },
    {
      "epoch": 0.04683377308707124,
      "grad_norm": 0.0680900365114212,
      "learning_rate": 0.00019692172383465258,
      "loss": 0.0521,
      "step": 71
    },
    {
      "epoch": 0.047493403693931395,
      "grad_norm": 0.07927345484495163,
      "learning_rate": 0.00019687774846086192,
      "loss": 0.0404,
      "step": 72
    },
    {
      "epoch": 0.048153034300791556,
      "grad_norm": 0.07084106653928757,
      "learning_rate": 0.00019683377308707124,
      "loss": 0.0519,
      "step": 73
    },
    {
      "epoch": 0.048812664907651716,
      "grad_norm": 0.09232115745544434,
      "learning_rate": 0.00019678979771328058,
      "loss": 0.0262,
      "step": 74
    },
    {
      "epoch": 0.04947229551451187,
      "grad_norm": 0.06073536351323128,
      "learning_rate": 0.0001967458223394899,
      "loss": 0.0359,
      "step": 75
    },
    {
      "epoch": 0.05013192612137203,
      "grad_norm": 0.13216532766819,
      "learning_rate": 0.00019670184696569923,
      "loss": 0.0562,
      "step": 76
    },
    {
      "epoch": 0.05079155672823219,
      "grad_norm": 0.06590700894594193,
      "learning_rate": 0.00019665787159190855,
      "loss": 0.0385,
      "step": 77
    },
    {
      "epoch": 0.051451187335092345,
      "grad_norm": 0.24434073269367218,
      "learning_rate": 0.00019661389621811786,
      "loss": 0.0834,
      "step": 78
    },
    {
      "epoch": 0.052110817941952506,
      "grad_norm": 0.056640658527612686,
      "learning_rate": 0.0001965699208443272,
      "loss": 0.0391,
      "step": 79
    },
    {
      "epoch": 0.052770448548812667,
      "grad_norm": 0.3446822762489319,
      "learning_rate": 0.00019652594547053652,
      "loss": 0.0894,
      "step": 80
    },
    {
      "epoch": 0.05343007915567282,
      "grad_norm": 0.10367447882890701,
      "learning_rate": 0.00019648197009674583,
      "loss": 0.0619,
      "step": 81
    },
    {
      "epoch": 0.05408970976253298,
      "grad_norm": 0.3536369502544403,
      "learning_rate": 0.00019643799472295517,
      "loss": 0.0512,
      "step": 82
    },
    {
      "epoch": 0.05474934036939314,
      "grad_norm": 0.2684752345085144,
      "learning_rate": 0.00019639401934916449,
      "loss": 0.0389,
      "step": 83
    },
    {
      "epoch": 0.055408970976253295,
      "grad_norm": 0.28515341877937317,
      "learning_rate": 0.0001963500439753738,
      "loss": 0.0138,
      "step": 84
    },
    {
      "epoch": 0.056068601583113456,
      "grad_norm": 0.12716767191886902,
      "learning_rate": 0.00019630606860158311,
      "loss": 0.0453,
      "step": 85
    },
    {
      "epoch": 0.05672823218997362,
      "grad_norm": 0.27634304761886597,
      "learning_rate": 0.00019626209322779246,
      "loss": 0.0635,
      "step": 86
    },
    {
      "epoch": 0.05738786279683377,
      "grad_norm": 0.13463714718818665,
      "learning_rate": 0.00019621811785400177,
      "loss": 0.0401,
      "step": 87
    },
    {
      "epoch": 0.05804749340369393,
      "grad_norm": 0.20833075046539307,
      "learning_rate": 0.00019617414248021108,
      "loss": 0.0491,
      "step": 88
    },
    {
      "epoch": 0.05870712401055409,
      "grad_norm": 0.12235816568136215,
      "learning_rate": 0.0001961301671064204,
      "loss": 0.0336,
      "step": 89
    },
    {
      "epoch": 0.059366754617414245,
      "grad_norm": 0.043249744921922684,
      "learning_rate": 0.00019608619173262974,
      "loss": 0.0224,
      "step": 90
    },
    {
      "epoch": 0.060026385224274406,
      "grad_norm": 0.1612648218870163,
      "learning_rate": 0.00019604221635883905,
      "loss": 0.0453,
      "step": 91
    },
    {
      "epoch": 0.06068601583113457,
      "grad_norm": 0.11032130569219589,
      "learning_rate": 0.00019599824098504837,
      "loss": 0.0434,
      "step": 92
    },
    {
      "epoch": 0.06134564643799472,
      "grad_norm": 0.25559529662132263,
      "learning_rate": 0.0001959542656112577,
      "loss": 0.0307,
      "step": 93
    },
    {
      "epoch": 0.06200527704485488,
      "grad_norm": 0.19734643399715424,
      "learning_rate": 0.00019591029023746702,
      "loss": 0.0396,
      "step": 94
    },
    {
      "epoch": 0.06266490765171503,
      "grad_norm": 0.1691751778125763,
      "learning_rate": 0.00019586631486367634,
      "loss": 0.0426,
      "step": 95
    },
    {
      "epoch": 0.0633245382585752,
      "grad_norm": 0.16714054346084595,
      "learning_rate": 0.00019582233948988568,
      "loss": 0.0305,
      "step": 96
    },
    {
      "epoch": 0.06398416886543536,
      "grad_norm": 0.1062399372458458,
      "learning_rate": 0.000195778364116095,
      "loss": 0.0303,
      "step": 97
    },
    {
      "epoch": 0.06464379947229551,
      "grad_norm": 0.2215929478406906,
      "learning_rate": 0.00019573438874230433,
      "loss": 0.0494,
      "step": 98
    },
    {
      "epoch": 0.06530343007915568,
      "grad_norm": 0.09904662519693375,
      "learning_rate": 0.00019569041336851365,
      "loss": 0.0262,
      "step": 99
    },
    {
      "epoch": 0.06596306068601583,
      "grad_norm": 0.21408338844776154,
      "learning_rate": 0.00019564643799472299,
      "loss": 0.0435,
      "step": 100
    },
    {
      "epoch": 0.06662269129287599,
      "grad_norm": 0.30659642815589905,
      "learning_rate": 0.0001956024626209323,
      "loss": 0.063,
      "step": 101
    },
    {
      "epoch": 0.06728232189973615,
      "grad_norm": 0.13222965598106384,
      "learning_rate": 0.00019555848724714161,
      "loss": 0.0354,
      "step": 102
    },
    {
      "epoch": 0.0679419525065963,
      "grad_norm": 0.14156505465507507,
      "learning_rate": 0.00019551451187335093,
      "loss": 0.0289,
      "step": 103
    },
    {
      "epoch": 0.06860158311345646,
      "grad_norm": 0.17455017566680908,
      "learning_rate": 0.00019547053649956027,
      "loss": 0.0495,
      "step": 104
    },
    {
      "epoch": 0.06926121372031663,
      "grad_norm": 0.2612893879413605,
      "learning_rate": 0.00019542656112576958,
      "loss": 0.0116,
      "step": 105
    },
    {
      "epoch": 0.06992084432717678,
      "grad_norm": 0.17791563272476196,
      "learning_rate": 0.0001953825857519789,
      "loss": 0.0703,
      "step": 106
    },
    {
      "epoch": 0.07058047493403694,
      "grad_norm": 0.28025487065315247,
      "learning_rate": 0.0001953386103781882,
      "loss": 0.0312,
      "step": 107
    },
    {
      "epoch": 0.0712401055408971,
      "grad_norm": 0.10029605031013489,
      "learning_rate": 0.00019529463500439755,
      "loss": 0.0392,
      "step": 108
    },
    {
      "epoch": 0.07189973614775726,
      "grad_norm": 0.3146810233592987,
      "learning_rate": 0.00019525065963060687,
      "loss": 0.0689,
      "step": 109
    },
    {
      "epoch": 0.07255936675461741,
      "grad_norm": 0.11498094350099564,
      "learning_rate": 0.00019520668425681618,
      "loss": 0.0402,
      "step": 110
    },
    {
      "epoch": 0.07321899736147758,
      "grad_norm": 0.20288977026939392,
      "learning_rate": 0.00019516270888302552,
      "loss": 0.0089,
      "step": 111
    },
    {
      "epoch": 0.07387862796833773,
      "grad_norm": 0.09606583416461945,
      "learning_rate": 0.00019511873350923484,
      "loss": 0.0422,
      "step": 112
    },
    {
      "epoch": 0.07453825857519789,
      "grad_norm": 0.128167524933815,
      "learning_rate": 0.00019507475813544415,
      "loss": 0.0055,
      "step": 113
    },
    {
      "epoch": 0.07519788918205805,
      "grad_norm": 0.13003094494342804,
      "learning_rate": 0.00019503078276165346,
      "loss": 0.048,
      "step": 114
    },
    {
      "epoch": 0.0758575197889182,
      "grad_norm": 0.3133968114852905,
      "learning_rate": 0.0001949868073878628,
      "loss": 0.0708,
      "step": 115
    },
    {
      "epoch": 0.07651715039577836,
      "grad_norm": 0.05762765184044838,
      "learning_rate": 0.00019494283201407212,
      "loss": 0.0185,
      "step": 116
    },
    {
      "epoch": 0.07717678100263853,
      "grad_norm": 0.10894982516765594,
      "learning_rate": 0.00019489885664028146,
      "loss": 0.0277,
      "step": 117
    },
    {
      "epoch": 0.07783641160949868,
      "grad_norm": 0.21002894639968872,
      "learning_rate": 0.00019485488126649077,
      "loss": 0.0464,
      "step": 118
    },
    {
      "epoch": 0.07849604221635884,
      "grad_norm": 0.2224273681640625,
      "learning_rate": 0.00019481090589270011,
      "loss": 0.0351,
      "step": 119
    },
    {
      "epoch": 0.079155672823219,
      "grad_norm": 0.1394512951374054,
      "learning_rate": 0.00019476693051890943,
      "loss": 0.0257,
      "step": 120
    },
    {
      "epoch": 0.07981530343007916,
      "grad_norm": 0.18453086912631989,
      "learning_rate": 0.00019472295514511874,
      "loss": 0.029,
      "step": 121
    },
    {
      "epoch": 0.08047493403693931,
      "grad_norm": 0.1256110519170761,
      "learning_rate": 0.00019467897977132808,
      "loss": 0.041,
      "step": 122
    },
    {
      "epoch": 0.08113456464379948,
      "grad_norm": 0.2566694915294647,
      "learning_rate": 0.0001946350043975374,
      "loss": 0.0787,
      "step": 123
    },
    {
      "epoch": 0.08179419525065963,
      "grad_norm": 0.1477961242198944,
      "learning_rate": 0.0001945910290237467,
      "loss": 0.0467,
      "step": 124
    },
    {
      "epoch": 0.08245382585751979,
      "grad_norm": 0.18613003194332123,
      "learning_rate": 0.00019454705364995603,
      "loss": 0.0083,
      "step": 125
    },
    {
      "epoch": 0.08311345646437995,
      "grad_norm": 0.30505865812301636,
      "learning_rate": 0.00019450307827616537,
      "loss": 0.0669,
      "step": 126
    },
    {
      "epoch": 0.08377308707124011,
      "grad_norm": 0.12369900941848755,
      "learning_rate": 0.00019445910290237468,
      "loss": 0.0264,
      "step": 127
    },
    {
      "epoch": 0.08443271767810026,
      "grad_norm": 0.09146395325660706,
      "learning_rate": 0.000194415127528584,
      "loss": 0.0357,
      "step": 128
    },
    {
      "epoch": 0.08509234828496043,
      "grad_norm": 0.23533299565315247,
      "learning_rate": 0.00019437115215479334,
      "loss": 0.0593,
      "step": 129
    },
    {
      "epoch": 0.08575197889182058,
      "grad_norm": 0.11334734410047531,
      "learning_rate": 0.00019432717678100265,
      "loss": 0.0419,
      "step": 130
    },
    {
      "epoch": 0.08641160949868074,
      "grad_norm": 0.2419799119234085,
      "learning_rate": 0.00019428320140721196,
      "loss": 0.034,
      "step": 131
    },
    {
      "epoch": 0.0870712401055409,
      "grad_norm": 0.2006964385509491,
      "learning_rate": 0.00019423922603342128,
      "loss": 0.0325,
      "step": 132
    },
    {
      "epoch": 0.08773087071240106,
      "grad_norm": 0.1248619481921196,
      "learning_rate": 0.00019419525065963062,
      "loss": 0.0396,
      "step": 133
    },
    {
      "epoch": 0.08839050131926121,
      "grad_norm": 0.08222834020853043,
      "learning_rate": 0.00019415127528583993,
      "loss": 0.0424,
      "step": 134
    },
    {
      "epoch": 0.08905013192612138,
      "grad_norm": 0.24916771054267883,
      "learning_rate": 0.00019410729991204925,
      "loss": 0.0501,
      "step": 135
    },
    {
      "epoch": 0.08970976253298153,
      "grad_norm": 0.14414404332637787,
      "learning_rate": 0.0001940633245382586,
      "loss": 0.0513,
      "step": 136
    },
    {
      "epoch": 0.09036939313984169,
      "grad_norm": 0.26187941431999207,
      "learning_rate": 0.0001940193491644679,
      "loss": 0.0572,
      "step": 137
    },
    {
      "epoch": 0.09102902374670185,
      "grad_norm": 0.19284501671791077,
      "learning_rate": 0.00019397537379067722,
      "loss": 0.0086,
      "step": 138
    },
    {
      "epoch": 0.09168865435356201,
      "grad_norm": 0.11635082960128784,
      "learning_rate": 0.00019393139841688656,
      "loss": 0.0169,
      "step": 139
    },
    {
      "epoch": 0.09234828496042216,
      "grad_norm": 0.13965992629528046,
      "learning_rate": 0.0001938874230430959,
      "loss": 0.0175,
      "step": 140
    },
    {
      "epoch": 0.09300791556728233,
      "grad_norm": 0.07542465627193451,
      "learning_rate": 0.0001938434476693052,
      "loss": 0.0198,
      "step": 141
    },
    {
      "epoch": 0.09366754617414248,
      "grad_norm": 0.05143353343009949,
      "learning_rate": 0.00019379947229551453,
      "loss": 0.0183,
      "step": 142
    },
    {
      "epoch": 0.09432717678100264,
      "grad_norm": 0.07734987884759903,
      "learning_rate": 0.00019375549692172384,
      "loss": 0.0228,
      "step": 143
    },
    {
      "epoch": 0.09498680738786279,
      "grad_norm": 0.15355023741722107,
      "learning_rate": 0.00019371152154793318,
      "loss": 0.0319,
      "step": 144
    },
    {
      "epoch": 0.09564643799472296,
      "grad_norm": 0.2588460147380829,
      "learning_rate": 0.0001936675461741425,
      "loss": 0.0572,
      "step": 145
    },
    {
      "epoch": 0.09630606860158311,
      "grad_norm": 0.15116266906261444,
      "learning_rate": 0.0001936235708003518,
      "loss": 0.0373,
      "step": 146
    },
    {
      "epoch": 0.09696569920844327,
      "grad_norm": 0.12183251976966858,
      "learning_rate": 0.00019357959542656115,
      "loss": 0.0159,
      "step": 147
    },
    {
      "epoch": 0.09762532981530343,
      "grad_norm": 0.1263515055179596,
      "learning_rate": 0.00019353562005277046,
      "loss": 0.0051,
      "step": 148
    },
    {
      "epoch": 0.09828496042216359,
      "grad_norm": 0.10101950913667679,
      "learning_rate": 0.00019349164467897978,
      "loss": 0.0382,
      "step": 149
    },
    {
      "epoch": 0.09894459102902374,
      "grad_norm": 0.16407382488250732,
      "learning_rate": 0.0001934476693051891,
      "loss": 0.0072,
      "step": 150
    },
    {
      "epoch": 0.09960422163588391,
      "grad_norm": 0.284460186958313,
      "learning_rate": 0.00019340369393139843,
      "loss": 0.0582,
      "step": 151
    },
    {
      "epoch": 0.10026385224274406,
      "grad_norm": 0.14486350119113922,
      "learning_rate": 0.00019335971855760775,
      "loss": 0.0278,
      "step": 152
    },
    {
      "epoch": 0.10092348284960422,
      "grad_norm": 0.12790547311306,
      "learning_rate": 0.00019331574318381706,
      "loss": 0.0257,
      "step": 153
    },
    {
      "epoch": 0.10158311345646438,
      "grad_norm": 0.13060295581817627,
      "learning_rate": 0.0001932717678100264,
      "loss": 0.0053,
      "step": 154
    },
    {
      "epoch": 0.10224274406332454,
      "grad_norm": 0.0999758392572403,
      "learning_rate": 0.00019322779243623572,
      "loss": 0.0225,
      "step": 155
    },
    {
      "epoch": 0.10290237467018469,
      "grad_norm": 0.07807367295026779,
      "learning_rate": 0.00019318381706244503,
      "loss": 0.0208,
      "step": 156
    },
    {
      "epoch": 0.10356200527704486,
      "grad_norm": 0.16557645797729492,
      "learning_rate": 0.00019313984168865434,
      "loss": 0.0394,
      "step": 157
    },
    {
      "epoch": 0.10422163588390501,
      "grad_norm": 0.061547163873910904,
      "learning_rate": 0.00019309586631486368,
      "loss": 0.0024,
      "step": 158
    },
    {
      "epoch": 0.10488126649076517,
      "grad_norm": 0.2129908800125122,
      "learning_rate": 0.000193051890941073,
      "loss": 0.03,
      "step": 159
    },
    {
      "epoch": 0.10554089709762533,
      "grad_norm": 0.1917123943567276,
      "learning_rate": 0.00019300791556728234,
      "loss": 0.0387,
      "step": 160
    },
    {
      "epoch": 0.10620052770448549,
      "grad_norm": 0.12426037341356277,
      "learning_rate": 0.00019296394019349165,
      "loss": 0.0206,
      "step": 161
    },
    {
      "epoch": 0.10686015831134564,
      "grad_norm": 0.15173159539699554,
      "learning_rate": 0.000192919964819701,
      "loss": 0.036,
      "step": 162
    },
    {
      "epoch": 0.10751978891820581,
      "grad_norm": 0.10376128554344177,
      "learning_rate": 0.0001928759894459103,
      "loss": 0.0313,
      "step": 163
    },
    {
      "epoch": 0.10817941952506596,
      "grad_norm": 0.18105846643447876,
      "learning_rate": 0.00019283201407211962,
      "loss": 0.0534,
      "step": 164
    },
    {
      "epoch": 0.10883905013192612,
      "grad_norm": 0.3053407371044159,
      "learning_rate": 0.00019278803869832896,
      "loss": 0.0351,
      "step": 165
    },
    {
      "epoch": 0.10949868073878628,
      "grad_norm": 0.15922747552394867,
      "learning_rate": 0.00019274406332453828,
      "loss": 0.0312,
      "step": 166
    },
    {
      "epoch": 0.11015831134564644,
      "grad_norm": 0.10466444492340088,
      "learning_rate": 0.0001927000879507476,
      "loss": 0.0298,
      "step": 167
    },
    {
      "epoch": 0.11081794195250659,
      "grad_norm": 0.1910073608160019,
      "learning_rate": 0.0001926561125769569,
      "loss": 0.0087,
      "step": 168
    },
    {
      "epoch": 0.11147757255936676,
      "grad_norm": 0.19217872619628906,
      "learning_rate": 0.00019261213720316625,
      "loss": 0.0297,
      "step": 169
    },
    {
      "epoch": 0.11213720316622691,
      "grad_norm": 0.09727902710437775,
      "learning_rate": 0.00019256816182937556,
      "loss": 0.0256,
      "step": 170
    },
    {
      "epoch": 0.11279683377308707,
      "grad_norm": 0.28956523537635803,
      "learning_rate": 0.00019252418645558487,
      "loss": 0.0546,
      "step": 171
    },
    {
      "epoch": 0.11345646437994723,
      "grad_norm": 0.10394792258739471,
      "learning_rate": 0.00019248021108179422,
      "loss": 0.023,
      "step": 172
    },
    {
      "epoch": 0.11411609498680739,
      "grad_norm": 0.06451471894979477,
      "learning_rate": 0.00019243623570800353,
      "loss": 0.0087,
      "step": 173
    },
    {
      "epoch": 0.11477572559366754,
      "grad_norm": 0.243513286113739,
      "learning_rate": 0.00019239226033421284,
      "loss": 0.0601,
      "step": 174
    },
    {
      "epoch": 0.11543535620052771,
      "grad_norm": 0.2900483012199402,
      "learning_rate": 0.00019234828496042216,
      "loss": 0.0448,
      "step": 175
    },
    {
      "epoch": 0.11609498680738786,
      "grad_norm": 0.10734273493289948,
      "learning_rate": 0.0001923043095866315,
      "loss": 0.0222,
      "step": 176
    },
    {
      "epoch": 0.11675461741424802,
      "grad_norm": 0.14786824584007263,
      "learning_rate": 0.0001922603342128408,
      "loss": 0.029,
      "step": 177
    },
    {
      "epoch": 0.11741424802110818,
      "grad_norm": 0.16831807792186737,
      "learning_rate": 0.00019221635883905013,
      "loss": 0.0377,
      "step": 178
    },
    {
      "epoch": 0.11807387862796834,
      "grad_norm": 0.387617290019989,
      "learning_rate": 0.00019217238346525944,
      "loss": 0.0366,
      "step": 179
    },
    {
      "epoch": 0.11873350923482849,
      "grad_norm": 0.12525829672813416,
      "learning_rate": 0.00019212840809146878,
      "loss": 0.0445,
      "step": 180
    },
    {
      "epoch": 0.11939313984168866,
      "grad_norm": 0.14774373173713684,
      "learning_rate": 0.00019208443271767812,
      "loss": 0.0462,
      "step": 181
    },
    {
      "epoch": 0.12005277044854881,
      "grad_norm": 0.11644310504198074,
      "learning_rate": 0.00019204045734388744,
      "loss": 0.0382,
      "step": 182
    },
    {
      "epoch": 0.12071240105540897,
      "grad_norm": 0.31700724363327026,
      "learning_rate": 0.00019199648197009678,
      "loss": 0.0779,
      "step": 183
    },
    {
      "epoch": 0.12137203166226913,
      "grad_norm": 0.12285193055868149,
      "learning_rate": 0.0001919525065963061,
      "loss": 0.0324,
      "step": 184
    },
    {
      "epoch": 0.12203166226912929,
      "grad_norm": 0.18638621270656586,
      "learning_rate": 0.0001919085312225154,
      "loss": 0.0209,
      "step": 185
    },
    {
      "epoch": 0.12269129287598944,
      "grad_norm": 0.12487637251615524,
      "learning_rate": 0.00019186455584872472,
      "loss": 0.0284,
      "step": 186
    },
    {
      "epoch": 0.12335092348284961,
      "grad_norm": 0.09531760215759277,
      "learning_rate": 0.00019182058047493406,
      "loss": 0.0272,
      "step": 187
    },
    {
      "epoch": 0.12401055408970976,
      "grad_norm": 0.1428140252828598,
      "learning_rate": 0.00019177660510114337,
      "loss": 0.0313,
      "step": 188
    },
    {
      "epoch": 0.12467018469656992,
      "grad_norm": 0.12039037048816681,
      "learning_rate": 0.0001917326297273527,
      "loss": 0.027,
      "step": 189
    },
    {
      "epoch": 0.12532981530343007,
      "grad_norm": 0.26814985275268555,
      "learning_rate": 0.00019168865435356203,
      "loss": 0.0574,
      "step": 190
    },
    {
      "epoch": 0.12598944591029024,
      "grad_norm": 0.15960603952407837,
      "learning_rate": 0.00019164467897977134,
      "loss": 0.0425,
      "step": 191
    },
    {
      "epoch": 0.1266490765171504,
      "grad_norm": 0.19683250784873962,
      "learning_rate": 0.00019160070360598066,
      "loss": 0.0134,
      "step": 192
    },
    {
      "epoch": 0.12730870712401055,
      "grad_norm": 0.1749458611011505,
      "learning_rate": 0.00019155672823218997,
      "loss": 0.0479,
      "step": 193
    },
    {
      "epoch": 0.1279683377308707,
      "grad_norm": 0.17689643800258636,
      "learning_rate": 0.0001915127528583993,
      "loss": 0.0257,
      "step": 194
    },
    {
      "epoch": 0.12862796833773088,
      "grad_norm": 0.1794043332338333,
      "learning_rate": 0.00019146877748460863,
      "loss": 0.0457,
      "step": 195
    },
    {
      "epoch": 0.12928759894459102,
      "grad_norm": 0.17437396943569183,
      "learning_rate": 0.00019142480211081794,
      "loss": 0.0483,
      "step": 196
    },
    {
      "epoch": 0.1299472295514512,
      "grad_norm": 0.12864597141742706,
      "learning_rate": 0.00019138082673702725,
      "loss": 0.0215,
      "step": 197
    },
    {
      "epoch": 0.13060686015831136,
      "grad_norm": 0.35680100321769714,
      "learning_rate": 0.0001913368513632366,
      "loss": 0.0136,
      "step": 198
    },
    {
      "epoch": 0.1312664907651715,
      "grad_norm": 0.23740121722221375,
      "learning_rate": 0.0001912928759894459,
      "loss": 0.0085,
      "step": 199
    },
    {
      "epoch": 0.13192612137203166,
      "grad_norm": 0.34183815121650696,
      "learning_rate": 0.00019124890061565522,
      "loss": 0.0604,
      "step": 200
    },
    {
      "epoch": 0.13258575197889183,
      "grad_norm": 0.29078608751296997,
      "learning_rate": 0.00019120492524186456,
      "loss": 0.0729,
      "step": 201
    },
    {
      "epoch": 0.13324538258575197,
      "grad_norm": 0.1477326899766922,
      "learning_rate": 0.00019116094986807388,
      "loss": 0.0438,
      "step": 202
    },
    {
      "epoch": 0.13390501319261214,
      "grad_norm": 0.09249796718358994,
      "learning_rate": 0.00019111697449428322,
      "loss": 0.0184,
      "step": 203
    },
    {
      "epoch": 0.1345646437994723,
      "grad_norm": 0.18043112754821777,
      "learning_rate": 0.00019107299912049253,
      "loss": 0.0463,
      "step": 204
    },
    {
      "epoch": 0.13522427440633245,
      "grad_norm": 0.09831412881612778,
      "learning_rate": 0.00019102902374670187,
      "loss": 0.0375,
      "step": 205
    },
    {
      "epoch": 0.1358839050131926,
      "grad_norm": 0.19680404663085938,
      "learning_rate": 0.0001909850483729112,
      "loss": 0.0091,
      "step": 206
    },
    {
      "epoch": 0.13654353562005278,
      "grad_norm": 0.2062271386384964,
      "learning_rate": 0.0001909410729991205,
      "loss": 0.0484,
      "step": 207
    },
    {
      "epoch": 0.13720316622691292,
      "grad_norm": 0.09508629143238068,
      "learning_rate": 0.00019089709762532984,
      "loss": 0.0322,
      "step": 208
    },
    {
      "epoch": 0.1378627968337731,
      "grad_norm": 0.13301989436149597,
      "learning_rate": 0.00019085312225153916,
      "loss": 0.0211,
      "step": 209
    },
    {
      "epoch": 0.13852242744063326,
      "grad_norm": 0.09851998090744019,
      "learning_rate": 0.00019080914687774847,
      "loss": 0.0455,
      "step": 210
    },
    {
      "epoch": 0.1391820580474934,
      "grad_norm": 0.1096734032034874,
      "learning_rate": 0.00019076517150395778,
      "loss": 0.0305,
      "step": 211
    },
    {
      "epoch": 0.13984168865435356,
      "grad_norm": 0.14769691228866577,
      "learning_rate": 0.00019072119613016713,
      "loss": 0.0416,
      "step": 212
    },
    {
      "epoch": 0.14050131926121373,
      "grad_norm": 0.09760124236345291,
      "learning_rate": 0.00019067722075637644,
      "loss": 0.0248,
      "step": 213
    },
    {
      "epoch": 0.14116094986807387,
      "grad_norm": 0.31745147705078125,
      "learning_rate": 0.00019063324538258575,
      "loss": 0.0621,
      "step": 214
    },
    {
      "epoch": 0.14182058047493404,
      "grad_norm": 0.1543177217245102,
      "learning_rate": 0.00019058927000879507,
      "loss": 0.0073,
      "step": 215
    },
    {
      "epoch": 0.1424802110817942,
      "grad_norm": 0.20271125435829163,
      "learning_rate": 0.0001905452946350044,
      "loss": 0.0529,
      "step": 216
    },
    {
      "epoch": 0.14313984168865435,
      "grad_norm": 0.10752251744270325,
      "learning_rate": 0.00019050131926121372,
      "loss": 0.0287,
      "step": 217
    },
    {
      "epoch": 0.1437994722955145,
      "grad_norm": 0.21865250170230865,
      "learning_rate": 0.00019045734388742304,
      "loss": 0.0273,
      "step": 218
    },
    {
      "epoch": 0.14445910290237468,
      "grad_norm": 0.14582155644893646,
      "learning_rate": 0.00019041336851363238,
      "loss": 0.0285,
      "step": 219
    },
    {
      "epoch": 0.14511873350923482,
      "grad_norm": 0.16580945253372192,
      "learning_rate": 0.0001903693931398417,
      "loss": 0.0106,
      "step": 220
    },
    {
      "epoch": 0.145778364116095,
      "grad_norm": 0.09719638526439667,
      "learning_rate": 0.000190325417766051,
      "loss": 0.028,
      "step": 221
    },
    {
      "epoch": 0.14643799472295516,
      "grad_norm": 0.3280295729637146,
      "learning_rate": 0.00019028144239226035,
      "loss": 0.0766,
      "step": 222
    },
    {
      "epoch": 0.1470976253298153,
      "grad_norm": 0.13194382190704346,
      "learning_rate": 0.00019023746701846966,
      "loss": 0.0483,
      "step": 223
    },
    {
      "epoch": 0.14775725593667546,
      "grad_norm": 0.10151534527540207,
      "learning_rate": 0.000190193491644679,
      "loss": 0.0236,
      "step": 224
    },
    {
      "epoch": 0.14841688654353563,
      "grad_norm": 0.14077937602996826,
      "learning_rate": 0.00019014951627088832,
      "loss": 0.0394,
      "step": 225
    },
    {
      "epoch": 0.14907651715039577,
      "grad_norm": 0.20061689615249634,
      "learning_rate": 0.00019010554089709766,
      "loss": 0.0085,
      "step": 226
    },
    {
      "epoch": 0.14973614775725594,
      "grad_norm": 0.2101844847202301,
      "learning_rate": 0.00019006156552330697,
      "loss": 0.0559,
      "step": 227
    },
    {
      "epoch": 0.1503957783641161,
      "grad_norm": 0.121984101831913,
      "learning_rate": 0.00019001759014951628,
      "loss": 0.037,
      "step": 228
    },
    {
      "epoch": 0.15105540897097625,
      "grad_norm": 0.2454075813293457,
      "learning_rate": 0.0001899736147757256,
      "loss": 0.0657,
      "step": 229
    },
    {
      "epoch": 0.1517150395778364,
      "grad_norm": 0.12167758494615555,
      "learning_rate": 0.00018992963940193494,
      "loss": 0.024,
      "step": 230
    },
    {
      "epoch": 0.15237467018469658,
      "grad_norm": 0.1468701809644699,
      "learning_rate": 0.00018988566402814425,
      "loss": 0.0609,
      "step": 231
    },
    {
      "epoch": 0.15303430079155672,
      "grad_norm": 0.2747187316417694,
      "learning_rate": 0.00018984168865435357,
      "loss": 0.0234,
      "step": 232
    },
    {
      "epoch": 0.1536939313984169,
      "grad_norm": 0.21161307394504547,
      "learning_rate": 0.00018979771328056288,
      "loss": 0.0535,
      "step": 233
    },
    {
      "epoch": 0.15435356200527706,
      "grad_norm": 0.11065617203712463,
      "learning_rate": 0.00018975373790677222,
      "loss": 0.027,
      "step": 234
    },
    {
      "epoch": 0.1550131926121372,
      "grad_norm": 0.2823605537414551,
      "learning_rate": 0.00018970976253298154,
      "loss": 0.0609,
      "step": 235
    },
    {
      "epoch": 0.15567282321899736,
      "grad_norm": 0.1574711799621582,
      "learning_rate": 0.00018966578715919085,
      "loss": 0.0218,
      "step": 236
    },
    {
      "epoch": 0.15633245382585753,
      "grad_norm": 0.1133563444018364,
      "learning_rate": 0.0001896218117854002,
      "loss": 0.0303,
      "step": 237
    },
    {
      "epoch": 0.15699208443271767,
      "grad_norm": 0.23183485865592957,
      "learning_rate": 0.0001895778364116095,
      "loss": 0.0196,
      "step": 238
    },
    {
      "epoch": 0.15765171503957784,
      "grad_norm": 0.11320910602807999,
      "learning_rate": 0.00018953386103781882,
      "loss": 0.0217,
      "step": 239
    },
    {
      "epoch": 0.158311345646438,
      "grad_norm": 0.10426139831542969,
      "learning_rate": 0.00018948988566402813,
      "loss": 0.0302,
      "step": 240
    },
    {
      "epoch": 0.15897097625329815,
      "grad_norm": 0.19469720125198364,
      "learning_rate": 0.00018944591029023747,
      "loss": 0.0309,
      "step": 241
    },
    {
      "epoch": 0.15963060686015831,
      "grad_norm": 0.29449549317359924,
      "learning_rate": 0.0001894019349164468,
      "loss": 0.0587,
      "step": 242
    },
    {
      "epoch": 0.16029023746701848,
      "grad_norm": 0.1358005553483963,
      "learning_rate": 0.0001893579595426561,
      "loss": 0.0315,
      "step": 243
    },
    {
      "epoch": 0.16094986807387862,
      "grad_norm": 0.14565633237361908,
      "learning_rate": 0.00018931398416886544,
      "loss": 0.0246,
      "step": 244
    },
    {
      "epoch": 0.1616094986807388,
      "grad_norm": 0.4777294397354126,
      "learning_rate": 0.00018927000879507478,
      "loss": 0.0238,
      "step": 245
    },
    {
      "epoch": 0.16226912928759896,
      "grad_norm": 0.3011987507343292,
      "learning_rate": 0.0001892260334212841,
      "loss": 0.0636,
      "step": 246
    },
    {
      "epoch": 0.1629287598944591,
      "grad_norm": 0.17785322666168213,
      "learning_rate": 0.0001891820580474934,
      "loss": 0.0512,
      "step": 247
    },
    {
      "epoch": 0.16358839050131926,
      "grad_norm": 0.12346488237380981,
      "learning_rate": 0.00018913808267370275,
      "loss": 0.0305,
      "step": 248
    },
    {
      "epoch": 0.16424802110817943,
      "grad_norm": 0.1267511397600174,
      "learning_rate": 0.00018909410729991207,
      "loss": 0.0399,
      "step": 249
    },
    {
      "epoch": 0.16490765171503957,
      "grad_norm": 0.23225951194763184,
      "learning_rate": 0.00018905013192612138,
      "loss": 0.0443,
      "step": 250
    },
    {
      "epoch": 0.16556728232189974,
      "grad_norm": 0.17008565366268158,
      "learning_rate": 0.0001890061565523307,
      "loss": 0.0261,
      "step": 251
    },
    {
      "epoch": 0.1662269129287599,
      "grad_norm": 0.16774429380893707,
      "learning_rate": 0.00018896218117854004,
      "loss": 0.0411,
      "step": 252
    },
    {
      "epoch": 0.16688654353562005,
      "grad_norm": 0.14850173890590668,
      "learning_rate": 0.00018891820580474935,
      "loss": 0.0406,
      "step": 253
    },
    {
      "epoch": 0.16754617414248021,
      "grad_norm": 0.17768721282482147,
      "learning_rate": 0.00018887423043095866,
      "loss": 0.0074,
      "step": 254
    },
    {
      "epoch": 0.16820580474934038,
      "grad_norm": 0.09923968464136124,
      "learning_rate": 0.000188830255057168,
      "loss": 0.004,
      "step": 255
    },
    {
      "epoch": 0.16886543535620052,
      "grad_norm": 0.16113226115703583,
      "learning_rate": 0.00018878627968337732,
      "loss": 0.036,
      "step": 256
    },
    {
      "epoch": 0.1695250659630607,
      "grad_norm": 0.3584884703159332,
      "learning_rate": 0.00018874230430958663,
      "loss": 0.0683,
      "step": 257
    },
    {
      "epoch": 0.17018469656992086,
      "grad_norm": 0.13502253592014313,
      "learning_rate": 0.00018869832893579595,
      "loss": 0.0449,
      "step": 258
    },
    {
      "epoch": 0.170844327176781,
      "grad_norm": 0.31017008423805237,
      "learning_rate": 0.0001886543535620053,
      "loss": 0.075,
      "step": 259
    },
    {
      "epoch": 0.17150395778364116,
      "grad_norm": 0.20371004939079285,
      "learning_rate": 0.0001886103781882146,
      "loss": 0.0543,
      "step": 260
    },
    {
      "epoch": 0.17216358839050133,
      "grad_norm": 0.20408768951892853,
      "learning_rate": 0.00018856640281442392,
      "loss": 0.0396,
      "step": 261
    },
    {
      "epoch": 0.17282321899736147,
      "grad_norm": 0.31677430868148804,
      "learning_rate": 0.00018852242744063326,
      "loss": 0.0403,
      "step": 262
    },
    {
      "epoch": 0.17348284960422164,
      "grad_norm": 0.2106190025806427,
      "learning_rate": 0.00018847845206684257,
      "loss": 0.0446,
      "step": 263
    },
    {
      "epoch": 0.1741424802110818,
      "grad_norm": 0.37700438499450684,
      "learning_rate": 0.00018843447669305189,
      "loss": 0.0197,
      "step": 264
    },
    {
      "epoch": 0.17480211081794195,
      "grad_norm": 0.17693425714969635,
      "learning_rate": 0.00018839050131926123,
      "loss": 0.0603,
      "step": 265
    },
    {
      "epoch": 0.17546174142480211,
      "grad_norm": 0.14948789775371552,
      "learning_rate": 0.00018834652594547054,
      "loss": 0.0081,
      "step": 266
    },
    {
      "epoch": 0.17612137203166228,
      "grad_norm": 0.45674338936805725,
      "learning_rate": 0.00018830255057167988,
      "loss": 0.0299,
      "step": 267
    },
    {
      "epoch": 0.17678100263852242,
      "grad_norm": 0.07432827353477478,
      "learning_rate": 0.0001882585751978892,
      "loss": 0.0035,
      "step": 268
    },
    {
      "epoch": 0.1774406332453826,
      "grad_norm": 0.2359493225812912,
      "learning_rate": 0.0001882145998240985,
      "loss": 0.0613,
      "step": 269
    },
    {
      "epoch": 0.17810026385224276,
      "grad_norm": 0.4908621907234192,
      "learning_rate": 0.00018817062445030785,
      "loss": 0.0579,
      "step": 270
    },
    {
      "epoch": 0.1787598944591029,
      "grad_norm": 0.30601686239242554,
      "learning_rate": 0.00018812664907651716,
      "loss": 0.0594,
      "step": 271
    },
    {
      "epoch": 0.17941952506596306,
      "grad_norm": 0.12472175061702728,
      "learning_rate": 0.00018808267370272648,
      "loss": 0.0253,
      "step": 272
    },
    {
      "epoch": 0.18007915567282323,
      "grad_norm": 0.15892568230628967,
      "learning_rate": 0.00018803869832893582,
      "loss": 0.0443,
      "step": 273
    },
    {
      "epoch": 0.18073878627968337,
      "grad_norm": 0.0834997296333313,
      "learning_rate": 0.00018799472295514513,
      "loss": 0.0384,
      "step": 274
    },
    {
      "epoch": 0.18139841688654354,
      "grad_norm": 0.14860966801643372,
      "learning_rate": 0.00018795074758135445,
      "loss": 0.0077,
      "step": 275
    },
    {
      "epoch": 0.1820580474934037,
      "grad_norm": 0.15168800950050354,
      "learning_rate": 0.00018790677220756376,
      "loss": 0.0193,
      "step": 276
    },
    {
      "epoch": 0.18271767810026385,
      "grad_norm": 0.2295731008052826,
      "learning_rate": 0.0001878627968337731,
      "loss": 0.0197,
      "step": 277
    },
    {
      "epoch": 0.18337730870712401,
      "grad_norm": 0.15327389538288116,
      "learning_rate": 0.00018781882145998242,
      "loss": 0.0303,
      "step": 278
    },
    {
      "epoch": 0.18403693931398418,
      "grad_norm": 0.15900860726833344,
      "learning_rate": 0.00018777484608619173,
      "loss": 0.032,
      "step": 279
    },
    {
      "epoch": 0.18469656992084432,
      "grad_norm": 0.179190993309021,
      "learning_rate": 0.00018773087071240107,
      "loss": 0.0523,
      "step": 280
    },
    {
      "epoch": 0.1853562005277045,
      "grad_norm": 0.15199297666549683,
      "learning_rate": 0.00018768689533861039,
      "loss": 0.053,
      "step": 281
    },
    {
      "epoch": 0.18601583113456466,
      "grad_norm": 0.05995456501841545,
      "learning_rate": 0.0001876429199648197,
      "loss": 0.0238,
      "step": 282
    },
    {
      "epoch": 0.1866754617414248,
      "grad_norm": 0.06031165271997452,
      "learning_rate": 0.00018759894459102901,
      "loss": 0.0138,
      "step": 283
    },
    {
      "epoch": 0.18733509234828497,
      "grad_norm": 0.05265991389751434,
      "learning_rate": 0.00018755496921723835,
      "loss": 0.0197,
      "step": 284
    },
    {
      "epoch": 0.1879947229551451,
      "grad_norm": 0.12140937149524689,
      "learning_rate": 0.00018751099384344767,
      "loss": 0.0414,
      "step": 285
    },
    {
      "epoch": 0.18865435356200527,
      "grad_norm": 0.10338076949119568,
      "learning_rate": 0.000187467018469657,
      "loss": 0.0245,
      "step": 286
    },
    {
      "epoch": 0.18931398416886544,
      "grad_norm": 0.14068648219108582,
      "learning_rate": 0.00018742304309586632,
      "loss": 0.0436,
      "step": 287
    },
    {
      "epoch": 0.18997361477572558,
      "grad_norm": 0.15604950487613678,
      "learning_rate": 0.00018737906772207566,
      "loss": 0.055,
      "step": 288
    },
    {
      "epoch": 0.19063324538258575,
      "grad_norm": 0.12546131014823914,
      "learning_rate": 0.00018733509234828498,
      "loss": 0.0063,
      "step": 289
    },
    {
      "epoch": 0.19129287598944592,
      "grad_norm": 0.13727931678295135,
      "learning_rate": 0.0001872911169744943,
      "loss": 0.0341,
      "step": 290
    },
    {
      "epoch": 0.19195250659630606,
      "grad_norm": 0.11068456619977951,
      "learning_rate": 0.00018724714160070363,
      "loss": 0.0114,
      "step": 291
    },
    {
      "epoch": 0.19261213720316622,
      "grad_norm": 0.13765092194080353,
      "learning_rate": 0.00018720316622691295,
      "loss": 0.0199,
      "step": 292
    },
    {
      "epoch": 0.1932717678100264,
      "grad_norm": 0.0722384825348854,
      "learning_rate": 0.00018715919085312226,
      "loss": 0.0254,
      "step": 293
    },
    {
      "epoch": 0.19393139841688653,
      "grad_norm": 0.10770145803689957,
      "learning_rate": 0.00018711521547933158,
      "loss": 0.0438,
      "step": 294
    },
    {
      "epoch": 0.1945910290237467,
      "grad_norm": 0.1323946714401245,
      "learning_rate": 0.00018707124010554092,
      "loss": 0.0444,
      "step": 295
    },
    {
      "epoch": 0.19525065963060687,
      "grad_norm": 0.13147789239883423,
      "learning_rate": 0.00018702726473175023,
      "loss": 0.0315,
      "step": 296
    },
    {
      "epoch": 0.195910290237467,
      "grad_norm": 0.0838061198592186,
      "learning_rate": 0.00018698328935795954,
      "loss": 0.0196,
      "step": 297
    },
    {
      "epoch": 0.19656992084432717,
      "grad_norm": 0.11696106940507889,
      "learning_rate": 0.00018693931398416889,
      "loss": 0.0056,
      "step": 298
    },
    {
      "epoch": 0.19722955145118734,
      "grad_norm": 0.28537291288375854,
      "learning_rate": 0.0001868953386103782,
      "loss": 0.071,
      "step": 299
    },
    {
      "epoch": 0.19788918205804748,
      "grad_norm": 0.08896739035844803,
      "learning_rate": 0.00018685136323658751,
      "loss": 0.033,
      "step": 300
    },
    {
      "epoch": 0.19854881266490765,
      "grad_norm": 0.1671205759048462,
      "learning_rate": 0.00018680738786279683,
      "loss": 0.0173,
      "step": 301
    },
    {
      "epoch": 0.19920844327176782,
      "grad_norm": 0.25466111302375793,
      "learning_rate": 0.00018676341248900617,
      "loss": 0.0595,
      "step": 302
    },
    {
      "epoch": 0.19986807387862796,
      "grad_norm": 0.234773188829422,
      "learning_rate": 0.00018671943711521548,
      "loss": 0.0118,
      "step": 303
    },
    {
      "epoch": 0.20052770448548812,
      "grad_norm": 0.16967210173606873,
      "learning_rate": 0.0001866754617414248,
      "loss": 0.022,
      "step": 304
    },
    {
      "epoch": 0.2011873350923483,
      "grad_norm": 0.06990941613912582,
      "learning_rate": 0.0001866314863676341,
      "loss": 0.0168,
      "step": 305
    },
    {
      "epoch": 0.20184696569920843,
      "grad_norm": 0.08854915201663971,
      "learning_rate": 0.00018658751099384345,
      "loss": 0.0127,
      "step": 306
    },
    {
      "epoch": 0.2025065963060686,
      "grad_norm": 0.15157340466976166,
      "learning_rate": 0.00018654353562005277,
      "loss": 0.0243,
      "step": 307
    },
    {
      "epoch": 0.20316622691292877,
      "grad_norm": 0.15561407804489136,
      "learning_rate": 0.0001864995602462621,
      "loss": 0.0273,
      "step": 308
    },
    {
      "epoch": 0.2038258575197889,
      "grad_norm": 0.06243356317281723,
      "learning_rate": 0.00018645558487247142,
      "loss": 0.0117,
      "step": 309
    },
    {
      "epoch": 0.20448548812664907,
      "grad_norm": 0.09370484203100204,
      "learning_rate": 0.00018641160949868076,
      "loss": 0.0223,
      "step": 310
    },
    {
      "epoch": 0.20514511873350924,
      "grad_norm": 0.4050877094268799,
      "learning_rate": 0.00018636763412489008,
      "loss": 0.1088,
      "step": 311
    },
    {
      "epoch": 0.20580474934036938,
      "grad_norm": 0.13015133142471313,
      "learning_rate": 0.0001863236587510994,
      "loss": 0.0054,
      "step": 312
    },
    {
      "epoch": 0.20646437994722955,
      "grad_norm": 0.25691625475883484,
      "learning_rate": 0.00018627968337730873,
      "loss": 0.0123,
      "step": 313
    },
    {
      "epoch": 0.20712401055408972,
      "grad_norm": 0.307771772146225,
      "learning_rate": 0.00018623570800351804,
      "loss": 0.0441,
      "step": 314
    },
    {
      "epoch": 0.20778364116094986,
      "grad_norm": 0.13112516701221466,
      "learning_rate": 0.00018619173262972736,
      "loss": 0.0287,
      "step": 315
    },
    {
      "epoch": 0.20844327176781002,
      "grad_norm": 0.16946454346179962,
      "learning_rate": 0.0001861477572559367,
      "loss": 0.044,
      "step": 316
    },
    {
      "epoch": 0.2091029023746702,
      "grad_norm": 0.1800411343574524,
      "learning_rate": 0.00018610378188214601,
      "loss": 0.0286,
      "step": 317
    },
    {
      "epoch": 0.20976253298153033,
      "grad_norm": 0.18840743601322174,
      "learning_rate": 0.00018605980650835533,
      "loss": 0.0457,
      "step": 318
    },
    {
      "epoch": 0.2104221635883905,
      "grad_norm": 0.42243489623069763,
      "learning_rate": 0.00018601583113456464,
      "loss": 0.0226,
      "step": 319
    },
    {
      "epoch": 0.21108179419525067,
      "grad_norm": 0.281595915555954,
      "learning_rate": 0.00018597185576077398,
      "loss": 0.0122,
      "step": 320
    },
    {
      "epoch": 0.2117414248021108,
      "grad_norm": 0.1960383504629135,
      "learning_rate": 0.0001859278803869833,
      "loss": 0.0363,
      "step": 321
    },
    {
      "epoch": 0.21240105540897097,
      "grad_norm": 0.06646469980478287,
      "learning_rate": 0.0001858839050131926,
      "loss": 0.0121,
      "step": 322
    },
    {
      "epoch": 0.21306068601583114,
      "grad_norm": 0.4216509163379669,
      "learning_rate": 0.00018583992963940192,
      "loss": 0.0802,
      "step": 323
    },
    {
      "epoch": 0.21372031662269128,
      "grad_norm": 0.15882079303264618,
      "learning_rate": 0.00018579595426561127,
      "loss": 0.0278,
      "step": 324
    },
    {
      "epoch": 0.21437994722955145,
      "grad_norm": 0.23440319299697876,
      "learning_rate": 0.00018575197889182058,
      "loss": 0.0642,
      "step": 325
    },
    {
      "epoch": 0.21503957783641162,
      "grad_norm": 0.051791660487651825,
      "learning_rate": 0.0001857080035180299,
      "loss": 0.0073,
      "step": 326
    },
    {
      "epoch": 0.21569920844327176,
      "grad_norm": 0.11632749438285828,
      "learning_rate": 0.00018566402814423923,
      "loss": 0.0052,
      "step": 327
    },
    {
      "epoch": 0.21635883905013192,
      "grad_norm": 0.08130896091461182,
      "learning_rate": 0.00018562005277044855,
      "loss": 0.0255,
      "step": 328
    },
    {
      "epoch": 0.2170184696569921,
      "grad_norm": 0.10192210227251053,
      "learning_rate": 0.0001855760773966579,
      "loss": 0.0193,
      "step": 329
    },
    {
      "epoch": 0.21767810026385223,
      "grad_norm": 0.11850395053625107,
      "learning_rate": 0.0001855321020228672,
      "loss": 0.0425,
      "step": 330
    },
    {
      "epoch": 0.2183377308707124,
      "grad_norm": 0.16518525779247284,
      "learning_rate": 0.00018548812664907654,
      "loss": 0.0086,
      "step": 331
    },
    {
      "epoch": 0.21899736147757257,
      "grad_norm": 0.10314742475748062,
      "learning_rate": 0.00018544415127528586,
      "loss": 0.0051,
      "step": 332
    },
    {
      "epoch": 0.2196569920844327,
      "grad_norm": 0.16333219408988953,
      "learning_rate": 0.00018540017590149517,
      "loss": 0.0086,
      "step": 333
    },
    {
      "epoch": 0.22031662269129287,
      "grad_norm": 0.07367107272148132,
      "learning_rate": 0.00018535620052770451,
      "loss": 0.0036,
      "step": 334
    },
    {
      "epoch": 0.22097625329815304,
      "grad_norm": 0.1596449464559555,
      "learning_rate": 0.00018531222515391383,
      "loss": 0.0411,
      "step": 335
    },
    {
      "epoch": 0.22163588390501318,
      "grad_norm": 0.4504897892475128,
      "learning_rate": 0.00018526824978012314,
      "loss": 0.0423,
      "step": 336
    },
    {
      "epoch": 0.22229551451187335,
      "grad_norm": 0.33370983600616455,
      "learning_rate": 0.00018522427440633246,
      "loss": 0.0589,
      "step": 337
    },
    {
      "epoch": 0.22295514511873352,
      "grad_norm": 0.12324994802474976,
      "learning_rate": 0.0001851802990325418,
      "loss": 0.0288,
      "step": 338
    },
    {
      "epoch": 0.22361477572559366,
      "grad_norm": 0.26627981662750244,
      "learning_rate": 0.0001851363236587511,
      "loss": 0.0631,
      "step": 339
    },
    {
      "epoch": 0.22427440633245382,
      "grad_norm": 0.12521807849407196,
      "learning_rate": 0.00018509234828496042,
      "loss": 0.026,
      "step": 340
    },
    {
      "epoch": 0.224934036939314,
      "grad_norm": 0.14315369725227356,
      "learning_rate": 0.00018504837291116974,
      "loss": 0.0305,
      "step": 341
    },
    {
      "epoch": 0.22559366754617413,
      "grad_norm": 0.10920839011669159,
      "learning_rate": 0.00018500439753737908,
      "loss": 0.0247,
      "step": 342
    },
    {
      "epoch": 0.2262532981530343,
      "grad_norm": 0.11852335184812546,
      "learning_rate": 0.0001849604221635884,
      "loss": 0.0413,
      "step": 343
    },
    {
      "epoch": 0.22691292875989447,
      "grad_norm": 0.11384479701519012,
      "learning_rate": 0.0001849164467897977,
      "loss": 0.0603,
      "step": 344
    },
    {
      "epoch": 0.2275725593667546,
      "grad_norm": 0.18584024906158447,
      "learning_rate": 0.00018487247141600705,
      "loss": 0.0392,
      "step": 345
    },
    {
      "epoch": 0.22823218997361477,
      "grad_norm": 0.19693401455879211,
      "learning_rate": 0.00018482849604221636,
      "loss": 0.0713,
      "step": 346
    },
    {
      "epoch": 0.22889182058047494,
      "grad_norm": 0.11212527751922607,
      "learning_rate": 0.00018478452066842568,
      "loss": 0.0241,
      "step": 347
    },
    {
      "epoch": 0.22955145118733508,
      "grad_norm": 0.083701491355896,
      "learning_rate": 0.000184740545294635,
      "loss": 0.0311,
      "step": 348
    },
    {
      "epoch": 0.23021108179419525,
      "grad_norm": 0.11869709938764572,
      "learning_rate": 0.00018469656992084433,
      "loss": 0.0543,
      "step": 349
    },
    {
      "epoch": 0.23087071240105542,
      "grad_norm": 0.25627174973487854,
      "learning_rate": 0.00018465259454705365,
      "loss": 0.063,
      "step": 350
    },
    {
      "epoch": 0.23153034300791556,
      "grad_norm": 0.3457930386066437,
      "learning_rate": 0.000184608619173263,
      "loss": 0.0952,
      "step": 351
    },
    {
      "epoch": 0.23218997361477572,
      "grad_norm": 0.14892838895320892,
      "learning_rate": 0.00018456464379947233,
      "loss": 0.0723,
      "step": 352
    },
    {
      "epoch": 0.2328496042216359,
      "grad_norm": 0.17938075959682465,
      "learning_rate": 0.00018452066842568164,
      "loss": 0.032,
      "step": 353
    },
    {
      "epoch": 0.23350923482849603,
      "grad_norm": 0.38997694849967957,
      "learning_rate": 0.00018447669305189096,
      "loss": 0.0421,
      "step": 354
    },
    {
      "epoch": 0.2341688654353562,
      "grad_norm": 0.3800145983695984,
      "learning_rate": 0.00018443271767810027,
      "loss": 0.032,
      "step": 355
    },
    {
      "epoch": 0.23482849604221637,
      "grad_norm": 0.22173549234867096,
      "learning_rate": 0.0001843887423043096,
      "loss": 0.0388,
      "step": 356
    },
    {
      "epoch": 0.2354881266490765,
      "grad_norm": 0.11998254805803299,
      "learning_rate": 0.00018434476693051892,
      "loss": 0.0358,
      "step": 357
    },
    {
      "epoch": 0.23614775725593667,
      "grad_norm": 0.1403476595878601,
      "learning_rate": 0.00018430079155672824,
      "loss": 0.044,
      "step": 358
    },
    {
      "epoch": 0.23680738786279684,
      "grad_norm": 0.11741739511489868,
      "learning_rate": 0.00018425681618293755,
      "loss": 0.0459,
      "step": 359
    },
    {
      "epoch": 0.23746701846965698,
      "grad_norm": 0.20706066489219666,
      "learning_rate": 0.0001842128408091469,
      "loss": 0.0465,
      "step": 360
    },
    {
      "epoch": 0.23812664907651715,
      "grad_norm": 0.13768641650676727,
      "learning_rate": 0.0001841688654353562,
      "loss": 0.0313,
      "step": 361
    },
    {
      "epoch": 0.23878627968337732,
      "grad_norm": 0.10456262528896332,
      "learning_rate": 0.00018412489006156552,
      "loss": 0.0326,
      "step": 362
    },
    {
      "epoch": 0.23944591029023746,
      "grad_norm": 0.06702134013175964,
      "learning_rate": 0.00018408091468777486,
      "loss": 0.0196,
      "step": 363
    },
    {
      "epoch": 0.24010554089709762,
      "grad_norm": 0.1867019683122635,
      "learning_rate": 0.00018403693931398418,
      "loss": 0.0451,
      "step": 364
    },
    {
      "epoch": 0.2407651715039578,
      "grad_norm": 0.1328548640012741,
      "learning_rate": 0.0001839929639401935,
      "loss": 0.0346,
      "step": 365
    },
    {
      "epoch": 0.24142480211081793,
      "grad_norm": 0.17640767991542816,
      "learning_rate": 0.0001839489885664028,
      "loss": 0.0573,
      "step": 366
    },
    {
      "epoch": 0.2420844327176781,
      "grad_norm": 0.08763616532087326,
      "learning_rate": 0.00018390501319261215,
      "loss": 0.0294,
      "step": 367
    },
    {
      "epoch": 0.24274406332453827,
      "grad_norm": 0.14130359888076782,
      "learning_rate": 0.00018386103781882146,
      "loss": 0.0604,
      "step": 368
    },
    {
      "epoch": 0.2434036939313984,
      "grad_norm": 0.13940328359603882,
      "learning_rate": 0.00018381706244503077,
      "loss": 0.0508,
      "step": 369
    },
    {
      "epoch": 0.24406332453825857,
      "grad_norm": 0.2298174947500229,
      "learning_rate": 0.00018377308707124011,
      "loss": 0.0238,
      "step": 370
    },
    {
      "epoch": 0.24472295514511874,
      "grad_norm": 0.25829386711120605,
      "learning_rate": 0.00018372911169744943,
      "loss": 0.0421,
      "step": 371
    },
    {
      "epoch": 0.24538258575197888,
      "grad_norm": 0.18463067710399628,
      "learning_rate": 0.00018368513632365877,
      "loss": 0.0375,
      "step": 372
    },
    {
      "epoch": 0.24604221635883905,
      "grad_norm": 0.12453096359968185,
      "learning_rate": 0.00018364116094986808,
      "loss": 0.0568,
      "step": 373
    },
    {
      "epoch": 0.24670184696569922,
      "grad_norm": 0.23674164712429047,
      "learning_rate": 0.00018359718557607742,
      "loss": 0.0561,
      "step": 374
    },
    {
      "epoch": 0.24736147757255936,
      "grad_norm": 0.09498414397239685,
      "learning_rate": 0.00018355321020228674,
      "loss": 0.0416,
      "step": 375
    },
    {
      "epoch": 0.24802110817941952,
      "grad_norm": 0.10955262929201126,
      "learning_rate": 0.00018350923482849605,
      "loss": 0.0461,
      "step": 376
    },
    {
      "epoch": 0.2486807387862797,
      "grad_norm": 0.10013560205698013,
      "learning_rate": 0.00018346525945470537,
      "loss": 0.0316,
      "step": 377
    },
    {
      "epoch": 0.24934036939313983,
      "grad_norm": 0.19489656388759613,
      "learning_rate": 0.0001834212840809147,
      "loss": 0.0108,
      "step": 378
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.15793240070343018,
      "learning_rate": 0.00018337730870712402,
      "loss": 0.0283,
      "step": 379
    },
    {
      "epoch": 0.25065963060686014,
      "grad_norm": 0.12917138636112213,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.035,
      "step": 380
    },
    {
      "epoch": 0.25131926121372034,
      "grad_norm": 0.19483838975429535,
      "learning_rate": 0.00018328935795954268,
      "loss": 0.0577,
      "step": 381
    },
    {
      "epoch": 0.2519788918205805,
      "grad_norm": 0.1283104568719864,
      "learning_rate": 0.000183245382585752,
      "loss": 0.0415,
      "step": 382
    },
    {
      "epoch": 0.2526385224274406,
      "grad_norm": 0.1305508315563202,
      "learning_rate": 0.0001832014072119613,
      "loss": 0.0348,
      "step": 383
    },
    {
      "epoch": 0.2532981530343008,
      "grad_norm": 0.056548215448856354,
      "learning_rate": 0.00018315743183817062,
      "loss": 0.0103,
      "step": 384
    },
    {
      "epoch": 0.25395778364116095,
      "grad_norm": 0.17468403279781342,
      "learning_rate": 0.00018311345646437996,
      "loss": 0.0323,
      "step": 385
    },
    {
      "epoch": 0.2546174142480211,
      "grad_norm": 0.20664536952972412,
      "learning_rate": 0.00018306948109058927,
      "loss": 0.0399,
      "step": 386
    },
    {
      "epoch": 0.2552770448548813,
      "grad_norm": 0.38402506709098816,
      "learning_rate": 0.0001830255057167986,
      "loss": 0.0811,
      "step": 387
    },
    {
      "epoch": 0.2559366754617414,
      "grad_norm": 0.3310551047325134,
      "learning_rate": 0.00018298153034300793,
      "loss": 0.0484,
      "step": 388
    },
    {
      "epoch": 0.25659630606860157,
      "grad_norm": 0.12981268763542175,
      "learning_rate": 0.00018293755496921724,
      "loss": 0.0246,
      "step": 389
    },
    {
      "epoch": 0.25725593667546176,
      "grad_norm": 0.2896742820739746,
      "learning_rate": 0.00018289357959542656,
      "loss": 0.0268,
      "step": 390
    },
    {
      "epoch": 0.2579155672823219,
      "grad_norm": 0.4253804385662079,
      "learning_rate": 0.0001828496042216359,
      "loss": 0.0385,
      "step": 391
    },
    {
      "epoch": 0.25857519788918204,
      "grad_norm": 0.37423262000083923,
      "learning_rate": 0.0001828056288478452,
      "loss": 0.039,
      "step": 392
    },
    {
      "epoch": 0.25923482849604224,
      "grad_norm": 0.12024305015802383,
      "learning_rate": 0.00018276165347405455,
      "loss": 0.0325,
      "step": 393
    },
    {
      "epoch": 0.2598944591029024,
      "grad_norm": 0.0844896212220192,
      "learning_rate": 0.00018271767810026387,
      "loss": 0.0201,
      "step": 394
    },
    {
      "epoch": 0.2605540897097625,
      "grad_norm": 0.1823677122592926,
      "learning_rate": 0.0001826737027264732,
      "loss": 0.0407,
      "step": 395
    },
    {
      "epoch": 0.2612137203166227,
      "grad_norm": 0.11427386105060577,
      "learning_rate": 0.00018262972735268252,
      "loss": 0.0212,
      "step": 396
    },
    {
      "epoch": 0.26187335092348285,
      "grad_norm": 0.04018102213740349,
      "learning_rate": 0.00018258575197889184,
      "loss": 0.002,
      "step": 397
    },
    {
      "epoch": 0.262532981530343,
      "grad_norm": 0.07761936634778976,
      "learning_rate": 0.00018254177660510115,
      "loss": 0.0227,
      "step": 398
    },
    {
      "epoch": 0.2631926121372032,
      "grad_norm": 0.20973189175128937,
      "learning_rate": 0.0001824978012313105,
      "loss": 0.0513,
      "step": 399
    },
    {
      "epoch": 0.2638522427440633,
      "grad_norm": 0.07394681870937347,
      "learning_rate": 0.0001824538258575198,
      "loss": 0.0119,
      "step": 400
    },
    {
      "epoch": 0.26451187335092347,
      "grad_norm": 0.35939738154411316,
      "learning_rate": 0.00018240985048372912,
      "loss": 0.0913,
      "step": 401
    },
    {
      "epoch": 0.26517150395778366,
      "grad_norm": 0.0686757043004036,
      "learning_rate": 0.00018236587510993843,
      "loss": 0.0164,
      "step": 402
    },
    {
      "epoch": 0.2658311345646438,
      "grad_norm": 0.03519360348582268,
      "learning_rate": 0.00018232189973614777,
      "loss": 0.0019,
      "step": 403
    },
    {
      "epoch": 0.26649076517150394,
      "grad_norm": 0.13784541189670563,
      "learning_rate": 0.0001822779243623571,
      "loss": 0.0432,
      "step": 404
    },
    {
      "epoch": 0.26715039577836414,
      "grad_norm": 0.20169013738632202,
      "learning_rate": 0.0001822339489885664,
      "loss": 0.0486,
      "step": 405
    },
    {
      "epoch": 0.2678100263852243,
      "grad_norm": 0.08839768171310425,
      "learning_rate": 0.00018218997361477574,
      "loss": 0.0051,
      "step": 406
    },
    {
      "epoch": 0.2684696569920844,
      "grad_norm": 0.1112482026219368,
      "learning_rate": 0.00018214599824098506,
      "loss": 0.0228,
      "step": 407
    },
    {
      "epoch": 0.2691292875989446,
      "grad_norm": 0.12330775707960129,
      "learning_rate": 0.00018210202286719437,
      "loss": 0.0428,
      "step": 408
    },
    {
      "epoch": 0.26978891820580475,
      "grad_norm": 0.1506921499967575,
      "learning_rate": 0.00018205804749340368,
      "loss": 0.0176,
      "step": 409
    },
    {
      "epoch": 0.2704485488126649,
      "grad_norm": 0.1967235952615738,
      "learning_rate": 0.00018201407211961303,
      "loss": 0.0241,
      "step": 410
    },
    {
      "epoch": 0.2711081794195251,
      "grad_norm": 0.10750004649162292,
      "learning_rate": 0.00018197009674582234,
      "loss": 0.0166,
      "step": 411
    },
    {
      "epoch": 0.2717678100263852,
      "grad_norm": 0.10917887836694717,
      "learning_rate": 0.00018192612137203165,
      "loss": 0.0318,
      "step": 412
    },
    {
      "epoch": 0.27242744063324537,
      "grad_norm": 0.13071107864379883,
      "learning_rate": 0.000181882145998241,
      "loss": 0.0611,
      "step": 413
    },
    {
      "epoch": 0.27308707124010556,
      "grad_norm": 0.1297711580991745,
      "learning_rate": 0.0001818381706244503,
      "loss": 0.0453,
      "step": 414
    },
    {
      "epoch": 0.2737467018469657,
      "grad_norm": 0.12107842415571213,
      "learning_rate": 0.00018179419525065965,
      "loss": 0.0069,
      "step": 415
    },
    {
      "epoch": 0.27440633245382584,
      "grad_norm": 0.09917732328176498,
      "learning_rate": 0.00018175021987686896,
      "loss": 0.0072,
      "step": 416
    },
    {
      "epoch": 0.27506596306068604,
      "grad_norm": 0.12330090254545212,
      "learning_rate": 0.0001817062445030783,
      "loss": 0.0223,
      "step": 417
    },
    {
      "epoch": 0.2757255936675462,
      "grad_norm": 0.08666965365409851,
      "learning_rate": 0.00018166226912928762,
      "loss": 0.0201,
      "step": 418
    },
    {
      "epoch": 0.2763852242744063,
      "grad_norm": 0.2583370804786682,
      "learning_rate": 0.00018161829375549693,
      "loss": 0.0525,
      "step": 419
    },
    {
      "epoch": 0.2770448548812665,
      "grad_norm": 0.28555062413215637,
      "learning_rate": 0.00018157431838170625,
      "loss": 0.06,
      "step": 420
    },
    {
      "epoch": 0.27770448548812665,
      "grad_norm": 0.2656480669975281,
      "learning_rate": 0.0001815303430079156,
      "loss": 0.0764,
      "step": 421
    },
    {
      "epoch": 0.2783641160949868,
      "grad_norm": 0.15808185935020447,
      "learning_rate": 0.0001814863676341249,
      "loss": 0.0446,
      "step": 422
    },
    {
      "epoch": 0.279023746701847,
      "grad_norm": 0.08544658124446869,
      "learning_rate": 0.00018144239226033422,
      "loss": 0.0265,
      "step": 423
    },
    {
      "epoch": 0.2796833773087071,
      "grad_norm": 0.10644915699958801,
      "learning_rate": 0.00018139841688654356,
      "loss": 0.0384,
      "step": 424
    },
    {
      "epoch": 0.28034300791556727,
      "grad_norm": 0.14895306527614594,
      "learning_rate": 0.00018135444151275287,
      "loss": 0.0252,
      "step": 425
    },
    {
      "epoch": 0.28100263852242746,
      "grad_norm": 0.13604417443275452,
      "learning_rate": 0.00018131046613896218,
      "loss": 0.0525,
      "step": 426
    },
    {
      "epoch": 0.2816622691292876,
      "grad_norm": 0.172395259141922,
      "learning_rate": 0.0001812664907651715,
      "loss": 0.0418,
      "step": 427
    },
    {
      "epoch": 0.28232189973614774,
      "grad_norm": 0.09892599284648895,
      "learning_rate": 0.00018122251539138084,
      "loss": 0.0255,
      "step": 428
    },
    {
      "epoch": 0.28298153034300794,
      "grad_norm": 0.16506482660770416,
      "learning_rate": 0.00018117854001759015,
      "loss": 0.0659,
      "step": 429
    },
    {
      "epoch": 0.2836411609498681,
      "grad_norm": 0.2031330168247223,
      "learning_rate": 0.00018113456464379947,
      "loss": 0.0661,
      "step": 430
    },
    {
      "epoch": 0.2843007915567282,
      "grad_norm": 0.129147008061409,
      "learning_rate": 0.0001810905892700088,
      "loss": 0.0214,
      "step": 431
    },
    {
      "epoch": 0.2849604221635884,
      "grad_norm": 0.12830795347690582,
      "learning_rate": 0.00018104661389621812,
      "loss": 0.034,
      "step": 432
    },
    {
      "epoch": 0.28562005277044855,
      "grad_norm": 0.11248774081468582,
      "learning_rate": 0.00018100263852242744,
      "loss": 0.0248,
      "step": 433
    },
    {
      "epoch": 0.2862796833773087,
      "grad_norm": 0.07769862562417984,
      "learning_rate": 0.00018095866314863678,
      "loss": 0.0281,
      "step": 434
    },
    {
      "epoch": 0.2869393139841689,
      "grad_norm": 0.17599773406982422,
      "learning_rate": 0.0001809146877748461,
      "loss": 0.0665,
      "step": 435
    },
    {
      "epoch": 0.287598944591029,
      "grad_norm": 0.11136054247617722,
      "learning_rate": 0.00018087071240105543,
      "loss": 0.0317,
      "step": 436
    },
    {
      "epoch": 0.28825857519788917,
      "grad_norm": 0.11924675107002258,
      "learning_rate": 0.00018082673702726475,
      "loss": 0.0267,
      "step": 437
    },
    {
      "epoch": 0.28891820580474936,
      "grad_norm": 0.12109523266553879,
      "learning_rate": 0.00018078276165347406,
      "loss": 0.0424,
      "step": 438
    },
    {
      "epoch": 0.2895778364116095,
      "grad_norm": 0.3063255846500397,
      "learning_rate": 0.0001807387862796834,
      "loss": 0.0503,
      "step": 439
    },
    {
      "epoch": 0.29023746701846964,
      "grad_norm": 0.2281317263841629,
      "learning_rate": 0.00018069481090589272,
      "loss": 0.0584,
      "step": 440
    },
    {
      "epoch": 0.29089709762532984,
      "grad_norm": 0.11766545474529266,
      "learning_rate": 0.00018065083553210203,
      "loss": 0.0254,
      "step": 441
    },
    {
      "epoch": 0.29155672823219,
      "grad_norm": 0.16301755607128143,
      "learning_rate": 0.00018060686015831137,
      "loss": 0.0642,
      "step": 442
    },
    {
      "epoch": 0.2922163588390501,
      "grad_norm": 0.22418160736560822,
      "learning_rate": 0.00018056288478452068,
      "loss": 0.0258,
      "step": 443
    },
    {
      "epoch": 0.2928759894459103,
      "grad_norm": 0.24746108055114746,
      "learning_rate": 0.00018051890941073,
      "loss": 0.0333,
      "step": 444
    },
    {
      "epoch": 0.29353562005277045,
      "grad_norm": 0.19385595619678497,
      "learning_rate": 0.0001804749340369393,
      "loss": 0.0587,
      "step": 445
    },
    {
      "epoch": 0.2941952506596306,
      "grad_norm": 0.22688081860542297,
      "learning_rate": 0.00018043095866314865,
      "loss": 0.0227,
      "step": 446
    },
    {
      "epoch": 0.2948548812664908,
      "grad_norm": 0.08501362800598145,
      "learning_rate": 0.00018038698328935797,
      "loss": 0.0238,
      "step": 447
    },
    {
      "epoch": 0.2955145118733509,
      "grad_norm": 0.21320873498916626,
      "learning_rate": 0.00018034300791556728,
      "loss": 0.0137,
      "step": 448
    },
    {
      "epoch": 0.29617414248021107,
      "grad_norm": 0.23259323835372925,
      "learning_rate": 0.00018029903254177662,
      "loss": 0.0531,
      "step": 449
    },
    {
      "epoch": 0.29683377308707126,
      "grad_norm": 0.08675897866487503,
      "learning_rate": 0.00018025505716798594,
      "loss": 0.0212,
      "step": 450
    },
    {
      "epoch": 0.2974934036939314,
      "grad_norm": 0.15247920155525208,
      "learning_rate": 0.00018021108179419525,
      "loss": 0.0367,
      "step": 451
    },
    {
      "epoch": 0.29815303430079154,
      "grad_norm": 0.13746310770511627,
      "learning_rate": 0.00018016710642040456,
      "loss": 0.0254,
      "step": 452
    },
    {
      "epoch": 0.29881266490765174,
      "grad_norm": 0.13185934722423553,
      "learning_rate": 0.0001801231310466139,
      "loss": 0.0284,
      "step": 453
    },
    {
      "epoch": 0.2994722955145119,
      "grad_norm": 0.15614476799964905,
      "learning_rate": 0.00018007915567282322,
      "loss": 0.0368,
      "step": 454
    },
    {
      "epoch": 0.300131926121372,
      "grad_norm": 0.32293975353240967,
      "learning_rate": 0.00018003518029903253,
      "loss": 0.0667,
      "step": 455
    },
    {
      "epoch": 0.3007915567282322,
      "grad_norm": 0.14336560666561127,
      "learning_rate": 0.00017999120492524187,
      "loss": 0.0554,
      "step": 456
    },
    {
      "epoch": 0.30145118733509235,
      "grad_norm": 0.1289646178483963,
      "learning_rate": 0.00017994722955145122,
      "loss": 0.0472,
      "step": 457
    },
    {
      "epoch": 0.3021108179419525,
      "grad_norm": 0.13255007565021515,
      "learning_rate": 0.00017990325417766053,
      "loss": 0.0334,
      "step": 458
    },
    {
      "epoch": 0.3027704485488127,
      "grad_norm": 0.2818635106086731,
      "learning_rate": 0.00017985927880386984,
      "loss": 0.0283,
      "step": 459
    },
    {
      "epoch": 0.3034300791556728,
      "grad_norm": 0.5482266545295715,
      "learning_rate": 0.00017981530343007918,
      "loss": 0.0328,
      "step": 460
    },
    {
      "epoch": 0.30408970976253297,
      "grad_norm": 0.15948981046676636,
      "learning_rate": 0.0001797713280562885,
      "loss": 0.0465,
      "step": 461
    },
    {
      "epoch": 0.30474934036939316,
      "grad_norm": 0.08475402742624283,
      "learning_rate": 0.0001797273526824978,
      "loss": 0.0253,
      "step": 462
    },
    {
      "epoch": 0.3054089709762533,
      "grad_norm": 0.1172502189874649,
      "learning_rate": 0.00017968337730870713,
      "loss": 0.0274,
      "step": 463
    },
    {
      "epoch": 0.30606860158311344,
      "grad_norm": 0.13966919481754303,
      "learning_rate": 0.00017963940193491647,
      "loss": 0.0308,
      "step": 464
    },
    {
      "epoch": 0.30672823218997364,
      "grad_norm": 0.061836469918489456,
      "learning_rate": 0.00017959542656112578,
      "loss": 0.0178,
      "step": 465
    },
    {
      "epoch": 0.3073878627968338,
      "grad_norm": 0.03612888231873512,
      "learning_rate": 0.0001795514511873351,
      "loss": 0.0021,
      "step": 466
    },
    {
      "epoch": 0.3080474934036939,
      "grad_norm": 0.23663577437400818,
      "learning_rate": 0.00017950747581354444,
      "loss": 0.051,
      "step": 467
    },
    {
      "epoch": 0.3087071240105541,
      "grad_norm": 0.05052686855196953,
      "learning_rate": 0.00017946350043975375,
      "loss": 0.003,
      "step": 468
    },
    {
      "epoch": 0.30936675461741425,
      "grad_norm": 0.3238077163696289,
      "learning_rate": 0.00017941952506596306,
      "loss": 0.0819,
      "step": 469
    },
    {
      "epoch": 0.3100263852242744,
      "grad_norm": 0.05849732831120491,
      "learning_rate": 0.00017937554969217238,
      "loss": 0.0143,
      "step": 470
    },
    {
      "epoch": 0.3106860158311346,
      "grad_norm": 0.07412716001272202,
      "learning_rate": 0.00017933157431838172,
      "loss": 0.0276,
      "step": 471
    },
    {
      "epoch": 0.3113456464379947,
      "grad_norm": 0.12468355149030685,
      "learning_rate": 0.00017928759894459103,
      "loss": 0.0475,
      "step": 472
    },
    {
      "epoch": 0.31200527704485487,
      "grad_norm": 0.08822087943553925,
      "learning_rate": 0.00017924362357080035,
      "loss": 0.0283,
      "step": 473
    },
    {
      "epoch": 0.31266490765171506,
      "grad_norm": 0.10951769351959229,
      "learning_rate": 0.00017919964819700966,
      "loss": 0.0255,
      "step": 474
    },
    {
      "epoch": 0.3133245382585752,
      "grad_norm": 0.16345462203025818,
      "learning_rate": 0.000179155672823219,
      "loss": 0.0545,
      "step": 475
    },
    {
      "epoch": 0.31398416886543534,
      "grad_norm": 0.12999388575553894,
      "learning_rate": 0.00017911169744942832,
      "loss": 0.0496,
      "step": 476
    },
    {
      "epoch": 0.31464379947229554,
      "grad_norm": 0.12021619826555252,
      "learning_rate": 0.00017906772207563766,
      "loss": 0.0189,
      "step": 477
    },
    {
      "epoch": 0.3153034300791557,
      "grad_norm": 0.13588258624076843,
      "learning_rate": 0.00017902374670184697,
      "loss": 0.0486,
      "step": 478
    },
    {
      "epoch": 0.3159630606860158,
      "grad_norm": 0.13996922969818115,
      "learning_rate": 0.0001789797713280563,
      "loss": 0.0221,
      "step": 479
    },
    {
      "epoch": 0.316622691292876,
      "grad_norm": 0.14807280898094177,
      "learning_rate": 0.00017893579595426563,
      "loss": 0.0185,
      "step": 480
    },
    {
      "epoch": 0.31728232189973615,
      "grad_norm": 0.11143828183412552,
      "learning_rate": 0.00017889182058047494,
      "loss": 0.0512,
      "step": 481
    },
    {
      "epoch": 0.3179419525065963,
      "grad_norm": 0.08998066931962967,
      "learning_rate": 0.00017884784520668428,
      "loss": 0.0293,
      "step": 482
    },
    {
      "epoch": 0.3186015831134565,
      "grad_norm": 0.13245800137519836,
      "learning_rate": 0.0001788038698328936,
      "loss": 0.0441,
      "step": 483
    },
    {
      "epoch": 0.31926121372031663,
      "grad_norm": 0.14057987928390503,
      "learning_rate": 0.0001787598944591029,
      "loss": 0.0123,
      "step": 484
    },
    {
      "epoch": 0.31992084432717677,
      "grad_norm": 0.13197457790374756,
      "learning_rate": 0.00017871591908531225,
      "loss": 0.0222,
      "step": 485
    },
    {
      "epoch": 0.32058047493403696,
      "grad_norm": 0.10755467414855957,
      "learning_rate": 0.00017867194371152156,
      "loss": 0.0067,
      "step": 486
    },
    {
      "epoch": 0.3212401055408971,
      "grad_norm": 0.14179617166519165,
      "learning_rate": 0.00017862796833773088,
      "loss": 0.0204,
      "step": 487
    },
    {
      "epoch": 0.32189973614775724,
      "grad_norm": 0.18936894834041595,
      "learning_rate": 0.0001785839929639402,
      "loss": 0.0256,
      "step": 488
    },
    {
      "epoch": 0.32255936675461744,
      "grad_norm": 0.27906423807144165,
      "learning_rate": 0.00017854001759014953,
      "loss": 0.0648,
      "step": 489
    },
    {
      "epoch": 0.3232189973614776,
      "grad_norm": 0.051390640437603,
      "learning_rate": 0.00017849604221635885,
      "loss": 0.0028,
      "step": 490
    },
    {
      "epoch": 0.3238786279683377,
      "grad_norm": 0.17598767578601837,
      "learning_rate": 0.00017845206684256816,
      "loss": 0.0414,
      "step": 491
    },
    {
      "epoch": 0.3245382585751979,
      "grad_norm": 0.21808840334415436,
      "learning_rate": 0.00017840809146877747,
      "loss": 0.0541,
      "step": 492
    },
    {
      "epoch": 0.32519788918205805,
      "grad_norm": 0.16083478927612305,
      "learning_rate": 0.00017836411609498682,
      "loss": 0.0426,
      "step": 493
    },
    {
      "epoch": 0.3258575197889182,
      "grad_norm": 0.12460670620203018,
      "learning_rate": 0.00017832014072119613,
      "loss": 0.0168,
      "step": 494
    },
    {
      "epoch": 0.3265171503957784,
      "grad_norm": 0.15590853989124298,
      "learning_rate": 0.00017827616534740544,
      "loss": 0.0296,
      "step": 495
    },
    {
      "epoch": 0.32717678100263853,
      "grad_norm": 0.17553310096263885,
      "learning_rate": 0.00017823218997361478,
      "loss": 0.0382,
      "step": 496
    },
    {
      "epoch": 0.32783641160949867,
      "grad_norm": 0.20945999026298523,
      "learning_rate": 0.0001781882145998241,
      "loss": 0.0384,
      "step": 497
    },
    {
      "epoch": 0.32849604221635886,
      "grad_norm": 0.2520866096019745,
      "learning_rate": 0.00017814423922603344,
      "loss": 0.0341,
      "step": 498
    },
    {
      "epoch": 0.329155672823219,
      "grad_norm": 0.17950865626335144,
      "learning_rate": 0.00017810026385224275,
      "loss": 0.0429,
      "step": 499
    },
    {
      "epoch": 0.32981530343007914,
      "grad_norm": 0.1381823718547821,
      "learning_rate": 0.0001780562884784521,
      "loss": 0.0315,
      "step": 500
    },
    {
      "epoch": 0.33047493403693934,
      "grad_norm": 0.12731702625751495,
      "learning_rate": 0.0001780123131046614,
      "loss": 0.0068,
      "step": 501
    },
    {
      "epoch": 0.3311345646437995,
      "grad_norm": 0.14003118872642517,
      "learning_rate": 0.00017796833773087072,
      "loss": 0.0162,
      "step": 502
    },
    {
      "epoch": 0.3317941952506596,
      "grad_norm": 0.09797767549753189,
      "learning_rate": 0.00017792436235708006,
      "loss": 0.0312,
      "step": 503
    },
    {
      "epoch": 0.3324538258575198,
      "grad_norm": 0.11008993536233902,
      "learning_rate": 0.00017788038698328938,
      "loss": 0.0107,
      "step": 504
    },
    {
      "epoch": 0.33311345646437995,
      "grad_norm": 0.41574472188949585,
      "learning_rate": 0.0001778364116094987,
      "loss": 0.0911,
      "step": 505
    },
    {
      "epoch": 0.3337730870712401,
      "grad_norm": 0.1508847177028656,
      "learning_rate": 0.000177792436235708,
      "loss": 0.0504,
      "step": 506
    },
    {
      "epoch": 0.3344327176781003,
      "grad_norm": 0.2990347743034363,
      "learning_rate": 0.00017774846086191735,
      "loss": 0.061,
      "step": 507
    },
    {
      "epoch": 0.33509234828496043,
      "grad_norm": 0.22952520847320557,
      "learning_rate": 0.00017770448548812666,
      "loss": 0.0588,
      "step": 508
    },
    {
      "epoch": 0.33575197889182057,
      "grad_norm": 0.3681064546108246,
      "learning_rate": 0.00017766051011433597,
      "loss": 0.0944,
      "step": 509
    },
    {
      "epoch": 0.33641160949868076,
      "grad_norm": 0.1399623304605484,
      "learning_rate": 0.0001776165347405453,
      "loss": 0.03,
      "step": 510
    },
    {
      "epoch": 0.3370712401055409,
      "grad_norm": 0.25820067524909973,
      "learning_rate": 0.00017757255936675463,
      "loss": 0.0674,
      "step": 511
    },
    {
      "epoch": 0.33773087071240104,
      "grad_norm": 0.32180172204971313,
      "learning_rate": 0.00017752858399296394,
      "loss": 0.0424,
      "step": 512
    },
    {
      "epoch": 0.33839050131926124,
      "grad_norm": 0.22695939242839813,
      "learning_rate": 0.00017748460861917326,
      "loss": 0.0393,
      "step": 513
    },
    {
      "epoch": 0.3390501319261214,
      "grad_norm": 0.2922760546207428,
      "learning_rate": 0.0001774406332453826,
      "loss": 0.0505,
      "step": 514
    },
    {
      "epoch": 0.3397097625329815,
      "grad_norm": 0.20311327278614044,
      "learning_rate": 0.0001773966578715919,
      "loss": 0.0252,
      "step": 515
    },
    {
      "epoch": 0.3403693931398417,
      "grad_norm": 0.15219224989414215,
      "learning_rate": 0.00017735268249780123,
      "loss": 0.0102,
      "step": 516
    },
    {
      "epoch": 0.34102902374670185,
      "grad_norm": 0.11788099259138107,
      "learning_rate": 0.00017730870712401054,
      "loss": 0.0469,
      "step": 517
    },
    {
      "epoch": 0.341688654353562,
      "grad_norm": 0.1571049839258194,
      "learning_rate": 0.00017726473175021988,
      "loss": 0.0323,
      "step": 518
    },
    {
      "epoch": 0.3423482849604222,
      "grad_norm": 0.1453552544116974,
      "learning_rate": 0.0001772207563764292,
      "loss": 0.0434,
      "step": 519
    },
    {
      "epoch": 0.34300791556728233,
      "grad_norm": 0.08905339986085892,
      "learning_rate": 0.00017717678100263854,
      "loss": 0.0175,
      "step": 520
    },
    {
      "epoch": 0.34366754617414247,
      "grad_norm": 0.33609575033187866,
      "learning_rate": 0.00017713280562884785,
      "loss": 0.0956,
      "step": 521
    },
    {
      "epoch": 0.34432717678100266,
      "grad_norm": 0.08100952953100204,
      "learning_rate": 0.0001770888302550572,
      "loss": 0.0143,
      "step": 522
    },
    {
      "epoch": 0.3449868073878628,
      "grad_norm": 0.07588719576597214,
      "learning_rate": 0.0001770448548812665,
      "loss": 0.0155,
      "step": 523
    },
    {
      "epoch": 0.34564643799472294,
      "grad_norm": 0.16540296375751495,
      "learning_rate": 0.00017700087950747582,
      "loss": 0.0559,
      "step": 524
    },
    {
      "epoch": 0.34630606860158314,
      "grad_norm": 0.060502782464027405,
      "learning_rate": 0.00017695690413368516,
      "loss": 0.017,
      "step": 525
    },
    {
      "epoch": 0.3469656992084433,
      "grad_norm": 0.2910462021827698,
      "learning_rate": 0.00017691292875989447,
      "loss": 0.0967,
      "step": 526
    },
    {
      "epoch": 0.3476253298153034,
      "grad_norm": 0.203109472990036,
      "learning_rate": 0.0001768689533861038,
      "loss": 0.0667,
      "step": 527
    },
    {
      "epoch": 0.3482849604221636,
      "grad_norm": 0.11342716217041016,
      "learning_rate": 0.0001768249780123131,
      "loss": 0.0342,
      "step": 528
    },
    {
      "epoch": 0.34894459102902375,
      "grad_norm": 0.09255530685186386,
      "learning_rate": 0.00017678100263852244,
      "loss": 0.0304,
      "step": 529
    },
    {
      "epoch": 0.3496042216358839,
      "grad_norm": 0.1405492126941681,
      "learning_rate": 0.00017673702726473176,
      "loss": 0.0598,
      "step": 530
    },
    {
      "epoch": 0.3502638522427441,
      "grad_norm": 0.2468791902065277,
      "learning_rate": 0.00017669305189094107,
      "loss": 0.0291,
      "step": 531
    },
    {
      "epoch": 0.35092348284960423,
      "grad_norm": 0.2715703547000885,
      "learning_rate": 0.0001766490765171504,
      "loss": 0.0629,
      "step": 532
    },
    {
      "epoch": 0.35158311345646437,
      "grad_norm": 0.27965304255485535,
      "learning_rate": 0.00017660510114335973,
      "loss": 0.0498,
      "step": 533
    },
    {
      "epoch": 0.35224274406332456,
      "grad_norm": 0.11333087831735611,
      "learning_rate": 0.00017656112576956904,
      "loss": 0.0634,
      "step": 534
    },
    {
      "epoch": 0.3529023746701847,
      "grad_norm": 0.09166604280471802,
      "learning_rate": 0.00017651715039577835,
      "loss": 0.0514,
      "step": 535
    },
    {
      "epoch": 0.35356200527704484,
      "grad_norm": 0.16664843261241913,
      "learning_rate": 0.0001764731750219877,
      "loss": 0.0131,
      "step": 536
    },
    {
      "epoch": 0.35422163588390504,
      "grad_norm": 0.06952808052301407,
      "learning_rate": 0.000176429199648197,
      "loss": 0.0145,
      "step": 537
    },
    {
      "epoch": 0.3548812664907652,
      "grad_norm": 0.10104518383741379,
      "learning_rate": 0.00017638522427440632,
      "loss": 0.0209,
      "step": 538
    },
    {
      "epoch": 0.3555408970976253,
      "grad_norm": 0.16494765877723694,
      "learning_rate": 0.00017634124890061566,
      "loss": 0.0565,
      "step": 539
    },
    {
      "epoch": 0.3562005277044855,
      "grad_norm": 0.153669536113739,
      "learning_rate": 0.00017629727352682498,
      "loss": 0.0383,
      "step": 540
    },
    {
      "epoch": 0.35686015831134565,
      "grad_norm": 0.09522706270217896,
      "learning_rate": 0.00017625329815303432,
      "loss": 0.0306,
      "step": 541
    },
    {
      "epoch": 0.3575197889182058,
      "grad_norm": 0.12431294471025467,
      "learning_rate": 0.00017620932277924363,
      "loss": 0.0405,
      "step": 542
    },
    {
      "epoch": 0.358179419525066,
      "grad_norm": 0.2440674901008606,
      "learning_rate": 0.00017616534740545297,
      "loss": 0.0704,
      "step": 543
    },
    {
      "epoch": 0.35883905013192613,
      "grad_norm": 0.08420421928167343,
      "learning_rate": 0.0001761213720316623,
      "loss": 0.0396,
      "step": 544
    },
    {
      "epoch": 0.35949868073878627,
      "grad_norm": 0.05449569225311279,
      "learning_rate": 0.0001760773966578716,
      "loss": 0.0166,
      "step": 545
    },
    {
      "epoch": 0.36015831134564646,
      "grad_norm": 0.1502639204263687,
      "learning_rate": 0.00017603342128408092,
      "loss": 0.0381,
      "step": 546
    },
    {
      "epoch": 0.3608179419525066,
      "grad_norm": 0.09017304331064224,
      "learning_rate": 0.00017598944591029026,
      "loss": 0.0193,
      "step": 547
    },
    {
      "epoch": 0.36147757255936674,
      "grad_norm": 0.07766924798488617,
      "learning_rate": 0.00017594547053649957,
      "loss": 0.0141,
      "step": 548
    },
    {
      "epoch": 0.36213720316622694,
      "grad_norm": 0.6279740333557129,
      "learning_rate": 0.00017590149516270889,
      "loss": 0.058,
      "step": 549
    },
    {
      "epoch": 0.3627968337730871,
      "grad_norm": 0.15317709743976593,
      "learning_rate": 0.00017585751978891823,
      "loss": 0.0108,
      "step": 550
    },
    {
      "epoch": 0.3634564643799472,
      "grad_norm": 0.15768329799175262,
      "learning_rate": 0.00017581354441512754,
      "loss": 0.0111,
      "step": 551
    },
    {
      "epoch": 0.3641160949868074,
      "grad_norm": 0.15749453008174896,
      "learning_rate": 0.00017576956904133685,
      "loss": 0.0114,
      "step": 552
    },
    {
      "epoch": 0.36477572559366755,
      "grad_norm": 0.08472646772861481,
      "learning_rate": 0.00017572559366754617,
      "loss": 0.0194,
      "step": 553
    },
    {
      "epoch": 0.3654353562005277,
      "grad_norm": 0.08876559138298035,
      "learning_rate": 0.0001756816182937555,
      "loss": 0.0108,
      "step": 554
    },
    {
      "epoch": 0.3660949868073879,
      "grad_norm": 0.1103358343243599,
      "learning_rate": 0.00017563764291996482,
      "loss": 0.0186,
      "step": 555
    },
    {
      "epoch": 0.36675461741424803,
      "grad_norm": 0.053025148808956146,
      "learning_rate": 0.00017559366754617414,
      "loss": 0.009,
      "step": 556
    },
    {
      "epoch": 0.36741424802110817,
      "grad_norm": 0.24557147920131683,
      "learning_rate": 0.00017554969217238348,
      "loss": 0.0402,
      "step": 557
    },
    {
      "epoch": 0.36807387862796836,
      "grad_norm": 0.24285441637039185,
      "learning_rate": 0.0001755057167985928,
      "loss": 0.0444,
      "step": 558
    },
    {
      "epoch": 0.3687335092348285,
      "grad_norm": 0.11464741080999374,
      "learning_rate": 0.0001754617414248021,
      "loss": 0.0217,
      "step": 559
    },
    {
      "epoch": 0.36939313984168864,
      "grad_norm": 0.1534205973148346,
      "learning_rate": 0.00017541776605101142,
      "loss": 0.0434,
      "step": 560
    },
    {
      "epoch": 0.37005277044854884,
      "grad_norm": 0.1622535139322281,
      "learning_rate": 0.00017537379067722076,
      "loss": 0.0205,
      "step": 561
    },
    {
      "epoch": 0.370712401055409,
      "grad_norm": 0.09242672473192215,
      "learning_rate": 0.0001753298153034301,
      "loss": 0.0072,
      "step": 562
    },
    {
      "epoch": 0.3713720316622691,
      "grad_norm": 0.1214994490146637,
      "learning_rate": 0.00017528583992963942,
      "loss": 0.0102,
      "step": 563
    },
    {
      "epoch": 0.3720316622691293,
      "grad_norm": 0.18668735027313232,
      "learning_rate": 0.00017524186455584873,
      "loss": 0.0318,
      "step": 564
    },
    {
      "epoch": 0.37269129287598945,
      "grad_norm": 0.3094939887523651,
      "learning_rate": 0.00017519788918205807,
      "loss": 0.0854,
      "step": 565
    },
    {
      "epoch": 0.3733509234828496,
      "grad_norm": 0.11820654571056366,
      "learning_rate": 0.00017515391380826739,
      "loss": 0.0071,
      "step": 566
    },
    {
      "epoch": 0.3740105540897098,
      "grad_norm": 0.08634255826473236,
      "learning_rate": 0.0001751099384344767,
      "loss": 0.0181,
      "step": 567
    },
    {
      "epoch": 0.37467018469656993,
      "grad_norm": 0.11999626457691193,
      "learning_rate": 0.00017506596306068604,
      "loss": 0.0099,
      "step": 568
    },
    {
      "epoch": 0.37532981530343007,
      "grad_norm": 0.12005443125963211,
      "learning_rate": 0.00017502198768689535,
      "loss": 0.0186,
      "step": 569
    },
    {
      "epoch": 0.3759894459102902,
      "grad_norm": 0.29930341243743896,
      "learning_rate": 0.00017497801231310467,
      "loss": 0.063,
      "step": 570
    },
    {
      "epoch": 0.3766490765171504,
      "grad_norm": 0.13971397280693054,
      "learning_rate": 0.00017493403693931398,
      "loss": 0.0493,
      "step": 571
    },
    {
      "epoch": 0.37730870712401055,
      "grad_norm": 0.05365758016705513,
      "learning_rate": 0.00017489006156552332,
      "loss": 0.009,
      "step": 572
    },
    {
      "epoch": 0.3779683377308707,
      "grad_norm": 0.11153816431760788,
      "learning_rate": 0.00017484608619173264,
      "loss": 0.0342,
      "step": 573
    },
    {
      "epoch": 0.3786279683377309,
      "grad_norm": 0.1781376749277115,
      "learning_rate": 0.00017480211081794195,
      "loss": 0.0602,
      "step": 574
    },
    {
      "epoch": 0.379287598944591,
      "grad_norm": 0.12304244935512543,
      "learning_rate": 0.0001747581354441513,
      "loss": 0.017,
      "step": 575
    },
    {
      "epoch": 0.37994722955145116,
      "grad_norm": 0.13509978353977203,
      "learning_rate": 0.0001747141600703606,
      "loss": 0.0254,
      "step": 576
    },
    {
      "epoch": 0.38060686015831136,
      "grad_norm": 0.09458523243665695,
      "learning_rate": 0.00017467018469656992,
      "loss": 0.0293,
      "step": 577
    },
    {
      "epoch": 0.3812664907651715,
      "grad_norm": 0.12338078767061234,
      "learning_rate": 0.00017462620932277923,
      "loss": 0.0308,
      "step": 578
    },
    {
      "epoch": 0.38192612137203164,
      "grad_norm": 0.1271258145570755,
      "learning_rate": 0.00017458223394898858,
      "loss": 0.0249,
      "step": 579
    },
    {
      "epoch": 0.38258575197889183,
      "grad_norm": 0.13460206985473633,
      "learning_rate": 0.0001745382585751979,
      "loss": 0.0436,
      "step": 580
    },
    {
      "epoch": 0.38324538258575197,
      "grad_norm": 0.10893967747688293,
      "learning_rate": 0.0001744942832014072,
      "loss": 0.0276,
      "step": 581
    },
    {
      "epoch": 0.3839050131926121,
      "grad_norm": 0.14189521968364716,
      "learning_rate": 0.00017445030782761654,
      "loss": 0.0313,
      "step": 582
    },
    {
      "epoch": 0.3845646437994723,
      "grad_norm": 0.13459742069244385,
      "learning_rate": 0.00017440633245382586,
      "loss": 0.0196,
      "step": 583
    },
    {
      "epoch": 0.38522427440633245,
      "grad_norm": 0.12679463624954224,
      "learning_rate": 0.0001743623570800352,
      "loss": 0.0424,
      "step": 584
    },
    {
      "epoch": 0.3858839050131926,
      "grad_norm": 0.059193387627601624,
      "learning_rate": 0.00017431838170624451,
      "loss": 0.0063,
      "step": 585
    },
    {
      "epoch": 0.3865435356200528,
      "grad_norm": 0.1217634454369545,
      "learning_rate": 0.00017427440633245385,
      "loss": 0.047,
      "step": 586
    },
    {
      "epoch": 0.3872031662269129,
      "grad_norm": 0.05458416789770126,
      "learning_rate": 0.00017423043095866317,
      "loss": 0.0114,
      "step": 587
    },
    {
      "epoch": 0.38786279683377306,
      "grad_norm": 0.0850694552063942,
      "learning_rate": 0.00017418645558487248,
      "loss": 0.022,
      "step": 588
    },
    {
      "epoch": 0.38852242744063326,
      "grad_norm": 0.26102539896965027,
      "learning_rate": 0.0001741424802110818,
      "loss": 0.0728,
      "step": 589
    },
    {
      "epoch": 0.3891820580474934,
      "grad_norm": 0.23015743494033813,
      "learning_rate": 0.00017409850483729114,
      "loss": 0.034,
      "step": 590
    },
    {
      "epoch": 0.38984168865435354,
      "grad_norm": 0.05954989045858383,
      "learning_rate": 0.00017405452946350045,
      "loss": 0.0034,
      "step": 591
    },
    {
      "epoch": 0.39050131926121373,
      "grad_norm": 0.11615171283483505,
      "learning_rate": 0.00017401055408970977,
      "loss": 0.0445,
      "step": 592
    },
    {
      "epoch": 0.39116094986807387,
      "grad_norm": 0.39702269434928894,
      "learning_rate": 0.0001739665787159191,
      "loss": 0.0864,
      "step": 593
    },
    {
      "epoch": 0.391820580474934,
      "grad_norm": 0.1561075896024704,
      "learning_rate": 0.00017392260334212842,
      "loss": 0.0431,
      "step": 594
    },
    {
      "epoch": 0.3924802110817942,
      "grad_norm": 0.16138938069343567,
      "learning_rate": 0.00017387862796833773,
      "loss": 0.0366,
      "step": 595
    },
    {
      "epoch": 0.39313984168865435,
      "grad_norm": 0.16959801316261292,
      "learning_rate": 0.00017383465259454705,
      "loss": 0.0396,
      "step": 596
    },
    {
      "epoch": 0.3937994722955145,
      "grad_norm": 0.19846434891223907,
      "learning_rate": 0.0001737906772207564,
      "loss": 0.0249,
      "step": 597
    },
    {
      "epoch": 0.3944591029023747,
      "grad_norm": 0.27855753898620605,
      "learning_rate": 0.0001737467018469657,
      "loss": 0.041,
      "step": 598
    },
    {
      "epoch": 0.3951187335092348,
      "grad_norm": 0.2733895778656006,
      "learning_rate": 0.00017370272647317502,
      "loss": 0.0342,
      "step": 599
    },
    {
      "epoch": 0.39577836411609496,
      "grad_norm": 0.19528038799762726,
      "learning_rate": 0.00017365875109938433,
      "loss": 0.0654,
      "step": 600
    },
    {
      "epoch": 0.39643799472295516,
      "grad_norm": 0.12669987976551056,
      "learning_rate": 0.00017361477572559367,
      "loss": 0.025,
      "step": 601
    },
    {
      "epoch": 0.3970976253298153,
      "grad_norm": 0.16816918551921844,
      "learning_rate": 0.000173570800351803,
      "loss": 0.0268,
      "step": 602
    },
    {
      "epoch": 0.39775725593667544,
      "grad_norm": 0.14748910069465637,
      "learning_rate": 0.00017352682497801233,
      "loss": 0.0374,
      "step": 603
    },
    {
      "epoch": 0.39841688654353563,
      "grad_norm": 0.11140201985836029,
      "learning_rate": 0.00017348284960422164,
      "loss": 0.02,
      "step": 604
    },
    {
      "epoch": 0.39907651715039577,
      "grad_norm": 0.18530671298503876,
      "learning_rate": 0.00017343887423043098,
      "loss": 0.0437,
      "step": 605
    },
    {
      "epoch": 0.3997361477572559,
      "grad_norm": 0.09564288705587387,
      "learning_rate": 0.0001733948988566403,
      "loss": 0.0264,
      "step": 606
    },
    {
      "epoch": 0.4003957783641161,
      "grad_norm": 0.11248395591974258,
      "learning_rate": 0.0001733509234828496,
      "loss": 0.0372,
      "step": 607
    },
    {
      "epoch": 0.40105540897097625,
      "grad_norm": 0.26821836829185486,
      "learning_rate": 0.00017330694810905895,
      "loss": 0.0685,
      "step": 608
    },
    {
      "epoch": 0.4017150395778364,
      "grad_norm": 0.1726316213607788,
      "learning_rate": 0.00017326297273526827,
      "loss": 0.058,
      "step": 609
    },
    {
      "epoch": 0.4023746701846966,
      "grad_norm": 0.14271146059036255,
      "learning_rate": 0.00017321899736147758,
      "loss": 0.0431,
      "step": 610
    },
    {
      "epoch": 0.4030343007915567,
      "grad_norm": 0.12576010823249817,
      "learning_rate": 0.00017317502198768692,
      "loss": 0.0493,
      "step": 611
    },
    {
      "epoch": 0.40369393139841686,
      "grad_norm": 0.10740651935338974,
      "learning_rate": 0.00017313104661389623,
      "loss": 0.029,
      "step": 612
    },
    {
      "epoch": 0.40435356200527706,
      "grad_norm": 0.10534904152154922,
      "learning_rate": 0.00017308707124010555,
      "loss": 0.0501,
      "step": 613
    },
    {
      "epoch": 0.4050131926121372,
      "grad_norm": 0.14784583449363708,
      "learning_rate": 0.00017304309586631486,
      "loss": 0.0159,
      "step": 614
    },
    {
      "epoch": 0.40567282321899734,
      "grad_norm": 0.1654161810874939,
      "learning_rate": 0.0001729991204925242,
      "loss": 0.0274,
      "step": 615
    },
    {
      "epoch": 0.40633245382585753,
      "grad_norm": 0.0964420959353447,
      "learning_rate": 0.00017295514511873352,
      "loss": 0.0306,
      "step": 616
    },
    {
      "epoch": 0.40699208443271767,
      "grad_norm": 0.10248211771249771,
      "learning_rate": 0.00017291116974494283,
      "loss": 0.0156,
      "step": 617
    },
    {
      "epoch": 0.4076517150395778,
      "grad_norm": 0.12284287810325623,
      "learning_rate": 0.00017286719437115215,
      "loss": 0.0201,
      "step": 618
    },
    {
      "epoch": 0.408311345646438,
      "grad_norm": 0.2426195740699768,
      "learning_rate": 0.0001728232189973615,
      "loss": 0.0824,
      "step": 619
    },
    {
      "epoch": 0.40897097625329815,
      "grad_norm": 0.09909763187170029,
      "learning_rate": 0.0001727792436235708,
      "loss": 0.0066,
      "step": 620
    },
    {
      "epoch": 0.4096306068601583,
      "grad_norm": 0.0986630916595459,
      "learning_rate": 0.00017273526824978011,
      "loss": 0.0231,
      "step": 621
    },
    {
      "epoch": 0.4102902374670185,
      "grad_norm": 0.09607844054698944,
      "learning_rate": 0.00017269129287598946,
      "loss": 0.0254,
      "step": 622
    },
    {
      "epoch": 0.4109498680738786,
      "grad_norm": 0.13871046900749207,
      "learning_rate": 0.00017264731750219877,
      "loss": 0.0355,
      "step": 623
    },
    {
      "epoch": 0.41160949868073876,
      "grad_norm": 0.3096542954444885,
      "learning_rate": 0.00017260334212840808,
      "loss": 0.0903,
      "step": 624
    },
    {
      "epoch": 0.41226912928759896,
      "grad_norm": 0.16437563300132751,
      "learning_rate": 0.00017255936675461742,
      "loss": 0.0239,
      "step": 625
    },
    {
      "epoch": 0.4129287598944591,
      "grad_norm": 0.14544640481472015,
      "learning_rate": 0.00017251539138082674,
      "loss": 0.0498,
      "step": 626
    },
    {
      "epoch": 0.41358839050131924,
      "grad_norm": 0.10620558261871338,
      "learning_rate": 0.00017247141600703608,
      "loss": 0.0281,
      "step": 627
    },
    {
      "epoch": 0.41424802110817943,
      "grad_norm": 0.13954584300518036,
      "learning_rate": 0.0001724274406332454,
      "loss": 0.01,
      "step": 628
    },
    {
      "epoch": 0.41490765171503957,
      "grad_norm": 0.12443288415670395,
      "learning_rate": 0.00017238346525945473,
      "loss": 0.0082,
      "step": 629
    },
    {
      "epoch": 0.4155672823218997,
      "grad_norm": 0.08447328209877014,
      "learning_rate": 0.00017233948988566405,
      "loss": 0.0269,
      "step": 630
    },
    {
      "epoch": 0.4162269129287599,
      "grad_norm": 0.05555072799324989,
      "learning_rate": 0.00017229551451187336,
      "loss": 0.0126,
      "step": 631
    },
    {
      "epoch": 0.41688654353562005,
      "grad_norm": 0.12880447506904602,
      "learning_rate": 0.00017225153913808268,
      "loss": 0.0211,
      "step": 632
    },
    {
      "epoch": 0.4175461741424802,
      "grad_norm": 0.157676562666893,
      "learning_rate": 0.00017220756376429202,
      "loss": 0.0425,
      "step": 633
    },
    {
      "epoch": 0.4182058047493404,
      "grad_norm": 0.08693722635507584,
      "learning_rate": 0.00017216358839050133,
      "loss": 0.0057,
      "step": 634
    },
    {
      "epoch": 0.4188654353562005,
      "grad_norm": 0.10858321189880371,
      "learning_rate": 0.00017211961301671065,
      "loss": 0.0308,
      "step": 635
    },
    {
      "epoch": 0.41952506596306066,
      "grad_norm": 0.1768357753753662,
      "learning_rate": 0.00017207563764291996,
      "loss": 0.0503,
      "step": 636
    },
    {
      "epoch": 0.42018469656992086,
      "grad_norm": 0.09593243151903152,
      "learning_rate": 0.0001720316622691293,
      "loss": 0.0293,
      "step": 637
    },
    {
      "epoch": 0.420844327176781,
      "grad_norm": 0.06336937844753265,
      "learning_rate": 0.00017198768689533861,
      "loss": 0.0149,
      "step": 638
    },
    {
      "epoch": 0.42150395778364114,
      "grad_norm": 0.0789070725440979,
      "learning_rate": 0.00017194371152154793,
      "loss": 0.0128,
      "step": 639
    },
    {
      "epoch": 0.42216358839050133,
      "grad_norm": 0.07617644220590591,
      "learning_rate": 0.00017189973614775727,
      "loss": 0.0216,
      "step": 640
    },
    {
      "epoch": 0.42282321899736147,
      "grad_norm": 0.17923715710639954,
      "learning_rate": 0.00017185576077396658,
      "loss": 0.0266,
      "step": 641
    },
    {
      "epoch": 0.4234828496042216,
      "grad_norm": 0.29969608783721924,
      "learning_rate": 0.0001718117854001759,
      "loss": 0.0748,
      "step": 642
    },
    {
      "epoch": 0.4241424802110818,
      "grad_norm": 0.13071812689304352,
      "learning_rate": 0.0001717678100263852,
      "loss": 0.0253,
      "step": 643
    },
    {
      "epoch": 0.42480211081794195,
      "grad_norm": 0.15491260588169098,
      "learning_rate": 0.00017172383465259455,
      "loss": 0.0421,
      "step": 644
    },
    {
      "epoch": 0.4254617414248021,
      "grad_norm": 0.24334624409675598,
      "learning_rate": 0.00017167985927880387,
      "loss": 0.0756,
      "step": 645
    },
    {
      "epoch": 0.4261213720316623,
      "grad_norm": 0.22025108337402344,
      "learning_rate": 0.0001716358839050132,
      "loss": 0.0234,
      "step": 646
    },
    {
      "epoch": 0.4267810026385224,
      "grad_norm": 0.18972614407539368,
      "learning_rate": 0.00017159190853122252,
      "loss": 0.0399,
      "step": 647
    },
    {
      "epoch": 0.42744063324538256,
      "grad_norm": 0.2991578280925751,
      "learning_rate": 0.00017154793315743186,
      "loss": 0.0214,
      "step": 648
    },
    {
      "epoch": 0.42810026385224276,
      "grad_norm": 0.1477070450782776,
      "learning_rate": 0.00017150395778364118,
      "loss": 0.0623,
      "step": 649
    },
    {
      "epoch": 0.4287598944591029,
      "grad_norm": 0.11711940914392471,
      "learning_rate": 0.0001714599824098505,
      "loss": 0.0389,
      "step": 650
    },
    {
      "epoch": 0.42941952506596304,
      "grad_norm": 0.1010143905878067,
      "learning_rate": 0.00017141600703605983,
      "loss": 0.0292,
      "step": 651
    },
    {
      "epoch": 0.43007915567282323,
      "grad_norm": 0.27926313877105713,
      "learning_rate": 0.00017137203166226915,
      "loss": 0.068,
      "step": 652
    },
    {
      "epoch": 0.43073878627968337,
      "grad_norm": 0.08412474393844604,
      "learning_rate": 0.00017132805628847846,
      "loss": 0.0052,
      "step": 653
    },
    {
      "epoch": 0.4313984168865435,
      "grad_norm": 0.14838393032550812,
      "learning_rate": 0.00017128408091468777,
      "loss": 0.0495,
      "step": 654
    },
    {
      "epoch": 0.4320580474934037,
      "grad_norm": 0.10675439983606339,
      "learning_rate": 0.00017124010554089711,
      "loss": 0.0183,
      "step": 655
    },
    {
      "epoch": 0.43271767810026385,
      "grad_norm": 0.11925056576728821,
      "learning_rate": 0.00017119613016710643,
      "loss": 0.037,
      "step": 656
    },
    {
      "epoch": 0.433377308707124,
      "grad_norm": 0.23897388577461243,
      "learning_rate": 0.00017115215479331574,
      "loss": 0.0716,
      "step": 657
    },
    {
      "epoch": 0.4340369393139842,
      "grad_norm": 0.1437949538230896,
      "learning_rate": 0.00017110817941952508,
      "loss": 0.0426,
      "step": 658
    },
    {
      "epoch": 0.4346965699208443,
      "grad_norm": 0.1784209907054901,
      "learning_rate": 0.0001710642040457344,
      "loss": 0.0343,
      "step": 659
    },
    {
      "epoch": 0.43535620052770446,
      "grad_norm": 0.19162200391292572,
      "learning_rate": 0.0001710202286719437,
      "loss": 0.0375,
      "step": 660
    },
    {
      "epoch": 0.43601583113456466,
      "grad_norm": 0.11864830553531647,
      "learning_rate": 0.00017097625329815303,
      "loss": 0.0417,
      "step": 661
    },
    {
      "epoch": 0.4366754617414248,
      "grad_norm": 0.19719339907169342,
      "learning_rate": 0.00017093227792436237,
      "loss": 0.0141,
      "step": 662
    },
    {
      "epoch": 0.43733509234828494,
      "grad_norm": 0.3453345000743866,
      "learning_rate": 0.00017088830255057168,
      "loss": 0.0771,
      "step": 663
    },
    {
      "epoch": 0.43799472295514513,
      "grad_norm": 0.16988348960876465,
      "learning_rate": 0.000170844327176781,
      "loss": 0.0476,
      "step": 664
    },
    {
      "epoch": 0.4386543535620053,
      "grad_norm": 0.1897146999835968,
      "learning_rate": 0.00017080035180299034,
      "loss": 0.0137,
      "step": 665
    },
    {
      "epoch": 0.4393139841688654,
      "grad_norm": 0.19516395032405853,
      "learning_rate": 0.00017075637642919965,
      "loss": 0.0192,
      "step": 666
    },
    {
      "epoch": 0.4399736147757256,
      "grad_norm": 0.12646600604057312,
      "learning_rate": 0.000170712401055409,
      "loss": 0.0337,
      "step": 667
    },
    {
      "epoch": 0.44063324538258575,
      "grad_norm": 0.13872069120407104,
      "learning_rate": 0.0001706684256816183,
      "loss": 0.0425,
      "step": 668
    },
    {
      "epoch": 0.4412928759894459,
      "grad_norm": 0.07439431548118591,
      "learning_rate": 0.00017062445030782765,
      "loss": 0.0214,
      "step": 669
    },
    {
      "epoch": 0.4419525065963061,
      "grad_norm": 0.09757578372955322,
      "learning_rate": 0.00017058047493403696,
      "loss": 0.0279,
      "step": 670
    },
    {
      "epoch": 0.4426121372031662,
      "grad_norm": 0.11254796385765076,
      "learning_rate": 0.00017053649956024627,
      "loss": 0.0307,
      "step": 671
    },
    {
      "epoch": 0.44327176781002636,
      "grad_norm": 0.3169785141944885,
      "learning_rate": 0.0001704925241864556,
      "loss": 0.0751,
      "step": 672
    },
    {
      "epoch": 0.44393139841688656,
      "grad_norm": 0.17285670340061188,
      "learning_rate": 0.00017044854881266493,
      "loss": 0.0412,
      "step": 673
    },
    {
      "epoch": 0.4445910290237467,
      "grad_norm": 0.10227113217115402,
      "learning_rate": 0.00017040457343887424,
      "loss": 0.0347,
      "step": 674
    },
    {
      "epoch": 0.44525065963060684,
      "grad_norm": 0.20437723398208618,
      "learning_rate": 0.00017036059806508356,
      "loss": 0.0132,
      "step": 675
    },
    {
      "epoch": 0.44591029023746703,
      "grad_norm": 0.20661160349845886,
      "learning_rate": 0.0001703166226912929,
      "loss": 0.0709,
      "step": 676
    },
    {
      "epoch": 0.4465699208443272,
      "grad_norm": 0.10978713631629944,
      "learning_rate": 0.0001702726473175022,
      "loss": 0.031,
      "step": 677
    },
    {
      "epoch": 0.4472295514511873,
      "grad_norm": 0.13026802241802216,
      "learning_rate": 0.00017022867194371153,
      "loss": 0.0199,
      "step": 678
    },
    {
      "epoch": 0.4478891820580475,
      "grad_norm": 0.17573313415050507,
      "learning_rate": 0.00017018469656992084,
      "loss": 0.0307,
      "step": 679
    },
    {
      "epoch": 0.44854881266490765,
      "grad_norm": 0.1286337673664093,
      "learning_rate": 0.00017014072119613018,
      "loss": 0.0288,
      "step": 680
    },
    {
      "epoch": 0.4492084432717678,
      "grad_norm": 0.13432836532592773,
      "learning_rate": 0.0001700967458223395,
      "loss": 0.0253,
      "step": 681
    },
    {
      "epoch": 0.449868073878628,
      "grad_norm": 0.137332946062088,
      "learning_rate": 0.0001700527704485488,
      "loss": 0.018,
      "step": 682
    },
    {
      "epoch": 0.4505277044854881,
      "grad_norm": 0.1680961549282074,
      "learning_rate": 0.00017000879507475815,
      "loss": 0.0252,
      "step": 683
    },
    {
      "epoch": 0.45118733509234826,
      "grad_norm": 0.1503438800573349,
      "learning_rate": 0.00016996481970096746,
      "loss": 0.0285,
      "step": 684
    },
    {
      "epoch": 0.45184696569920846,
      "grad_norm": 0.20819221436977386,
      "learning_rate": 0.00016992084432717678,
      "loss": 0.048,
      "step": 685
    },
    {
      "epoch": 0.4525065963060686,
      "grad_norm": 0.0681789219379425,
      "learning_rate": 0.0001698768689533861,
      "loss": 0.0144,
      "step": 686
    },
    {
      "epoch": 0.45316622691292874,
      "grad_norm": 0.11056903004646301,
      "learning_rate": 0.00016983289357959543,
      "loss": 0.025,
      "step": 687
    },
    {
      "epoch": 0.45382585751978893,
      "grad_norm": 0.06471849232912064,
      "learning_rate": 0.00016978891820580475,
      "loss": 0.0036,
      "step": 688
    },
    {
      "epoch": 0.4544854881266491,
      "grad_norm": 0.13875260949134827,
      "learning_rate": 0.0001697449428320141,
      "loss": 0.0234,
      "step": 689
    },
    {
      "epoch": 0.4551451187335092,
      "grad_norm": 0.249802827835083,
      "learning_rate": 0.0001697009674582234,
      "loss": 0.0601,
      "step": 690
    },
    {
      "epoch": 0.4558047493403694,
      "grad_norm": 0.239874005317688,
      "learning_rate": 0.00016965699208443274,
      "loss": 0.0472,
      "step": 691
    },
    {
      "epoch": 0.45646437994722955,
      "grad_norm": 0.32739171385765076,
      "learning_rate": 0.00016961301671064206,
      "loss": 0.0611,
      "step": 692
    },
    {
      "epoch": 0.4571240105540897,
      "grad_norm": 0.13187430799007416,
      "learning_rate": 0.00016956904133685137,
      "loss": 0.0287,
      "step": 693
    },
    {
      "epoch": 0.4577836411609499,
      "grad_norm": 0.15504947304725647,
      "learning_rate": 0.0001695250659630607,
      "loss": 0.0093,
      "step": 694
    },
    {
      "epoch": 0.45844327176781,
      "grad_norm": 0.16062529385089874,
      "learning_rate": 0.00016948109058927003,
      "loss": 0.0449,
      "step": 695
    },
    {
      "epoch": 0.45910290237467016,
      "grad_norm": 0.26844239234924316,
      "learning_rate": 0.00016943711521547934,
      "loss": 0.0184,
      "step": 696
    },
    {
      "epoch": 0.45976253298153036,
      "grad_norm": 0.16579926013946533,
      "learning_rate": 0.00016939313984168865,
      "loss": 0.0175,
      "step": 697
    },
    {
      "epoch": 0.4604221635883905,
      "grad_norm": 0.1426556408405304,
      "learning_rate": 0.000169349164467898,
      "loss": 0.0397,
      "step": 698
    },
    {
      "epoch": 0.46108179419525064,
      "grad_norm": 0.10163968056440353,
      "learning_rate": 0.0001693051890941073,
      "loss": 0.0205,
      "step": 699
    },
    {
      "epoch": 0.46174142480211083,
      "grad_norm": 0.09195016324520111,
      "learning_rate": 0.00016926121372031662,
      "loss": 0.0061,
      "step": 700
    },
    {
      "epoch": 0.462401055408971,
      "grad_norm": 0.18080900609493256,
      "learning_rate": 0.00016921723834652596,
      "loss": 0.0464,
      "step": 701
    },
    {
      "epoch": 0.4630606860158311,
      "grad_norm": 0.18538020551204681,
      "learning_rate": 0.00016917326297273528,
      "loss": 0.0396,
      "step": 702
    },
    {
      "epoch": 0.4637203166226913,
      "grad_norm": 0.0946730226278305,
      "learning_rate": 0.0001691292875989446,
      "loss": 0.0266,
      "step": 703
    },
    {
      "epoch": 0.46437994722955145,
      "grad_norm": 0.04454510658979416,
      "learning_rate": 0.0001690853122251539,
      "loss": 0.0027,
      "step": 704
    },
    {
      "epoch": 0.4650395778364116,
      "grad_norm": 0.29219353199005127,
      "learning_rate": 0.00016904133685136325,
      "loss": 0.0948,
      "step": 705
    },
    {
      "epoch": 0.4656992084432718,
      "grad_norm": 0.3077755570411682,
      "learning_rate": 0.00016899736147757256,
      "loss": 0.0846,
      "step": 706
    },
    {
      "epoch": 0.4663588390501319,
      "grad_norm": 0.1003488227725029,
      "learning_rate": 0.00016895338610378187,
      "loss": 0.0268,
      "step": 707
    },
    {
      "epoch": 0.46701846965699206,
      "grad_norm": 0.07244012504816055,
      "learning_rate": 0.00016890941072999122,
      "loss": 0.022,
      "step": 708
    },
    {
      "epoch": 0.46767810026385226,
      "grad_norm": 0.09854458272457123,
      "learning_rate": 0.00016886543535620053,
      "loss": 0.0218,
      "step": 709
    },
    {
      "epoch": 0.4683377308707124,
      "grad_norm": 0.15189984440803528,
      "learning_rate": 0.00016882145998240987,
      "loss": 0.023,
      "step": 710
    },
    {
      "epoch": 0.46899736147757254,
      "grad_norm": 0.16658373177051544,
      "learning_rate": 0.00016877748460861918,
      "loss": 0.0633,
      "step": 711
    },
    {
      "epoch": 0.46965699208443273,
      "grad_norm": 0.11164552718400955,
      "learning_rate": 0.00016873350923482853,
      "loss": 0.0515,
      "step": 712
    },
    {
      "epoch": 0.4703166226912929,
      "grad_norm": 0.1381760984659195,
      "learning_rate": 0.00016868953386103784,
      "loss": 0.0501,
      "step": 713
    },
    {
      "epoch": 0.470976253298153,
      "grad_norm": 0.180783212184906,
      "learning_rate": 0.00016864555848724715,
      "loss": 0.0383,
      "step": 714
    },
    {
      "epoch": 0.4716358839050132,
      "grad_norm": 0.15020851790905,
      "learning_rate": 0.00016860158311345647,
      "loss": 0.0347,
      "step": 715
    },
    {
      "epoch": 0.47229551451187335,
      "grad_norm": 0.21067741513252258,
      "learning_rate": 0.0001685576077396658,
      "loss": 0.0188,
      "step": 716
    },
    {
      "epoch": 0.4729551451187335,
      "grad_norm": 0.18821308016777039,
      "learning_rate": 0.00016851363236587512,
      "loss": 0.0257,
      "step": 717
    },
    {
      "epoch": 0.4736147757255937,
      "grad_norm": 0.10825753957033157,
      "learning_rate": 0.00016846965699208444,
      "loss": 0.034,
      "step": 718
    },
    {
      "epoch": 0.4742744063324538,
      "grad_norm": 0.1268332302570343,
      "learning_rate": 0.00016842568161829378,
      "loss": 0.0156,
      "step": 719
    },
    {
      "epoch": 0.47493403693931396,
      "grad_norm": 0.19828951358795166,
      "learning_rate": 0.0001683817062445031,
      "loss": 0.0275,
      "step": 720
    },
    {
      "epoch": 0.47559366754617416,
      "grad_norm": 0.1330440640449524,
      "learning_rate": 0.0001683377308707124,
      "loss": 0.0253,
      "step": 721
    },
    {
      "epoch": 0.4762532981530343,
      "grad_norm": 0.24470682442188263,
      "learning_rate": 0.00016829375549692172,
      "loss": 0.0353,
      "step": 722
    },
    {
      "epoch": 0.47691292875989444,
      "grad_norm": 0.07278455048799515,
      "learning_rate": 0.00016824978012313106,
      "loss": 0.0136,
      "step": 723
    },
    {
      "epoch": 0.47757255936675463,
      "grad_norm": 0.0699383020401001,
      "learning_rate": 0.00016820580474934037,
      "loss": 0.0043,
      "step": 724
    },
    {
      "epoch": 0.4782321899736148,
      "grad_norm": 0.16378137469291687,
      "learning_rate": 0.0001681618293755497,
      "loss": 0.0328,
      "step": 725
    },
    {
      "epoch": 0.4788918205804749,
      "grad_norm": 0.11612925678491592,
      "learning_rate": 0.000168117854001759,
      "loss": 0.0202,
      "step": 726
    },
    {
      "epoch": 0.4795514511873351,
      "grad_norm": 0.2824101448059082,
      "learning_rate": 0.00016807387862796834,
      "loss": 0.0521,
      "step": 727
    },
    {
      "epoch": 0.48021108179419525,
      "grad_norm": 0.13710707426071167,
      "learning_rate": 0.00016802990325417766,
      "loss": 0.0228,
      "step": 728
    },
    {
      "epoch": 0.4808707124010554,
      "grad_norm": 0.14824001491069794,
      "learning_rate": 0.00016798592788038697,
      "loss": 0.0288,
      "step": 729
    },
    {
      "epoch": 0.4815303430079156,
      "grad_norm": 0.13887721300125122,
      "learning_rate": 0.0001679419525065963,
      "loss": 0.031,
      "step": 730
    },
    {
      "epoch": 0.4821899736147757,
      "grad_norm": 0.15214954316616058,
      "learning_rate": 0.00016789797713280563,
      "loss": 0.0236,
      "step": 731
    },
    {
      "epoch": 0.48284960422163586,
      "grad_norm": 0.1885674148797989,
      "learning_rate": 0.00016785400175901497,
      "loss": 0.0428,
      "step": 732
    },
    {
      "epoch": 0.48350923482849606,
      "grad_norm": 0.13549697399139404,
      "learning_rate": 0.00016781002638522428,
      "loss": 0.0084,
      "step": 733
    },
    {
      "epoch": 0.4841688654353562,
      "grad_norm": 0.17942377924919128,
      "learning_rate": 0.00016776605101143362,
      "loss": 0.0294,
      "step": 734
    },
    {
      "epoch": 0.48482849604221634,
      "grad_norm": 0.2669023275375366,
      "learning_rate": 0.00016772207563764294,
      "loss": 0.0635,
      "step": 735
    },
    {
      "epoch": 0.48548812664907653,
      "grad_norm": 0.21253515779972076,
      "learning_rate": 0.00016767810026385225,
      "loss": 0.0547,
      "step": 736
    },
    {
      "epoch": 0.4861477572559367,
      "grad_norm": 0.1683473140001297,
      "learning_rate": 0.0001676341248900616,
      "loss": 0.0271,
      "step": 737
    },
    {
      "epoch": 0.4868073878627968,
      "grad_norm": 0.17070162296295166,
      "learning_rate": 0.0001675901495162709,
      "loss": 0.0152,
      "step": 738
    },
    {
      "epoch": 0.487467018469657,
      "grad_norm": 0.3062136173248291,
      "learning_rate": 0.00016754617414248022,
      "loss": 0.1041,
      "step": 739
    },
    {
      "epoch": 0.48812664907651715,
      "grad_norm": 0.24565961956977844,
      "learning_rate": 0.00016750219876868953,
      "loss": 0.0486,
      "step": 740
    },
    {
      "epoch": 0.4887862796833773,
      "grad_norm": 0.09109683334827423,
      "learning_rate": 0.00016745822339489887,
      "loss": 0.006,
      "step": 741
    },
    {
      "epoch": 0.4894459102902375,
      "grad_norm": 0.17389611899852753,
      "learning_rate": 0.0001674142480211082,
      "loss": 0.0565,
      "step": 742
    },
    {
      "epoch": 0.4901055408970976,
      "grad_norm": 0.0947922021150589,
      "learning_rate": 0.0001673702726473175,
      "loss": 0.0184,
      "step": 743
    },
    {
      "epoch": 0.49076517150395776,
      "grad_norm": 0.08004336059093475,
      "learning_rate": 0.00016732629727352682,
      "loss": 0.0059,
      "step": 744
    },
    {
      "epoch": 0.49142480211081796,
      "grad_norm": 0.09362511336803436,
      "learning_rate": 0.00016728232189973616,
      "loss": 0.0067,
      "step": 745
    },
    {
      "epoch": 0.4920844327176781,
      "grad_norm": 0.09706075489521027,
      "learning_rate": 0.00016723834652594547,
      "loss": 0.0304,
      "step": 746
    },
    {
      "epoch": 0.49274406332453824,
      "grad_norm": 0.12087903171777725,
      "learning_rate": 0.00016719437115215479,
      "loss": 0.0254,
      "step": 747
    },
    {
      "epoch": 0.49340369393139843,
      "grad_norm": 0.18939432501792908,
      "learning_rate": 0.00016715039577836413,
      "loss": 0.042,
      "step": 748
    },
    {
      "epoch": 0.4940633245382586,
      "grad_norm": 0.18836718797683716,
      "learning_rate": 0.00016710642040457344,
      "loss": 0.0442,
      "step": 749
    },
    {
      "epoch": 0.4947229551451187,
      "grad_norm": 0.08189081400632858,
      "learning_rate": 0.00016706244503078275,
      "loss": 0.006,
      "step": 750
    },
    {
      "epoch": 0.4953825857519789,
      "grad_norm": 0.1043143942952156,
      "learning_rate": 0.0001670184696569921,
      "loss": 0.0088,
      "step": 751
    },
    {
      "epoch": 0.49604221635883905,
      "grad_norm": 0.11912861466407776,
      "learning_rate": 0.0001669744942832014,
      "loss": 0.0332,
      "step": 752
    },
    {
      "epoch": 0.4967018469656992,
      "grad_norm": 0.12317201495170593,
      "learning_rate": 0.00016693051890941075,
      "loss": 0.0324,
      "step": 753
    },
    {
      "epoch": 0.4973614775725594,
      "grad_norm": 0.08862116932868958,
      "learning_rate": 0.00016688654353562006,
      "loss": 0.0241,
      "step": 754
    },
    {
      "epoch": 0.4980211081794195,
      "grad_norm": 0.11721250414848328,
      "learning_rate": 0.0001668425681618294,
      "loss": 0.0421,
      "step": 755
    },
    {
      "epoch": 0.49868073878627966,
      "grad_norm": 0.24449339509010315,
      "learning_rate": 0.00016679859278803872,
      "loss": 0.068,
      "step": 756
    },
    {
      "epoch": 0.49934036939313986,
      "grad_norm": 0.1834089308977127,
      "learning_rate": 0.00016675461741424803,
      "loss": 0.048,
      "step": 757
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.13484910130500793,
      "learning_rate": 0.00016671064204045735,
      "loss": 0.0457,
      "step": 758
    },
    {
      "epoch": 0.5006596306068601,
      "grad_norm": 0.15829245746135712,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.0152,
      "step": 759
    },
    {
      "epoch": 0.5013192612137203,
      "grad_norm": 0.16184395551681519,
      "learning_rate": 0.000166622691292876,
      "loss": 0.024,
      "step": 760
    },
    {
      "epoch": 0.5019788918205804,
      "grad_norm": 0.19768235087394714,
      "learning_rate": 0.00016657871591908532,
      "loss": 0.035,
      "step": 761
    },
    {
      "epoch": 0.5026385224274407,
      "grad_norm": 0.11875247955322266,
      "learning_rate": 0.00016653474054529466,
      "loss": 0.0206,
      "step": 762
    },
    {
      "epoch": 0.5032981530343008,
      "grad_norm": 0.1290995329618454,
      "learning_rate": 0.00016649076517150397,
      "loss": 0.0188,
      "step": 763
    },
    {
      "epoch": 0.503957783641161,
      "grad_norm": 0.09279179573059082,
      "learning_rate": 0.00016644678979771329,
      "loss": 0.0206,
      "step": 764
    },
    {
      "epoch": 0.5046174142480211,
      "grad_norm": 0.15528860688209534,
      "learning_rate": 0.0001664028144239226,
      "loss": 0.0463,
      "step": 765
    },
    {
      "epoch": 0.5052770448548812,
      "grad_norm": 0.10247810184955597,
      "learning_rate": 0.00016635883905013194,
      "loss": 0.0341,
      "step": 766
    },
    {
      "epoch": 0.5059366754617414,
      "grad_norm": 0.26573941111564636,
      "learning_rate": 0.00016631486367634125,
      "loss": 0.053,
      "step": 767
    },
    {
      "epoch": 0.5065963060686016,
      "grad_norm": 0.09183336794376373,
      "learning_rate": 0.00016627088830255057,
      "loss": 0.0203,
      "step": 768
    },
    {
      "epoch": 0.5072559366754618,
      "grad_norm": 0.2392536848783493,
      "learning_rate": 0.00016622691292875988,
      "loss": 0.0472,
      "step": 769
    },
    {
      "epoch": 0.5079155672823219,
      "grad_norm": 0.15285541117191315,
      "learning_rate": 0.00016618293755496922,
      "loss": 0.0284,
      "step": 770
    },
    {
      "epoch": 0.508575197889182,
      "grad_norm": 0.1571388840675354,
      "learning_rate": 0.00016613896218117854,
      "loss": 0.0384,
      "step": 771
    },
    {
      "epoch": 0.5092348284960422,
      "grad_norm": 0.13082726299762726,
      "learning_rate": 0.00016609498680738785,
      "loss": 0.0112,
      "step": 772
    },
    {
      "epoch": 0.5098944591029023,
      "grad_norm": 0.12676246464252472,
      "learning_rate": 0.0001660510114335972,
      "loss": 0.0395,
      "step": 773
    },
    {
      "epoch": 0.5105540897097626,
      "grad_norm": 0.15130093693733215,
      "learning_rate": 0.00016600703605980653,
      "loss": 0.0379,
      "step": 774
    },
    {
      "epoch": 0.5112137203166227,
      "grad_norm": 0.24176988005638123,
      "learning_rate": 0.00016596306068601585,
      "loss": 0.0188,
      "step": 775
    },
    {
      "epoch": 0.5118733509234829,
      "grad_norm": 0.10990148782730103,
      "learning_rate": 0.00016591908531222516,
      "loss": 0.017,
      "step": 776
    },
    {
      "epoch": 0.512532981530343,
      "grad_norm": 0.11432012170553207,
      "learning_rate": 0.0001658751099384345,
      "loss": 0.0273,
      "step": 777
    },
    {
      "epoch": 0.5131926121372031,
      "grad_norm": 0.17256824672222137,
      "learning_rate": 0.00016583113456464382,
      "loss": 0.0288,
      "step": 778
    },
    {
      "epoch": 0.5138522427440633,
      "grad_norm": 0.2287413328886032,
      "learning_rate": 0.00016578715919085313,
      "loss": 0.0509,
      "step": 779
    },
    {
      "epoch": 0.5145118733509235,
      "grad_norm": 0.21687175333499908,
      "learning_rate": 0.00016574318381706247,
      "loss": 0.0291,
      "step": 780
    },
    {
      "epoch": 0.5151715039577837,
      "grad_norm": 0.20919537544250488,
      "learning_rate": 0.00016569920844327179,
      "loss": 0.0269,
      "step": 781
    },
    {
      "epoch": 0.5158311345646438,
      "grad_norm": 0.07335364818572998,
      "learning_rate": 0.0001656552330694811,
      "loss": 0.008,
      "step": 782
    },
    {
      "epoch": 0.5164907651715039,
      "grad_norm": 0.24343697726726532,
      "learning_rate": 0.0001656112576956904,
      "loss": 0.0329,
      "step": 783
    },
    {
      "epoch": 0.5171503957783641,
      "grad_norm": 0.1775093376636505,
      "learning_rate": 0.00016556728232189975,
      "loss": 0.0339,
      "step": 784
    },
    {
      "epoch": 0.5178100263852242,
      "grad_norm": 0.15460079908370972,
      "learning_rate": 0.00016552330694810907,
      "loss": 0.0214,
      "step": 785
    },
    {
      "epoch": 0.5184696569920845,
      "grad_norm": 0.10496518015861511,
      "learning_rate": 0.00016547933157431838,
      "loss": 0.0064,
      "step": 786
    },
    {
      "epoch": 0.5191292875989446,
      "grad_norm": 0.1062120795249939,
      "learning_rate": 0.0001654353562005277,
      "loss": 0.0206,
      "step": 787
    },
    {
      "epoch": 0.5197889182058048,
      "grad_norm": 0.0770968496799469,
      "learning_rate": 0.00016539138082673704,
      "loss": 0.0057,
      "step": 788
    },
    {
      "epoch": 0.5204485488126649,
      "grad_norm": 0.20542390644550323,
      "learning_rate": 0.00016534740545294635,
      "loss": 0.0309,
      "step": 789
    },
    {
      "epoch": 0.521108179419525,
      "grad_norm": 0.16939209401607513,
      "learning_rate": 0.00016530343007915566,
      "loss": 0.0295,
      "step": 790
    },
    {
      "epoch": 0.5217678100263852,
      "grad_norm": 0.13170386850833893,
      "learning_rate": 0.000165259454705365,
      "loss": 0.0298,
      "step": 791
    },
    {
      "epoch": 0.5224274406332454,
      "grad_norm": 0.07829774916172028,
      "learning_rate": 0.00016521547933157432,
      "loss": 0.0097,
      "step": 792
    },
    {
      "epoch": 0.5230870712401056,
      "grad_norm": 0.13823619484901428,
      "learning_rate": 0.00016517150395778363,
      "loss": 0.017,
      "step": 793
    },
    {
      "epoch": 0.5237467018469657,
      "grad_norm": 0.15901830792427063,
      "learning_rate": 0.00016512752858399298,
      "loss": 0.0118,
      "step": 794
    },
    {
      "epoch": 0.5244063324538258,
      "grad_norm": 0.12909024953842163,
      "learning_rate": 0.0001650835532102023,
      "loss": 0.0112,
      "step": 795
    },
    {
      "epoch": 0.525065963060686,
      "grad_norm": 0.07755231112241745,
      "learning_rate": 0.00016503957783641163,
      "loss": 0.0128,
      "step": 796
    },
    {
      "epoch": 0.5257255936675461,
      "grad_norm": 0.2716083526611328,
      "learning_rate": 0.00016499560246262094,
      "loss": 0.0654,
      "step": 797
    },
    {
      "epoch": 0.5263852242744064,
      "grad_norm": 0.06811270117759705,
      "learning_rate": 0.00016495162708883029,
      "loss": 0.0121,
      "step": 798
    },
    {
      "epoch": 0.5270448548812665,
      "grad_norm": 0.08879679441452026,
      "learning_rate": 0.0001649076517150396,
      "loss": 0.0166,
      "step": 799
    },
    {
      "epoch": 0.5277044854881267,
      "grad_norm": 0.09889304637908936,
      "learning_rate": 0.0001648636763412489,
      "loss": 0.0167,
      "step": 800
    },
    {
      "epoch": 0.5283641160949868,
      "grad_norm": 0.12466204911470413,
      "learning_rate": 0.00016481970096745823,
      "loss": 0.0119,
      "step": 801
    },
    {
      "epoch": 0.5290237467018469,
      "grad_norm": 0.17922843992710114,
      "learning_rate": 0.00016477572559366757,
      "loss": 0.0313,
      "step": 802
    },
    {
      "epoch": 0.5296833773087071,
      "grad_norm": 0.17628532648086548,
      "learning_rate": 0.00016473175021987688,
      "loss": 0.048,
      "step": 803
    },
    {
      "epoch": 0.5303430079155673,
      "grad_norm": 0.09787875413894653,
      "learning_rate": 0.0001646877748460862,
      "loss": 0.0242,
      "step": 804
    },
    {
      "epoch": 0.5310026385224275,
      "grad_norm": 0.1326446384191513,
      "learning_rate": 0.0001646437994722955,
      "loss": 0.0255,
      "step": 805
    },
    {
      "epoch": 0.5316622691292876,
      "grad_norm": 0.12446925044059753,
      "learning_rate": 0.00016459982409850485,
      "loss": 0.0234,
      "step": 806
    },
    {
      "epoch": 0.5323218997361477,
      "grad_norm": 0.11294776946306229,
      "learning_rate": 0.00016455584872471416,
      "loss": 0.0238,
      "step": 807
    },
    {
      "epoch": 0.5329815303430079,
      "grad_norm": 0.1418045461177826,
      "learning_rate": 0.00016451187335092348,
      "loss": 0.0298,
      "step": 808
    },
    {
      "epoch": 0.533641160949868,
      "grad_norm": 0.12874659895896912,
      "learning_rate": 0.00016446789797713282,
      "loss": 0.0339,
      "step": 809
    },
    {
      "epoch": 0.5343007915567283,
      "grad_norm": 0.18263046443462372,
      "learning_rate": 0.00016442392260334213,
      "loss": 0.0528,
      "step": 810
    },
    {
      "epoch": 0.5349604221635884,
      "grad_norm": 0.08413567394018173,
      "learning_rate": 0.00016437994722955145,
      "loss": 0.0196,
      "step": 811
    },
    {
      "epoch": 0.5356200527704486,
      "grad_norm": 0.0979427620768547,
      "learning_rate": 0.00016433597185576076,
      "loss": 0.0058,
      "step": 812
    },
    {
      "epoch": 0.5362796833773087,
      "grad_norm": 0.1473580002784729,
      "learning_rate": 0.0001642919964819701,
      "loss": 0.0365,
      "step": 813
    },
    {
      "epoch": 0.5369393139841688,
      "grad_norm": 0.2155044823884964,
      "learning_rate": 0.00016424802110817942,
      "loss": 0.0775,
      "step": 814
    },
    {
      "epoch": 0.537598944591029,
      "grad_norm": 0.1093115359544754,
      "learning_rate": 0.00016420404573438876,
      "loss": 0.0068,
      "step": 815
    },
    {
      "epoch": 0.5382585751978892,
      "grad_norm": 0.10064983367919922,
      "learning_rate": 0.00016416007036059807,
      "loss": 0.0342,
      "step": 816
    },
    {
      "epoch": 0.5389182058047494,
      "grad_norm": 0.11435982584953308,
      "learning_rate": 0.0001641160949868074,
      "loss": 0.0108,
      "step": 817
    },
    {
      "epoch": 0.5395778364116095,
      "grad_norm": 0.05945746973156929,
      "learning_rate": 0.00016407211961301673,
      "loss": 0.0124,
      "step": 818
    },
    {
      "epoch": 0.5402374670184696,
      "grad_norm": 0.0685262531042099,
      "learning_rate": 0.00016402814423922604,
      "loss": 0.0138,
      "step": 819
    },
    {
      "epoch": 0.5408970976253298,
      "grad_norm": 0.19727307558059692,
      "learning_rate": 0.00016398416886543538,
      "loss": 0.0544,
      "step": 820
    },
    {
      "epoch": 0.5415567282321899,
      "grad_norm": 0.17107714712619781,
      "learning_rate": 0.0001639401934916447,
      "loss": 0.0641,
      "step": 821
    },
    {
      "epoch": 0.5422163588390502,
      "grad_norm": 0.07629573345184326,
      "learning_rate": 0.000163896218117854,
      "loss": 0.02,
      "step": 822
    },
    {
      "epoch": 0.5428759894459103,
      "grad_norm": 0.11815351992845535,
      "learning_rate": 0.00016385224274406332,
      "loss": 0.0086,
      "step": 823
    },
    {
      "epoch": 0.5435356200527705,
      "grad_norm": 0.11437845975160599,
      "learning_rate": 0.00016380826737027266,
      "loss": 0.0315,
      "step": 824
    },
    {
      "epoch": 0.5441952506596306,
      "grad_norm": 0.08114766329526901,
      "learning_rate": 0.00016376429199648198,
      "loss": 0.0165,
      "step": 825
    },
    {
      "epoch": 0.5448548812664907,
      "grad_norm": 0.20141105353832245,
      "learning_rate": 0.0001637203166226913,
      "loss": 0.0575,
      "step": 826
    },
    {
      "epoch": 0.5455145118733509,
      "grad_norm": 0.09921807795763016,
      "learning_rate": 0.00016367634124890063,
      "loss": 0.0073,
      "step": 827
    },
    {
      "epoch": 0.5461741424802111,
      "grad_norm": 0.0695984810590744,
      "learning_rate": 0.00016363236587510995,
      "loss": 0.0256,
      "step": 828
    },
    {
      "epoch": 0.5468337730870713,
      "grad_norm": 0.06928963214159012,
      "learning_rate": 0.00016358839050131926,
      "loss": 0.0123,
      "step": 829
    },
    {
      "epoch": 0.5474934036939314,
      "grad_norm": 0.07559728622436523,
      "learning_rate": 0.00016354441512752858,
      "loss": 0.0257,
      "step": 830
    },
    {
      "epoch": 0.5481530343007915,
      "grad_norm": 0.12516073882579803,
      "learning_rate": 0.00016350043975373792,
      "loss": 0.0439,
      "step": 831
    },
    {
      "epoch": 0.5488126649076517,
      "grad_norm": 0.1136423870921135,
      "learning_rate": 0.00016345646437994723,
      "loss": 0.0308,
      "step": 832
    },
    {
      "epoch": 0.5494722955145118,
      "grad_norm": 0.06928764283657074,
      "learning_rate": 0.00016341248900615654,
      "loss": 0.0045,
      "step": 833
    },
    {
      "epoch": 0.5501319261213721,
      "grad_norm": 0.07665978372097015,
      "learning_rate": 0.00016336851363236589,
      "loss": 0.0101,
      "step": 834
    },
    {
      "epoch": 0.5507915567282322,
      "grad_norm": 0.06622395664453506,
      "learning_rate": 0.0001633245382585752,
      "loss": 0.004,
      "step": 835
    },
    {
      "epoch": 0.5514511873350924,
      "grad_norm": 0.07649010419845581,
      "learning_rate": 0.00016328056288478451,
      "loss": 0.028,
      "step": 836
    },
    {
      "epoch": 0.5521108179419525,
      "grad_norm": 0.21328362822532654,
      "learning_rate": 0.00016323658751099385,
      "loss": 0.0622,
      "step": 837
    },
    {
      "epoch": 0.5527704485488126,
      "grad_norm": 0.10636536777019501,
      "learning_rate": 0.00016319261213720317,
      "loss": 0.0245,
      "step": 838
    },
    {
      "epoch": 0.5534300791556728,
      "grad_norm": 0.06261996179819107,
      "learning_rate": 0.0001631486367634125,
      "loss": 0.0231,
      "step": 839
    },
    {
      "epoch": 0.554089709762533,
      "grad_norm": 0.08158326148986816,
      "learning_rate": 0.00016310466138962182,
      "loss": 0.0123,
      "step": 840
    },
    {
      "epoch": 0.5547493403693932,
      "grad_norm": 0.28988441824913025,
      "learning_rate": 0.00016306068601583114,
      "loss": 0.0745,
      "step": 841
    },
    {
      "epoch": 0.5554089709762533,
      "grad_norm": 0.06478836387395859,
      "learning_rate": 0.00016301671064204048,
      "loss": 0.0171,
      "step": 842
    },
    {
      "epoch": 0.5560686015831134,
      "grad_norm": 0.10714208334684372,
      "learning_rate": 0.0001629727352682498,
      "loss": 0.0265,
      "step": 843
    },
    {
      "epoch": 0.5567282321899736,
      "grad_norm": 0.1045900508761406,
      "learning_rate": 0.0001629287598944591,
      "loss": 0.0071,
      "step": 844
    },
    {
      "epoch": 0.5573878627968337,
      "grad_norm": 0.11891687661409378,
      "learning_rate": 0.00016288478452066845,
      "loss": 0.021,
      "step": 845
    },
    {
      "epoch": 0.558047493403694,
      "grad_norm": 0.2228630632162094,
      "learning_rate": 0.00016284080914687776,
      "loss": 0.0473,
      "step": 846
    },
    {
      "epoch": 0.5587071240105541,
      "grad_norm": 0.18387839198112488,
      "learning_rate": 0.00016279683377308708,
      "loss": 0.0571,
      "step": 847
    },
    {
      "epoch": 0.5593667546174143,
      "grad_norm": 0.1295257806777954,
      "learning_rate": 0.0001627528583992964,
      "loss": 0.0402,
      "step": 848
    },
    {
      "epoch": 0.5600263852242744,
      "grad_norm": 0.1746588945388794,
      "learning_rate": 0.00016270888302550573,
      "loss": 0.0163,
      "step": 849
    },
    {
      "epoch": 0.5606860158311345,
      "grad_norm": 0.1574990600347519,
      "learning_rate": 0.00016266490765171504,
      "loss": 0.0214,
      "step": 850
    },
    {
      "epoch": 0.5613456464379947,
      "grad_norm": 0.1672859787940979,
      "learning_rate": 0.00016262093227792436,
      "loss": 0.0532,
      "step": 851
    },
    {
      "epoch": 0.5620052770448549,
      "grad_norm": 0.08692740648984909,
      "learning_rate": 0.0001625769569041337,
      "loss": 0.0127,
      "step": 852
    },
    {
      "epoch": 0.5626649076517151,
      "grad_norm": 0.14617761969566345,
      "learning_rate": 0.00016253298153034301,
      "loss": 0.0281,
      "step": 853
    },
    {
      "epoch": 0.5633245382585752,
      "grad_norm": 0.19108228385448456,
      "learning_rate": 0.00016248900615655233,
      "loss": 0.0397,
      "step": 854
    },
    {
      "epoch": 0.5639841688654353,
      "grad_norm": 0.15531682968139648,
      "learning_rate": 0.00016244503078276164,
      "loss": 0.0386,
      "step": 855
    },
    {
      "epoch": 0.5646437994722955,
      "grad_norm": 0.09716622531414032,
      "learning_rate": 0.00016240105540897098,
      "loss": 0.0064,
      "step": 856
    },
    {
      "epoch": 0.5653034300791556,
      "grad_norm": 0.16020314395427704,
      "learning_rate": 0.0001623570800351803,
      "loss": 0.0434,
      "step": 857
    },
    {
      "epoch": 0.5659630606860159,
      "grad_norm": 0.1453268826007843,
      "learning_rate": 0.00016231310466138964,
      "loss": 0.0274,
      "step": 858
    },
    {
      "epoch": 0.566622691292876,
      "grad_norm": 0.10597624629735947,
      "learning_rate": 0.00016226912928759895,
      "loss": 0.0261,
      "step": 859
    },
    {
      "epoch": 0.5672823218997362,
      "grad_norm": 0.10441892594099045,
      "learning_rate": 0.0001622251539138083,
      "loss": 0.016,
      "step": 860
    },
    {
      "epoch": 0.5679419525065963,
      "grad_norm": 0.10103638470172882,
      "learning_rate": 0.0001621811785400176,
      "loss": 0.0202,
      "step": 861
    },
    {
      "epoch": 0.5686015831134564,
      "grad_norm": 0.11365985870361328,
      "learning_rate": 0.00016213720316622692,
      "loss": 0.0152,
      "step": 862
    },
    {
      "epoch": 0.5692612137203166,
      "grad_norm": 0.09640052914619446,
      "learning_rate": 0.00016209322779243626,
      "loss": 0.0283,
      "step": 863
    },
    {
      "epoch": 0.5699208443271768,
      "grad_norm": 0.07413340359926224,
      "learning_rate": 0.00016204925241864558,
      "loss": 0.0146,
      "step": 864
    },
    {
      "epoch": 0.570580474934037,
      "grad_norm": 0.12051556259393692,
      "learning_rate": 0.0001620052770448549,
      "loss": 0.038,
      "step": 865
    },
    {
      "epoch": 0.5712401055408971,
      "grad_norm": 0.23685699701309204,
      "learning_rate": 0.0001619613016710642,
      "loss": 0.057,
      "step": 866
    },
    {
      "epoch": 0.5718997361477572,
      "grad_norm": 0.11726243048906326,
      "learning_rate": 0.00016191732629727354,
      "loss": 0.0214,
      "step": 867
    },
    {
      "epoch": 0.5725593667546174,
      "grad_norm": 0.10750575363636017,
      "learning_rate": 0.00016187335092348286,
      "loss": 0.024,
      "step": 868
    },
    {
      "epoch": 0.5732189973614775,
      "grad_norm": 0.0985398218035698,
      "learning_rate": 0.00016182937554969217,
      "loss": 0.0222,
      "step": 869
    },
    {
      "epoch": 0.5738786279683378,
      "grad_norm": 0.08084539324045181,
      "learning_rate": 0.00016178540017590151,
      "loss": 0.0144,
      "step": 870
    },
    {
      "epoch": 0.5745382585751979,
      "grad_norm": 0.17210683226585388,
      "learning_rate": 0.00016174142480211083,
      "loss": 0.0439,
      "step": 871
    },
    {
      "epoch": 0.575197889182058,
      "grad_norm": 0.13360704481601715,
      "learning_rate": 0.00016169744942832014,
      "loss": 0.0229,
      "step": 872
    },
    {
      "epoch": 0.5758575197889182,
      "grad_norm": 0.2410958707332611,
      "learning_rate": 0.00016165347405452946,
      "loss": 0.0572,
      "step": 873
    },
    {
      "epoch": 0.5765171503957783,
      "grad_norm": 0.1375456154346466,
      "learning_rate": 0.0001616094986807388,
      "loss": 0.0089,
      "step": 874
    },
    {
      "epoch": 0.5771767810026385,
      "grad_norm": 0.2247239053249359,
      "learning_rate": 0.0001615655233069481,
      "loss": 0.0649,
      "step": 875
    },
    {
      "epoch": 0.5778364116094987,
      "grad_norm": 0.13344936072826385,
      "learning_rate": 0.00016152154793315742,
      "loss": 0.0351,
      "step": 876
    },
    {
      "epoch": 0.5784960422163589,
      "grad_norm": 0.19980163872241974,
      "learning_rate": 0.00016147757255936674,
      "loss": 0.049,
      "step": 877
    },
    {
      "epoch": 0.579155672823219,
      "grad_norm": 0.13127416372299194,
      "learning_rate": 0.00016143359718557608,
      "loss": 0.0388,
      "step": 878
    },
    {
      "epoch": 0.5798153034300791,
      "grad_norm": 0.1394331157207489,
      "learning_rate": 0.00016138962181178542,
      "loss": 0.0423,
      "step": 879
    },
    {
      "epoch": 0.5804749340369393,
      "grad_norm": 0.23271070420742035,
      "learning_rate": 0.00016134564643799473,
      "loss": 0.067,
      "step": 880
    },
    {
      "epoch": 0.5811345646437994,
      "grad_norm": 0.1587025672197342,
      "learning_rate": 0.00016130167106420408,
      "loss": 0.0295,
      "step": 881
    },
    {
      "epoch": 0.5817941952506597,
      "grad_norm": 0.15456527471542358,
      "learning_rate": 0.0001612576956904134,
      "loss": 0.0379,
      "step": 882
    },
    {
      "epoch": 0.5824538258575198,
      "grad_norm": 0.150905042886734,
      "learning_rate": 0.0001612137203166227,
      "loss": 0.0405,
      "step": 883
    },
    {
      "epoch": 0.58311345646438,
      "grad_norm": 0.16252924501895905,
      "learning_rate": 0.00016116974494283202,
      "loss": 0.0472,
      "step": 884
    },
    {
      "epoch": 0.5837730870712401,
      "grad_norm": 0.17589141428470612,
      "learning_rate": 0.00016112576956904136,
      "loss": 0.0281,
      "step": 885
    },
    {
      "epoch": 0.5844327176781002,
      "grad_norm": 0.16443116962909698,
      "learning_rate": 0.00016108179419525067,
      "loss": 0.0205,
      "step": 886
    },
    {
      "epoch": 0.5850923482849604,
      "grad_norm": 0.13737007975578308,
      "learning_rate": 0.00016103781882146,
      "loss": 0.0205,
      "step": 887
    },
    {
      "epoch": 0.5857519788918206,
      "grad_norm": 0.08024121820926666,
      "learning_rate": 0.00016099384344766933,
      "loss": 0.023,
      "step": 888
    },
    {
      "epoch": 0.5864116094986808,
      "grad_norm": 0.10417483001947403,
      "learning_rate": 0.00016094986807387864,
      "loss": 0.0267,
      "step": 889
    },
    {
      "epoch": 0.5870712401055409,
      "grad_norm": 0.13819114863872528,
      "learning_rate": 0.00016090589270008796,
      "loss": 0.0217,
      "step": 890
    },
    {
      "epoch": 0.587730870712401,
      "grad_norm": 0.1462433636188507,
      "learning_rate": 0.00016086191732629727,
      "loss": 0.0321,
      "step": 891
    },
    {
      "epoch": 0.5883905013192612,
      "grad_norm": 0.1836545765399933,
      "learning_rate": 0.0001608179419525066,
      "loss": 0.0467,
      "step": 892
    },
    {
      "epoch": 0.5890501319261213,
      "grad_norm": 0.06258735060691833,
      "learning_rate": 0.00016077396657871592,
      "loss": 0.0138,
      "step": 893
    },
    {
      "epoch": 0.5897097625329816,
      "grad_norm": 0.19848643243312836,
      "learning_rate": 0.00016072999120492524,
      "loss": 0.0485,
      "step": 894
    },
    {
      "epoch": 0.5903693931398417,
      "grad_norm": 0.05641037970781326,
      "learning_rate": 0.00016068601583113455,
      "loss": 0.0038,
      "step": 895
    },
    {
      "epoch": 0.5910290237467019,
      "grad_norm": 0.1724139302968979,
      "learning_rate": 0.0001606420404573439,
      "loss": 0.0432,
      "step": 896
    },
    {
      "epoch": 0.591688654353562,
      "grad_norm": 0.08092499524354935,
      "learning_rate": 0.0001605980650835532,
      "loss": 0.0056,
      "step": 897
    },
    {
      "epoch": 0.5923482849604221,
      "grad_norm": 0.14691054821014404,
      "learning_rate": 0.00016055408970976252,
      "loss": 0.0307,
      "step": 898
    },
    {
      "epoch": 0.5930079155672823,
      "grad_norm": 0.11350515484809875,
      "learning_rate": 0.00016051011433597186,
      "loss": 0.0356,
      "step": 899
    },
    {
      "epoch": 0.5936675461741425,
      "grad_norm": 0.0784933939576149,
      "learning_rate": 0.00016046613896218118,
      "loss": 0.0053,
      "step": 900
    },
    {
      "epoch": 0.5943271767810027,
      "grad_norm": 0.07258125394582748,
      "learning_rate": 0.00016042216358839052,
      "loss": 0.0144,
      "step": 901
    },
    {
      "epoch": 0.5949868073878628,
      "grad_norm": 0.08221647143363953,
      "learning_rate": 0.00016037818821459983,
      "loss": 0.0054,
      "step": 902
    },
    {
      "epoch": 0.5956464379947229,
      "grad_norm": 0.25402069091796875,
      "learning_rate": 0.00016033421284080917,
      "loss": 0.0488,
      "step": 903
    },
    {
      "epoch": 0.5963060686015831,
      "grad_norm": 0.1378733366727829,
      "learning_rate": 0.0001602902374670185,
      "loss": 0.0314,
      "step": 904
    },
    {
      "epoch": 0.5969656992084432,
      "grad_norm": 0.11056813597679138,
      "learning_rate": 0.0001602462620932278,
      "loss": 0.0252,
      "step": 905
    },
    {
      "epoch": 0.5976253298153035,
      "grad_norm": 0.2209276407957077,
      "learning_rate": 0.00016020228671943714,
      "loss": 0.0513,
      "step": 906
    },
    {
      "epoch": 0.5982849604221636,
      "grad_norm": 0.11637742817401886,
      "learning_rate": 0.00016015831134564646,
      "loss": 0.0234,
      "step": 907
    },
    {
      "epoch": 0.5989445910290238,
      "grad_norm": 0.09707394242286682,
      "learning_rate": 0.00016011433597185577,
      "loss": 0.0111,
      "step": 908
    },
    {
      "epoch": 0.5996042216358839,
      "grad_norm": 0.08183937519788742,
      "learning_rate": 0.00016007036059806508,
      "loss": 0.0202,
      "step": 909
    },
    {
      "epoch": 0.600263852242744,
      "grad_norm": 0.12094836682081223,
      "learning_rate": 0.00016002638522427442,
      "loss": 0.0284,
      "step": 910
    },
    {
      "epoch": 0.6009234828496042,
      "grad_norm": 0.09794480353593826,
      "learning_rate": 0.00015998240985048374,
      "loss": 0.022,
      "step": 911
    },
    {
      "epoch": 0.6015831134564644,
      "grad_norm": 0.1491338014602661,
      "learning_rate": 0.00015993843447669305,
      "loss": 0.0348,
      "step": 912
    },
    {
      "epoch": 0.6022427440633246,
      "grad_norm": 0.14360669255256653,
      "learning_rate": 0.00015989445910290237,
      "loss": 0.038,
      "step": 913
    },
    {
      "epoch": 0.6029023746701847,
      "grad_norm": 0.1561790555715561,
      "learning_rate": 0.0001598504837291117,
      "loss": 0.0209,
      "step": 914
    },
    {
      "epoch": 0.6035620052770448,
      "grad_norm": 0.08833194524049759,
      "learning_rate": 0.00015980650835532102,
      "loss": 0.0132,
      "step": 915
    },
    {
      "epoch": 0.604221635883905,
      "grad_norm": 0.1346270591020584,
      "learning_rate": 0.00015976253298153034,
      "loss": 0.0267,
      "step": 916
    },
    {
      "epoch": 0.6048812664907651,
      "grad_norm": 0.3076432943344116,
      "learning_rate": 0.00015971855760773968,
      "loss": 0.0696,
      "step": 917
    },
    {
      "epoch": 0.6055408970976254,
      "grad_norm": 0.13718098402023315,
      "learning_rate": 0.000159674582233949,
      "loss": 0.0361,
      "step": 918
    },
    {
      "epoch": 0.6062005277044855,
      "grad_norm": 0.12586481869220734,
      "learning_rate": 0.0001596306068601583,
      "loss": 0.0165,
      "step": 919
    },
    {
      "epoch": 0.6068601583113457,
      "grad_norm": 0.14634259045124054,
      "learning_rate": 0.00015958663148636765,
      "loss": 0.0292,
      "step": 920
    },
    {
      "epoch": 0.6075197889182058,
      "grad_norm": 0.2459302395582199,
      "learning_rate": 0.00015954265611257696,
      "loss": 0.0479,
      "step": 921
    },
    {
      "epoch": 0.6081794195250659,
      "grad_norm": 0.26423442363739014,
      "learning_rate": 0.0001594986807387863,
      "loss": 0.0428,
      "step": 922
    },
    {
      "epoch": 0.6088390501319261,
      "grad_norm": 0.12176792323589325,
      "learning_rate": 0.00015945470536499561,
      "loss": 0.0111,
      "step": 923
    },
    {
      "epoch": 0.6094986807387863,
      "grad_norm": 0.15606682002544403,
      "learning_rate": 0.00015941072999120496,
      "loss": 0.0361,
      "step": 924
    },
    {
      "epoch": 0.6101583113456465,
      "grad_norm": 0.1871809959411621,
      "learning_rate": 0.00015936675461741427,
      "loss": 0.0126,
      "step": 925
    },
    {
      "epoch": 0.6108179419525066,
      "grad_norm": 0.14851655066013336,
      "learning_rate": 0.00015932277924362358,
      "loss": 0.0511,
      "step": 926
    },
    {
      "epoch": 0.6114775725593667,
      "grad_norm": 0.1062006801366806,
      "learning_rate": 0.0001592788038698329,
      "loss": 0.0291,
      "step": 927
    },
    {
      "epoch": 0.6121372031662269,
      "grad_norm": 0.14740480482578278,
      "learning_rate": 0.00015923482849604224,
      "loss": 0.0304,
      "step": 928
    },
    {
      "epoch": 0.612796833773087,
      "grad_norm": 0.07930178940296173,
      "learning_rate": 0.00015919085312225155,
      "loss": 0.0049,
      "step": 929
    },
    {
      "epoch": 0.6134564643799473,
      "grad_norm": 0.10649827122688293,
      "learning_rate": 0.00015914687774846087,
      "loss": 0.0142,
      "step": 930
    },
    {
      "epoch": 0.6141160949868074,
      "grad_norm": 0.09115011245012283,
      "learning_rate": 0.00015910290237467018,
      "loss": 0.0128,
      "step": 931
    },
    {
      "epoch": 0.6147757255936676,
      "grad_norm": 0.06659293919801712,
      "learning_rate": 0.00015905892700087952,
      "loss": 0.0145,
      "step": 932
    },
    {
      "epoch": 0.6154353562005277,
      "grad_norm": 0.19745437800884247,
      "learning_rate": 0.00015901495162708884,
      "loss": 0.0317,
      "step": 933
    },
    {
      "epoch": 0.6160949868073878,
      "grad_norm": 0.054526008665561676,
      "learning_rate": 0.00015897097625329815,
      "loss": 0.0032,
      "step": 934
    },
    {
      "epoch": 0.616754617414248,
      "grad_norm": 0.24129463732242584,
      "learning_rate": 0.0001589270008795075,
      "loss": 0.0348,
      "step": 935
    },
    {
      "epoch": 0.6174142480211082,
      "grad_norm": 0.04278230294585228,
      "learning_rate": 0.0001588830255057168,
      "loss": 0.0025,
      "step": 936
    },
    {
      "epoch": 0.6180738786279684,
      "grad_norm": 0.13796864449977875,
      "learning_rate": 0.00015883905013192612,
      "loss": 0.0159,
      "step": 937
    },
    {
      "epoch": 0.6187335092348285,
      "grad_norm": 0.06337767094373703,
      "learning_rate": 0.00015879507475813543,
      "loss": 0.0083,
      "step": 938
    },
    {
      "epoch": 0.6193931398416886,
      "grad_norm": 0.07446835935115814,
      "learning_rate": 0.00015875109938434477,
      "loss": 0.0066,
      "step": 939
    },
    {
      "epoch": 0.6200527704485488,
      "grad_norm": 0.21451464295387268,
      "learning_rate": 0.0001587071240105541,
      "loss": 0.0353,
      "step": 940
    },
    {
      "epoch": 0.6207124010554089,
      "grad_norm": 0.1440437138080597,
      "learning_rate": 0.0001586631486367634,
      "loss": 0.0198,
      "step": 941
    },
    {
      "epoch": 0.6213720316622692,
      "grad_norm": 0.10582882910966873,
      "learning_rate": 0.00015861917326297274,
      "loss": 0.0157,
      "step": 942
    },
    {
      "epoch": 0.6220316622691293,
      "grad_norm": 0.11347091943025589,
      "learning_rate": 0.00015857519788918206,
      "loss": 0.0229,
      "step": 943
    },
    {
      "epoch": 0.6226912928759895,
      "grad_norm": 0.22886668145656586,
      "learning_rate": 0.0001585312225153914,
      "loss": 0.022,
      "step": 944
    },
    {
      "epoch": 0.6233509234828496,
      "grad_norm": 0.12520091235637665,
      "learning_rate": 0.0001584872471416007,
      "loss": 0.0237,
      "step": 945
    },
    {
      "epoch": 0.6240105540897097,
      "grad_norm": 0.26471760869026184,
      "learning_rate": 0.00015844327176781005,
      "loss": 0.0609,
      "step": 946
    },
    {
      "epoch": 0.6246701846965699,
      "grad_norm": 0.11332923918962479,
      "learning_rate": 0.00015839929639401937,
      "loss": 0.0097,
      "step": 947
    },
    {
      "epoch": 0.6253298153034301,
      "grad_norm": 0.1581287980079651,
      "learning_rate": 0.00015835532102022868,
      "loss": 0.025,
      "step": 948
    },
    {
      "epoch": 0.6259894459102903,
      "grad_norm": 0.46915915608406067,
      "learning_rate": 0.000158311345646438,
      "loss": 0.1089,
      "step": 949
    },
    {
      "epoch": 0.6266490765171504,
      "grad_norm": 0.21147440373897552,
      "learning_rate": 0.00015826737027264734,
      "loss": 0.0225,
      "step": 950
    },
    {
      "epoch": 0.6273087071240105,
      "grad_norm": 0.20106656849384308,
      "learning_rate": 0.00015822339489885665,
      "loss": 0.0431,
      "step": 951
    },
    {
      "epoch": 0.6279683377308707,
      "grad_norm": 0.15368051826953888,
      "learning_rate": 0.00015817941952506596,
      "loss": 0.0266,
      "step": 952
    },
    {
      "epoch": 0.6286279683377308,
      "grad_norm": 0.17008207738399506,
      "learning_rate": 0.0001581354441512753,
      "loss": 0.0242,
      "step": 953
    },
    {
      "epoch": 0.6292875989445911,
      "grad_norm": 0.14930576086044312,
      "learning_rate": 0.00015809146877748462,
      "loss": 0.0427,
      "step": 954
    },
    {
      "epoch": 0.6299472295514512,
      "grad_norm": 0.4089459180831909,
      "learning_rate": 0.00015804749340369393,
      "loss": 0.112,
      "step": 955
    },
    {
      "epoch": 0.6306068601583114,
      "grad_norm": 0.10803564637899399,
      "learning_rate": 0.00015800351802990325,
      "loss": 0.0081,
      "step": 956
    },
    {
      "epoch": 0.6312664907651715,
      "grad_norm": 0.12273850291967392,
      "learning_rate": 0.0001579595426561126,
      "loss": 0.0327,
      "step": 957
    },
    {
      "epoch": 0.6319261213720316,
      "grad_norm": 0.0818730816245079,
      "learning_rate": 0.0001579155672823219,
      "loss": 0.0323,
      "step": 958
    },
    {
      "epoch": 0.6325857519788918,
      "grad_norm": 0.13661757111549377,
      "learning_rate": 0.00015787159190853122,
      "loss": 0.0106,
      "step": 959
    },
    {
      "epoch": 0.633245382585752,
      "grad_norm": 0.19437672197818756,
      "learning_rate": 0.00015782761653474056,
      "loss": 0.0479,
      "step": 960
    },
    {
      "epoch": 0.6339050131926122,
      "grad_norm": 0.11600769311189651,
      "learning_rate": 0.00015778364116094987,
      "loss": 0.0323,
      "step": 961
    },
    {
      "epoch": 0.6345646437994723,
      "grad_norm": 0.08743426948785782,
      "learning_rate": 0.00015773966578715918,
      "loss": 0.0401,
      "step": 962
    },
    {
      "epoch": 0.6352242744063324,
      "grad_norm": 0.09629420191049576,
      "learning_rate": 0.00015769569041336853,
      "loss": 0.0298,
      "step": 963
    },
    {
      "epoch": 0.6358839050131926,
      "grad_norm": 0.09650755673646927,
      "learning_rate": 0.00015765171503957784,
      "loss": 0.0461,
      "step": 964
    },
    {
      "epoch": 0.6365435356200527,
      "grad_norm": 0.11365342140197754,
      "learning_rate": 0.00015760773966578718,
      "loss": 0.0331,
      "step": 965
    },
    {
      "epoch": 0.637203166226913,
      "grad_norm": 0.1234249547123909,
      "learning_rate": 0.0001575637642919965,
      "loss": 0.0493,
      "step": 966
    },
    {
      "epoch": 0.6378627968337731,
      "grad_norm": 0.17661775648593903,
      "learning_rate": 0.0001575197889182058,
      "loss": 0.0648,
      "step": 967
    },
    {
      "epoch": 0.6385224274406333,
      "grad_norm": 0.14137084782123566,
      "learning_rate": 0.00015747581354441515,
      "loss": 0.0111,
      "step": 968
    },
    {
      "epoch": 0.6391820580474934,
      "grad_norm": 0.135686993598938,
      "learning_rate": 0.00015743183817062446,
      "loss": 0.0209,
      "step": 969
    },
    {
      "epoch": 0.6398416886543535,
      "grad_norm": 0.11412093788385391,
      "learning_rate": 0.00015738786279683378,
      "loss": 0.0382,
      "step": 970
    },
    {
      "epoch": 0.6405013192612137,
      "grad_norm": 0.07197537273168564,
      "learning_rate": 0.00015734388742304312,
      "loss": 0.0187,
      "step": 971
    },
    {
      "epoch": 0.6411609498680739,
      "grad_norm": 0.0985872745513916,
      "learning_rate": 0.00015729991204925243,
      "loss": 0.0068,
      "step": 972
    },
    {
      "epoch": 0.6418205804749341,
      "grad_norm": 0.1737734079360962,
      "learning_rate": 0.00015725593667546175,
      "loss": 0.0502,
      "step": 973
    },
    {
      "epoch": 0.6424802110817942,
      "grad_norm": 0.10803570598363876,
      "learning_rate": 0.00015721196130167106,
      "loss": 0.0372,
      "step": 974
    },
    {
      "epoch": 0.6431398416886543,
      "grad_norm": 0.18142466247081757,
      "learning_rate": 0.0001571679859278804,
      "loss": 0.035,
      "step": 975
    },
    {
      "epoch": 0.6437994722955145,
      "grad_norm": 0.11020297557115555,
      "learning_rate": 0.00015712401055408972,
      "loss": 0.0312,
      "step": 976
    },
    {
      "epoch": 0.6444591029023746,
      "grad_norm": 0.2217065542936325,
      "learning_rate": 0.00015708003518029903,
      "loss": 0.0335,
      "step": 977
    },
    {
      "epoch": 0.6451187335092349,
      "grad_norm": 0.14916415512561798,
      "learning_rate": 0.00015703605980650837,
      "loss": 0.039,
      "step": 978
    },
    {
      "epoch": 0.645778364116095,
      "grad_norm": 0.07461486011743546,
      "learning_rate": 0.00015699208443271768,
      "loss": 0.0109,
      "step": 979
    },
    {
      "epoch": 0.6464379947229552,
      "grad_norm": 0.08587154000997543,
      "learning_rate": 0.000156948109058927,
      "loss": 0.0193,
      "step": 980
    },
    {
      "epoch": 0.6470976253298153,
      "grad_norm": 0.08579342067241669,
      "learning_rate": 0.0001569041336851363,
      "loss": 0.0111,
      "step": 981
    },
    {
      "epoch": 0.6477572559366754,
      "grad_norm": 0.11165007948875427,
      "learning_rate": 0.00015686015831134565,
      "loss": 0.0292,
      "step": 982
    },
    {
      "epoch": 0.6484168865435356,
      "grad_norm": 0.10112418234348297,
      "learning_rate": 0.00015681618293755497,
      "loss": 0.0293,
      "step": 983
    },
    {
      "epoch": 0.6490765171503958,
      "grad_norm": 0.11943784356117249,
      "learning_rate": 0.0001567722075637643,
      "loss": 0.0264,
      "step": 984
    },
    {
      "epoch": 0.649736147757256,
      "grad_norm": 0.12381291389465332,
      "learning_rate": 0.00015672823218997362,
      "loss": 0.0082,
      "step": 985
    },
    {
      "epoch": 0.6503957783641161,
      "grad_norm": 0.269265741109848,
      "learning_rate": 0.00015668425681618296,
      "loss": 0.065,
      "step": 986
    },
    {
      "epoch": 0.6510554089709762,
      "grad_norm": 0.14796003699302673,
      "learning_rate": 0.00015664028144239228,
      "loss": 0.0371,
      "step": 987
    },
    {
      "epoch": 0.6517150395778364,
      "grad_norm": 0.5739474296569824,
      "learning_rate": 0.0001565963060686016,
      "loss": 0.0686,
      "step": 988
    },
    {
      "epoch": 0.6523746701846965,
      "grad_norm": 0.17836593091487885,
      "learning_rate": 0.00015655233069481093,
      "loss": 0.0108,
      "step": 989
    },
    {
      "epoch": 0.6530343007915568,
      "grad_norm": 0.18586622178554535,
      "learning_rate": 0.00015650835532102025,
      "loss": 0.0167,
      "step": 990
    },
    {
      "epoch": 0.6536939313984169,
      "grad_norm": 0.20984943211078644,
      "learning_rate": 0.00015646437994722956,
      "loss": 0.0536,
      "step": 991
    },
    {
      "epoch": 0.6543535620052771,
      "grad_norm": 0.14788976311683655,
      "learning_rate": 0.00015642040457343887,
      "loss": 0.0421,
      "step": 992
    },
    {
      "epoch": 0.6550131926121372,
      "grad_norm": 0.15083199739456177,
      "learning_rate": 0.00015637642919964822,
      "loss": 0.0469,
      "step": 993
    },
    {
      "epoch": 0.6556728232189973,
      "grad_norm": 0.29549872875213623,
      "learning_rate": 0.00015633245382585753,
      "loss": 0.0739,
      "step": 994
    },
    {
      "epoch": 0.6563324538258575,
      "grad_norm": 0.1467006802558899,
      "learning_rate": 0.00015628847845206684,
      "loss": 0.0423,
      "step": 995
    },
    {
      "epoch": 0.6569920844327177,
      "grad_norm": 0.0846908688545227,
      "learning_rate": 0.00015624450307827618,
      "loss": 0.0146,
      "step": 996
    },
    {
      "epoch": 0.6576517150395779,
      "grad_norm": 0.11508435755968094,
      "learning_rate": 0.0001562005277044855,
      "loss": 0.0325,
      "step": 997
    },
    {
      "epoch": 0.658311345646438,
      "grad_norm": 0.13174307346343994,
      "learning_rate": 0.0001561565523306948,
      "loss": 0.036,
      "step": 998
    },
    {
      "epoch": 0.6589709762532981,
      "grad_norm": 0.2578682005405426,
      "learning_rate": 0.00015611257695690413,
      "loss": 0.0631,
      "step": 999
    },
    {
      "epoch": 0.6596306068601583,
      "grad_norm": 0.1076539009809494,
      "learning_rate": 0.00015606860158311347,
      "loss": 0.0278,
      "step": 1000
    },
    {
      "epoch": 0.6602902374670184,
      "grad_norm": 0.13605403900146484,
      "learning_rate": 0.00015602462620932278,
      "loss": 0.0258,
      "step": 1001
    },
    {
      "epoch": 0.6609498680738787,
      "grad_norm": 0.48534172773361206,
      "learning_rate": 0.0001559806508355321,
      "loss": 0.0192,
      "step": 1002
    },
    {
      "epoch": 0.6616094986807388,
      "grad_norm": 0.15487313270568848,
      "learning_rate": 0.0001559366754617414,
      "loss": 0.0108,
      "step": 1003
    },
    {
      "epoch": 0.662269129287599,
      "grad_norm": 0.15927378833293915,
      "learning_rate": 0.00015589270008795075,
      "loss": 0.0399,
      "step": 1004
    },
    {
      "epoch": 0.6629287598944591,
      "grad_norm": 0.19018100202083588,
      "learning_rate": 0.00015584872471416006,
      "loss": 0.0399,
      "step": 1005
    },
    {
      "epoch": 0.6635883905013192,
      "grad_norm": 0.10342439264059067,
      "learning_rate": 0.0001558047493403694,
      "loss": 0.0234,
      "step": 1006
    },
    {
      "epoch": 0.6642480211081794,
      "grad_norm": 0.07611986249685287,
      "learning_rate": 0.00015576077396657872,
      "loss": 0.0129,
      "step": 1007
    },
    {
      "epoch": 0.6649076517150396,
      "grad_norm": 0.1673540621995926,
      "learning_rate": 0.00015571679859278806,
      "loss": 0.0428,
      "step": 1008
    },
    {
      "epoch": 0.6655672823218998,
      "grad_norm": 0.1906452476978302,
      "learning_rate": 0.00015567282321899737,
      "loss": 0.0409,
      "step": 1009
    },
    {
      "epoch": 0.6662269129287599,
      "grad_norm": 0.3691564202308655,
      "learning_rate": 0.0001556288478452067,
      "loss": 0.0747,
      "step": 1010
    },
    {
      "epoch": 0.66688654353562,
      "grad_norm": 0.1758706122636795,
      "learning_rate": 0.00015558487247141603,
      "loss": 0.055,
      "step": 1011
    },
    {
      "epoch": 0.6675461741424802,
      "grad_norm": 0.22103512287139893,
      "learning_rate": 0.00015554089709762534,
      "loss": 0.0621,
      "step": 1012
    },
    {
      "epoch": 0.6682058047493403,
      "grad_norm": 0.15778981149196625,
      "learning_rate": 0.00015549692172383466,
      "loss": 0.0444,
      "step": 1013
    },
    {
      "epoch": 0.6688654353562006,
      "grad_norm": 0.2332177609205246,
      "learning_rate": 0.000155452946350044,
      "loss": 0.033,
      "step": 1014
    },
    {
      "epoch": 0.6695250659630607,
      "grad_norm": 0.21355974674224854,
      "learning_rate": 0.0001554089709762533,
      "loss": 0.0433,
      "step": 1015
    },
    {
      "epoch": 0.6701846965699209,
      "grad_norm": 0.32643255591392517,
      "learning_rate": 0.00015536499560246263,
      "loss": 0.0308,
      "step": 1016
    },
    {
      "epoch": 0.670844327176781,
      "grad_norm": 0.18646511435508728,
      "learning_rate": 0.00015532102022867194,
      "loss": 0.0677,
      "step": 1017
    },
    {
      "epoch": 0.6715039577836411,
      "grad_norm": 0.2442457526922226,
      "learning_rate": 0.00015527704485488128,
      "loss": 0.0426,
      "step": 1018
    },
    {
      "epoch": 0.6721635883905013,
      "grad_norm": 0.16497278213500977,
      "learning_rate": 0.0001552330694810906,
      "loss": 0.02,
      "step": 1019
    },
    {
      "epoch": 0.6728232189973615,
      "grad_norm": 0.10226771235466003,
      "learning_rate": 0.0001551890941072999,
      "loss": 0.0307,
      "step": 1020
    },
    {
      "epoch": 0.6734828496042217,
      "grad_norm": 0.09462656080722809,
      "learning_rate": 0.00015514511873350922,
      "loss": 0.0164,
      "step": 1021
    },
    {
      "epoch": 0.6741424802110818,
      "grad_norm": 0.07883922010660172,
      "learning_rate": 0.00015510114335971856,
      "loss": 0.019,
      "step": 1022
    },
    {
      "epoch": 0.674802110817942,
      "grad_norm": 0.21317118406295776,
      "learning_rate": 0.00015505716798592788,
      "loss": 0.0277,
      "step": 1023
    },
    {
      "epoch": 0.6754617414248021,
      "grad_norm": 0.2575826644897461,
      "learning_rate": 0.0001550131926121372,
      "loss": 0.0198,
      "step": 1024
    },
    {
      "epoch": 0.6761213720316622,
      "grad_norm": 0.19067777693271637,
      "learning_rate": 0.00015496921723834653,
      "loss": 0.0453,
      "step": 1025
    },
    {
      "epoch": 0.6767810026385225,
      "grad_norm": 0.3654399514198303,
      "learning_rate": 0.00015492524186455585,
      "loss": 0.0741,
      "step": 1026
    },
    {
      "epoch": 0.6774406332453826,
      "grad_norm": 0.06156393513083458,
      "learning_rate": 0.0001548812664907652,
      "loss": 0.0104,
      "step": 1027
    },
    {
      "epoch": 0.6781002638522428,
      "grad_norm": 0.23557965457439423,
      "learning_rate": 0.0001548372911169745,
      "loss": 0.0524,
      "step": 1028
    },
    {
      "epoch": 0.6787598944591029,
      "grad_norm": 0.06950674206018448,
      "learning_rate": 0.00015479331574318384,
      "loss": 0.0185,
      "step": 1029
    },
    {
      "epoch": 0.679419525065963,
      "grad_norm": 0.1712658554315567,
      "learning_rate": 0.00015474934036939316,
      "loss": 0.0317,
      "step": 1030
    },
    {
      "epoch": 0.6800791556728232,
      "grad_norm": 0.15258488059043884,
      "learning_rate": 0.00015470536499560247,
      "loss": 0.0572,
      "step": 1031
    },
    {
      "epoch": 0.6807387862796834,
      "grad_norm": 0.1701010763645172,
      "learning_rate": 0.0001546613896218118,
      "loss": 0.0352,
      "step": 1032
    },
    {
      "epoch": 0.6813984168865436,
      "grad_norm": 0.1473444253206253,
      "learning_rate": 0.00015461741424802113,
      "loss": 0.0403,
      "step": 1033
    },
    {
      "epoch": 0.6820580474934037,
      "grad_norm": 0.1350085586309433,
      "learning_rate": 0.00015457343887423044,
      "loss": 0.0242,
      "step": 1034
    },
    {
      "epoch": 0.6827176781002638,
      "grad_norm": 0.16764503717422485,
      "learning_rate": 0.00015452946350043975,
      "loss": 0.0182,
      "step": 1035
    },
    {
      "epoch": 0.683377308707124,
      "grad_norm": 0.31583908200263977,
      "learning_rate": 0.0001544854881266491,
      "loss": 0.0269,
      "step": 1036
    },
    {
      "epoch": 0.6840369393139841,
      "grad_norm": 0.10506128519773483,
      "learning_rate": 0.0001544415127528584,
      "loss": 0.0315,
      "step": 1037
    },
    {
      "epoch": 0.6846965699208444,
      "grad_norm": 0.47811004519462585,
      "learning_rate": 0.00015439753737906772,
      "loss": 0.0416,
      "step": 1038
    },
    {
      "epoch": 0.6853562005277045,
      "grad_norm": 0.15494322776794434,
      "learning_rate": 0.00015435356200527704,
      "loss": 0.0226,
      "step": 1039
    },
    {
      "epoch": 0.6860158311345647,
      "grad_norm": 0.08502530306577682,
      "learning_rate": 0.00015430958663148638,
      "loss": 0.0164,
      "step": 1040
    },
    {
      "epoch": 0.6866754617414248,
      "grad_norm": 0.09847616404294968,
      "learning_rate": 0.0001542656112576957,
      "loss": 0.0069,
      "step": 1041
    },
    {
      "epoch": 0.6873350923482849,
      "grad_norm": 0.11679835617542267,
      "learning_rate": 0.000154221635883905,
      "loss": 0.0242,
      "step": 1042
    },
    {
      "epoch": 0.6879947229551451,
      "grad_norm": 0.0547482967376709,
      "learning_rate": 0.00015417766051011435,
      "loss": 0.0102,
      "step": 1043
    },
    {
      "epoch": 0.6886543535620053,
      "grad_norm": 0.06231018900871277,
      "learning_rate": 0.00015413368513632366,
      "loss": 0.0065,
      "step": 1044
    },
    {
      "epoch": 0.6893139841688655,
      "grad_norm": 0.19784489274024963,
      "learning_rate": 0.00015408970976253298,
      "loss": 0.0478,
      "step": 1045
    },
    {
      "epoch": 0.6899736147757256,
      "grad_norm": 0.09717239439487457,
      "learning_rate": 0.0001540457343887423,
      "loss": 0.0167,
      "step": 1046
    },
    {
      "epoch": 0.6906332453825857,
      "grad_norm": 0.4859819710254669,
      "learning_rate": 0.00015400175901495163,
      "loss": 0.1096,
      "step": 1047
    },
    {
      "epoch": 0.6912928759894459,
      "grad_norm": 0.04957457631826401,
      "learning_rate": 0.00015395778364116094,
      "loss": 0.0069,
      "step": 1048
    },
    {
      "epoch": 0.691952506596306,
      "grad_norm": 0.2587793469429016,
      "learning_rate": 0.00015391380826737029,
      "loss": 0.0605,
      "step": 1049
    },
    {
      "epoch": 0.6926121372031663,
      "grad_norm": 0.04869779199361801,
      "learning_rate": 0.00015386983289357963,
      "loss": 0.0116,
      "step": 1050
    },
    {
      "epoch": 0.6932717678100264,
      "grad_norm": 0.14741291105747223,
      "learning_rate": 0.00015382585751978894,
      "loss": 0.0304,
      "step": 1051
    },
    {
      "epoch": 0.6939313984168866,
      "grad_norm": 0.279996395111084,
      "learning_rate": 0.00015378188214599825,
      "loss": 0.0771,
      "step": 1052
    },
    {
      "epoch": 0.6945910290237467,
      "grad_norm": 0.12454206496477127,
      "learning_rate": 0.00015373790677220757,
      "loss": 0.0203,
      "step": 1053
    },
    {
      "epoch": 0.6952506596306068,
      "grad_norm": 0.1441766619682312,
      "learning_rate": 0.0001536939313984169,
      "loss": 0.0463,
      "step": 1054
    },
    {
      "epoch": 0.695910290237467,
      "grad_norm": 0.13774041831493378,
      "learning_rate": 0.00015364995602462622,
      "loss": 0.0361,
      "step": 1055
    },
    {
      "epoch": 0.6965699208443272,
      "grad_norm": 0.1935005635023117,
      "learning_rate": 0.00015360598065083554,
      "loss": 0.0319,
      "step": 1056
    },
    {
      "epoch": 0.6972295514511874,
      "grad_norm": 0.1527407169342041,
      "learning_rate": 0.00015356200527704485,
      "loss": 0.0144,
      "step": 1057
    },
    {
      "epoch": 0.6978891820580475,
      "grad_norm": 0.25606435537338257,
      "learning_rate": 0.0001535180299032542,
      "loss": 0.0253,
      "step": 1058
    },
    {
      "epoch": 0.6985488126649076,
      "grad_norm": 0.18142572045326233,
      "learning_rate": 0.0001534740545294635,
      "loss": 0.0233,
      "step": 1059
    },
    {
      "epoch": 0.6992084432717678,
      "grad_norm": 0.11222656071186066,
      "learning_rate": 0.00015343007915567282,
      "loss": 0.0277,
      "step": 1060
    },
    {
      "epoch": 0.6998680738786279,
      "grad_norm": 0.11700859665870667,
      "learning_rate": 0.00015338610378188216,
      "loss": 0.0205,
      "step": 1061
    },
    {
      "epoch": 0.7005277044854882,
      "grad_norm": 0.07980934530496597,
      "learning_rate": 0.00015334212840809148,
      "loss": 0.0119,
      "step": 1062
    },
    {
      "epoch": 0.7011873350923483,
      "grad_norm": 0.04784706234931946,
      "learning_rate": 0.0001532981530343008,
      "loss": 0.009,
      "step": 1063
    },
    {
      "epoch": 0.7018469656992085,
      "grad_norm": 0.09032504260540009,
      "learning_rate": 0.0001532541776605101,
      "loss": 0.0171,
      "step": 1064
    },
    {
      "epoch": 0.7025065963060686,
      "grad_norm": 0.27711087465286255,
      "learning_rate": 0.00015321020228671944,
      "loss": 0.0611,
      "step": 1065
    },
    {
      "epoch": 0.7031662269129287,
      "grad_norm": 0.2623445987701416,
      "learning_rate": 0.00015316622691292876,
      "loss": 0.0597,
      "step": 1066
    },
    {
      "epoch": 0.7038258575197889,
      "grad_norm": 0.19574065506458282,
      "learning_rate": 0.00015312225153913807,
      "loss": 0.0502,
      "step": 1067
    },
    {
      "epoch": 0.7044854881266491,
      "grad_norm": 0.10838811844587326,
      "learning_rate": 0.0001530782761653474,
      "loss": 0.0255,
      "step": 1068
    },
    {
      "epoch": 0.7051451187335093,
      "grad_norm": 0.12475432455539703,
      "learning_rate": 0.00015303430079155673,
      "loss": 0.02,
      "step": 1069
    },
    {
      "epoch": 0.7058047493403694,
      "grad_norm": 0.07454540580511093,
      "learning_rate": 0.00015299032541776607,
      "loss": 0.0167,
      "step": 1070
    },
    {
      "epoch": 0.7064643799472295,
      "grad_norm": 0.38088977336883545,
      "learning_rate": 0.00015294635004397538,
      "loss": 0.1182,
      "step": 1071
    },
    {
      "epoch": 0.7071240105540897,
      "grad_norm": 0.10724731534719467,
      "learning_rate": 0.00015290237467018472,
      "loss": 0.0291,
      "step": 1072
    },
    {
      "epoch": 0.7077836411609498,
      "grad_norm": 0.09993357211351395,
      "learning_rate": 0.00015285839929639404,
      "loss": 0.024,
      "step": 1073
    },
    {
      "epoch": 0.7084432717678101,
      "grad_norm": 0.12846559286117554,
      "learning_rate": 0.00015281442392260335,
      "loss": 0.0187,
      "step": 1074
    },
    {
      "epoch": 0.7091029023746702,
      "grad_norm": 0.12419798225164413,
      "learning_rate": 0.00015277044854881267,
      "loss": 0.0344,
      "step": 1075
    },
    {
      "epoch": 0.7097625329815304,
      "grad_norm": 0.22292475402355194,
      "learning_rate": 0.000152726473175022,
      "loss": 0.0436,
      "step": 1076
    },
    {
      "epoch": 0.7104221635883905,
      "grad_norm": 0.18017816543579102,
      "learning_rate": 0.00015268249780123132,
      "loss": 0.0143,
      "step": 1077
    },
    {
      "epoch": 0.7110817941952506,
      "grad_norm": 0.09987972676753998,
      "learning_rate": 0.00015263852242744063,
      "loss": 0.0305,
      "step": 1078
    },
    {
      "epoch": 0.7117414248021108,
      "grad_norm": 0.10090066492557526,
      "learning_rate": 0.00015259454705364998,
      "loss": 0.0209,
      "step": 1079
    },
    {
      "epoch": 0.712401055408971,
      "grad_norm": 0.11489244550466537,
      "learning_rate": 0.0001525505716798593,
      "loss": 0.0409,
      "step": 1080
    },
    {
      "epoch": 0.7130606860158312,
      "grad_norm": 0.8420856595039368,
      "learning_rate": 0.0001525065963060686,
      "loss": 0.0843,
      "step": 1081
    },
    {
      "epoch": 0.7137203166226913,
      "grad_norm": 0.2139727771282196,
      "learning_rate": 0.00015246262093227792,
      "loss": 0.0709,
      "step": 1082
    },
    {
      "epoch": 0.7143799472295514,
      "grad_norm": 0.1132567971944809,
      "learning_rate": 0.00015241864555848726,
      "loss": 0.019,
      "step": 1083
    },
    {
      "epoch": 0.7150395778364116,
      "grad_norm": 0.12938563525676727,
      "learning_rate": 0.00015237467018469657,
      "loss": 0.0408,
      "step": 1084
    },
    {
      "epoch": 0.7156992084432717,
      "grad_norm": 0.10310547798871994,
      "learning_rate": 0.00015233069481090589,
      "loss": 0.0317,
      "step": 1085
    },
    {
      "epoch": 0.716358839050132,
      "grad_norm": 0.06401032954454422,
      "learning_rate": 0.00015228671943711523,
      "loss": 0.0168,
      "step": 1086
    },
    {
      "epoch": 0.7170184696569921,
      "grad_norm": 0.09109515696763992,
      "learning_rate": 0.00015224274406332454,
      "loss": 0.022,
      "step": 1087
    },
    {
      "epoch": 0.7176781002638523,
      "grad_norm": 0.07810726016759872,
      "learning_rate": 0.00015219876868953386,
      "loss": 0.0194,
      "step": 1088
    },
    {
      "epoch": 0.7183377308707124,
      "grad_norm": 0.13174159824848175,
      "learning_rate": 0.0001521547933157432,
      "loss": 0.0338,
      "step": 1089
    },
    {
      "epoch": 0.7189973614775725,
      "grad_norm": 0.07021994888782501,
      "learning_rate": 0.0001521108179419525,
      "loss": 0.0146,
      "step": 1090
    },
    {
      "epoch": 0.7196569920844327,
      "grad_norm": 0.1110735610127449,
      "learning_rate": 0.00015206684256816185,
      "loss": 0.0328,
      "step": 1091
    },
    {
      "epoch": 0.7203166226912929,
      "grad_norm": 0.7329087853431702,
      "learning_rate": 0.00015202286719437117,
      "loss": 0.0613,
      "step": 1092
    },
    {
      "epoch": 0.7209762532981531,
      "grad_norm": 0.18205001950263977,
      "learning_rate": 0.00015197889182058048,
      "loss": 0.0535,
      "step": 1093
    },
    {
      "epoch": 0.7216358839050132,
      "grad_norm": 0.1592993438243866,
      "learning_rate": 0.00015193491644678982,
      "loss": 0.0403,
      "step": 1094
    },
    {
      "epoch": 0.7222955145118733,
      "grad_norm": 0.09413556754589081,
      "learning_rate": 0.00015189094107299913,
      "loss": 0.018,
      "step": 1095
    },
    {
      "epoch": 0.7229551451187335,
      "grad_norm": 0.23609623312950134,
      "learning_rate": 0.00015184696569920845,
      "loss": 0.0415,
      "step": 1096
    },
    {
      "epoch": 0.7236147757255936,
      "grad_norm": 0.09255460649728775,
      "learning_rate": 0.0001518029903254178,
      "loss": 0.0074,
      "step": 1097
    },
    {
      "epoch": 0.7242744063324539,
      "grad_norm": 0.1699853539466858,
      "learning_rate": 0.0001517590149516271,
      "loss": 0.0499,
      "step": 1098
    },
    {
      "epoch": 0.724934036939314,
      "grad_norm": 0.09596843272447586,
      "learning_rate": 0.00015171503957783642,
      "loss": 0.0191,
      "step": 1099
    },
    {
      "epoch": 0.7255936675461742,
      "grad_norm": 0.09993493556976318,
      "learning_rate": 0.00015167106420404573,
      "loss": 0.0296,
      "step": 1100
    },
    {
      "epoch": 0.7262532981530343,
      "grad_norm": 0.10386785119771957,
      "learning_rate": 0.00015162708883025507,
      "loss": 0.0179,
      "step": 1101
    },
    {
      "epoch": 0.7269129287598944,
      "grad_norm": 0.08137441426515579,
      "learning_rate": 0.00015158311345646439,
      "loss": 0.0065,
      "step": 1102
    },
    {
      "epoch": 0.7275725593667546,
      "grad_norm": 0.12285342067480087,
      "learning_rate": 0.0001515391380826737,
      "loss": 0.0321,
      "step": 1103
    },
    {
      "epoch": 0.7282321899736148,
      "grad_norm": 0.12240971624851227,
      "learning_rate": 0.00015149516270888304,
      "loss": 0.0283,
      "step": 1104
    },
    {
      "epoch": 0.728891820580475,
      "grad_norm": 0.14870119094848633,
      "learning_rate": 0.00015145118733509236,
      "loss": 0.0361,
      "step": 1105
    },
    {
      "epoch": 0.7295514511873351,
      "grad_norm": 0.2626568078994751,
      "learning_rate": 0.00015140721196130167,
      "loss": 0.0774,
      "step": 1106
    },
    {
      "epoch": 0.7302110817941952,
      "grad_norm": 0.16298460960388184,
      "learning_rate": 0.00015136323658751098,
      "loss": 0.0352,
      "step": 1107
    },
    {
      "epoch": 0.7308707124010554,
      "grad_norm": 0.07011725753545761,
      "learning_rate": 0.00015131926121372032,
      "loss": 0.0158,
      "step": 1108
    },
    {
      "epoch": 0.7315303430079155,
      "grad_norm": 0.09363522380590439,
      "learning_rate": 0.00015127528583992964,
      "loss": 0.0288,
      "step": 1109
    },
    {
      "epoch": 0.7321899736147758,
      "grad_norm": 0.11241000145673752,
      "learning_rate": 0.00015123131046613895,
      "loss": 0.0331,
      "step": 1110
    },
    {
      "epoch": 0.7328496042216359,
      "grad_norm": 0.08750134706497192,
      "learning_rate": 0.0001511873350923483,
      "loss": 0.0282,
      "step": 1111
    },
    {
      "epoch": 0.7335092348284961,
      "grad_norm": 0.16730190813541412,
      "learning_rate": 0.0001511433597185576,
      "loss": 0.0493,
      "step": 1112
    },
    {
      "epoch": 0.7341688654353562,
      "grad_norm": 0.1700805425643921,
      "learning_rate": 0.00015109938434476695,
      "loss": 0.0441,
      "step": 1113
    },
    {
      "epoch": 0.7348284960422163,
      "grad_norm": 0.1306026130914688,
      "learning_rate": 0.00015105540897097626,
      "loss": 0.0277,
      "step": 1114
    },
    {
      "epoch": 0.7354881266490765,
      "grad_norm": 0.22116461396217346,
      "learning_rate": 0.0001510114335971856,
      "loss": 0.0368,
      "step": 1115
    },
    {
      "epoch": 0.7361477572559367,
      "grad_norm": 0.15216684341430664,
      "learning_rate": 0.00015096745822339492,
      "loss": 0.0154,
      "step": 1116
    },
    {
      "epoch": 0.7368073878627969,
      "grad_norm": 0.18610331416130066,
      "learning_rate": 0.00015092348284960423,
      "loss": 0.0166,
      "step": 1117
    },
    {
      "epoch": 0.737467018469657,
      "grad_norm": 0.167336106300354,
      "learning_rate": 0.00015087950747581354,
      "loss": 0.0403,
      "step": 1118
    },
    {
      "epoch": 0.7381266490765171,
      "grad_norm": 0.0707329586148262,
      "learning_rate": 0.00015083553210202289,
      "loss": 0.0082,
      "step": 1119
    },
    {
      "epoch": 0.7387862796833773,
      "grad_norm": 0.09035850316286087,
      "learning_rate": 0.0001507915567282322,
      "loss": 0.0184,
      "step": 1120
    },
    {
      "epoch": 0.7394459102902374,
      "grad_norm": 0.07237391918897629,
      "learning_rate": 0.00015074758135444151,
      "loss": 0.0133,
      "step": 1121
    },
    {
      "epoch": 0.7401055408970977,
      "grad_norm": 0.2025974690914154,
      "learning_rate": 0.00015070360598065086,
      "loss": 0.046,
      "step": 1122
    },
    {
      "epoch": 0.7407651715039578,
      "grad_norm": 0.12884214520454407,
      "learning_rate": 0.00015065963060686017,
      "loss": 0.0315,
      "step": 1123
    },
    {
      "epoch": 0.741424802110818,
      "grad_norm": 0.11418743431568146,
      "learning_rate": 0.00015061565523306948,
      "loss": 0.0227,
      "step": 1124
    },
    {
      "epoch": 0.7420844327176781,
      "grad_norm": 0.2529642581939697,
      "learning_rate": 0.0001505716798592788,
      "loss": 0.0461,
      "step": 1125
    },
    {
      "epoch": 0.7427440633245382,
      "grad_norm": 0.24269109964370728,
      "learning_rate": 0.00015052770448548814,
      "loss": 0.0586,
      "step": 1126
    },
    {
      "epoch": 0.7434036939313984,
      "grad_norm": 0.24893821775913239,
      "learning_rate": 0.00015048372911169745,
      "loss": 0.0458,
      "step": 1127
    },
    {
      "epoch": 0.7440633245382586,
      "grad_norm": 0.09815414994955063,
      "learning_rate": 0.00015043975373790677,
      "loss": 0.0071,
      "step": 1128
    },
    {
      "epoch": 0.7447229551451188,
      "grad_norm": 0.16130994260311127,
      "learning_rate": 0.00015039577836411608,
      "loss": 0.0373,
      "step": 1129
    },
    {
      "epoch": 0.7453825857519789,
      "grad_norm": 0.1290937066078186,
      "learning_rate": 0.00015035180299032542,
      "loss": 0.0247,
      "step": 1130
    },
    {
      "epoch": 0.746042216358839,
      "grad_norm": 0.1623949557542801,
      "learning_rate": 0.00015030782761653473,
      "loss": 0.0305,
      "step": 1131
    },
    {
      "epoch": 0.7467018469656992,
      "grad_norm": 0.1786722093820572,
      "learning_rate": 0.00015026385224274408,
      "loss": 0.0494,
      "step": 1132
    },
    {
      "epoch": 0.7473614775725593,
      "grad_norm": 0.255016028881073,
      "learning_rate": 0.0001502198768689534,
      "loss": 0.0633,
      "step": 1133
    },
    {
      "epoch": 0.7480211081794196,
      "grad_norm": 0.27951112389564514,
      "learning_rate": 0.00015017590149516273,
      "loss": 0.0436,
      "step": 1134
    },
    {
      "epoch": 0.7486807387862797,
      "grad_norm": 0.27387458086013794,
      "learning_rate": 0.00015013192612137204,
      "loss": 0.0257,
      "step": 1135
    },
    {
      "epoch": 0.7493403693931399,
      "grad_norm": 0.08721756935119629,
      "learning_rate": 0.00015008795074758136,
      "loss": 0.014,
      "step": 1136
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.07264388352632523,
      "learning_rate": 0.0001500439753737907,
      "loss": 0.0052,
      "step": 1137
    },
    {
      "epoch": 0.7506596306068601,
      "grad_norm": 0.029796576127409935,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.002,
      "step": 1138
    },
    {
      "epoch": 0.7513192612137203,
      "grad_norm": 0.21070197224617004,
      "learning_rate": 0.00014995602462620933,
      "loss": 0.0516,
      "step": 1139
    },
    {
      "epoch": 0.7519788918205804,
      "grad_norm": 0.13773980736732483,
      "learning_rate": 0.00014991204925241867,
      "loss": 0.0303,
      "step": 1140
    },
    {
      "epoch": 0.7526385224274407,
      "grad_norm": 0.054930493235588074,
      "learning_rate": 0.00014986807387862798,
      "loss": 0.0166,
      "step": 1141
    },
    {
      "epoch": 0.7532981530343008,
      "grad_norm": 0.05685633420944214,
      "learning_rate": 0.0001498240985048373,
      "loss": 0.015,
      "step": 1142
    },
    {
      "epoch": 0.753957783641161,
      "grad_norm": 0.05582136660814285,
      "learning_rate": 0.0001497801231310466,
      "loss": 0.0135,
      "step": 1143
    },
    {
      "epoch": 0.7546174142480211,
      "grad_norm": 0.09212943911552429,
      "learning_rate": 0.00014973614775725595,
      "loss": 0.0268,
      "step": 1144
    },
    {
      "epoch": 0.7552770448548812,
      "grad_norm": 0.21025791764259338,
      "learning_rate": 0.00014969217238346527,
      "loss": 0.0533,
      "step": 1145
    },
    {
      "epoch": 0.7559366754617414,
      "grad_norm": 0.14615651965141296,
      "learning_rate": 0.00014964819700967458,
      "loss": 0.0364,
      "step": 1146
    },
    {
      "epoch": 0.7565963060686016,
      "grad_norm": 0.10061509162187576,
      "learning_rate": 0.00014960422163588392,
      "loss": 0.0246,
      "step": 1147
    },
    {
      "epoch": 0.7572559366754618,
      "grad_norm": 0.20791466534137726,
      "learning_rate": 0.00014956024626209323,
      "loss": 0.0513,
      "step": 1148
    },
    {
      "epoch": 0.7579155672823219,
      "grad_norm": 0.14291352033615112,
      "learning_rate": 0.00014951627088830255,
      "loss": 0.0463,
      "step": 1149
    },
    {
      "epoch": 0.758575197889182,
      "grad_norm": 0.06363843381404877,
      "learning_rate": 0.00014947229551451186,
      "loss": 0.0204,
      "step": 1150
    },
    {
      "epoch": 0.7592348284960422,
      "grad_norm": 0.06722096353769302,
      "learning_rate": 0.0001494283201407212,
      "loss": 0.0228,
      "step": 1151
    },
    {
      "epoch": 0.7598944591029023,
      "grad_norm": 0.16037046909332275,
      "learning_rate": 0.00014938434476693052,
      "loss": 0.0372,
      "step": 1152
    },
    {
      "epoch": 0.7605540897097626,
      "grad_norm": 0.08683419972658157,
      "learning_rate": 0.00014934036939313983,
      "loss": 0.0067,
      "step": 1153
    },
    {
      "epoch": 0.7612137203166227,
      "grad_norm": 0.12339337915182114,
      "learning_rate": 0.00014929639401934917,
      "loss": 0.0378,
      "step": 1154
    },
    {
      "epoch": 0.7618733509234829,
      "grad_norm": 0.14152918756008148,
      "learning_rate": 0.00014925241864555851,
      "loss": 0.0292,
      "step": 1155
    },
    {
      "epoch": 0.762532981530343,
      "grad_norm": 0.13841411471366882,
      "learning_rate": 0.00014920844327176783,
      "loss": 0.0394,
      "step": 1156
    },
    {
      "epoch": 0.7631926121372031,
      "grad_norm": 0.14338889718055725,
      "learning_rate": 0.00014916446789797714,
      "loss": 0.0333,
      "step": 1157
    },
    {
      "epoch": 0.7638522427440633,
      "grad_norm": 0.12224128097295761,
      "learning_rate": 0.00014912049252418648,
      "loss": 0.0303,
      "step": 1158
    },
    {
      "epoch": 0.7645118733509235,
      "grad_norm": 0.12962348759174347,
      "learning_rate": 0.0001490765171503958,
      "loss": 0.0125,
      "step": 1159
    },
    {
      "epoch": 0.7651715039577837,
      "grad_norm": 0.17277273535728455,
      "learning_rate": 0.0001490325417766051,
      "loss": 0.0554,
      "step": 1160
    },
    {
      "epoch": 0.7658311345646438,
      "grad_norm": 0.08710812032222748,
      "learning_rate": 0.00014898856640281442,
      "loss": 0.0176,
      "step": 1161
    },
    {
      "epoch": 0.7664907651715039,
      "grad_norm": 0.14732180535793304,
      "learning_rate": 0.00014894459102902377,
      "loss": 0.0171,
      "step": 1162
    },
    {
      "epoch": 0.7671503957783641,
      "grad_norm": 0.16634072363376617,
      "learning_rate": 0.00014890061565523308,
      "loss": 0.046,
      "step": 1163
    },
    {
      "epoch": 0.7678100263852242,
      "grad_norm": 0.09661095589399338,
      "learning_rate": 0.0001488566402814424,
      "loss": 0.0164,
      "step": 1164
    },
    {
      "epoch": 0.7684696569920845,
      "grad_norm": 0.3226813077926636,
      "learning_rate": 0.00014881266490765173,
      "loss": 0.0608,
      "step": 1165
    },
    {
      "epoch": 0.7691292875989446,
      "grad_norm": 0.03305474668741226,
      "learning_rate": 0.00014876868953386105,
      "loss": 0.0017,
      "step": 1166
    },
    {
      "epoch": 0.7697889182058048,
      "grad_norm": 0.21746405959129333,
      "learning_rate": 0.00014872471416007036,
      "loss": 0.0329,
      "step": 1167
    },
    {
      "epoch": 0.7704485488126649,
      "grad_norm": 0.12151490151882172,
      "learning_rate": 0.00014868073878627968,
      "loss": 0.0272,
      "step": 1168
    },
    {
      "epoch": 0.771108179419525,
      "grad_norm": 0.17121681571006775,
      "learning_rate": 0.00014863676341248902,
      "loss": 0.0366,
      "step": 1169
    },
    {
      "epoch": 0.7717678100263852,
      "grad_norm": 0.08975103497505188,
      "learning_rate": 0.00014859278803869833,
      "loss": 0.018,
      "step": 1170
    },
    {
      "epoch": 0.7724274406332454,
      "grad_norm": 0.1666780710220337,
      "learning_rate": 0.00014854881266490765,
      "loss": 0.0264,
      "step": 1171
    },
    {
      "epoch": 0.7730870712401056,
      "grad_norm": 0.4702862501144409,
      "learning_rate": 0.00014850483729111696,
      "loss": 0.103,
      "step": 1172
    },
    {
      "epoch": 0.7737467018469657,
      "grad_norm": 0.08853144943714142,
      "learning_rate": 0.0001484608619173263,
      "loss": 0.0054,
      "step": 1173
    },
    {
      "epoch": 0.7744063324538258,
      "grad_norm": 0.23726508021354675,
      "learning_rate": 0.00014841688654353561,
      "loss": 0.0415,
      "step": 1174
    },
    {
      "epoch": 0.775065963060686,
      "grad_norm": 0.18575792014598846,
      "learning_rate": 0.00014837291116974496,
      "loss": 0.0267,
      "step": 1175
    },
    {
      "epoch": 0.7757255936675461,
      "grad_norm": 0.2986917197704315,
      "learning_rate": 0.00014832893579595427,
      "loss": 0.0262,
      "step": 1176
    },
    {
      "epoch": 0.7763852242744064,
      "grad_norm": 0.24924983084201813,
      "learning_rate": 0.0001482849604221636,
      "loss": 0.0444,
      "step": 1177
    },
    {
      "epoch": 0.7770448548812665,
      "grad_norm": 0.258334755897522,
      "learning_rate": 0.00014824098504837292,
      "loss": 0.0209,
      "step": 1178
    },
    {
      "epoch": 0.7777044854881267,
      "grad_norm": 0.1042453944683075,
      "learning_rate": 0.00014819700967458224,
      "loss": 0.007,
      "step": 1179
    },
    {
      "epoch": 0.7783641160949868,
      "grad_norm": 0.2734003961086273,
      "learning_rate": 0.00014815303430079158,
      "loss": 0.0261,
      "step": 1180
    },
    {
      "epoch": 0.7790237467018469,
      "grad_norm": 0.12018900364637375,
      "learning_rate": 0.0001481090589270009,
      "loss": 0.0184,
      "step": 1181
    },
    {
      "epoch": 0.7796833773087071,
      "grad_norm": 0.03656395152211189,
      "learning_rate": 0.0001480650835532102,
      "loss": 0.0022,
      "step": 1182
    },
    {
      "epoch": 0.7803430079155673,
      "grad_norm": 0.1691662222146988,
      "learning_rate": 0.00014802110817941955,
      "loss": 0.0384,
      "step": 1183
    },
    {
      "epoch": 0.7810026385224275,
      "grad_norm": 0.19660137593746185,
      "learning_rate": 0.00014797713280562886,
      "loss": 0.0432,
      "step": 1184
    },
    {
      "epoch": 0.7816622691292876,
      "grad_norm": 0.10865235328674316,
      "learning_rate": 0.00014793315743183818,
      "loss": 0.024,
      "step": 1185
    },
    {
      "epoch": 0.7823218997361477,
      "grad_norm": 0.12858985364437103,
      "learning_rate": 0.0001478891820580475,
      "loss": 0.0264,
      "step": 1186
    },
    {
      "epoch": 0.7829815303430079,
      "grad_norm": 0.1542041152715683,
      "learning_rate": 0.00014784520668425683,
      "loss": 0.0304,
      "step": 1187
    },
    {
      "epoch": 0.783641160949868,
      "grad_norm": 0.13013005256652832,
      "learning_rate": 0.00014780123131046615,
      "loss": 0.0251,
      "step": 1188
    },
    {
      "epoch": 0.7843007915567283,
      "grad_norm": 0.06414933502674103,
      "learning_rate": 0.00014775725593667546,
      "loss": 0.0043,
      "step": 1189
    },
    {
      "epoch": 0.7849604221635884,
      "grad_norm": 0.2554457485675812,
      "learning_rate": 0.00014771328056288477,
      "loss": 0.0389,
      "step": 1190
    },
    {
      "epoch": 0.7856200527704486,
      "grad_norm": 0.08315733820199966,
      "learning_rate": 0.00014766930518909411,
      "loss": 0.0177,
      "step": 1191
    },
    {
      "epoch": 0.7862796833773087,
      "grad_norm": 0.1026112362742424,
      "learning_rate": 0.00014762532981530343,
      "loss": 0.0252,
      "step": 1192
    },
    {
      "epoch": 0.7869393139841688,
      "grad_norm": 0.17099954187870026,
      "learning_rate": 0.00014758135444151274,
      "loss": 0.0367,
      "step": 1193
    },
    {
      "epoch": 0.787598944591029,
      "grad_norm": 0.13773764669895172,
      "learning_rate": 0.00014753737906772208,
      "loss": 0.0418,
      "step": 1194
    },
    {
      "epoch": 0.7882585751978892,
      "grad_norm": 0.10139429569244385,
      "learning_rate": 0.0001474934036939314,
      "loss": 0.034,
      "step": 1195
    },
    {
      "epoch": 0.7889182058047494,
      "grad_norm": 0.0975533127784729,
      "learning_rate": 0.00014744942832014074,
      "loss": 0.0176,
      "step": 1196
    },
    {
      "epoch": 0.7895778364116095,
      "grad_norm": 0.14308832585811615,
      "learning_rate": 0.00014740545294635005,
      "loss": 0.0452,
      "step": 1197
    },
    {
      "epoch": 0.7902374670184696,
      "grad_norm": 0.09715338796377182,
      "learning_rate": 0.0001473614775725594,
      "loss": 0.0266,
      "step": 1198
    },
    {
      "epoch": 0.7908970976253298,
      "grad_norm": 0.22028984129428864,
      "learning_rate": 0.0001473175021987687,
      "loss": 0.0619,
      "step": 1199
    },
    {
      "epoch": 0.7915567282321899,
      "grad_norm": 0.14724934101104736,
      "learning_rate": 0.00014727352682497802,
      "loss": 0.0188,
      "step": 1200
    },
    {
      "epoch": 0.7922163588390502,
      "grad_norm": 0.14176255464553833,
      "learning_rate": 0.00014722955145118736,
      "loss": 0.0371,
      "step": 1201
    },
    {
      "epoch": 0.7928759894459103,
      "grad_norm": 0.2141813188791275,
      "learning_rate": 0.00014718557607739668,
      "loss": 0.0614,
      "step": 1202
    },
    {
      "epoch": 0.7935356200527705,
      "grad_norm": 0.22877326607704163,
      "learning_rate": 0.000147141600703606,
      "loss": 0.0364,
      "step": 1203
    },
    {
      "epoch": 0.7941952506596306,
      "grad_norm": 0.1719219833612442,
      "learning_rate": 0.0001470976253298153,
      "loss": 0.0239,
      "step": 1204
    },
    {
      "epoch": 0.7948548812664907,
      "grad_norm": 0.17348599433898926,
      "learning_rate": 0.00014705364995602465,
      "loss": 0.0223,
      "step": 1205
    },
    {
      "epoch": 0.7955145118733509,
      "grad_norm": 0.1077476441860199,
      "learning_rate": 0.00014700967458223396,
      "loss": 0.0263,
      "step": 1206
    },
    {
      "epoch": 0.7961741424802111,
      "grad_norm": 0.07950446009635925,
      "learning_rate": 0.00014696569920844327,
      "loss": 0.0195,
      "step": 1207
    },
    {
      "epoch": 0.7968337730870713,
      "grad_norm": 0.2656054198741913,
      "learning_rate": 0.0001469217238346526,
      "loss": 0.0552,
      "step": 1208
    },
    {
      "epoch": 0.7974934036939314,
      "grad_norm": 0.18435898423194885,
      "learning_rate": 0.00014687774846086193,
      "loss": 0.0459,
      "step": 1209
    },
    {
      "epoch": 0.7981530343007915,
      "grad_norm": 0.10144588351249695,
      "learning_rate": 0.00014683377308707124,
      "loss": 0.0248,
      "step": 1210
    },
    {
      "epoch": 0.7988126649076517,
      "grad_norm": 0.12228976935148239,
      "learning_rate": 0.00014678979771328056,
      "loss": 0.024,
      "step": 1211
    },
    {
      "epoch": 0.7994722955145118,
      "grad_norm": 0.14885996282100677,
      "learning_rate": 0.0001467458223394899,
      "loss": 0.0366,
      "step": 1212
    },
    {
      "epoch": 0.8001319261213721,
      "grad_norm": 0.08718258142471313,
      "learning_rate": 0.0001467018469656992,
      "loss": 0.0125,
      "step": 1213
    },
    {
      "epoch": 0.8007915567282322,
      "grad_norm": 0.150014728307724,
      "learning_rate": 0.00014665787159190853,
      "loss": 0.026,
      "step": 1214
    },
    {
      "epoch": 0.8014511873350924,
      "grad_norm": 0.2707614600658417,
      "learning_rate": 0.00014661389621811784,
      "loss": 0.0616,
      "step": 1215
    },
    {
      "epoch": 0.8021108179419525,
      "grad_norm": 0.12168911099433899,
      "learning_rate": 0.00014656992084432718,
      "loss": 0.019,
      "step": 1216
    },
    {
      "epoch": 0.8027704485488126,
      "grad_norm": 0.12318751215934753,
      "learning_rate": 0.0001465259454705365,
      "loss": 0.0343,
      "step": 1217
    },
    {
      "epoch": 0.8034300791556728,
      "grad_norm": 0.10471601784229279,
      "learning_rate": 0.00014648197009674584,
      "loss": 0.0073,
      "step": 1218
    },
    {
      "epoch": 0.804089709762533,
      "grad_norm": 0.10562022030353546,
      "learning_rate": 0.00014643799472295515,
      "loss": 0.0138,
      "step": 1219
    },
    {
      "epoch": 0.8047493403693932,
      "grad_norm": 0.32828986644744873,
      "learning_rate": 0.0001463940193491645,
      "loss": 0.094,
      "step": 1220
    },
    {
      "epoch": 0.8054089709762533,
      "grad_norm": 0.07620206475257874,
      "learning_rate": 0.0001463500439753738,
      "loss": 0.0119,
      "step": 1221
    },
    {
      "epoch": 0.8060686015831134,
      "grad_norm": 0.22666795551776886,
      "learning_rate": 0.00014630606860158312,
      "loss": 0.0616,
      "step": 1222
    },
    {
      "epoch": 0.8067282321899736,
      "grad_norm": 0.0935964286327362,
      "learning_rate": 0.00014626209322779246,
      "loss": 0.0202,
      "step": 1223
    },
    {
      "epoch": 0.8073878627968337,
      "grad_norm": 0.10287509858608246,
      "learning_rate": 0.00014621811785400177,
      "loss": 0.0216,
      "step": 1224
    },
    {
      "epoch": 0.808047493403694,
      "grad_norm": 0.10089585930109024,
      "learning_rate": 0.0001461741424802111,
      "loss": 0.0272,
      "step": 1225
    },
    {
      "epoch": 0.8087071240105541,
      "grad_norm": 0.14899571239948273,
      "learning_rate": 0.0001461301671064204,
      "loss": 0.0391,
      "step": 1226
    },
    {
      "epoch": 0.8093667546174143,
      "grad_norm": 0.13873064517974854,
      "learning_rate": 0.00014608619173262974,
      "loss": 0.0342,
      "step": 1227
    },
    {
      "epoch": 0.8100263852242744,
      "grad_norm": 0.1265122890472412,
      "learning_rate": 0.00014604221635883906,
      "loss": 0.0532,
      "step": 1228
    },
    {
      "epoch": 0.8106860158311345,
      "grad_norm": 0.13974390923976898,
      "learning_rate": 0.00014599824098504837,
      "loss": 0.037,
      "step": 1229
    },
    {
      "epoch": 0.8113456464379947,
      "grad_norm": 0.09109117090702057,
      "learning_rate": 0.0001459542656112577,
      "loss": 0.018,
      "step": 1230
    },
    {
      "epoch": 0.8120052770448549,
      "grad_norm": 0.12234988063573837,
      "learning_rate": 0.00014591029023746703,
      "loss": 0.0235,
      "step": 1231
    },
    {
      "epoch": 0.8126649076517151,
      "grad_norm": 0.0939435288310051,
      "learning_rate": 0.00014586631486367634,
      "loss": 0.0198,
      "step": 1232
    },
    {
      "epoch": 0.8133245382585752,
      "grad_norm": 0.18718048930168152,
      "learning_rate": 0.00014582233948988565,
      "loss": 0.0492,
      "step": 1233
    },
    {
      "epoch": 0.8139841688654353,
      "grad_norm": 0.17891019582748413,
      "learning_rate": 0.000145778364116095,
      "loss": 0.0254,
      "step": 1234
    },
    {
      "epoch": 0.8146437994722955,
      "grad_norm": 0.13952727615833282,
      "learning_rate": 0.0001457343887423043,
      "loss": 0.0263,
      "step": 1235
    },
    {
      "epoch": 0.8153034300791556,
      "grad_norm": 0.14642462134361267,
      "learning_rate": 0.00014569041336851362,
      "loss": 0.0365,
      "step": 1236
    },
    {
      "epoch": 0.8159630606860159,
      "grad_norm": 0.1176135465502739,
      "learning_rate": 0.00014564643799472296,
      "loss": 0.031,
      "step": 1237
    },
    {
      "epoch": 0.816622691292876,
      "grad_norm": 0.08808088302612305,
      "learning_rate": 0.00014560246262093228,
      "loss": 0.0191,
      "step": 1238
    },
    {
      "epoch": 0.8172823218997362,
      "grad_norm": 0.18374067544937134,
      "learning_rate": 0.00014555848724714162,
      "loss": 0.0323,
      "step": 1239
    },
    {
      "epoch": 0.8179419525065963,
      "grad_norm": 0.1290590614080429,
      "learning_rate": 0.00014551451187335093,
      "loss": 0.0334,
      "step": 1240
    },
    {
      "epoch": 0.8186015831134564,
      "grad_norm": 0.1516120582818985,
      "learning_rate": 0.00014547053649956027,
      "loss": 0.0537,
      "step": 1241
    },
    {
      "epoch": 0.8192612137203166,
      "grad_norm": 0.11060772836208344,
      "learning_rate": 0.0001454265611257696,
      "loss": 0.0236,
      "step": 1242
    },
    {
      "epoch": 0.8199208443271768,
      "grad_norm": 0.2270449846982956,
      "learning_rate": 0.0001453825857519789,
      "loss": 0.0516,
      "step": 1243
    },
    {
      "epoch": 0.820580474934037,
      "grad_norm": 0.10795996338129044,
      "learning_rate": 0.00014533861037818822,
      "loss": 0.0269,
      "step": 1244
    },
    {
      "epoch": 0.8212401055408971,
      "grad_norm": 0.12526631355285645,
      "learning_rate": 0.00014529463500439756,
      "loss": 0.0162,
      "step": 1245
    },
    {
      "epoch": 0.8218997361477572,
      "grad_norm": 0.21156255900859833,
      "learning_rate": 0.00014525065963060687,
      "loss": 0.0429,
      "step": 1246
    },
    {
      "epoch": 0.8225593667546174,
      "grad_norm": 0.07933206111192703,
      "learning_rate": 0.00014520668425681618,
      "loss": 0.0077,
      "step": 1247
    },
    {
      "epoch": 0.8232189973614775,
      "grad_norm": 0.1327560693025589,
      "learning_rate": 0.00014516270888302553,
      "loss": 0.0323,
      "step": 1248
    },
    {
      "epoch": 0.8238786279683378,
      "grad_norm": 0.18084308505058289,
      "learning_rate": 0.00014511873350923484,
      "loss": 0.0564,
      "step": 1249
    },
    {
      "epoch": 0.8245382585751979,
      "grad_norm": 0.23599852621555328,
      "learning_rate": 0.00014507475813544415,
      "loss": 0.0532,
      "step": 1250
    },
    {
      "epoch": 0.825197889182058,
      "grad_norm": 0.07861629873514175,
      "learning_rate": 0.00014503078276165347,
      "loss": 0.0117,
      "step": 1251
    },
    {
      "epoch": 0.8258575197889182,
      "grad_norm": 0.26975560188293457,
      "learning_rate": 0.0001449868073878628,
      "loss": 0.0765,
      "step": 1252
    },
    {
      "epoch": 0.8265171503957783,
      "grad_norm": 0.11733822524547577,
      "learning_rate": 0.00014494283201407212,
      "loss": 0.0234,
      "step": 1253
    },
    {
      "epoch": 0.8271767810026385,
      "grad_norm": 0.1287408322095871,
      "learning_rate": 0.00014489885664028144,
      "loss": 0.0099,
      "step": 1254
    },
    {
      "epoch": 0.8278364116094987,
      "grad_norm": 0.11043570190668106,
      "learning_rate": 0.00014485488126649078,
      "loss": 0.0202,
      "step": 1255
    },
    {
      "epoch": 0.8284960422163589,
      "grad_norm": 0.15035487711429596,
      "learning_rate": 0.0001448109058927001,
      "loss": 0.0296,
      "step": 1256
    },
    {
      "epoch": 0.829155672823219,
      "grad_norm": 0.09346380829811096,
      "learning_rate": 0.0001447669305189094,
      "loss": 0.0061,
      "step": 1257
    },
    {
      "epoch": 0.8298153034300791,
      "grad_norm": 0.20183919370174408,
      "learning_rate": 0.00014472295514511872,
      "loss": 0.0498,
      "step": 1258
    },
    {
      "epoch": 0.8304749340369393,
      "grad_norm": 0.266988068819046,
      "learning_rate": 0.00014467897977132806,
      "loss": 0.037,
      "step": 1259
    },
    {
      "epoch": 0.8311345646437994,
      "grad_norm": 0.13506411015987396,
      "learning_rate": 0.0001446350043975374,
      "loss": 0.0305,
      "step": 1260
    },
    {
      "epoch": 0.8317941952506597,
      "grad_norm": 0.10283072292804718,
      "learning_rate": 0.00014459102902374672,
      "loss": 0.0179,
      "step": 1261
    },
    {
      "epoch": 0.8324538258575198,
      "grad_norm": 0.13362711668014526,
      "learning_rate": 0.00014454705364995603,
      "loss": 0.0226,
      "step": 1262
    },
    {
      "epoch": 0.83311345646438,
      "grad_norm": 0.08259125798940659,
      "learning_rate": 0.00014450307827616537,
      "loss": 0.0093,
      "step": 1263
    },
    {
      "epoch": 0.8337730870712401,
      "grad_norm": 0.18642807006835938,
      "learning_rate": 0.00014445910290237468,
      "loss": 0.0526,
      "step": 1264
    },
    {
      "epoch": 0.8344327176781002,
      "grad_norm": 0.07984864711761475,
      "learning_rate": 0.000144415127528584,
      "loss": 0.0177,
      "step": 1265
    },
    {
      "epoch": 0.8350923482849604,
      "grad_norm": 0.14551129937171936,
      "learning_rate": 0.00014437115215479334,
      "loss": 0.0393,
      "step": 1266
    },
    {
      "epoch": 0.8357519788918206,
      "grad_norm": 0.13752523064613342,
      "learning_rate": 0.00014432717678100265,
      "loss": 0.0097,
      "step": 1267
    },
    {
      "epoch": 0.8364116094986808,
      "grad_norm": 0.07517698407173157,
      "learning_rate": 0.00014428320140721197,
      "loss": 0.0068,
      "step": 1268
    },
    {
      "epoch": 0.8370712401055409,
      "grad_norm": 0.10392218828201294,
      "learning_rate": 0.00014423922603342128,
      "loss": 0.0068,
      "step": 1269
    },
    {
      "epoch": 0.837730870712401,
      "grad_norm": 0.20159578323364258,
      "learning_rate": 0.00014419525065963062,
      "loss": 0.0511,
      "step": 1270
    },
    {
      "epoch": 0.8383905013192612,
      "grad_norm": 0.05239509418606758,
      "learning_rate": 0.00014415127528583994,
      "loss": 0.0111,
      "step": 1271
    },
    {
      "epoch": 0.8390501319261213,
      "grad_norm": 0.07003647834062576,
      "learning_rate": 0.00014410729991204925,
      "loss": 0.0101,
      "step": 1272
    },
    {
      "epoch": 0.8397097625329816,
      "grad_norm": 0.1692083179950714,
      "learning_rate": 0.0001440633245382586,
      "loss": 0.0355,
      "step": 1273
    },
    {
      "epoch": 0.8403693931398417,
      "grad_norm": 0.08963880687952042,
      "learning_rate": 0.0001440193491644679,
      "loss": 0.0093,
      "step": 1274
    },
    {
      "epoch": 0.8410290237467019,
      "grad_norm": 0.18244701623916626,
      "learning_rate": 0.00014397537379067722,
      "loss": 0.0466,
      "step": 1275
    },
    {
      "epoch": 0.841688654353562,
      "grad_norm": 0.13303536176681519,
      "learning_rate": 0.00014393139841688653,
      "loss": 0.0299,
      "step": 1276
    },
    {
      "epoch": 0.8423482849604221,
      "grad_norm": 0.09780425578355789,
      "learning_rate": 0.00014388742304309587,
      "loss": 0.0225,
      "step": 1277
    },
    {
      "epoch": 0.8430079155672823,
      "grad_norm": 0.09323029220104218,
      "learning_rate": 0.0001438434476693052,
      "loss": 0.0211,
      "step": 1278
    },
    {
      "epoch": 0.8436675461741425,
      "grad_norm": 0.06148659437894821,
      "learning_rate": 0.0001437994722955145,
      "loss": 0.0038,
      "step": 1279
    },
    {
      "epoch": 0.8443271767810027,
      "grad_norm": 0.05174132436513901,
      "learning_rate": 0.00014375549692172384,
      "loss": 0.0031,
      "step": 1280
    },
    {
      "epoch": 0.8449868073878628,
      "grad_norm": 0.09371868520975113,
      "learning_rate": 0.00014371152154793316,
      "loss": 0.0142,
      "step": 1281
    },
    {
      "epoch": 0.8456464379947229,
      "grad_norm": 0.12266460806131363,
      "learning_rate": 0.0001436675461741425,
      "loss": 0.0343,
      "step": 1282
    },
    {
      "epoch": 0.8463060686015831,
      "grad_norm": 0.07019917666912079,
      "learning_rate": 0.0001436235708003518,
      "loss": 0.0048,
      "step": 1283
    },
    {
      "epoch": 0.8469656992084432,
      "grad_norm": 0.29698646068573,
      "learning_rate": 0.00014357959542656115,
      "loss": 0.0862,
      "step": 1284
    },
    {
      "epoch": 0.8476253298153035,
      "grad_norm": 0.07559879869222641,
      "learning_rate": 0.00014353562005277047,
      "loss": 0.0152,
      "step": 1285
    },
    {
      "epoch": 0.8482849604221636,
      "grad_norm": 0.1548224687576294,
      "learning_rate": 0.00014349164467897978,
      "loss": 0.0239,
      "step": 1286
    },
    {
      "epoch": 0.8489445910290238,
      "grad_norm": 0.08191896975040436,
      "learning_rate": 0.0001434476693051891,
      "loss": 0.0166,
      "step": 1287
    },
    {
      "epoch": 0.8496042216358839,
      "grad_norm": 0.09388023614883423,
      "learning_rate": 0.00014340369393139844,
      "loss": 0.0185,
      "step": 1288
    },
    {
      "epoch": 0.850263852242744,
      "grad_norm": 0.15337370336055756,
      "learning_rate": 0.00014335971855760775,
      "loss": 0.0228,
      "step": 1289
    },
    {
      "epoch": 0.8509234828496042,
      "grad_norm": 0.3032304346561432,
      "learning_rate": 0.00014331574318381706,
      "loss": 0.0326,
      "step": 1290
    },
    {
      "epoch": 0.8515831134564644,
      "grad_norm": 0.1170523390173912,
      "learning_rate": 0.0001432717678100264,
      "loss": 0.0392,
      "step": 1291
    },
    {
      "epoch": 0.8522427440633246,
      "grad_norm": 0.1462438702583313,
      "learning_rate": 0.00014322779243623572,
      "loss": 0.0209,
      "step": 1292
    },
    {
      "epoch": 0.8529023746701847,
      "grad_norm": 0.281231552362442,
      "learning_rate": 0.00014318381706244503,
      "loss": 0.0825,
      "step": 1293
    },
    {
      "epoch": 0.8535620052770448,
      "grad_norm": 0.1950969249010086,
      "learning_rate": 0.00014313984168865435,
      "loss": 0.0495,
      "step": 1294
    },
    {
      "epoch": 0.854221635883905,
      "grad_norm": 0.14262135326862335,
      "learning_rate": 0.0001430958663148637,
      "loss": 0.0299,
      "step": 1295
    },
    {
      "epoch": 0.8548812664907651,
      "grad_norm": 0.08876535296440125,
      "learning_rate": 0.000143051890941073,
      "loss": 0.0122,
      "step": 1296
    },
    {
      "epoch": 0.8555408970976254,
      "grad_norm": 0.11767338216304779,
      "learning_rate": 0.00014300791556728232,
      "loss": 0.0199,
      "step": 1297
    },
    {
      "epoch": 0.8562005277044855,
      "grad_norm": 0.2595147490501404,
      "learning_rate": 0.00014296394019349163,
      "loss": 0.0213,
      "step": 1298
    },
    {
      "epoch": 0.8568601583113457,
      "grad_norm": 0.1563408374786377,
      "learning_rate": 0.00014291996481970097,
      "loss": 0.05,
      "step": 1299
    },
    {
      "epoch": 0.8575197889182058,
      "grad_norm": 0.16566471755504608,
      "learning_rate": 0.00014287598944591029,
      "loss": 0.0155,
      "step": 1300
    },
    {
      "epoch": 0.8581794195250659,
      "grad_norm": 0.0943496897816658,
      "learning_rate": 0.00014283201407211963,
      "loss": 0.0172,
      "step": 1301
    },
    {
      "epoch": 0.8588390501319261,
      "grad_norm": 0.07889246940612793,
      "learning_rate": 0.00014278803869832894,
      "loss": 0.0139,
      "step": 1302
    },
    {
      "epoch": 0.8594986807387863,
      "grad_norm": 0.08166167140007019,
      "learning_rate": 0.00014274406332453828,
      "loss": 0.0138,
      "step": 1303
    },
    {
      "epoch": 0.8601583113456465,
      "grad_norm": 0.09045393764972687,
      "learning_rate": 0.0001427000879507476,
      "loss": 0.0137,
      "step": 1304
    },
    {
      "epoch": 0.8608179419525066,
      "grad_norm": 0.07021123915910721,
      "learning_rate": 0.0001426561125769569,
      "loss": 0.0132,
      "step": 1305
    },
    {
      "epoch": 0.8614775725593667,
      "grad_norm": 0.1563224047422409,
      "learning_rate": 0.00014261213720316625,
      "loss": 0.0346,
      "step": 1306
    },
    {
      "epoch": 0.8621372031662269,
      "grad_norm": 0.17002050578594208,
      "learning_rate": 0.00014256816182937556,
      "loss": 0.0324,
      "step": 1307
    },
    {
      "epoch": 0.862796833773087,
      "grad_norm": 0.06564031541347504,
      "learning_rate": 0.00014252418645558488,
      "loss": 0.0121,
      "step": 1308
    },
    {
      "epoch": 0.8634564643799473,
      "grad_norm": 0.22095881402492523,
      "learning_rate": 0.00014248021108179422,
      "loss": 0.0323,
      "step": 1309
    },
    {
      "epoch": 0.8641160949868074,
      "grad_norm": 0.045915208756923676,
      "learning_rate": 0.00014243623570800353,
      "loss": 0.006,
      "step": 1310
    },
    {
      "epoch": 0.8647757255936676,
      "grad_norm": 0.1573653370141983,
      "learning_rate": 0.00014239226033421285,
      "loss": 0.0257,
      "step": 1311
    },
    {
      "epoch": 0.8654353562005277,
      "grad_norm": 0.15514235198497772,
      "learning_rate": 0.00014234828496042216,
      "loss": 0.0346,
      "step": 1312
    },
    {
      "epoch": 0.8660949868073878,
      "grad_norm": 0.044752173125743866,
      "learning_rate": 0.0001423043095866315,
      "loss": 0.0086,
      "step": 1313
    },
    {
      "epoch": 0.866754617414248,
      "grad_norm": 0.293486088514328,
      "learning_rate": 0.00014226033421284082,
      "loss": 0.0563,
      "step": 1314
    },
    {
      "epoch": 0.8674142480211082,
      "grad_norm": 0.22442936897277832,
      "learning_rate": 0.00014221635883905013,
      "loss": 0.0659,
      "step": 1315
    },
    {
      "epoch": 0.8680738786279684,
      "grad_norm": 0.12776990234851837,
      "learning_rate": 0.00014217238346525944,
      "loss": 0.0259,
      "step": 1316
    },
    {
      "epoch": 0.8687335092348285,
      "grad_norm": 0.1338663399219513,
      "learning_rate": 0.00014212840809146879,
      "loss": 0.0098,
      "step": 1317
    },
    {
      "epoch": 0.8693931398416886,
      "grad_norm": 0.3490079343318939,
      "learning_rate": 0.0001420844327176781,
      "loss": 0.0587,
      "step": 1318
    },
    {
      "epoch": 0.8700527704485488,
      "grad_norm": 0.1996694654226303,
      "learning_rate": 0.0001420404573438874,
      "loss": 0.0516,
      "step": 1319
    },
    {
      "epoch": 0.8707124010554089,
      "grad_norm": 0.15276199579238892,
      "learning_rate": 0.00014199648197009675,
      "loss": 0.0162,
      "step": 1320
    },
    {
      "epoch": 0.8713720316622692,
      "grad_norm": 0.1346679925918579,
      "learning_rate": 0.00014195250659630607,
      "loss": 0.0369,
      "step": 1321
    },
    {
      "epoch": 0.8720316622691293,
      "grad_norm": 0.20684269070625305,
      "learning_rate": 0.00014190853122251538,
      "loss": 0.0279,
      "step": 1322
    },
    {
      "epoch": 0.8726912928759895,
      "grad_norm": 0.20856115221977234,
      "learning_rate": 0.00014186455584872472,
      "loss": 0.0338,
      "step": 1323
    },
    {
      "epoch": 0.8733509234828496,
      "grad_norm": 0.16655179858207703,
      "learning_rate": 0.00014182058047493404,
      "loss": 0.0312,
      "step": 1324
    },
    {
      "epoch": 0.8740105540897097,
      "grad_norm": 0.11420699954032898,
      "learning_rate": 0.00014177660510114338,
      "loss": 0.0136,
      "step": 1325
    },
    {
      "epoch": 0.8746701846965699,
      "grad_norm": 0.124714195728302,
      "learning_rate": 0.0001417326297273527,
      "loss": 0.0328,
      "step": 1326
    },
    {
      "epoch": 0.8753298153034301,
      "grad_norm": 0.3159627914428711,
      "learning_rate": 0.00014168865435356203,
      "loss": 0.1004,
      "step": 1327
    },
    {
      "epoch": 0.8759894459102903,
      "grad_norm": 0.1728256642818451,
      "learning_rate": 0.00014164467897977135,
      "loss": 0.0237,
      "step": 1328
    },
    {
      "epoch": 0.8766490765171504,
      "grad_norm": 0.13123871386051178,
      "learning_rate": 0.00014160070360598066,
      "loss": 0.0281,
      "step": 1329
    },
    {
      "epoch": 0.8773087071240105,
      "grad_norm": 0.21968139708042145,
      "learning_rate": 0.00014155672823218998,
      "loss": 0.0412,
      "step": 1330
    },
    {
      "epoch": 0.8779683377308707,
      "grad_norm": 0.09749391674995422,
      "learning_rate": 0.00014151275285839932,
      "loss": 0.0202,
      "step": 1331
    },
    {
      "epoch": 0.8786279683377308,
      "grad_norm": 0.1240428239107132,
      "learning_rate": 0.00014146877748460863,
      "loss": 0.0297,
      "step": 1332
    },
    {
      "epoch": 0.8792875989445911,
      "grad_norm": 0.1886092573404312,
      "learning_rate": 0.00014142480211081794,
      "loss": 0.0536,
      "step": 1333
    },
    {
      "epoch": 0.8799472295514512,
      "grad_norm": 0.1138390377163887,
      "learning_rate": 0.00014138082673702726,
      "loss": 0.032,
      "step": 1334
    },
    {
      "epoch": 0.8806068601583114,
      "grad_norm": 0.18579861521720886,
      "learning_rate": 0.0001413368513632366,
      "loss": 0.0547,
      "step": 1335
    },
    {
      "epoch": 0.8812664907651715,
      "grad_norm": 0.21592891216278076,
      "learning_rate": 0.0001412928759894459,
      "loss": 0.0727,
      "step": 1336
    },
    {
      "epoch": 0.8819261213720316,
      "grad_norm": 0.08176259696483612,
      "learning_rate": 0.00014124890061565523,
      "loss": 0.0209,
      "step": 1337
    },
    {
      "epoch": 0.8825857519788918,
      "grad_norm": 0.06549037247896194,
      "learning_rate": 0.00014120492524186457,
      "loss": 0.0152,
      "step": 1338
    },
    {
      "epoch": 0.883245382585752,
      "grad_norm": 0.08579954504966736,
      "learning_rate": 0.00014116094986807388,
      "loss": 0.0293,
      "step": 1339
    },
    {
      "epoch": 0.8839050131926122,
      "grad_norm": 0.10942251235246658,
      "learning_rate": 0.0001411169744942832,
      "loss": 0.0204,
      "step": 1340
    },
    {
      "epoch": 0.8845646437994723,
      "grad_norm": 0.09873675554990768,
      "learning_rate": 0.0001410729991204925,
      "loss": 0.0157,
      "step": 1341
    },
    {
      "epoch": 0.8852242744063324,
      "grad_norm": 0.09376165270805359,
      "learning_rate": 0.00014102902374670185,
      "loss": 0.0186,
      "step": 1342
    },
    {
      "epoch": 0.8858839050131926,
      "grad_norm": 0.08669188618659973,
      "learning_rate": 0.00014098504837291117,
      "loss": 0.0168,
      "step": 1343
    },
    {
      "epoch": 0.8865435356200527,
      "grad_norm": 0.08325769752264023,
      "learning_rate": 0.0001409410729991205,
      "loss": 0.0163,
      "step": 1344
    },
    {
      "epoch": 0.887203166226913,
      "grad_norm": 0.10199886560440063,
      "learning_rate": 0.00014089709762532982,
      "loss": 0.0313,
      "step": 1345
    },
    {
      "epoch": 0.8878627968337731,
      "grad_norm": 0.08334241062402725,
      "learning_rate": 0.00014085312225153916,
      "loss": 0.0215,
      "step": 1346
    },
    {
      "epoch": 0.8885224274406333,
      "grad_norm": 0.08929333090782166,
      "learning_rate": 0.00014080914687774848,
      "loss": 0.0146,
      "step": 1347
    },
    {
      "epoch": 0.8891820580474934,
      "grad_norm": 0.33808550238609314,
      "learning_rate": 0.0001407651715039578,
      "loss": 0.0884,
      "step": 1348
    },
    {
      "epoch": 0.8898416886543535,
      "grad_norm": 0.16011489927768707,
      "learning_rate": 0.00014072119613016713,
      "loss": 0.037,
      "step": 1349
    },
    {
      "epoch": 0.8905013192612137,
      "grad_norm": 0.06436391174793243,
      "learning_rate": 0.00014067722075637644,
      "loss": 0.0123,
      "step": 1350
    },
    {
      "epoch": 0.8911609498680739,
      "grad_norm": 0.1120365560054779,
      "learning_rate": 0.00014063324538258576,
      "loss": 0.0367,
      "step": 1351
    },
    {
      "epoch": 0.8918205804749341,
      "grad_norm": 0.08241221308708191,
      "learning_rate": 0.00014058927000879507,
      "loss": 0.021,
      "step": 1352
    },
    {
      "epoch": 0.8924802110817942,
      "grad_norm": 0.16637222468852997,
      "learning_rate": 0.0001405452946350044,
      "loss": 0.0395,
      "step": 1353
    },
    {
      "epoch": 0.8931398416886543,
      "grad_norm": 0.17845404148101807,
      "learning_rate": 0.00014050131926121373,
      "loss": 0.048,
      "step": 1354
    },
    {
      "epoch": 0.8937994722955145,
      "grad_norm": 0.1448507159948349,
      "learning_rate": 0.00014045734388742304,
      "loss": 0.0404,
      "step": 1355
    },
    {
      "epoch": 0.8944591029023746,
      "grad_norm": 0.21283674240112305,
      "learning_rate": 0.00014041336851363238,
      "loss": 0.0573,
      "step": 1356
    },
    {
      "epoch": 0.8951187335092349,
      "grad_norm": 0.1098385602235794,
      "learning_rate": 0.0001403693931398417,
      "loss": 0.0161,
      "step": 1357
    },
    {
      "epoch": 0.895778364116095,
      "grad_norm": 0.13666486740112305,
      "learning_rate": 0.000140325417766051,
      "loss": 0.0408,
      "step": 1358
    },
    {
      "epoch": 0.8964379947229552,
      "grad_norm": 0.22571367025375366,
      "learning_rate": 0.00014028144239226032,
      "loss": 0.0227,
      "step": 1359
    },
    {
      "epoch": 0.8970976253298153,
      "grad_norm": 0.13301482796669006,
      "learning_rate": 0.00014023746701846967,
      "loss": 0.043,
      "step": 1360
    },
    {
      "epoch": 0.8977572559366754,
      "grad_norm": 0.14945071935653687,
      "learning_rate": 0.00014019349164467898,
      "loss": 0.0325,
      "step": 1361
    },
    {
      "epoch": 0.8984168865435356,
      "grad_norm": 0.11216725409030914,
      "learning_rate": 0.0001401495162708883,
      "loss": 0.0279,
      "step": 1362
    },
    {
      "epoch": 0.8990765171503958,
      "grad_norm": 0.19254687428474426,
      "learning_rate": 0.00014010554089709763,
      "loss": 0.0412,
      "step": 1363
    },
    {
      "epoch": 0.899736147757256,
      "grad_norm": 0.17095187306404114,
      "learning_rate": 0.00014006156552330695,
      "loss": 0.071,
      "step": 1364
    },
    {
      "epoch": 0.9003957783641161,
      "grad_norm": 0.14502452313899994,
      "learning_rate": 0.00014001759014951626,
      "loss": 0.0109,
      "step": 1365
    },
    {
      "epoch": 0.9010554089709762,
      "grad_norm": 0.16917142271995544,
      "learning_rate": 0.0001399736147757256,
      "loss": 0.0414,
      "step": 1366
    },
    {
      "epoch": 0.9017150395778364,
      "grad_norm": 0.13072580099105835,
      "learning_rate": 0.00013992963940193494,
      "loss": 0.0383,
      "step": 1367
    },
    {
      "epoch": 0.9023746701846965,
      "grad_norm": 0.16975946724414825,
      "learning_rate": 0.00013988566402814426,
      "loss": 0.044,
      "step": 1368
    },
    {
      "epoch": 0.9030343007915568,
      "grad_norm": 0.1344100534915924,
      "learning_rate": 0.00013984168865435357,
      "loss": 0.0379,
      "step": 1369
    },
    {
      "epoch": 0.9036939313984169,
      "grad_norm": 0.1509508192539215,
      "learning_rate": 0.00013979771328056289,
      "loss": 0.0374,
      "step": 1370
    },
    {
      "epoch": 0.9043535620052771,
      "grad_norm": 0.13903911411762238,
      "learning_rate": 0.00013975373790677223,
      "loss": 0.0376,
      "step": 1371
    },
    {
      "epoch": 0.9050131926121372,
      "grad_norm": 0.11929338425397873,
      "learning_rate": 0.00013970976253298154,
      "loss": 0.0411,
      "step": 1372
    },
    {
      "epoch": 0.9056728232189973,
      "grad_norm": 0.07643572241067886,
      "learning_rate": 0.00013966578715919086,
      "loss": 0.0147,
      "step": 1373
    },
    {
      "epoch": 0.9063324538258575,
      "grad_norm": 0.13500478863716125,
      "learning_rate": 0.0001396218117854002,
      "loss": 0.026,
      "step": 1374
    },
    {
      "epoch": 0.9069920844327177,
      "grad_norm": 0.1231357753276825,
      "learning_rate": 0.0001395778364116095,
      "loss": 0.01,
      "step": 1375
    },
    {
      "epoch": 0.9076517150395779,
      "grad_norm": 0.14031130075454712,
      "learning_rate": 0.00013953386103781882,
      "loss": 0.0415,
      "step": 1376
    },
    {
      "epoch": 0.908311345646438,
      "grad_norm": 0.0862111747264862,
      "learning_rate": 0.00013948988566402814,
      "loss": 0.0163,
      "step": 1377
    },
    {
      "epoch": 0.9089709762532981,
      "grad_norm": 0.10504557937383652,
      "learning_rate": 0.00013944591029023748,
      "loss": 0.0208,
      "step": 1378
    },
    {
      "epoch": 0.9096306068601583,
      "grad_norm": 0.07509136945009232,
      "learning_rate": 0.0001394019349164468,
      "loss": 0.0152,
      "step": 1379
    },
    {
      "epoch": 0.9102902374670184,
      "grad_norm": 0.06189476698637009,
      "learning_rate": 0.0001393579595426561,
      "loss": 0.0153,
      "step": 1380
    },
    {
      "epoch": 0.9109498680738787,
      "grad_norm": 0.11700501292943954,
      "learning_rate": 0.00013931398416886545,
      "loss": 0.0357,
      "step": 1381
    },
    {
      "epoch": 0.9116094986807388,
      "grad_norm": 0.12965664267539978,
      "learning_rate": 0.00013927000879507476,
      "loss": 0.03,
      "step": 1382
    },
    {
      "epoch": 0.912269129287599,
      "grad_norm": 0.0972810760140419,
      "learning_rate": 0.00013922603342128408,
      "loss": 0.0126,
      "step": 1383
    },
    {
      "epoch": 0.9129287598944591,
      "grad_norm": 0.1544565111398697,
      "learning_rate": 0.0001391820580474934,
      "loss": 0.032,
      "step": 1384
    },
    {
      "epoch": 0.9135883905013192,
      "grad_norm": 0.18777737021446228,
      "learning_rate": 0.00013913808267370273,
      "loss": 0.0396,
      "step": 1385
    },
    {
      "epoch": 0.9142480211081794,
      "grad_norm": 0.16748462617397308,
      "learning_rate": 0.00013909410729991205,
      "loss": 0.0298,
      "step": 1386
    },
    {
      "epoch": 0.9149076517150396,
      "grad_norm": 0.09444555640220642,
      "learning_rate": 0.00013905013192612139,
      "loss": 0.0189,
      "step": 1387
    },
    {
      "epoch": 0.9155672823218998,
      "grad_norm": 0.174606055021286,
      "learning_rate": 0.0001390061565523307,
      "loss": 0.0451,
      "step": 1388
    },
    {
      "epoch": 0.9162269129287599,
      "grad_norm": 0.2531820833683014,
      "learning_rate": 0.00013896218117854004,
      "loss": 0.0702,
      "step": 1389
    },
    {
      "epoch": 0.91688654353562,
      "grad_norm": 0.11356265097856522,
      "learning_rate": 0.00013891820580474936,
      "loss": 0.0126,
      "step": 1390
    },
    {
      "epoch": 0.9175461741424802,
      "grad_norm": 0.10554396361112595,
      "learning_rate": 0.00013887423043095867,
      "loss": 0.03,
      "step": 1391
    },
    {
      "epoch": 0.9182058047493403,
      "grad_norm": 0.22750453650951385,
      "learning_rate": 0.000138830255057168,
      "loss": 0.0458,
      "step": 1392
    },
    {
      "epoch": 0.9188654353562006,
      "grad_norm": 0.1572130024433136,
      "learning_rate": 0.00013878627968337732,
      "loss": 0.0309,
      "step": 1393
    },
    {
      "epoch": 0.9195250659630607,
      "grad_norm": 0.17781557142734528,
      "learning_rate": 0.00013874230430958664,
      "loss": 0.0236,
      "step": 1394
    },
    {
      "epoch": 0.9201846965699209,
      "grad_norm": 0.10994111746549606,
      "learning_rate": 0.00013869832893579595,
      "loss": 0.016,
      "step": 1395
    },
    {
      "epoch": 0.920844327176781,
      "grad_norm": 0.1530650556087494,
      "learning_rate": 0.0001386543535620053,
      "loss": 0.0467,
      "step": 1396
    },
    {
      "epoch": 0.9215039577836411,
      "grad_norm": 0.1340877115726471,
      "learning_rate": 0.0001386103781882146,
      "loss": 0.0429,
      "step": 1397
    },
    {
      "epoch": 0.9221635883905013,
      "grad_norm": 0.14443635940551758,
      "learning_rate": 0.00013856640281442392,
      "loss": 0.0175,
      "step": 1398
    },
    {
      "epoch": 0.9228232189973615,
      "grad_norm": 0.1688997745513916,
      "learning_rate": 0.00013852242744063326,
      "loss": 0.0612,
      "step": 1399
    },
    {
      "epoch": 0.9234828496042217,
      "grad_norm": 0.20491701364517212,
      "learning_rate": 0.00013847845206684258,
      "loss": 0.0559,
      "step": 1400
    },
    {
      "epoch": 0.9241424802110818,
      "grad_norm": 0.13896392285823822,
      "learning_rate": 0.0001384344766930519,
      "loss": 0.0336,
      "step": 1401
    },
    {
      "epoch": 0.924802110817942,
      "grad_norm": 0.19579200446605682,
      "learning_rate": 0.0001383905013192612,
      "loss": 0.0607,
      "step": 1402
    },
    {
      "epoch": 0.9254617414248021,
      "grad_norm": 0.11532715708017349,
      "learning_rate": 0.00013834652594547055,
      "loss": 0.0309,
      "step": 1403
    },
    {
      "epoch": 0.9261213720316622,
      "grad_norm": 0.0862959697842598,
      "learning_rate": 0.00013830255057167986,
      "loss": 0.0153,
      "step": 1404
    },
    {
      "epoch": 0.9267810026385225,
      "grad_norm": 0.10423921793699265,
      "learning_rate": 0.00013825857519788917,
      "loss": 0.0271,
      "step": 1405
    },
    {
      "epoch": 0.9274406332453826,
      "grad_norm": 0.08387498557567596,
      "learning_rate": 0.00013821459982409851,
      "loss": 0.0062,
      "step": 1406
    },
    {
      "epoch": 0.9281002638522428,
      "grad_norm": 0.11579256504774094,
      "learning_rate": 0.00013817062445030783,
      "loss": 0.009,
      "step": 1407
    },
    {
      "epoch": 0.9287598944591029,
      "grad_norm": 0.09461778402328491,
      "learning_rate": 0.00013812664907651717,
      "loss": 0.0202,
      "step": 1408
    },
    {
      "epoch": 0.929419525065963,
      "grad_norm": 0.2067088782787323,
      "learning_rate": 0.00013808267370272648,
      "loss": 0.0588,
      "step": 1409
    },
    {
      "epoch": 0.9300791556728232,
      "grad_norm": 0.1987171471118927,
      "learning_rate": 0.00013803869832893582,
      "loss": 0.0505,
      "step": 1410
    },
    {
      "epoch": 0.9307387862796834,
      "grad_norm": 0.19550925493240356,
      "learning_rate": 0.00013799472295514514,
      "loss": 0.0649,
      "step": 1411
    },
    {
      "epoch": 0.9313984168865436,
      "grad_norm": 0.12751424312591553,
      "learning_rate": 0.00013795074758135445,
      "loss": 0.0276,
      "step": 1412
    },
    {
      "epoch": 0.9320580474934037,
      "grad_norm": 0.11977527290582657,
      "learning_rate": 0.00013790677220756377,
      "loss": 0.0401,
      "step": 1413
    },
    {
      "epoch": 0.9327176781002638,
      "grad_norm": 0.1224345862865448,
      "learning_rate": 0.0001378627968337731,
      "loss": 0.0309,
      "step": 1414
    },
    {
      "epoch": 0.933377308707124,
      "grad_norm": 0.08867436647415161,
      "learning_rate": 0.00013781882145998242,
      "loss": 0.017,
      "step": 1415
    },
    {
      "epoch": 0.9340369393139841,
      "grad_norm": 0.08907410502433777,
      "learning_rate": 0.00013777484608619174,
      "loss": 0.0186,
      "step": 1416
    },
    {
      "epoch": 0.9346965699208444,
      "grad_norm": 0.12770384550094604,
      "learning_rate": 0.00013773087071240108,
      "loss": 0.0156,
      "step": 1417
    },
    {
      "epoch": 0.9353562005277045,
      "grad_norm": 0.10536859929561615,
      "learning_rate": 0.0001376868953386104,
      "loss": 0.0194,
      "step": 1418
    },
    {
      "epoch": 0.9360158311345647,
      "grad_norm": 0.10853305459022522,
      "learning_rate": 0.0001376429199648197,
      "loss": 0.0329,
      "step": 1419
    },
    {
      "epoch": 0.9366754617414248,
      "grad_norm": 0.1748216599225998,
      "learning_rate": 0.00013759894459102902,
      "loss": 0.048,
      "step": 1420
    },
    {
      "epoch": 0.9373350923482849,
      "grad_norm": 0.15145626664161682,
      "learning_rate": 0.00013755496921723836,
      "loss": 0.0359,
      "step": 1421
    },
    {
      "epoch": 0.9379947229551451,
      "grad_norm": 0.1458299458026886,
      "learning_rate": 0.00013751099384344767,
      "loss": 0.0264,
      "step": 1422
    },
    {
      "epoch": 0.9386543535620053,
      "grad_norm": 0.1351323127746582,
      "learning_rate": 0.000137467018469657,
      "loss": 0.0342,
      "step": 1423
    },
    {
      "epoch": 0.9393139841688655,
      "grad_norm": 0.1268400102853775,
      "learning_rate": 0.0001374230430958663,
      "loss": 0.0273,
      "step": 1424
    },
    {
      "epoch": 0.9399736147757256,
      "grad_norm": 0.17962677776813507,
      "learning_rate": 0.00013737906772207564,
      "loss": 0.0589,
      "step": 1425
    },
    {
      "epoch": 0.9406332453825857,
      "grad_norm": 0.1407070904970169,
      "learning_rate": 0.00013733509234828496,
      "loss": 0.0231,
      "step": 1426
    },
    {
      "epoch": 0.9412928759894459,
      "grad_norm": 0.07474631816148758,
      "learning_rate": 0.00013729111697449427,
      "loss": 0.0132,
      "step": 1427
    },
    {
      "epoch": 0.941952506596306,
      "grad_norm": 0.1300322562456131,
      "learning_rate": 0.0001372471416007036,
      "loss": 0.0231,
      "step": 1428
    },
    {
      "epoch": 0.9426121372031663,
      "grad_norm": 0.1802651584148407,
      "learning_rate": 0.00013720316622691292,
      "loss": 0.0251,
      "step": 1429
    },
    {
      "epoch": 0.9432717678100264,
      "grad_norm": 0.2056036740541458,
      "learning_rate": 0.00013715919085312227,
      "loss": 0.0305,
      "step": 1430
    },
    {
      "epoch": 0.9439313984168866,
      "grad_norm": 0.14322225749492645,
      "learning_rate": 0.00013711521547933158,
      "loss": 0.0095,
      "step": 1431
    },
    {
      "epoch": 0.9445910290237467,
      "grad_norm": 0.08642763644456863,
      "learning_rate": 0.00013707124010554092,
      "loss": 0.0104,
      "step": 1432
    },
    {
      "epoch": 0.9452506596306068,
      "grad_norm": 0.07629302144050598,
      "learning_rate": 0.00013702726473175024,
      "loss": 0.0096,
      "step": 1433
    },
    {
      "epoch": 0.945910290237467,
      "grad_norm": 0.2438475787639618,
      "learning_rate": 0.00013698328935795955,
      "loss": 0.0407,
      "step": 1434
    },
    {
      "epoch": 0.9465699208443272,
      "grad_norm": 0.14563535153865814,
      "learning_rate": 0.0001369393139841689,
      "loss": 0.0273,
      "step": 1435
    },
    {
      "epoch": 0.9472295514511874,
      "grad_norm": 0.16298136115074158,
      "learning_rate": 0.0001368953386103782,
      "loss": 0.0328,
      "step": 1436
    },
    {
      "epoch": 0.9478891820580475,
      "grad_norm": 0.19234518706798553,
      "learning_rate": 0.00013685136323658752,
      "loss": 0.0304,
      "step": 1437
    },
    {
      "epoch": 0.9485488126649076,
      "grad_norm": 0.24605469405651093,
      "learning_rate": 0.00013680738786279683,
      "loss": 0.0471,
      "step": 1438
    },
    {
      "epoch": 0.9492084432717678,
      "grad_norm": 0.11248517036437988,
      "learning_rate": 0.00013676341248900617,
      "loss": 0.0222,
      "step": 1439
    },
    {
      "epoch": 0.9498680738786279,
      "grad_norm": 0.09104634821414948,
      "learning_rate": 0.0001367194371152155,
      "loss": 0.0122,
      "step": 1440
    },
    {
      "epoch": 0.9505277044854882,
      "grad_norm": 0.24818916618824005,
      "learning_rate": 0.0001366754617414248,
      "loss": 0.0479,
      "step": 1441
    },
    {
      "epoch": 0.9511873350923483,
      "grad_norm": 0.13978122174739838,
      "learning_rate": 0.00013663148636763411,
      "loss": 0.0134,
      "step": 1442
    },
    {
      "epoch": 0.9518469656992085,
      "grad_norm": 0.1366599053144455,
      "learning_rate": 0.00013658751099384346,
      "loss": 0.0182,
      "step": 1443
    },
    {
      "epoch": 0.9525065963060686,
      "grad_norm": 0.09141973406076431,
      "learning_rate": 0.00013654353562005277,
      "loss": 0.0091,
      "step": 1444
    },
    {
      "epoch": 0.9531662269129287,
      "grad_norm": 0.15576261281967163,
      "learning_rate": 0.00013649956024626208,
      "loss": 0.0342,
      "step": 1445
    },
    {
      "epoch": 0.9538258575197889,
      "grad_norm": 0.18730528652668,
      "learning_rate": 0.00013645558487247142,
      "loss": 0.0208,
      "step": 1446
    },
    {
      "epoch": 0.9544854881266491,
      "grad_norm": 0.13546380400657654,
      "learning_rate": 0.00013641160949868074,
      "loss": 0.033,
      "step": 1447
    },
    {
      "epoch": 0.9551451187335093,
      "grad_norm": 0.33218470215797424,
      "learning_rate": 0.00013636763412489005,
      "loss": 0.0652,
      "step": 1448
    },
    {
      "epoch": 0.9558047493403694,
      "grad_norm": 0.1990104466676712,
      "learning_rate": 0.0001363236587510994,
      "loss": 0.0281,
      "step": 1449
    },
    {
      "epoch": 0.9564643799472295,
      "grad_norm": 0.18450461328029633,
      "learning_rate": 0.0001362796833773087,
      "loss": 0.043,
      "step": 1450
    },
    {
      "epoch": 0.9571240105540897,
      "grad_norm": 0.1633075475692749,
      "learning_rate": 0.00013623570800351805,
      "loss": 0.0242,
      "step": 1451
    },
    {
      "epoch": 0.9577836411609498,
      "grad_norm": 0.09079620242118835,
      "learning_rate": 0.00013619173262972736,
      "loss": 0.0053,
      "step": 1452
    },
    {
      "epoch": 0.9584432717678101,
      "grad_norm": 0.21772818267345428,
      "learning_rate": 0.0001361477572559367,
      "loss": 0.0671,
      "step": 1453
    },
    {
      "epoch": 0.9591029023746702,
      "grad_norm": 0.11405884474515915,
      "learning_rate": 0.00013610378188214602,
      "loss": 0.0286,
      "step": 1454
    },
    {
      "epoch": 0.9597625329815304,
      "grad_norm": 0.21953897178173065,
      "learning_rate": 0.00013605980650835533,
      "loss": 0.0171,
      "step": 1455
    },
    {
      "epoch": 0.9604221635883905,
      "grad_norm": 0.12651553750038147,
      "learning_rate": 0.00013601583113456465,
      "loss": 0.0366,
      "step": 1456
    },
    {
      "epoch": 0.9610817941952506,
      "grad_norm": 0.14760233461856842,
      "learning_rate": 0.000135971855760774,
      "loss": 0.0328,
      "step": 1457
    },
    {
      "epoch": 0.9617414248021108,
      "grad_norm": 0.0651514008641243,
      "learning_rate": 0.0001359278803869833,
      "loss": 0.0121,
      "step": 1458
    },
    {
      "epoch": 0.962401055408971,
      "grad_norm": 0.07470578700304031,
      "learning_rate": 0.00013588390501319261,
      "loss": 0.0047,
      "step": 1459
    },
    {
      "epoch": 0.9630606860158312,
      "grad_norm": 0.22181552648544312,
      "learning_rate": 0.00013583992963940193,
      "loss": 0.042,
      "step": 1460
    },
    {
      "epoch": 0.9637203166226913,
      "grad_norm": 0.19369708001613617,
      "learning_rate": 0.00013579595426561127,
      "loss": 0.0483,
      "step": 1461
    },
    {
      "epoch": 0.9643799472295514,
      "grad_norm": 0.192474365234375,
      "learning_rate": 0.00013575197889182058,
      "loss": 0.0544,
      "step": 1462
    },
    {
      "epoch": 0.9650395778364116,
      "grad_norm": 0.13801313936710358,
      "learning_rate": 0.0001357080035180299,
      "loss": 0.0296,
      "step": 1463
    },
    {
      "epoch": 0.9656992084432717,
      "grad_norm": 0.14092716574668884,
      "learning_rate": 0.00013566402814423924,
      "loss": 0.0363,
      "step": 1464
    },
    {
      "epoch": 0.966358839050132,
      "grad_norm": 0.1475318968296051,
      "learning_rate": 0.00013562005277044855,
      "loss": 0.0474,
      "step": 1465
    },
    {
      "epoch": 0.9670184696569921,
      "grad_norm": 0.1308039426803589,
      "learning_rate": 0.00013557607739665787,
      "loss": 0.0338,
      "step": 1466
    },
    {
      "epoch": 0.9676781002638523,
      "grad_norm": 0.11696742475032806,
      "learning_rate": 0.00013553210202286718,
      "loss": 0.0321,
      "step": 1467
    },
    {
      "epoch": 0.9683377308707124,
      "grad_norm": 0.11275412887334824,
      "learning_rate": 0.00013548812664907652,
      "loss": 0.0072,
      "step": 1468
    },
    {
      "epoch": 0.9689973614775725,
      "grad_norm": 0.11244922876358032,
      "learning_rate": 0.00013544415127528584,
      "loss": 0.0286,
      "step": 1469
    },
    {
      "epoch": 0.9696569920844327,
      "grad_norm": 0.10941194742918015,
      "learning_rate": 0.00013540017590149515,
      "loss": 0.0306,
      "step": 1470
    },
    {
      "epoch": 0.9703166226912929,
      "grad_norm": 0.13597334921360016,
      "learning_rate": 0.0001353562005277045,
      "loss": 0.0301,
      "step": 1471
    },
    {
      "epoch": 0.9709762532981531,
      "grad_norm": 0.1315174251794815,
      "learning_rate": 0.00013531222515391383,
      "loss": 0.0089,
      "step": 1472
    },
    {
      "epoch": 0.9716358839050132,
      "grad_norm": 0.2824278175830841,
      "learning_rate": 0.00013526824978012315,
      "loss": 0.0836,
      "step": 1473
    },
    {
      "epoch": 0.9722955145118733,
      "grad_norm": 0.13090477883815765,
      "learning_rate": 0.00013522427440633246,
      "loss": 0.0176,
      "step": 1474
    },
    {
      "epoch": 0.9729551451187335,
      "grad_norm": 0.14466793835163116,
      "learning_rate": 0.0001351802990325418,
      "loss": 0.0309,
      "step": 1475
    },
    {
      "epoch": 0.9736147757255936,
      "grad_norm": 0.09564338624477386,
      "learning_rate": 0.00013513632365875111,
      "loss": 0.0153,
      "step": 1476
    },
    {
      "epoch": 0.9742744063324539,
      "grad_norm": 0.0979272723197937,
      "learning_rate": 0.00013509234828496043,
      "loss": 0.0061,
      "step": 1477
    },
    {
      "epoch": 0.974934036939314,
      "grad_norm": 0.16389422118663788,
      "learning_rate": 0.00013504837291116974,
      "loss": 0.042,
      "step": 1478
    },
    {
      "epoch": 0.9755936675461742,
      "grad_norm": 0.2719620168209076,
      "learning_rate": 0.00013500439753737908,
      "loss": 0.0557,
      "step": 1479
    },
    {
      "epoch": 0.9762532981530343,
      "grad_norm": 0.10780271142721176,
      "learning_rate": 0.0001349604221635884,
      "loss": 0.0164,
      "step": 1480
    },
    {
      "epoch": 0.9769129287598944,
      "grad_norm": 0.12940935790538788,
      "learning_rate": 0.0001349164467897977,
      "loss": 0.0325,
      "step": 1481
    },
    {
      "epoch": 0.9775725593667546,
      "grad_norm": 0.11078586429357529,
      "learning_rate": 0.00013487247141600705,
      "loss": 0.0293,
      "step": 1482
    },
    {
      "epoch": 0.9782321899736148,
      "grad_norm": 0.22676342725753784,
      "learning_rate": 0.00013482849604221637,
      "loss": 0.0462,
      "step": 1483
    },
    {
      "epoch": 0.978891820580475,
      "grad_norm": 0.14810596406459808,
      "learning_rate": 0.00013478452066842568,
      "loss": 0.0242,
      "step": 1484
    },
    {
      "epoch": 0.9795514511873351,
      "grad_norm": 0.1205986961722374,
      "learning_rate": 0.000134740545294635,
      "loss": 0.0118,
      "step": 1485
    },
    {
      "epoch": 0.9802110817941952,
      "grad_norm": 0.13070084154605865,
      "learning_rate": 0.00013469656992084434,
      "loss": 0.0355,
      "step": 1486
    },
    {
      "epoch": 0.9808707124010554,
      "grad_norm": 0.15466423332691193,
      "learning_rate": 0.00013465259454705365,
      "loss": 0.025,
      "step": 1487
    },
    {
      "epoch": 0.9815303430079155,
      "grad_norm": 0.11172927170991898,
      "learning_rate": 0.00013460861917326296,
      "loss": 0.0157,
      "step": 1488
    },
    {
      "epoch": 0.9821899736147758,
      "grad_norm": 0.20864300429821014,
      "learning_rate": 0.0001345646437994723,
      "loss": 0.0258,
      "step": 1489
    },
    {
      "epoch": 0.9828496042216359,
      "grad_norm": 0.25203242897987366,
      "learning_rate": 0.00013452066842568162,
      "loss": 0.0405,
      "step": 1490
    },
    {
      "epoch": 0.9835092348284961,
      "grad_norm": 0.0959254652261734,
      "learning_rate": 0.00013447669305189093,
      "loss": 0.0122,
      "step": 1491
    },
    {
      "epoch": 0.9841688654353562,
      "grad_norm": 0.099158376455307,
      "learning_rate": 0.00013443271767810027,
      "loss": 0.0216,
      "step": 1492
    },
    {
      "epoch": 0.9848284960422163,
      "grad_norm": 0.19597625732421875,
      "learning_rate": 0.0001343887423043096,
      "loss": 0.0135,
      "step": 1493
    },
    {
      "epoch": 0.9854881266490765,
      "grad_norm": 0.1351017951965332,
      "learning_rate": 0.00013434476693051893,
      "loss": 0.0237,
      "step": 1494
    },
    {
      "epoch": 0.9861477572559367,
      "grad_norm": 0.08852732181549072,
      "learning_rate": 0.00013430079155672824,
      "loss": 0.0107,
      "step": 1495
    },
    {
      "epoch": 0.9868073878627969,
      "grad_norm": 0.308567613363266,
      "learning_rate": 0.00013425681618293756,
      "loss": 0.0625,
      "step": 1496
    },
    {
      "epoch": 0.987467018469657,
      "grad_norm": 0.14887158572673798,
      "learning_rate": 0.0001342128408091469,
      "loss": 0.0313,
      "step": 1497
    },
    {
      "epoch": 0.9881266490765171,
      "grad_norm": 0.21746350824832916,
      "learning_rate": 0.0001341688654353562,
      "loss": 0.038,
      "step": 1498
    },
    {
      "epoch": 0.9887862796833773,
      "grad_norm": 0.13009917736053467,
      "learning_rate": 0.00013412489006156553,
      "loss": 0.0201,
      "step": 1499
    },
    {
      "epoch": 0.9894459102902374,
      "grad_norm": 0.2146957963705063,
      "learning_rate": 0.00013408091468777487,
      "loss": 0.0441,
      "step": 1500
    },
    {
      "epoch": 0.9901055408970977,
      "grad_norm": 0.17439879477024078,
      "learning_rate": 0.00013403693931398418,
      "loss": 0.0332,
      "step": 1501
    },
    {
      "epoch": 0.9907651715039578,
      "grad_norm": 0.1243106797337532,
      "learning_rate": 0.0001339929639401935,
      "loss": 0.0251,
      "step": 1502
    },
    {
      "epoch": 0.991424802110818,
      "grad_norm": 0.41834357380867004,
      "learning_rate": 0.0001339489885664028,
      "loss": 0.1038,
      "step": 1503
    },
    {
      "epoch": 0.9920844327176781,
      "grad_norm": 0.15863096714019775,
      "learning_rate": 0.00013390501319261215,
      "loss": 0.0303,
      "step": 1504
    },
    {
      "epoch": 0.9927440633245382,
      "grad_norm": 0.20410676300525665,
      "learning_rate": 0.00013386103781882146,
      "loss": 0.0146,
      "step": 1505
    },
    {
      "epoch": 0.9934036939313984,
      "grad_norm": 0.16025516390800476,
      "learning_rate": 0.00013381706244503078,
      "loss": 0.0148,
      "step": 1506
    },
    {
      "epoch": 0.9940633245382586,
      "grad_norm": 0.117454893887043,
      "learning_rate": 0.00013377308707124012,
      "loss": 0.0259,
      "step": 1507
    },
    {
      "epoch": 0.9947229551451188,
      "grad_norm": 0.188646137714386,
      "learning_rate": 0.00013372911169744943,
      "loss": 0.0368,
      "step": 1508
    },
    {
      "epoch": 0.9953825857519789,
      "grad_norm": 0.14104565978050232,
      "learning_rate": 0.00013368513632365875,
      "loss": 0.0257,
      "step": 1509
    },
    {
      "epoch": 0.996042216358839,
      "grad_norm": 0.20529550313949585,
      "learning_rate": 0.00013364116094986806,
      "loss": 0.0487,
      "step": 1510
    },
    {
      "epoch": 0.9967018469656992,
      "grad_norm": 0.1505880355834961,
      "learning_rate": 0.0001335971855760774,
      "loss": 0.0116,
      "step": 1511
    },
    {
      "epoch": 0.9973614775725593,
      "grad_norm": 0.14477118849754333,
      "learning_rate": 0.00013355321020228672,
      "loss": 0.0433,
      "step": 1512
    },
    {
      "epoch": 0.9980211081794196,
      "grad_norm": 0.12461160123348236,
      "learning_rate": 0.00013350923482849606,
      "loss": 0.039,
      "step": 1513
    },
    {
      "epoch": 0.9986807387862797,
      "grad_norm": 0.09497349709272385,
      "learning_rate": 0.00013346525945470537,
      "loss": 0.0243,
      "step": 1514
    },
    {
      "epoch": 0.9993403693931399,
      "grad_norm": 0.12263771146535873,
      "learning_rate": 0.0001334212840809147,
      "loss": 0.0416,
      "step": 1515
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.1482406109571457,
      "learning_rate": 0.00013337730870712403,
      "loss": 0.0312,
      "step": 1516
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6954022988505747,
      "eval_f1": 0.4732392757182841,
      "eval_keras_BCE": 0.8198239803314209,
      "eval_loss": 0.8196753263473511,
      "eval_precision": 0.7567053869557355,
      "eval_runtime": 5.2197,
      "eval_samples_per_second": 33.336,
      "eval_steps_per_second": 4.215,
      "eval_weigthed BCE": 0.5484339594841003,
      "step": 1516
    },
    {
      "epoch": 1.0006596306068603,
      "grad_norm": 0.12217436730861664,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.0244,
      "step": 1517
    },
    {
      "epoch": 1.0013192612137203,
      "grad_norm": 0.1241198405623436,
      "learning_rate": 0.00013328935795954268,
      "loss": 0.034,
      "step": 1518
    },
    {
      "epoch": 1.0019788918205805,
      "grad_norm": 0.3031664788722992,
      "learning_rate": 0.000133245382585752,
      "loss": 0.0551,
      "step": 1519
    },
    {
      "epoch": 1.0026385224274406,
      "grad_norm": 0.09875001013278961,
      "learning_rate": 0.0001332014072119613,
      "loss": 0.0148,
      "step": 1520
    },
    {
      "epoch": 1.0032981530343008,
      "grad_norm": 0.09050743281841278,
      "learning_rate": 0.00013315743183817062,
      "loss": 0.0066,
      "step": 1521
    },
    {
      "epoch": 1.003957783641161,
      "grad_norm": 0.10361717641353607,
      "learning_rate": 0.00013311345646437996,
      "loss": 0.02,
      "step": 1522
    },
    {
      "epoch": 1.004617414248021,
      "grad_norm": 0.07558707147836685,
      "learning_rate": 0.00013306948109058928,
      "loss": 0.01,
      "step": 1523
    },
    {
      "epoch": 1.0052770448548813,
      "grad_norm": 0.12972496449947357,
      "learning_rate": 0.0001330255057167986,
      "loss": 0.0093,
      "step": 1524
    },
    {
      "epoch": 1.0059366754617414,
      "grad_norm": 0.15659183263778687,
      "learning_rate": 0.00013298153034300793,
      "loss": 0.0346,
      "step": 1525
    },
    {
      "epoch": 1.0065963060686016,
      "grad_norm": 0.07787283509969711,
      "learning_rate": 0.00013293755496921725,
      "loss": 0.0129,
      "step": 1526
    },
    {
      "epoch": 1.0072559366754616,
      "grad_norm": 0.16323362290859222,
      "learning_rate": 0.00013289357959542656,
      "loss": 0.0167,
      "step": 1527
    },
    {
      "epoch": 1.007915567282322,
      "grad_norm": 0.1824597418308258,
      "learning_rate": 0.00013284960422163587,
      "loss": 0.0252,
      "step": 1528
    },
    {
      "epoch": 1.0085751978891822,
      "grad_norm": 0.07741467654705048,
      "learning_rate": 0.00013280562884784522,
      "loss": 0.0091,
      "step": 1529
    },
    {
      "epoch": 1.0092348284960422,
      "grad_norm": 0.13670098781585693,
      "learning_rate": 0.00013276165347405453,
      "loss": 0.0201,
      "step": 1530
    },
    {
      "epoch": 1.0098944591029024,
      "grad_norm": 0.0732647180557251,
      "learning_rate": 0.00013271767810026384,
      "loss": 0.0044,
      "step": 1531
    },
    {
      "epoch": 1.0105540897097625,
      "grad_norm": 0.1773679256439209,
      "learning_rate": 0.00013267370272647318,
      "loss": 0.0376,
      "step": 1532
    },
    {
      "epoch": 1.0112137203166227,
      "grad_norm": 0.18986600637435913,
      "learning_rate": 0.0001326297273526825,
      "loss": 0.0254,
      "step": 1533
    },
    {
      "epoch": 1.0118733509234827,
      "grad_norm": 0.07797729969024658,
      "learning_rate": 0.0001325857519788918,
      "loss": 0.0081,
      "step": 1534
    },
    {
      "epoch": 1.012532981530343,
      "grad_norm": 0.12688325345516205,
      "learning_rate": 0.00013254177660510115,
      "loss": 0.0137,
      "step": 1535
    },
    {
      "epoch": 1.0131926121372032,
      "grad_norm": 0.18708744645118713,
      "learning_rate": 0.00013249780123131047,
      "loss": 0.033,
      "step": 1536
    },
    {
      "epoch": 1.0138522427440633,
      "grad_norm": 0.195429727435112,
      "learning_rate": 0.0001324538258575198,
      "loss": 0.0418,
      "step": 1537
    },
    {
      "epoch": 1.0145118733509235,
      "grad_norm": 0.1427510380744934,
      "learning_rate": 0.00013240985048372912,
      "loss": 0.0165,
      "step": 1538
    },
    {
      "epoch": 1.0151715039577835,
      "grad_norm": 0.1573333591222763,
      "learning_rate": 0.00013236587510993844,
      "loss": 0.0133,
      "step": 1539
    },
    {
      "epoch": 1.0158311345646438,
      "grad_norm": 0.1539282351732254,
      "learning_rate": 0.00013232189973614778,
      "loss": 0.0171,
      "step": 1540
    },
    {
      "epoch": 1.016490765171504,
      "grad_norm": 0.12983863055706024,
      "learning_rate": 0.0001322779243623571,
      "loss": 0.01,
      "step": 1541
    },
    {
      "epoch": 1.017150395778364,
      "grad_norm": 0.1915886253118515,
      "learning_rate": 0.0001322339489885664,
      "loss": 0.011,
      "step": 1542
    },
    {
      "epoch": 1.0178100263852243,
      "grad_norm": 0.15930427610874176,
      "learning_rate": 0.00013218997361477575,
      "loss": 0.0148,
      "step": 1543
    },
    {
      "epoch": 1.0184696569920844,
      "grad_norm": 0.1437217891216278,
      "learning_rate": 0.00013214599824098506,
      "loss": 0.0245,
      "step": 1544
    },
    {
      "epoch": 1.0191292875989446,
      "grad_norm": 0.06748661398887634,
      "learning_rate": 0.00013210202286719437,
      "loss": 0.0031,
      "step": 1545
    },
    {
      "epoch": 1.0197889182058049,
      "grad_norm": 0.08109118044376373,
      "learning_rate": 0.0001320580474934037,
      "loss": 0.008,
      "step": 1546
    },
    {
      "epoch": 1.020448548812665,
      "grad_norm": 0.062494393438100815,
      "learning_rate": 0.00013201407211961303,
      "loss": 0.0049,
      "step": 1547
    },
    {
      "epoch": 1.0211081794195251,
      "grad_norm": 0.10211991518735886,
      "learning_rate": 0.00013197009674582234,
      "loss": 0.0123,
      "step": 1548
    },
    {
      "epoch": 1.0217678100263852,
      "grad_norm": 0.3827899992465973,
      "learning_rate": 0.00013192612137203166,
      "loss": 0.0441,
      "step": 1549
    },
    {
      "epoch": 1.0224274406332454,
      "grad_norm": 0.27079689502716064,
      "learning_rate": 0.000131882145998241,
      "loss": 0.0552,
      "step": 1550
    },
    {
      "epoch": 1.0230870712401055,
      "grad_norm": 0.157638818025589,
      "learning_rate": 0.0001318381706244503,
      "loss": 0.0074,
      "step": 1551
    },
    {
      "epoch": 1.0237467018469657,
      "grad_norm": 0.11963241547346115,
      "learning_rate": 0.00013179419525065963,
      "loss": 0.0082,
      "step": 1552
    },
    {
      "epoch": 1.024406332453826,
      "grad_norm": 0.04319344460964203,
      "learning_rate": 0.00013175021987686894,
      "loss": 0.003,
      "step": 1553
    },
    {
      "epoch": 1.025065963060686,
      "grad_norm": 0.14569398760795593,
      "learning_rate": 0.00013170624450307828,
      "loss": 0.0067,
      "step": 1554
    },
    {
      "epoch": 1.0257255936675462,
      "grad_norm": 0.26398172974586487,
      "learning_rate": 0.0001316622691292876,
      "loss": 0.0333,
      "step": 1555
    },
    {
      "epoch": 1.0263852242744063,
      "grad_norm": 0.38382554054260254,
      "learning_rate": 0.00013161829375549694,
      "loss": 0.076,
      "step": 1556
    },
    {
      "epoch": 1.0270448548812665,
      "grad_norm": 0.06421700865030289,
      "learning_rate": 0.00013157431838170625,
      "loss": 0.0045,
      "step": 1557
    },
    {
      "epoch": 1.0277044854881265,
      "grad_norm": 0.1005040630698204,
      "learning_rate": 0.0001315303430079156,
      "loss": 0.0225,
      "step": 1558
    },
    {
      "epoch": 1.0283641160949868,
      "grad_norm": 0.34813401103019714,
      "learning_rate": 0.0001314863676341249,
      "loss": 0.0652,
      "step": 1559
    },
    {
      "epoch": 1.029023746701847,
      "grad_norm": 0.20053613185882568,
      "learning_rate": 0.00013144239226033422,
      "loss": 0.0215,
      "step": 1560
    },
    {
      "epoch": 1.029683377308707,
      "grad_norm": 0.11894561350345612,
      "learning_rate": 0.00013139841688654356,
      "loss": 0.0165,
      "step": 1561
    },
    {
      "epoch": 1.0303430079155673,
      "grad_norm": 0.12171633541584015,
      "learning_rate": 0.00013135444151275287,
      "loss": 0.0167,
      "step": 1562
    },
    {
      "epoch": 1.0310026385224274,
      "grad_norm": 0.11730366200208664,
      "learning_rate": 0.0001313104661389622,
      "loss": 0.0088,
      "step": 1563
    },
    {
      "epoch": 1.0316622691292876,
      "grad_norm": 0.11351627856492996,
      "learning_rate": 0.0001312664907651715,
      "loss": 0.0263,
      "step": 1564
    },
    {
      "epoch": 1.0323218997361479,
      "grad_norm": 0.11037493497133255,
      "learning_rate": 0.00013122251539138084,
      "loss": 0.0238,
      "step": 1565
    },
    {
      "epoch": 1.0329815303430079,
      "grad_norm": 0.16431593894958496,
      "learning_rate": 0.00013117854001759016,
      "loss": 0.0391,
      "step": 1566
    },
    {
      "epoch": 1.0336411609498681,
      "grad_norm": 0.13414587080478668,
      "learning_rate": 0.00013113456464379947,
      "loss": 0.0304,
      "step": 1567
    },
    {
      "epoch": 1.0343007915567282,
      "grad_norm": 0.12087713181972504,
      "learning_rate": 0.0001310905892700088,
      "loss": 0.0168,
      "step": 1568
    },
    {
      "epoch": 1.0349604221635884,
      "grad_norm": 0.17194172739982605,
      "learning_rate": 0.00013104661389621813,
      "loss": 0.0501,
      "step": 1569
    },
    {
      "epoch": 1.0356200527704487,
      "grad_norm": 0.12663893401622772,
      "learning_rate": 0.00013100263852242744,
      "loss": 0.0261,
      "step": 1570
    },
    {
      "epoch": 1.0362796833773087,
      "grad_norm": 0.1474326252937317,
      "learning_rate": 0.00013095866314863675,
      "loss": 0.0103,
      "step": 1571
    },
    {
      "epoch": 1.036939313984169,
      "grad_norm": 0.14970703423023224,
      "learning_rate": 0.0001309146877748461,
      "loss": 0.0341,
      "step": 1572
    },
    {
      "epoch": 1.037598944591029,
      "grad_norm": 0.10989683121442795,
      "learning_rate": 0.0001308707124010554,
      "loss": 0.0183,
      "step": 1573
    },
    {
      "epoch": 1.0382585751978892,
      "grad_norm": 0.1103563904762268,
      "learning_rate": 0.00013082673702726472,
      "loss": 0.0149,
      "step": 1574
    },
    {
      "epoch": 1.0389182058047493,
      "grad_norm": 0.08973529934883118,
      "learning_rate": 0.00013078276165347404,
      "loss": 0.008,
      "step": 1575
    },
    {
      "epoch": 1.0395778364116095,
      "grad_norm": 0.16768106818199158,
      "learning_rate": 0.00013073878627968338,
      "loss": 0.0418,
      "step": 1576
    },
    {
      "epoch": 1.0402374670184698,
      "grad_norm": 0.1341806948184967,
      "learning_rate": 0.00013069481090589272,
      "loss": 0.017,
      "step": 1577
    },
    {
      "epoch": 1.0408970976253298,
      "grad_norm": 0.1129240021109581,
      "learning_rate": 0.00013065083553210203,
      "loss": 0.0198,
      "step": 1578
    },
    {
      "epoch": 1.04155672823219,
      "grad_norm": 0.11522343754768372,
      "learning_rate": 0.00013060686015831137,
      "loss": 0.0115,
      "step": 1579
    },
    {
      "epoch": 1.04221635883905,
      "grad_norm": 0.1081114336848259,
      "learning_rate": 0.0001305628847845207,
      "loss": 0.028,
      "step": 1580
    },
    {
      "epoch": 1.0428759894459103,
      "grad_norm": 0.23561863601207733,
      "learning_rate": 0.00013051890941073,
      "loss": 0.025,
      "step": 1581
    },
    {
      "epoch": 1.0435356200527703,
      "grad_norm": 0.18750229477882385,
      "learning_rate": 0.00013047493403693932,
      "loss": 0.0267,
      "step": 1582
    },
    {
      "epoch": 1.0441952506596306,
      "grad_norm": 0.14041145145893097,
      "learning_rate": 0.00013043095866314866,
      "loss": 0.0147,
      "step": 1583
    },
    {
      "epoch": 1.0448548812664908,
      "grad_norm": 0.21981385350227356,
      "learning_rate": 0.00013038698328935797,
      "loss": 0.0337,
      "step": 1584
    },
    {
      "epoch": 1.0455145118733509,
      "grad_norm": 0.17911286652088165,
      "learning_rate": 0.00013034300791556729,
      "loss": 0.0288,
      "step": 1585
    },
    {
      "epoch": 1.0461741424802111,
      "grad_norm": 0.12778793275356293,
      "learning_rate": 0.00013029903254177663,
      "loss": 0.0139,
      "step": 1586
    },
    {
      "epoch": 1.0468337730870712,
      "grad_norm": 0.21307620406150818,
      "learning_rate": 0.00013025505716798594,
      "loss": 0.0225,
      "step": 1587
    },
    {
      "epoch": 1.0474934036939314,
      "grad_norm": 0.1439308077096939,
      "learning_rate": 0.00013021108179419525,
      "loss": 0.0066,
      "step": 1588
    },
    {
      "epoch": 1.0481530343007917,
      "grad_norm": 0.11966919153928757,
      "learning_rate": 0.00013016710642040457,
      "loss": 0.0098,
      "step": 1589
    },
    {
      "epoch": 1.0488126649076517,
      "grad_norm": 0.28569260239601135,
      "learning_rate": 0.0001301231310466139,
      "loss": 0.0305,
      "step": 1590
    },
    {
      "epoch": 1.049472295514512,
      "grad_norm": 0.1278732270002365,
      "learning_rate": 0.00013007915567282322,
      "loss": 0.0158,
      "step": 1591
    },
    {
      "epoch": 1.050131926121372,
      "grad_norm": 0.10357601195573807,
      "learning_rate": 0.00013003518029903254,
      "loss": 0.0028,
      "step": 1592
    },
    {
      "epoch": 1.0507915567282322,
      "grad_norm": 0.2041146606206894,
      "learning_rate": 0.00012999120492524185,
      "loss": 0.018,
      "step": 1593
    },
    {
      "epoch": 1.0514511873350922,
      "grad_norm": 0.038759496062994,
      "learning_rate": 0.0001299472295514512,
      "loss": 0.0016,
      "step": 1594
    },
    {
      "epoch": 1.0521108179419525,
      "grad_norm": 0.1905675232410431,
      "learning_rate": 0.0001299032541776605,
      "loss": 0.0124,
      "step": 1595
    },
    {
      "epoch": 1.0527704485488127,
      "grad_norm": 0.03788888454437256,
      "learning_rate": 0.00012985927880386982,
      "loss": 0.0016,
      "step": 1596
    },
    {
      "epoch": 1.0534300791556728,
      "grad_norm": 0.140933558344841,
      "learning_rate": 0.00012981530343007916,
      "loss": 0.0136,
      "step": 1597
    },
    {
      "epoch": 1.054089709762533,
      "grad_norm": 0.42731550335884094,
      "learning_rate": 0.00012977132805628848,
      "loss": 0.0676,
      "step": 1598
    },
    {
      "epoch": 1.054749340369393,
      "grad_norm": 0.19123390316963196,
      "learning_rate": 0.00012972735268249782,
      "loss": 0.0115,
      "step": 1599
    },
    {
      "epoch": 1.0554089709762533,
      "grad_norm": 0.11079668253660202,
      "learning_rate": 0.00012968337730870713,
      "loss": 0.0086,
      "step": 1600
    },
    {
      "epoch": 1.0560686015831136,
      "grad_norm": 0.24623073637485504,
      "learning_rate": 0.00012963940193491647,
      "loss": 0.0329,
      "step": 1601
    },
    {
      "epoch": 1.0567282321899736,
      "grad_norm": 0.12399668246507645,
      "learning_rate": 0.00012959542656112579,
      "loss": 0.0091,
      "step": 1602
    },
    {
      "epoch": 1.0573878627968338,
      "grad_norm": 0.25798872113227844,
      "learning_rate": 0.0001295514511873351,
      "loss": 0.0198,
      "step": 1603
    },
    {
      "epoch": 1.0580474934036939,
      "grad_norm": 0.31529712677001953,
      "learning_rate": 0.00012950747581354444,
      "loss": 0.0384,
      "step": 1604
    },
    {
      "epoch": 1.0587071240105541,
      "grad_norm": 0.2045164555311203,
      "learning_rate": 0.00012946350043975375,
      "loss": 0.0326,
      "step": 1605
    },
    {
      "epoch": 1.0593667546174141,
      "grad_norm": 0.22370213270187378,
      "learning_rate": 0.00012941952506596307,
      "loss": 0.0112,
      "step": 1606
    },
    {
      "epoch": 1.0600263852242744,
      "grad_norm": 0.2636738717556,
      "learning_rate": 0.00012937554969217238,
      "loss": 0.0418,
      "step": 1607
    },
    {
      "epoch": 1.0606860158311346,
      "grad_norm": 0.12695489823818207,
      "learning_rate": 0.00012933157431838172,
      "loss": 0.0049,
      "step": 1608
    },
    {
      "epoch": 1.0613456464379947,
      "grad_norm": 0.13766269385814667,
      "learning_rate": 0.00012928759894459104,
      "loss": 0.0084,
      "step": 1609
    },
    {
      "epoch": 1.062005277044855,
      "grad_norm": 0.1912616789340973,
      "learning_rate": 0.00012924362357080035,
      "loss": 0.022,
      "step": 1610
    },
    {
      "epoch": 1.062664907651715,
      "grad_norm": 0.177733913064003,
      "learning_rate": 0.00012919964819700967,
      "loss": 0.0164,
      "step": 1611
    },
    {
      "epoch": 1.0633245382585752,
      "grad_norm": 0.08002712577581406,
      "learning_rate": 0.000129155672823219,
      "loss": 0.0071,
      "step": 1612
    },
    {
      "epoch": 1.0639841688654355,
      "grad_norm": 0.2600023150444031,
      "learning_rate": 0.00012911169744942832,
      "loss": 0.0292,
      "step": 1613
    },
    {
      "epoch": 1.0646437994722955,
      "grad_norm": 0.12317296117544174,
      "learning_rate": 0.00012906772207563763,
      "loss": 0.0184,
      "step": 1614
    },
    {
      "epoch": 1.0653034300791557,
      "grad_norm": 0.17966850101947784,
      "learning_rate": 0.00012902374670184698,
      "loss": 0.0276,
      "step": 1615
    },
    {
      "epoch": 1.0659630606860158,
      "grad_norm": 0.051193613559007645,
      "learning_rate": 0.0001289797713280563,
      "loss": 0.0025,
      "step": 1616
    },
    {
      "epoch": 1.066622691292876,
      "grad_norm": 0.15267500281333923,
      "learning_rate": 0.0001289357959542656,
      "loss": 0.0323,
      "step": 1617
    },
    {
      "epoch": 1.0672823218997363,
      "grad_norm": 0.13108862936496735,
      "learning_rate": 0.00012889182058047494,
      "loss": 0.011,
      "step": 1618
    },
    {
      "epoch": 1.0679419525065963,
      "grad_norm": 0.40352487564086914,
      "learning_rate": 0.00012884784520668426,
      "loss": 0.0603,
      "step": 1619
    },
    {
      "epoch": 1.0686015831134565,
      "grad_norm": 0.22832824289798737,
      "learning_rate": 0.0001288038698328936,
      "loss": 0.02,
      "step": 1620
    },
    {
      "epoch": 1.0692612137203166,
      "grad_norm": 0.2333211600780487,
      "learning_rate": 0.0001287598944591029,
      "loss": 0.0481,
      "step": 1621
    },
    {
      "epoch": 1.0699208443271768,
      "grad_norm": 0.08282286673784256,
      "learning_rate": 0.00012871591908531225,
      "loss": 0.009,
      "step": 1622
    },
    {
      "epoch": 1.0705804749340369,
      "grad_norm": 0.2547234892845154,
      "learning_rate": 0.00012867194371152157,
      "loss": 0.0624,
      "step": 1623
    },
    {
      "epoch": 1.071240105540897,
      "grad_norm": 0.19787809252738953,
      "learning_rate": 0.00012862796833773088,
      "loss": 0.0481,
      "step": 1624
    },
    {
      "epoch": 1.0718997361477574,
      "grad_norm": 0.13686788082122803,
      "learning_rate": 0.0001285839929639402,
      "loss": 0.009,
      "step": 1625
    },
    {
      "epoch": 1.0725593667546174,
      "grad_norm": 0.2554897367954254,
      "learning_rate": 0.00012854001759014954,
      "loss": 0.0384,
      "step": 1626
    },
    {
      "epoch": 1.0732189973614776,
      "grad_norm": 0.28351616859436035,
      "learning_rate": 0.00012849604221635885,
      "loss": 0.0422,
      "step": 1627
    },
    {
      "epoch": 1.0738786279683377,
      "grad_norm": 0.27928072214126587,
      "learning_rate": 0.00012845206684256817,
      "loss": 0.0268,
      "step": 1628
    },
    {
      "epoch": 1.074538258575198,
      "grad_norm": 0.19119977951049805,
      "learning_rate": 0.00012840809146877748,
      "loss": 0.0313,
      "step": 1629
    },
    {
      "epoch": 1.075197889182058,
      "grad_norm": 0.10855486989021301,
      "learning_rate": 0.00012836411609498682,
      "loss": 0.0181,
      "step": 1630
    },
    {
      "epoch": 1.0758575197889182,
      "grad_norm": 0.09300641715526581,
      "learning_rate": 0.00012832014072119613,
      "loss": 0.012,
      "step": 1631
    },
    {
      "epoch": 1.0765171503957784,
      "grad_norm": 0.07491765916347504,
      "learning_rate": 0.00012827616534740545,
      "loss": 0.0098,
      "step": 1632
    },
    {
      "epoch": 1.0771767810026385,
      "grad_norm": 0.041640009731054306,
      "learning_rate": 0.0001282321899736148,
      "loss": 0.0026,
      "step": 1633
    },
    {
      "epoch": 1.0778364116094987,
      "grad_norm": 0.05763350427150726,
      "learning_rate": 0.0001281882145998241,
      "loss": 0.0095,
      "step": 1634
    },
    {
      "epoch": 1.0784960422163588,
      "grad_norm": 0.06729630380868912,
      "learning_rate": 0.00012814423922603342,
      "loss": 0.0074,
      "step": 1635
    },
    {
      "epoch": 1.079155672823219,
      "grad_norm": 0.056280434131622314,
      "learning_rate": 0.00012810026385224273,
      "loss": 0.0089,
      "step": 1636
    },
    {
      "epoch": 1.0798153034300793,
      "grad_norm": 0.28879761695861816,
      "learning_rate": 0.00012805628847845207,
      "loss": 0.0305,
      "step": 1637
    },
    {
      "epoch": 1.0804749340369393,
      "grad_norm": 0.15812507271766663,
      "learning_rate": 0.00012801231310466139,
      "loss": 0.0299,
      "step": 1638
    },
    {
      "epoch": 1.0811345646437995,
      "grad_norm": 0.1495274007320404,
      "learning_rate": 0.0001279683377308707,
      "loss": 0.0241,
      "step": 1639
    },
    {
      "epoch": 1.0817941952506596,
      "grad_norm": 0.23497061431407928,
      "learning_rate": 0.00012792436235708004,
      "loss": 0.0537,
      "step": 1640
    },
    {
      "epoch": 1.0824538258575198,
      "grad_norm": 0.2197745442390442,
      "learning_rate": 0.00012788038698328936,
      "loss": 0.0167,
      "step": 1641
    },
    {
      "epoch": 1.08311345646438,
      "grad_norm": 0.22414115071296692,
      "learning_rate": 0.0001278364116094987,
      "loss": 0.0563,
      "step": 1642
    },
    {
      "epoch": 1.08377308707124,
      "grad_norm": 0.2993340492248535,
      "learning_rate": 0.000127792436235708,
      "loss": 0.078,
      "step": 1643
    },
    {
      "epoch": 1.0844327176781003,
      "grad_norm": 0.0671381950378418,
      "learning_rate": 0.00012774846086191735,
      "loss": 0.0046,
      "step": 1644
    },
    {
      "epoch": 1.0850923482849604,
      "grad_norm": 0.13503393530845642,
      "learning_rate": 0.00012770448548812667,
      "loss": 0.0265,
      "step": 1645
    },
    {
      "epoch": 1.0857519788918206,
      "grad_norm": 0.11959384381771088,
      "learning_rate": 0.00012766051011433598,
      "loss": 0.0188,
      "step": 1646
    },
    {
      "epoch": 1.0864116094986807,
      "grad_norm": 0.13389135897159576,
      "learning_rate": 0.0001276165347405453,
      "loss": 0.0303,
      "step": 1647
    },
    {
      "epoch": 1.087071240105541,
      "grad_norm": 0.21524257957935333,
      "learning_rate": 0.00012757255936675463,
      "loss": 0.0513,
      "step": 1648
    },
    {
      "epoch": 1.0877308707124012,
      "grad_norm": 0.12315645813941956,
      "learning_rate": 0.00012752858399296395,
      "loss": 0.0291,
      "step": 1649
    },
    {
      "epoch": 1.0883905013192612,
      "grad_norm": 0.20215503871440887,
      "learning_rate": 0.00012748460861917326,
      "loss": 0.0265,
      "step": 1650
    },
    {
      "epoch": 1.0890501319261214,
      "grad_norm": 0.12833811342716217,
      "learning_rate": 0.0001274406332453826,
      "loss": 0.0351,
      "step": 1651
    },
    {
      "epoch": 1.0897097625329815,
      "grad_norm": 0.21518975496292114,
      "learning_rate": 0.00012739665787159192,
      "loss": 0.0254,
      "step": 1652
    },
    {
      "epoch": 1.0903693931398417,
      "grad_norm": 0.16950687766075134,
      "learning_rate": 0.00012735268249780123,
      "loss": 0.0156,
      "step": 1653
    },
    {
      "epoch": 1.0910290237467017,
      "grad_norm": 0.12779302895069122,
      "learning_rate": 0.00012730870712401055,
      "loss": 0.0313,
      "step": 1654
    },
    {
      "epoch": 1.091688654353562,
      "grad_norm": 0.1008978858590126,
      "learning_rate": 0.00012726473175021989,
      "loss": 0.0151,
      "step": 1655
    },
    {
      "epoch": 1.0923482849604222,
      "grad_norm": 0.11377675086259842,
      "learning_rate": 0.0001272207563764292,
      "loss": 0.0241,
      "step": 1656
    },
    {
      "epoch": 1.0930079155672823,
      "grad_norm": 0.07398030906915665,
      "learning_rate": 0.00012717678100263851,
      "loss": 0.0122,
      "step": 1657
    },
    {
      "epoch": 1.0936675461741425,
      "grad_norm": 0.2568483054637909,
      "learning_rate": 0.00012713280562884786,
      "loss": 0.0603,
      "step": 1658
    },
    {
      "epoch": 1.0943271767810026,
      "grad_norm": 0.1558941900730133,
      "learning_rate": 0.00012708883025505717,
      "loss": 0.0416,
      "step": 1659
    },
    {
      "epoch": 1.0949868073878628,
      "grad_norm": 0.09164055436849594,
      "learning_rate": 0.00012704485488126648,
      "loss": 0.0143,
      "step": 1660
    },
    {
      "epoch": 1.095646437994723,
      "grad_norm": 0.10811841487884521,
      "learning_rate": 0.00012700087950747582,
      "loss": 0.0142,
      "step": 1661
    },
    {
      "epoch": 1.096306068601583,
      "grad_norm": 0.07810735702514648,
      "learning_rate": 0.00012695690413368514,
      "loss": 0.0096,
      "step": 1662
    },
    {
      "epoch": 1.0969656992084433,
      "grad_norm": 0.08838704973459244,
      "learning_rate": 0.00012691292875989448,
      "loss": 0.0128,
      "step": 1663
    },
    {
      "epoch": 1.0976253298153034,
      "grad_norm": 0.10400494188070297,
      "learning_rate": 0.0001268689533861038,
      "loss": 0.0127,
      "step": 1664
    },
    {
      "epoch": 1.0982849604221636,
      "grad_norm": 0.06611592322587967,
      "learning_rate": 0.0001268249780123131,
      "loss": 0.0068,
      "step": 1665
    },
    {
      "epoch": 1.0989445910290236,
      "grad_norm": 0.33467453718185425,
      "learning_rate": 0.00012678100263852245,
      "loss": 0.0684,
      "step": 1666
    },
    {
      "epoch": 1.099604221635884,
      "grad_norm": 0.4460216164588928,
      "learning_rate": 0.00012673702726473176,
      "loss": 0.0623,
      "step": 1667
    },
    {
      "epoch": 1.1002638522427441,
      "grad_norm": 0.1695583462715149,
      "learning_rate": 0.00012669305189094108,
      "loss": 0.0318,
      "step": 1668
    },
    {
      "epoch": 1.1009234828496042,
      "grad_norm": 0.43028536438941956,
      "learning_rate": 0.00012664907651715042,
      "loss": 0.0472,
      "step": 1669
    },
    {
      "epoch": 1.1015831134564644,
      "grad_norm": 0.2541775405406952,
      "learning_rate": 0.00012660510114335973,
      "loss": 0.0311,
      "step": 1670
    },
    {
      "epoch": 1.1022427440633245,
      "grad_norm": 0.2493414729833603,
      "learning_rate": 0.00012656112576956905,
      "loss": 0.0501,
      "step": 1671
    },
    {
      "epoch": 1.1029023746701847,
      "grad_norm": 0.31085285544395447,
      "learning_rate": 0.00012651715039577836,
      "loss": 0.0469,
      "step": 1672
    },
    {
      "epoch": 1.103562005277045,
      "grad_norm": 0.11305251717567444,
      "learning_rate": 0.0001264731750219877,
      "loss": 0.017,
      "step": 1673
    },
    {
      "epoch": 1.104221635883905,
      "grad_norm": 0.2559122145175934,
      "learning_rate": 0.00012642919964819701,
      "loss": 0.0664,
      "step": 1674
    },
    {
      "epoch": 1.1048812664907652,
      "grad_norm": 0.2083989381790161,
      "learning_rate": 0.00012638522427440633,
      "loss": 0.046,
      "step": 1675
    },
    {
      "epoch": 1.1055408970976253,
      "grad_norm": 0.16319715976715088,
      "learning_rate": 0.00012634124890061567,
      "loss": 0.0375,
      "step": 1676
    },
    {
      "epoch": 1.1062005277044855,
      "grad_norm": 0.12565213441848755,
      "learning_rate": 0.00012629727352682498,
      "loss": 0.0194,
      "step": 1677
    },
    {
      "epoch": 1.1068601583113455,
      "grad_norm": 0.1601046323776245,
      "learning_rate": 0.0001262532981530343,
      "loss": 0.0245,
      "step": 1678
    },
    {
      "epoch": 1.1075197889182058,
      "grad_norm": 0.10715432465076447,
      "learning_rate": 0.0001262093227792436,
      "loss": 0.0189,
      "step": 1679
    },
    {
      "epoch": 1.108179419525066,
      "grad_norm": 0.22720158100128174,
      "learning_rate": 0.00012616534740545295,
      "loss": 0.064,
      "step": 1680
    },
    {
      "epoch": 1.108839050131926,
      "grad_norm": 0.09442596137523651,
      "learning_rate": 0.00012612137203166227,
      "loss": 0.0139,
      "step": 1681
    },
    {
      "epoch": 1.1094986807387863,
      "grad_norm": 0.10501570254564285,
      "learning_rate": 0.0001260773966578716,
      "loss": 0.0246,
      "step": 1682
    },
    {
      "epoch": 1.1101583113456464,
      "grad_norm": 0.07752154767513275,
      "learning_rate": 0.00012603342128408092,
      "loss": 0.0166,
      "step": 1683
    },
    {
      "epoch": 1.1108179419525066,
      "grad_norm": 0.1363438367843628,
      "learning_rate": 0.00012598944591029026,
      "loss": 0.0229,
      "step": 1684
    },
    {
      "epoch": 1.1114775725593669,
      "grad_norm": 0.10588543117046356,
      "learning_rate": 0.00012594547053649958,
      "loss": 0.0238,
      "step": 1685
    },
    {
      "epoch": 1.1121372031662269,
      "grad_norm": 0.11245787888765335,
      "learning_rate": 0.0001259014951627089,
      "loss": 0.0169,
      "step": 1686
    },
    {
      "epoch": 1.1127968337730871,
      "grad_norm": 0.2613503336906433,
      "learning_rate": 0.00012585751978891823,
      "loss": 0.0608,
      "step": 1687
    },
    {
      "epoch": 1.1134564643799472,
      "grad_norm": 0.08720838278532028,
      "learning_rate": 0.00012581354441512755,
      "loss": 0.006,
      "step": 1688
    },
    {
      "epoch": 1.1141160949868074,
      "grad_norm": 0.1325674206018448,
      "learning_rate": 0.00012576956904133686,
      "loss": 0.0221,
      "step": 1689
    },
    {
      "epoch": 1.1147757255936674,
      "grad_norm": 0.09933234006166458,
      "learning_rate": 0.00012572559366754617,
      "loss": 0.0206,
      "step": 1690
    },
    {
      "epoch": 1.1154353562005277,
      "grad_norm": 0.11779292672872543,
      "learning_rate": 0.00012568161829375551,
      "loss": 0.028,
      "step": 1691
    },
    {
      "epoch": 1.116094986807388,
      "grad_norm": 0.1154516413807869,
      "learning_rate": 0.00012563764291996483,
      "loss": 0.02,
      "step": 1692
    },
    {
      "epoch": 1.116754617414248,
      "grad_norm": 0.1146373376250267,
      "learning_rate": 0.00012559366754617414,
      "loss": 0.0174,
      "step": 1693
    },
    {
      "epoch": 1.1174142480211082,
      "grad_norm": 0.09165140241384506,
      "learning_rate": 0.00012554969217238348,
      "loss": 0.0158,
      "step": 1694
    },
    {
      "epoch": 1.1180738786279683,
      "grad_norm": 0.15215709805488586,
      "learning_rate": 0.0001255057167985928,
      "loss": 0.0261,
      "step": 1695
    },
    {
      "epoch": 1.1187335092348285,
      "grad_norm": 0.15228226780891418,
      "learning_rate": 0.0001254617414248021,
      "loss": 0.0296,
      "step": 1696
    },
    {
      "epoch": 1.1193931398416888,
      "grad_norm": 0.07819265872240067,
      "learning_rate": 0.00012541776605101143,
      "loss": 0.0099,
      "step": 1697
    },
    {
      "epoch": 1.1200527704485488,
      "grad_norm": 0.13207204639911652,
      "learning_rate": 0.00012537379067722077,
      "loss": 0.0089,
      "step": 1698
    },
    {
      "epoch": 1.120712401055409,
      "grad_norm": 0.10318981856107712,
      "learning_rate": 0.00012532981530343008,
      "loss": 0.0171,
      "step": 1699
    },
    {
      "epoch": 1.121372031662269,
      "grad_norm": 0.06968246400356293,
      "learning_rate": 0.0001252858399296394,
      "loss": 0.0042,
      "step": 1700
    },
    {
      "epoch": 1.1220316622691293,
      "grad_norm": 0.12299109250307083,
      "learning_rate": 0.0001252418645558487,
      "loss": 0.0096,
      "step": 1701
    },
    {
      "epoch": 1.1226912928759893,
      "grad_norm": 0.16373270750045776,
      "learning_rate": 0.00012519788918205805,
      "loss": 0.0229,
      "step": 1702
    },
    {
      "epoch": 1.1233509234828496,
      "grad_norm": 0.04287328943610191,
      "learning_rate": 0.00012515391380826736,
      "loss": 0.0024,
      "step": 1703
    },
    {
      "epoch": 1.1240105540897098,
      "grad_norm": 0.10796511918306351,
      "learning_rate": 0.0001251099384344767,
      "loss": 0.0142,
      "step": 1704
    },
    {
      "epoch": 1.1246701846965699,
      "grad_norm": 0.22500424087047577,
      "learning_rate": 0.00012506596306068602,
      "loss": 0.0295,
      "step": 1705
    },
    {
      "epoch": 1.1253298153034301,
      "grad_norm": 0.09691405296325684,
      "learning_rate": 0.00012502198768689536,
      "loss": 0.0167,
      "step": 1706
    },
    {
      "epoch": 1.1259894459102902,
      "grad_norm": 0.06719887256622314,
      "learning_rate": 0.00012497801231310467,
      "loss": 0.0062,
      "step": 1707
    },
    {
      "epoch": 1.1266490765171504,
      "grad_norm": 0.36314690113067627,
      "learning_rate": 0.000124934036939314,
      "loss": 0.059,
      "step": 1708
    },
    {
      "epoch": 1.1273087071240107,
      "grad_norm": 0.1989104002714157,
      "learning_rate": 0.00012489006156552333,
      "loss": 0.0212,
      "step": 1709
    },
    {
      "epoch": 1.1279683377308707,
      "grad_norm": 0.15332458913326263,
      "learning_rate": 0.00012484608619173264,
      "loss": 0.0203,
      "step": 1710
    },
    {
      "epoch": 1.128627968337731,
      "grad_norm": 0.0949581041932106,
      "learning_rate": 0.00012480211081794196,
      "loss": 0.0064,
      "step": 1711
    },
    {
      "epoch": 1.129287598944591,
      "grad_norm": 0.2981232702732086,
      "learning_rate": 0.0001247581354441513,
      "loss": 0.0533,
      "step": 1712
    },
    {
      "epoch": 1.1299472295514512,
      "grad_norm": 0.17292781174182892,
      "learning_rate": 0.0001247141600703606,
      "loss": 0.0146,
      "step": 1713
    },
    {
      "epoch": 1.1306068601583115,
      "grad_norm": 0.20859389007091522,
      "learning_rate": 0.00012467018469656993,
      "loss": 0.02,
      "step": 1714
    },
    {
      "epoch": 1.1312664907651715,
      "grad_norm": 0.2532958984375,
      "learning_rate": 0.00012462620932277924,
      "loss": 0.0334,
      "step": 1715
    },
    {
      "epoch": 1.1319261213720317,
      "grad_norm": 0.25761324167251587,
      "learning_rate": 0.00012458223394898858,
      "loss": 0.0147,
      "step": 1716
    },
    {
      "epoch": 1.1325857519788918,
      "grad_norm": 0.1948249787092209,
      "learning_rate": 0.0001245382585751979,
      "loss": 0.0218,
      "step": 1717
    },
    {
      "epoch": 1.133245382585752,
      "grad_norm": 0.192152202129364,
      "learning_rate": 0.0001244942832014072,
      "loss": 0.0272,
      "step": 1718
    },
    {
      "epoch": 1.133905013192612,
      "grad_norm": 0.2221912443637848,
      "learning_rate": 0.00012445030782761652,
      "loss": 0.0384,
      "step": 1719
    },
    {
      "epoch": 1.1345646437994723,
      "grad_norm": 0.2757875919342041,
      "learning_rate": 0.00012440633245382586,
      "loss": 0.053,
      "step": 1720
    },
    {
      "epoch": 1.1352242744063323,
      "grad_norm": 0.048639070242643356,
      "learning_rate": 0.00012436235708003518,
      "loss": 0.0043,
      "step": 1721
    },
    {
      "epoch": 1.1358839050131926,
      "grad_norm": 0.05419806391000748,
      "learning_rate": 0.0001243183817062445,
      "loss": 0.0025,
      "step": 1722
    },
    {
      "epoch": 1.1365435356200528,
      "grad_norm": 0.27272751927375793,
      "learning_rate": 0.00012427440633245383,
      "loss": 0.0332,
      "step": 1723
    },
    {
      "epoch": 1.1372031662269129,
      "grad_norm": 0.18637876212596893,
      "learning_rate": 0.00012423043095866315,
      "loss": 0.0156,
      "step": 1724
    },
    {
      "epoch": 1.1378627968337731,
      "grad_norm": 0.21684062480926514,
      "learning_rate": 0.0001241864555848725,
      "loss": 0.0465,
      "step": 1725
    },
    {
      "epoch": 1.1385224274406331,
      "grad_norm": 0.15251681208610535,
      "learning_rate": 0.0001241424802110818,
      "loss": 0.012,
      "step": 1726
    },
    {
      "epoch": 1.1391820580474934,
      "grad_norm": 0.16359256207942963,
      "learning_rate": 0.00012409850483729114,
      "loss": 0.0272,
      "step": 1727
    },
    {
      "epoch": 1.1398416886543536,
      "grad_norm": 0.13916784524917603,
      "learning_rate": 0.00012405452946350046,
      "loss": 0.0183,
      "step": 1728
    },
    {
      "epoch": 1.1405013192612137,
      "grad_norm": 0.16043110191822052,
      "learning_rate": 0.00012401055408970977,
      "loss": 0.0162,
      "step": 1729
    },
    {
      "epoch": 1.141160949868074,
      "grad_norm": 0.1991848647594452,
      "learning_rate": 0.0001239665787159191,
      "loss": 0.0309,
      "step": 1730
    },
    {
      "epoch": 1.141820580474934,
      "grad_norm": 0.15220768749713898,
      "learning_rate": 0.00012392260334212843,
      "loss": 0.0216,
      "step": 1731
    },
    {
      "epoch": 1.1424802110817942,
      "grad_norm": 0.1342124044895172,
      "learning_rate": 0.00012387862796833774,
      "loss": 0.0159,
      "step": 1732
    },
    {
      "epoch": 1.1431398416886545,
      "grad_norm": 0.15668639540672302,
      "learning_rate": 0.00012383465259454705,
      "loss": 0.019,
      "step": 1733
    },
    {
      "epoch": 1.1437994722955145,
      "grad_norm": 0.38781091570854187,
      "learning_rate": 0.0001237906772207564,
      "loss": 0.0785,
      "step": 1734
    },
    {
      "epoch": 1.1444591029023747,
      "grad_norm": 0.2057795226573944,
      "learning_rate": 0.0001237467018469657,
      "loss": 0.0371,
      "step": 1735
    },
    {
      "epoch": 1.1451187335092348,
      "grad_norm": 0.1667100489139557,
      "learning_rate": 0.00012370272647317502,
      "loss": 0.0253,
      "step": 1736
    },
    {
      "epoch": 1.145778364116095,
      "grad_norm": 0.1493515521287918,
      "learning_rate": 0.00012365875109938434,
      "loss": 0.0331,
      "step": 1737
    },
    {
      "epoch": 1.1464379947229553,
      "grad_norm": 0.2361277937889099,
      "learning_rate": 0.00012361477572559368,
      "loss": 0.019,
      "step": 1738
    },
    {
      "epoch": 1.1470976253298153,
      "grad_norm": 0.12232338637113571,
      "learning_rate": 0.000123570800351803,
      "loss": 0.0068,
      "step": 1739
    },
    {
      "epoch": 1.1477572559366755,
      "grad_norm": 0.1772538125514984,
      "learning_rate": 0.0001235268249780123,
      "loss": 0.0193,
      "step": 1740
    },
    {
      "epoch": 1.1484168865435356,
      "grad_norm": 0.1439252346754074,
      "learning_rate": 0.00012348284960422165,
      "loss": 0.0252,
      "step": 1741
    },
    {
      "epoch": 1.1490765171503958,
      "grad_norm": 0.1487504094839096,
      "learning_rate": 0.00012343887423043096,
      "loss": 0.0173,
      "step": 1742
    },
    {
      "epoch": 1.1497361477572559,
      "grad_norm": 0.16313312947750092,
      "learning_rate": 0.00012339489885664027,
      "loss": 0.0327,
      "step": 1743
    },
    {
      "epoch": 1.150395778364116,
      "grad_norm": 0.15102410316467285,
      "learning_rate": 0.0001233509234828496,
      "loss": 0.0141,
      "step": 1744
    },
    {
      "epoch": 1.1510554089709761,
      "grad_norm": 0.16853076219558716,
      "learning_rate": 0.00012330694810905893,
      "loss": 0.0194,
      "step": 1745
    },
    {
      "epoch": 1.1517150395778364,
      "grad_norm": 0.30562713742256165,
      "learning_rate": 0.00012326297273526824,
      "loss": 0.0423,
      "step": 1746
    },
    {
      "epoch": 1.1523746701846966,
      "grad_norm": 0.19701437652111053,
      "learning_rate": 0.00012321899736147758,
      "loss": 0.0148,
      "step": 1747
    },
    {
      "epoch": 1.1530343007915567,
      "grad_norm": 0.23136578500270844,
      "learning_rate": 0.00012317502198768693,
      "loss": 0.0328,
      "step": 1748
    },
    {
      "epoch": 1.153693931398417,
      "grad_norm": 0.1564289927482605,
      "learning_rate": 0.00012313104661389624,
      "loss": 0.0257,
      "step": 1749
    },
    {
      "epoch": 1.154353562005277,
      "grad_norm": 0.1311463713645935,
      "learning_rate": 0.00012308707124010555,
      "loss": 0.0201,
      "step": 1750
    },
    {
      "epoch": 1.1550131926121372,
      "grad_norm": 0.2689535319805145,
      "learning_rate": 0.00012304309586631487,
      "loss": 0.0396,
      "step": 1751
    },
    {
      "epoch": 1.1556728232189974,
      "grad_norm": 0.24904881417751312,
      "learning_rate": 0.0001229991204925242,
      "loss": 0.0221,
      "step": 1752
    },
    {
      "epoch": 1.1563324538258575,
      "grad_norm": 0.1964343637228012,
      "learning_rate": 0.00012295514511873352,
      "loss": 0.0196,
      "step": 1753
    },
    {
      "epoch": 1.1569920844327177,
      "grad_norm": 0.26558852195739746,
      "learning_rate": 0.00012291116974494284,
      "loss": 0.0274,
      "step": 1754
    },
    {
      "epoch": 1.1576517150395778,
      "grad_norm": 0.16943489015102386,
      "learning_rate": 0.00012286719437115215,
      "loss": 0.0217,
      "step": 1755
    },
    {
      "epoch": 1.158311345646438,
      "grad_norm": 0.1346386820077896,
      "learning_rate": 0.0001228232189973615,
      "loss": 0.0073,
      "step": 1756
    },
    {
      "epoch": 1.1589709762532983,
      "grad_norm": 0.13631068170070648,
      "learning_rate": 0.0001227792436235708,
      "loss": 0.01,
      "step": 1757
    },
    {
      "epoch": 1.1596306068601583,
      "grad_norm": 0.1236032098531723,
      "learning_rate": 0.00012273526824978012,
      "loss": 0.0169,
      "step": 1758
    },
    {
      "epoch": 1.1602902374670185,
      "grad_norm": 0.20682965219020844,
      "learning_rate": 0.00012269129287598946,
      "loss": 0.0271,
      "step": 1759
    },
    {
      "epoch": 1.1609498680738786,
      "grad_norm": 0.2824173867702484,
      "learning_rate": 0.00012264731750219877,
      "loss": 0.0379,
      "step": 1760
    },
    {
      "epoch": 1.1616094986807388,
      "grad_norm": 0.13276292383670807,
      "learning_rate": 0.0001226033421284081,
      "loss": 0.0199,
      "step": 1761
    },
    {
      "epoch": 1.162269129287599,
      "grad_norm": 0.2091647982597351,
      "learning_rate": 0.0001225593667546174,
      "loss": 0.0224,
      "step": 1762
    },
    {
      "epoch": 1.162928759894459,
      "grad_norm": 0.3713323175907135,
      "learning_rate": 0.00012251539138082674,
      "loss": 0.0515,
      "step": 1763
    },
    {
      "epoch": 1.1635883905013193,
      "grad_norm": 0.14128750562667847,
      "learning_rate": 0.00012247141600703606,
      "loss": 0.0161,
      "step": 1764
    },
    {
      "epoch": 1.1642480211081794,
      "grad_norm": 0.13925175368785858,
      "learning_rate": 0.00012242744063324537,
      "loss": 0.0218,
      "step": 1765
    },
    {
      "epoch": 1.1649076517150396,
      "grad_norm": 0.24837450683116913,
      "learning_rate": 0.0001223834652594547,
      "loss": 0.0243,
      "step": 1766
    },
    {
      "epoch": 1.1655672823218997,
      "grad_norm": 0.1805095225572586,
      "learning_rate": 0.00012233948988566403,
      "loss": 0.0353,
      "step": 1767
    },
    {
      "epoch": 1.16622691292876,
      "grad_norm": 0.20083458721637726,
      "learning_rate": 0.00012229551451187337,
      "loss": 0.0376,
      "step": 1768
    },
    {
      "epoch": 1.16688654353562,
      "grad_norm": 0.3197582960128784,
      "learning_rate": 0.00012225153913808268,
      "loss": 0.0256,
      "step": 1769
    },
    {
      "epoch": 1.1675461741424802,
      "grad_norm": 0.3359129726886749,
      "learning_rate": 0.00012220756376429202,
      "loss": 0.0204,
      "step": 1770
    },
    {
      "epoch": 1.1682058047493404,
      "grad_norm": 0.20086582005023956,
      "learning_rate": 0.00012216358839050134,
      "loss": 0.0291,
      "step": 1771
    },
    {
      "epoch": 1.1688654353562005,
      "grad_norm": 0.14072327315807343,
      "learning_rate": 0.00012211961301671065,
      "loss": 0.0163,
      "step": 1772
    },
    {
      "epoch": 1.1695250659630607,
      "grad_norm": 0.39633941650390625,
      "learning_rate": 0.00012207563764291996,
      "loss": 0.065,
      "step": 1773
    },
    {
      "epoch": 1.1701846965699207,
      "grad_norm": 0.12771452963352203,
      "learning_rate": 0.0001220316622691293,
      "loss": 0.0168,
      "step": 1774
    },
    {
      "epoch": 1.170844327176781,
      "grad_norm": 0.2158251404762268,
      "learning_rate": 0.00012198768689533862,
      "loss": 0.0587,
      "step": 1775
    },
    {
      "epoch": 1.1715039577836412,
      "grad_norm": 0.14667558670043945,
      "learning_rate": 0.00012194371152154793,
      "loss": 0.0239,
      "step": 1776
    },
    {
      "epoch": 1.1721635883905013,
      "grad_norm": 0.16421771049499512,
      "learning_rate": 0.00012189973614775727,
      "loss": 0.0157,
      "step": 1777
    },
    {
      "epoch": 1.1728232189973615,
      "grad_norm": 0.11208193749189377,
      "learning_rate": 0.00012185576077396659,
      "loss": 0.0067,
      "step": 1778
    },
    {
      "epoch": 1.1734828496042216,
      "grad_norm": 0.11677540093660355,
      "learning_rate": 0.0001218117854001759,
      "loss": 0.0182,
      "step": 1779
    },
    {
      "epoch": 1.1741424802110818,
      "grad_norm": 0.1753077507019043,
      "learning_rate": 0.00012176781002638522,
      "loss": 0.0287,
      "step": 1780
    },
    {
      "epoch": 1.174802110817942,
      "grad_norm": 0.1760816127061844,
      "learning_rate": 0.00012172383465259456,
      "loss": 0.0174,
      "step": 1781
    },
    {
      "epoch": 1.175461741424802,
      "grad_norm": 0.07780112326145172,
      "learning_rate": 0.00012167985927880387,
      "loss": 0.0038,
      "step": 1782
    },
    {
      "epoch": 1.1761213720316623,
      "grad_norm": 0.19535435736179352,
      "learning_rate": 0.0001216358839050132,
      "loss": 0.016,
      "step": 1783
    },
    {
      "epoch": 1.1767810026385224,
      "grad_norm": 0.1232534795999527,
      "learning_rate": 0.00012159190853122253,
      "loss": 0.0231,
      "step": 1784
    },
    {
      "epoch": 1.1774406332453826,
      "grad_norm": 0.222138911485672,
      "learning_rate": 0.00012154793315743185,
      "loss": 0.0313,
      "step": 1785
    },
    {
      "epoch": 1.1781002638522429,
      "grad_norm": 0.35400867462158203,
      "learning_rate": 0.00012150395778364117,
      "loss": 0.0558,
      "step": 1786
    },
    {
      "epoch": 1.178759894459103,
      "grad_norm": 0.12118765711784363,
      "learning_rate": 0.00012145998240985048,
      "loss": 0.0225,
      "step": 1787
    },
    {
      "epoch": 1.1794195250659631,
      "grad_norm": 0.1822734922170639,
      "learning_rate": 0.00012141600703605982,
      "loss": 0.0277,
      "step": 1788
    },
    {
      "epoch": 1.1800791556728232,
      "grad_norm": 0.18679723143577576,
      "learning_rate": 0.00012137203166226914,
      "loss": 0.0307,
      "step": 1789
    },
    {
      "epoch": 1.1807387862796834,
      "grad_norm": 0.15913130342960358,
      "learning_rate": 0.00012132805628847845,
      "loss": 0.0177,
      "step": 1790
    },
    {
      "epoch": 1.1813984168865435,
      "grad_norm": 0.12672875821590424,
      "learning_rate": 0.00012128408091468776,
      "loss": 0.0149,
      "step": 1791
    },
    {
      "epoch": 1.1820580474934037,
      "grad_norm": 0.2312317192554474,
      "learning_rate": 0.0001212401055408971,
      "loss": 0.0324,
      "step": 1792
    },
    {
      "epoch": 1.1827176781002637,
      "grad_norm": 0.23323816061019897,
      "learning_rate": 0.00012119613016710642,
      "loss": 0.0483,
      "step": 1793
    },
    {
      "epoch": 1.183377308707124,
      "grad_norm": 0.174357607960701,
      "learning_rate": 0.00012115215479331575,
      "loss": 0.0114,
      "step": 1794
    },
    {
      "epoch": 1.1840369393139842,
      "grad_norm": 0.144354447722435,
      "learning_rate": 0.00012110817941952507,
      "loss": 0.0193,
      "step": 1795
    },
    {
      "epoch": 1.1846965699208443,
      "grad_norm": 0.1358659714460373,
      "learning_rate": 0.0001210642040457344,
      "loss": 0.0222,
      "step": 1796
    },
    {
      "epoch": 1.1853562005277045,
      "grad_norm": 0.1470065861940384,
      "learning_rate": 0.00012102022867194372,
      "loss": 0.0286,
      "step": 1797
    },
    {
      "epoch": 1.1860158311345645,
      "grad_norm": 0.3195502758026123,
      "learning_rate": 0.00012097625329815303,
      "loss": 0.0287,
      "step": 1798
    },
    {
      "epoch": 1.1866754617414248,
      "grad_norm": 0.15515926480293274,
      "learning_rate": 0.00012093227792436237,
      "loss": 0.0153,
      "step": 1799
    },
    {
      "epoch": 1.187335092348285,
      "grad_norm": 0.16260914504528046,
      "learning_rate": 0.00012088830255057168,
      "loss": 0.0316,
      "step": 1800
    },
    {
      "epoch": 1.187994722955145,
      "grad_norm": 0.0948043167591095,
      "learning_rate": 0.000120844327176781,
      "loss": 0.0076,
      "step": 1801
    },
    {
      "epoch": 1.1886543535620053,
      "grad_norm": 0.18205273151397705,
      "learning_rate": 0.00012080035180299034,
      "loss": 0.0174,
      "step": 1802
    },
    {
      "epoch": 1.1893139841688654,
      "grad_norm": 0.1532396674156189,
      "learning_rate": 0.00012075637642919965,
      "loss": 0.0154,
      "step": 1803
    },
    {
      "epoch": 1.1899736147757256,
      "grad_norm": 0.1540450006723404,
      "learning_rate": 0.00012071240105540898,
      "loss": 0.0146,
      "step": 1804
    },
    {
      "epoch": 1.1906332453825859,
      "grad_norm": 0.18085359036922455,
      "learning_rate": 0.0001206684256816183,
      "loss": 0.0201,
      "step": 1805
    },
    {
      "epoch": 1.1912928759894459,
      "grad_norm": 0.15901713073253632,
      "learning_rate": 0.00012062445030782764,
      "loss": 0.0081,
      "step": 1806
    },
    {
      "epoch": 1.1919525065963061,
      "grad_norm": 0.06374458223581314,
      "learning_rate": 0.00012058047493403695,
      "loss": 0.0059,
      "step": 1807
    },
    {
      "epoch": 1.1926121372031662,
      "grad_norm": 0.13965632021427155,
      "learning_rate": 0.00012053649956024626,
      "loss": 0.0148,
      "step": 1808
    },
    {
      "epoch": 1.1932717678100264,
      "grad_norm": 0.1493040770292282,
      "learning_rate": 0.00012049252418645558,
      "loss": 0.0151,
      "step": 1809
    },
    {
      "epoch": 1.1939313984168864,
      "grad_norm": 0.154572993516922,
      "learning_rate": 0.00012044854881266492,
      "loss": 0.0155,
      "step": 1810
    },
    {
      "epoch": 1.1945910290237467,
      "grad_norm": 0.2486594170331955,
      "learning_rate": 0.00012040457343887423,
      "loss": 0.0277,
      "step": 1811
    },
    {
      "epoch": 1.195250659630607,
      "grad_norm": 0.1538306474685669,
      "learning_rate": 0.00012036059806508355,
      "loss": 0.0193,
      "step": 1812
    },
    {
      "epoch": 1.195910290237467,
      "grad_norm": 0.18603549897670746,
      "learning_rate": 0.00012031662269129289,
      "loss": 0.0183,
      "step": 1813
    },
    {
      "epoch": 1.1965699208443272,
      "grad_norm": 0.516547679901123,
      "learning_rate": 0.0001202726473175022,
      "loss": 0.0763,
      "step": 1814
    },
    {
      "epoch": 1.1972295514511873,
      "grad_norm": 0.10552812367677689,
      "learning_rate": 0.00012022867194371153,
      "loss": 0.0085,
      "step": 1815
    },
    {
      "epoch": 1.1978891820580475,
      "grad_norm": 0.24311690032482147,
      "learning_rate": 0.00012018469656992084,
      "loss": 0.0521,
      "step": 1816
    },
    {
      "epoch": 1.1985488126649075,
      "grad_norm": 0.13860584795475006,
      "learning_rate": 0.00012014072119613018,
      "loss": 0.0127,
      "step": 1817
    },
    {
      "epoch": 1.1992084432717678,
      "grad_norm": 0.2886667549610138,
      "learning_rate": 0.0001200967458223395,
      "loss": 0.0284,
      "step": 1818
    },
    {
      "epoch": 1.199868073878628,
      "grad_norm": 0.1987171173095703,
      "learning_rate": 0.00012005277044854881,
      "loss": 0.0292,
      "step": 1819
    },
    {
      "epoch": 1.200527704485488,
      "grad_norm": 0.2063312977552414,
      "learning_rate": 0.00012000879507475815,
      "loss": 0.0232,
      "step": 1820
    },
    {
      "epoch": 1.2011873350923483,
      "grad_norm": 0.3239278793334961,
      "learning_rate": 0.00011996481970096747,
      "loss": 0.0284,
      "step": 1821
    },
    {
      "epoch": 1.2018469656992083,
      "grad_norm": 0.1518269032239914,
      "learning_rate": 0.00011992084432717678,
      "loss": 0.0172,
      "step": 1822
    },
    {
      "epoch": 1.2025065963060686,
      "grad_norm": 0.1444176882505417,
      "learning_rate": 0.0001198768689533861,
      "loss": 0.0213,
      "step": 1823
    },
    {
      "epoch": 1.2031662269129288,
      "grad_norm": 0.15711672604084015,
      "learning_rate": 0.00011983289357959544,
      "loss": 0.02,
      "step": 1824
    },
    {
      "epoch": 1.2038258575197889,
      "grad_norm": 0.15018482506275177,
      "learning_rate": 0.00011978891820580475,
      "loss": 0.0213,
      "step": 1825
    },
    {
      "epoch": 1.2044854881266491,
      "grad_norm": 0.2589740753173828,
      "learning_rate": 0.00011974494283201408,
      "loss": 0.031,
      "step": 1826
    },
    {
      "epoch": 1.2051451187335092,
      "grad_norm": 0.13028134405612946,
      "learning_rate": 0.00011970096745822339,
      "loss": 0.0079,
      "step": 1827
    },
    {
      "epoch": 1.2058047493403694,
      "grad_norm": 0.126622274518013,
      "learning_rate": 0.00011965699208443273,
      "loss": 0.0168,
      "step": 1828
    },
    {
      "epoch": 1.2064643799472297,
      "grad_norm": 0.16215677559375763,
      "learning_rate": 0.00011961301671064205,
      "loss": 0.0255,
      "step": 1829
    },
    {
      "epoch": 1.2071240105540897,
      "grad_norm": 0.12555937469005585,
      "learning_rate": 0.00011956904133685136,
      "loss": 0.0115,
      "step": 1830
    },
    {
      "epoch": 1.20778364116095,
      "grad_norm": 0.22619879245758057,
      "learning_rate": 0.0001195250659630607,
      "loss": 0.0281,
      "step": 1831
    },
    {
      "epoch": 1.20844327176781,
      "grad_norm": 0.05128670856356621,
      "learning_rate": 0.00011948109058927002,
      "loss": 0.0039,
      "step": 1832
    },
    {
      "epoch": 1.2091029023746702,
      "grad_norm": 0.10115319490432739,
      "learning_rate": 0.00011943711521547933,
      "loss": 0.0104,
      "step": 1833
    },
    {
      "epoch": 1.2097625329815302,
      "grad_norm": 0.14600087702274323,
      "learning_rate": 0.00011939313984168864,
      "loss": 0.017,
      "step": 1834
    },
    {
      "epoch": 1.2104221635883905,
      "grad_norm": 0.0666356012225151,
      "learning_rate": 0.00011934916446789799,
      "loss": 0.0069,
      "step": 1835
    },
    {
      "epoch": 1.2110817941952507,
      "grad_norm": 0.5091881155967712,
      "learning_rate": 0.0001193051890941073,
      "loss": 0.0832,
      "step": 1836
    },
    {
      "epoch": 1.2117414248021108,
      "grad_norm": 0.054258476942777634,
      "learning_rate": 0.00011926121372031663,
      "loss": 0.0031,
      "step": 1837
    },
    {
      "epoch": 1.212401055408971,
      "grad_norm": 0.10141773521900177,
      "learning_rate": 0.00011921723834652597,
      "loss": 0.0098,
      "step": 1838
    },
    {
      "epoch": 1.213060686015831,
      "grad_norm": 0.13304878771305084,
      "learning_rate": 0.00011917326297273528,
      "loss": 0.0089,
      "step": 1839
    },
    {
      "epoch": 1.2137203166226913,
      "grad_norm": 0.18040242791175842,
      "learning_rate": 0.0001191292875989446,
      "loss": 0.0263,
      "step": 1840
    },
    {
      "epoch": 1.2143799472295513,
      "grad_norm": 0.12211735546588898,
      "learning_rate": 0.00011908531222515391,
      "loss": 0.0082,
      "step": 1841
    },
    {
      "epoch": 1.2150395778364116,
      "grad_norm": 0.18522833287715912,
      "learning_rate": 0.00011904133685136325,
      "loss": 0.0226,
      "step": 1842
    },
    {
      "epoch": 1.2156992084432718,
      "grad_norm": 0.1818353682756424,
      "learning_rate": 0.00011899736147757256,
      "loss": 0.0207,
      "step": 1843
    },
    {
      "epoch": 1.2163588390501319,
      "grad_norm": 0.28945133090019226,
      "learning_rate": 0.00011895338610378188,
      "loss": 0.0514,
      "step": 1844
    },
    {
      "epoch": 1.2170184696569921,
      "grad_norm": 0.11137576401233673,
      "learning_rate": 0.0001189094107299912,
      "loss": 0.0104,
      "step": 1845
    },
    {
      "epoch": 1.2176781002638521,
      "grad_norm": 0.4494888484477997,
      "learning_rate": 0.00011886543535620053,
      "loss": 0.0441,
      "step": 1846
    },
    {
      "epoch": 1.2183377308707124,
      "grad_norm": 0.18837182223796844,
      "learning_rate": 0.00011882145998240986,
      "loss": 0.0152,
      "step": 1847
    },
    {
      "epoch": 1.2189973614775726,
      "grad_norm": 0.14509651064872742,
      "learning_rate": 0.00011877748460861918,
      "loss": 0.0165,
      "step": 1848
    },
    {
      "epoch": 1.2196569920844327,
      "grad_norm": 0.13284268975257874,
      "learning_rate": 0.00011873350923482852,
      "loss": 0.0072,
      "step": 1849
    },
    {
      "epoch": 1.220316622691293,
      "grad_norm": 0.11147887259721756,
      "learning_rate": 0.00011868953386103783,
      "loss": 0.0096,
      "step": 1850
    },
    {
      "epoch": 1.220976253298153,
      "grad_norm": 0.09355675429105759,
      "learning_rate": 0.00011864555848724714,
      "loss": 0.0117,
      "step": 1851
    },
    {
      "epoch": 1.2216358839050132,
      "grad_norm": 0.15695491433143616,
      "learning_rate": 0.00011860158311345646,
      "loss": 0.0161,
      "step": 1852
    },
    {
      "epoch": 1.2222955145118735,
      "grad_norm": 0.16607433557510376,
      "learning_rate": 0.0001185576077396658,
      "loss": 0.0224,
      "step": 1853
    },
    {
      "epoch": 1.2229551451187335,
      "grad_norm": 0.37643900513648987,
      "learning_rate": 0.00011851363236587511,
      "loss": 0.0471,
      "step": 1854
    },
    {
      "epoch": 1.2236147757255937,
      "grad_norm": 0.13836470246315002,
      "learning_rate": 0.00011846965699208443,
      "loss": 0.0106,
      "step": 1855
    },
    {
      "epoch": 1.2242744063324538,
      "grad_norm": 0.24934351444244385,
      "learning_rate": 0.00011842568161829377,
      "loss": 0.0145,
      "step": 1856
    },
    {
      "epoch": 1.224934036939314,
      "grad_norm": 0.40193572640419006,
      "learning_rate": 0.00011838170624450308,
      "loss": 0.0307,
      "step": 1857
    },
    {
      "epoch": 1.225593667546174,
      "grad_norm": 0.13907893002033234,
      "learning_rate": 0.00011833773087071241,
      "loss": 0.02,
      "step": 1858
    },
    {
      "epoch": 1.2262532981530343,
      "grad_norm": 0.2040724903345108,
      "learning_rate": 0.00011829375549692172,
      "loss": 0.0161,
      "step": 1859
    },
    {
      "epoch": 1.2269129287598945,
      "grad_norm": 0.43999072909355164,
      "learning_rate": 0.00011824978012313106,
      "loss": 0.0668,
      "step": 1860
    },
    {
      "epoch": 1.2275725593667546,
      "grad_norm": 0.23893216252326965,
      "learning_rate": 0.00011820580474934038,
      "loss": 0.0317,
      "step": 1861
    },
    {
      "epoch": 1.2282321899736148,
      "grad_norm": 0.2937983572483063,
      "learning_rate": 0.00011816182937554969,
      "loss": 0.0281,
      "step": 1862
    },
    {
      "epoch": 1.2288918205804749,
      "grad_norm": 0.2799767851829529,
      "learning_rate": 0.000118117854001759,
      "loss": 0.0419,
      "step": 1863
    },
    {
      "epoch": 1.229551451187335,
      "grad_norm": 0.3971564769744873,
      "learning_rate": 0.00011807387862796835,
      "loss": 0.0523,
      "step": 1864
    },
    {
      "epoch": 1.2302110817941951,
      "grad_norm": 0.1505676507949829,
      "learning_rate": 0.00011802990325417766,
      "loss": 0.0147,
      "step": 1865
    },
    {
      "epoch": 1.2308707124010554,
      "grad_norm": 0.26761484146118164,
      "learning_rate": 0.00011798592788038698,
      "loss": 0.0344,
      "step": 1866
    },
    {
      "epoch": 1.2315303430079156,
      "grad_norm": 0.2432822734117508,
      "learning_rate": 0.00011794195250659632,
      "loss": 0.0252,
      "step": 1867
    },
    {
      "epoch": 1.2321899736147757,
      "grad_norm": 0.13525690138339996,
      "learning_rate": 0.00011789797713280563,
      "loss": 0.0077,
      "step": 1868
    },
    {
      "epoch": 1.232849604221636,
      "grad_norm": 0.14639438688755035,
      "learning_rate": 0.00011785400175901496,
      "loss": 0.0292,
      "step": 1869
    },
    {
      "epoch": 1.233509234828496,
      "grad_norm": 0.11590005457401276,
      "learning_rate": 0.00011781002638522427,
      "loss": 0.0118,
      "step": 1870
    },
    {
      "epoch": 1.2341688654353562,
      "grad_norm": 0.1645921915769577,
      "learning_rate": 0.00011776605101143361,
      "loss": 0.0243,
      "step": 1871
    },
    {
      "epoch": 1.2348284960422165,
      "grad_norm": 0.16731804609298706,
      "learning_rate": 0.00011772207563764293,
      "loss": 0.0198,
      "step": 1872
    },
    {
      "epoch": 1.2354881266490765,
      "grad_norm": 0.07751071453094482,
      "learning_rate": 0.00011767810026385224,
      "loss": 0.0043,
      "step": 1873
    },
    {
      "epoch": 1.2361477572559367,
      "grad_norm": 0.19808736443519592,
      "learning_rate": 0.00011763412489006158,
      "loss": 0.0441,
      "step": 1874
    },
    {
      "epoch": 1.2368073878627968,
      "grad_norm": 0.11866918951272964,
      "learning_rate": 0.0001175901495162709,
      "loss": 0.0168,
      "step": 1875
    },
    {
      "epoch": 1.237467018469657,
      "grad_norm": 0.14140015840530396,
      "learning_rate": 0.00011754617414248021,
      "loss": 0.0226,
      "step": 1876
    },
    {
      "epoch": 1.2381266490765173,
      "grad_norm": 0.12062643468379974,
      "learning_rate": 0.00011750219876868954,
      "loss": 0.0185,
      "step": 1877
    },
    {
      "epoch": 1.2387862796833773,
      "grad_norm": 0.13635699450969696,
      "learning_rate": 0.00011745822339489887,
      "loss": 0.0123,
      "step": 1878
    },
    {
      "epoch": 1.2394459102902375,
      "grad_norm": 0.11275829374790192,
      "learning_rate": 0.00011741424802110819,
      "loss": 0.0141,
      "step": 1879
    },
    {
      "epoch": 1.2401055408970976,
      "grad_norm": 0.15119506418704987,
      "learning_rate": 0.0001173702726473175,
      "loss": 0.0273,
      "step": 1880
    },
    {
      "epoch": 1.2407651715039578,
      "grad_norm": 0.18669886887073517,
      "learning_rate": 0.00011732629727352682,
      "loss": 0.0282,
      "step": 1881
    },
    {
      "epoch": 1.2414248021108178,
      "grad_norm": 0.26534584164619446,
      "learning_rate": 0.00011728232189973616,
      "loss": 0.0384,
      "step": 1882
    },
    {
      "epoch": 1.242084432717678,
      "grad_norm": 0.10639768838882446,
      "learning_rate": 0.00011723834652594548,
      "loss": 0.0141,
      "step": 1883
    },
    {
      "epoch": 1.2427440633245384,
      "grad_norm": 0.1418975293636322,
      "learning_rate": 0.00011719437115215479,
      "loss": 0.0197,
      "step": 1884
    },
    {
      "epoch": 1.2434036939313984,
      "grad_norm": 0.22474561631679535,
      "learning_rate": 0.00011715039577836413,
      "loss": 0.0194,
      "step": 1885
    },
    {
      "epoch": 1.2440633245382586,
      "grad_norm": 0.18900813162326813,
      "learning_rate": 0.00011710642040457344,
      "loss": 0.0277,
      "step": 1886
    },
    {
      "epoch": 1.2447229551451187,
      "grad_norm": 0.15213660895824432,
      "learning_rate": 0.00011706244503078276,
      "loss": 0.0129,
      "step": 1887
    },
    {
      "epoch": 1.245382585751979,
      "grad_norm": 0.27598968148231506,
      "learning_rate": 0.00011701846965699209,
      "loss": 0.0391,
      "step": 1888
    },
    {
      "epoch": 1.246042216358839,
      "grad_norm": 0.1986742466688156,
      "learning_rate": 0.00011697449428320141,
      "loss": 0.0312,
      "step": 1889
    },
    {
      "epoch": 1.2467018469656992,
      "grad_norm": 0.1834077686071396,
      "learning_rate": 0.00011693051890941074,
      "loss": 0.0137,
      "step": 1890
    },
    {
      "epoch": 1.2473614775725594,
      "grad_norm": 0.18208055198192596,
      "learning_rate": 0.00011688654353562006,
      "loss": 0.0226,
      "step": 1891
    },
    {
      "epoch": 1.2480211081794195,
      "grad_norm": 3.3058416843414307,
      "learning_rate": 0.0001168425681618294,
      "loss": 0.0294,
      "step": 1892
    },
    {
      "epoch": 1.2486807387862797,
      "grad_norm": 0.2061961591243744,
      "learning_rate": 0.00011679859278803871,
      "loss": 0.0106,
      "step": 1893
    },
    {
      "epoch": 1.2493403693931397,
      "grad_norm": 0.15786609053611755,
      "learning_rate": 0.00011675461741424802,
      "loss": 0.0087,
      "step": 1894
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.1463945508003235,
      "learning_rate": 0.00011671064204045734,
      "loss": 0.01,
      "step": 1895
    },
    {
      "epoch": 1.2506596306068603,
      "grad_norm": 0.07690180838108063,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.0036,
      "step": 1896
    },
    {
      "epoch": 1.2513192612137203,
      "grad_norm": 0.21884946525096893,
      "learning_rate": 0.00011662269129287599,
      "loss": 0.0233,
      "step": 1897
    },
    {
      "epoch": 1.2519788918205805,
      "grad_norm": 0.09434393793344498,
      "learning_rate": 0.00011657871591908531,
      "loss": 0.0076,
      "step": 1898
    },
    {
      "epoch": 1.2526385224274406,
      "grad_norm": 0.08665984123945236,
      "learning_rate": 0.00011653474054529465,
      "loss": 0.0055,
      "step": 1899
    },
    {
      "epoch": 1.2532981530343008,
      "grad_norm": 0.025098783895373344,
      "learning_rate": 0.00011649076517150396,
      "loss": 0.0009,
      "step": 1900
    },
    {
      "epoch": 1.253957783641161,
      "grad_norm": 0.2428634762763977,
      "learning_rate": 0.00011644678979771329,
      "loss": 0.0257,
      "step": 1901
    },
    {
      "epoch": 1.254617414248021,
      "grad_norm": 0.4962817132472992,
      "learning_rate": 0.0001164028144239226,
      "loss": 0.0814,
      "step": 1902
    },
    {
      "epoch": 1.2552770448548813,
      "grad_norm": 0.22302202880382538,
      "learning_rate": 0.00011635883905013194,
      "loss": 0.0319,
      "step": 1903
    },
    {
      "epoch": 1.2559366754617414,
      "grad_norm": 0.12983092665672302,
      "learning_rate": 0.00011631486367634126,
      "loss": 0.0109,
      "step": 1904
    },
    {
      "epoch": 1.2565963060686016,
      "grad_norm": 0.13978852331638336,
      "learning_rate": 0.00011627088830255057,
      "loss": 0.0194,
      "step": 1905
    },
    {
      "epoch": 1.2572559366754619,
      "grad_norm": 0.26403865218162537,
      "learning_rate": 0.00011622691292875989,
      "loss": 0.0405,
      "step": 1906
    },
    {
      "epoch": 1.257915567282322,
      "grad_norm": 0.37602418661117554,
      "learning_rate": 0.00011618293755496923,
      "loss": 0.0372,
      "step": 1907
    },
    {
      "epoch": 1.258575197889182,
      "grad_norm": 0.21373389661312103,
      "learning_rate": 0.00011613896218117854,
      "loss": 0.024,
      "step": 1908
    },
    {
      "epoch": 1.2592348284960422,
      "grad_norm": 0.2296474277973175,
      "learning_rate": 0.00011609498680738786,
      "loss": 0.0458,
      "step": 1909
    },
    {
      "epoch": 1.2598944591029024,
      "grad_norm": 0.14934368431568146,
      "learning_rate": 0.0001160510114335972,
      "loss": 0.0144,
      "step": 1910
    },
    {
      "epoch": 1.2605540897097625,
      "grad_norm": 0.33576855063438416,
      "learning_rate": 0.00011600703605980652,
      "loss": 0.052,
      "step": 1911
    },
    {
      "epoch": 1.2612137203166227,
      "grad_norm": 0.17293433845043182,
      "learning_rate": 0.00011596306068601584,
      "loss": 0.0139,
      "step": 1912
    },
    {
      "epoch": 1.2618733509234827,
      "grad_norm": 0.21787822246551514,
      "learning_rate": 0.00011591908531222515,
      "loss": 0.0397,
      "step": 1913
    },
    {
      "epoch": 1.262532981530343,
      "grad_norm": 0.3148297965526581,
      "learning_rate": 0.00011587510993843449,
      "loss": 0.029,
      "step": 1914
    },
    {
      "epoch": 1.2631926121372032,
      "grad_norm": 0.30773887038230896,
      "learning_rate": 0.00011583113456464381,
      "loss": 0.0247,
      "step": 1915
    },
    {
      "epoch": 1.2638522427440633,
      "grad_norm": 0.27138885855674744,
      "learning_rate": 0.00011578715919085312,
      "loss": 0.0361,
      "step": 1916
    },
    {
      "epoch": 1.2645118733509235,
      "grad_norm": 0.15513254702091217,
      "learning_rate": 0.00011574318381706246,
      "loss": 0.0092,
      "step": 1917
    },
    {
      "epoch": 1.2651715039577835,
      "grad_norm": 0.12165360152721405,
      "learning_rate": 0.00011569920844327178,
      "loss": 0.0152,
      "step": 1918
    },
    {
      "epoch": 1.2658311345646438,
      "grad_norm": 0.10959172993898392,
      "learning_rate": 0.00011565523306948109,
      "loss": 0.0095,
      "step": 1919
    },
    {
      "epoch": 1.266490765171504,
      "grad_norm": 0.09215395152568817,
      "learning_rate": 0.00011561125769569042,
      "loss": 0.0101,
      "step": 1920
    },
    {
      "epoch": 1.267150395778364,
      "grad_norm": 0.10988961905241013,
      "learning_rate": 0.00011556728232189974,
      "loss": 0.0114,
      "step": 1921
    },
    {
      "epoch": 1.2678100263852243,
      "grad_norm": 0.33919641375541687,
      "learning_rate": 0.00011552330694810907,
      "loss": 0.0154,
      "step": 1922
    },
    {
      "epoch": 1.2684696569920844,
      "grad_norm": 0.04251240938901901,
      "learning_rate": 0.00011547933157431839,
      "loss": 0.0029,
      "step": 1923
    },
    {
      "epoch": 1.2691292875989446,
      "grad_norm": 0.14504437148571014,
      "learning_rate": 0.0001154353562005277,
      "loss": 0.0182,
      "step": 1924
    },
    {
      "epoch": 1.2697889182058049,
      "grad_norm": 0.04396429657936096,
      "learning_rate": 0.00011539138082673704,
      "loss": 0.0022,
      "step": 1925
    },
    {
      "epoch": 1.270448548812665,
      "grad_norm": 0.3875526189804077,
      "learning_rate": 0.00011534740545294636,
      "loss": 0.0858,
      "step": 1926
    },
    {
      "epoch": 1.2711081794195251,
      "grad_norm": 0.17655156552791595,
      "learning_rate": 0.00011530343007915567,
      "loss": 0.0196,
      "step": 1927
    },
    {
      "epoch": 1.2717678100263852,
      "grad_norm": 0.12464241683483124,
      "learning_rate": 0.00011525945470536501,
      "loss": 0.019,
      "step": 1928
    },
    {
      "epoch": 1.2724274406332454,
      "grad_norm": 0.18558895587921143,
      "learning_rate": 0.00011521547933157432,
      "loss": 0.0069,
      "step": 1929
    },
    {
      "epoch": 1.2730870712401057,
      "grad_norm": 0.2569718062877655,
      "learning_rate": 0.00011517150395778364,
      "loss": 0.0277,
      "step": 1930
    },
    {
      "epoch": 1.2737467018469657,
      "grad_norm": 0.3146857023239136,
      "learning_rate": 0.00011512752858399297,
      "loss": 0.0255,
      "step": 1931
    },
    {
      "epoch": 1.2744063324538257,
      "grad_norm": 0.2823721766471863,
      "learning_rate": 0.0001150835532102023,
      "loss": 0.0303,
      "step": 1932
    },
    {
      "epoch": 1.275065963060686,
      "grad_norm": 0.2600044906139374,
      "learning_rate": 0.00011503957783641162,
      "loss": 0.0395,
      "step": 1933
    },
    {
      "epoch": 1.2757255936675462,
      "grad_norm": 0.14102664589881897,
      "learning_rate": 0.00011499560246262093,
      "loss": 0.0149,
      "step": 1934
    },
    {
      "epoch": 1.2763852242744063,
      "grad_norm": 0.1888095736503601,
      "learning_rate": 0.00011495162708883028,
      "loss": 0.0217,
      "step": 1935
    },
    {
      "epoch": 1.2770448548812665,
      "grad_norm": 0.15800225734710693,
      "learning_rate": 0.00011490765171503959,
      "loss": 0.0128,
      "step": 1936
    },
    {
      "epoch": 1.2777044854881265,
      "grad_norm": 0.15124884247779846,
      "learning_rate": 0.0001148636763412489,
      "loss": 0.0185,
      "step": 1937
    },
    {
      "epoch": 1.2783641160949868,
      "grad_norm": 0.1605682373046875,
      "learning_rate": 0.00011481970096745822,
      "loss": 0.0105,
      "step": 1938
    },
    {
      "epoch": 1.279023746701847,
      "grad_norm": 0.23890458047389984,
      "learning_rate": 0.00011477572559366756,
      "loss": 0.0315,
      "step": 1939
    },
    {
      "epoch": 1.279683377308707,
      "grad_norm": 0.30415084958076477,
      "learning_rate": 0.00011473175021987687,
      "loss": 0.0357,
      "step": 1940
    },
    {
      "epoch": 1.2803430079155673,
      "grad_norm": 0.23647934198379517,
      "learning_rate": 0.00011468777484608619,
      "loss": 0.0297,
      "step": 1941
    },
    {
      "epoch": 1.2810026385224274,
      "grad_norm": 0.1303078979253769,
      "learning_rate": 0.00011464379947229551,
      "loss": 0.0144,
      "step": 1942
    },
    {
      "epoch": 1.2816622691292876,
      "grad_norm": 0.239386647939682,
      "learning_rate": 0.00011459982409850486,
      "loss": 0.0232,
      "step": 1943
    },
    {
      "epoch": 1.2823218997361479,
      "grad_norm": 0.16825957596302032,
      "learning_rate": 0.00011455584872471417,
      "loss": 0.0191,
      "step": 1944
    },
    {
      "epoch": 1.2829815303430079,
      "grad_norm": 0.24394190311431885,
      "learning_rate": 0.00011451187335092348,
      "loss": 0.0243,
      "step": 1945
    },
    {
      "epoch": 1.2836411609498681,
      "grad_norm": 0.1627008467912674,
      "learning_rate": 0.00011446789797713282,
      "loss": 0.0273,
      "step": 1946
    },
    {
      "epoch": 1.2843007915567282,
      "grad_norm": 0.09787502139806747,
      "learning_rate": 0.00011442392260334214,
      "loss": 0.0099,
      "step": 1947
    },
    {
      "epoch": 1.2849604221635884,
      "grad_norm": 0.3716208040714264,
      "learning_rate": 0.00011437994722955145,
      "loss": 0.0452,
      "step": 1948
    },
    {
      "epoch": 1.2856200527704487,
      "grad_norm": 0.18262067437171936,
      "learning_rate": 0.00011433597185576077,
      "loss": 0.0146,
      "step": 1949
    },
    {
      "epoch": 1.2862796833773087,
      "grad_norm": 0.30962878465652466,
      "learning_rate": 0.00011429199648197011,
      "loss": 0.0415,
      "step": 1950
    },
    {
      "epoch": 1.286939313984169,
      "grad_norm": 0.15545566380023956,
      "learning_rate": 0.00011424802110817942,
      "loss": 0.0104,
      "step": 1951
    },
    {
      "epoch": 1.287598944591029,
      "grad_norm": 0.05233421549201012,
      "learning_rate": 0.00011420404573438875,
      "loss": 0.0025,
      "step": 1952
    },
    {
      "epoch": 1.2882585751978892,
      "grad_norm": 0.10265669971704483,
      "learning_rate": 0.00011416007036059808,
      "loss": 0.0147,
      "step": 1953
    },
    {
      "epoch": 1.2889182058047495,
      "grad_norm": 0.04460013657808304,
      "learning_rate": 0.0001141160949868074,
      "loss": 0.0034,
      "step": 1954
    },
    {
      "epoch": 1.2895778364116095,
      "grad_norm": 0.22170744836330414,
      "learning_rate": 0.00011407211961301672,
      "loss": 0.0383,
      "step": 1955
    },
    {
      "epoch": 1.2902374670184695,
      "grad_norm": 0.23546208441257477,
      "learning_rate": 0.00011402814423922603,
      "loss": 0.0324,
      "step": 1956
    },
    {
      "epoch": 1.2908970976253298,
      "grad_norm": 0.15753819048404694,
      "learning_rate": 0.00011398416886543537,
      "loss": 0.0118,
      "step": 1957
    },
    {
      "epoch": 1.29155672823219,
      "grad_norm": 0.1566392332315445,
      "learning_rate": 0.00011394019349164469,
      "loss": 0.0174,
      "step": 1958
    },
    {
      "epoch": 1.29221635883905,
      "grad_norm": 0.07094810158014297,
      "learning_rate": 0.000113896218117854,
      "loss": 0.0076,
      "step": 1959
    },
    {
      "epoch": 1.2928759894459103,
      "grad_norm": 0.19518175721168518,
      "learning_rate": 0.00011385224274406331,
      "loss": 0.0205,
      "step": 1960
    },
    {
      "epoch": 1.2935356200527703,
      "grad_norm": 0.18459860980510712,
      "learning_rate": 0.00011380826737027266,
      "loss": 0.03,
      "step": 1961
    },
    {
      "epoch": 1.2941952506596306,
      "grad_norm": 0.13329124450683594,
      "learning_rate": 0.00011376429199648197,
      "loss": 0.0075,
      "step": 1962
    },
    {
      "epoch": 1.2948548812664908,
      "grad_norm": 0.11106763780117035,
      "learning_rate": 0.0001137203166226913,
      "loss": 0.0091,
      "step": 1963
    },
    {
      "epoch": 1.2955145118733509,
      "grad_norm": 0.0958850234746933,
      "learning_rate": 0.00011367634124890062,
      "loss": 0.0083,
      "step": 1964
    },
    {
      "epoch": 1.2961741424802111,
      "grad_norm": 0.17684413492679596,
      "learning_rate": 0.00011363236587510995,
      "loss": 0.0338,
      "step": 1965
    },
    {
      "epoch": 1.2968337730870712,
      "grad_norm": 0.11483846604824066,
      "learning_rate": 0.00011358839050131927,
      "loss": 0.0075,
      "step": 1966
    },
    {
      "epoch": 1.2974934036939314,
      "grad_norm": 0.11513563245534897,
      "learning_rate": 0.00011354441512752858,
      "loss": 0.013,
      "step": 1967
    },
    {
      "epoch": 1.2981530343007917,
      "grad_norm": 0.11149391531944275,
      "learning_rate": 0.00011350043975373792,
      "loss": 0.0105,
      "step": 1968
    },
    {
      "epoch": 1.2988126649076517,
      "grad_norm": 0.2496776580810547,
      "learning_rate": 0.00011345646437994724,
      "loss": 0.0434,
      "step": 1969
    },
    {
      "epoch": 1.299472295514512,
      "grad_norm": 0.11535017937421799,
      "learning_rate": 0.00011341248900615655,
      "loss": 0.0108,
      "step": 1970
    },
    {
      "epoch": 1.300131926121372,
      "grad_norm": 0.4125826358795166,
      "learning_rate": 0.00011336851363236589,
      "loss": 0.0329,
      "step": 1971
    },
    {
      "epoch": 1.3007915567282322,
      "grad_norm": 0.13006268441677094,
      "learning_rate": 0.0001133245382585752,
      "loss": 0.0137,
      "step": 1972
    },
    {
      "epoch": 1.3014511873350925,
      "grad_norm": 0.06896587461233139,
      "learning_rate": 0.00011328056288478452,
      "loss": 0.0038,
      "step": 1973
    },
    {
      "epoch": 1.3021108179419525,
      "grad_norm": 0.07474656403064728,
      "learning_rate": 0.00011323658751099385,
      "loss": 0.0072,
      "step": 1974
    },
    {
      "epoch": 1.3027704485488127,
      "grad_norm": 0.08978227525949478,
      "learning_rate": 0.00011319261213720317,
      "loss": 0.0086,
      "step": 1975
    },
    {
      "epoch": 1.3034300791556728,
      "grad_norm": 0.10530360043048859,
      "learning_rate": 0.0001131486367634125,
      "loss": 0.0064,
      "step": 1976
    },
    {
      "epoch": 1.304089709762533,
      "grad_norm": 0.12116270512342453,
      "learning_rate": 0.00011310466138962181,
      "loss": 0.0162,
      "step": 1977
    },
    {
      "epoch": 1.3047493403693933,
      "grad_norm": 0.2550329864025116,
      "learning_rate": 0.00011306068601583113,
      "loss": 0.0337,
      "step": 1978
    },
    {
      "epoch": 1.3054089709762533,
      "grad_norm": 0.2408512681722641,
      "learning_rate": 0.00011301671064204047,
      "loss": 0.036,
      "step": 1979
    },
    {
      "epoch": 1.3060686015831133,
      "grad_norm": 0.32851481437683105,
      "learning_rate": 0.00011297273526824978,
      "loss": 0.0365,
      "step": 1980
    },
    {
      "epoch": 1.3067282321899736,
      "grad_norm": 0.2302454560995102,
      "learning_rate": 0.0001129287598944591,
      "loss": 0.0237,
      "step": 1981
    },
    {
      "epoch": 1.3073878627968338,
      "grad_norm": 0.2225346714258194,
      "learning_rate": 0.00011288478452066844,
      "loss": 0.0305,
      "step": 1982
    },
    {
      "epoch": 1.3080474934036939,
      "grad_norm": 0.16092438995838165,
      "learning_rate": 0.00011284080914687775,
      "loss": 0.014,
      "step": 1983
    },
    {
      "epoch": 1.3087071240105541,
      "grad_norm": 0.23549550771713257,
      "learning_rate": 0.00011279683377308708,
      "loss": 0.0166,
      "step": 1984
    },
    {
      "epoch": 1.3093667546174141,
      "grad_norm": 0.4307876229286194,
      "learning_rate": 0.0001127528583992964,
      "loss": 0.0295,
      "step": 1985
    },
    {
      "epoch": 1.3100263852242744,
      "grad_norm": 0.1557452231645584,
      "learning_rate": 0.00011270888302550574,
      "loss": 0.0137,
      "step": 1986
    },
    {
      "epoch": 1.3106860158311346,
      "grad_norm": 0.15145841240882874,
      "learning_rate": 0.00011266490765171505,
      "loss": 0.0186,
      "step": 1987
    },
    {
      "epoch": 1.3113456464379947,
      "grad_norm": 0.27343958616256714,
      "learning_rate": 0.00011262093227792436,
      "loss": 0.0459,
      "step": 1988
    },
    {
      "epoch": 1.312005277044855,
      "grad_norm": 0.2264215648174286,
      "learning_rate": 0.0001125769569041337,
      "loss": 0.0456,
      "step": 1989
    },
    {
      "epoch": 1.312664907651715,
      "grad_norm": 0.1676846444606781,
      "learning_rate": 0.00011253298153034302,
      "loss": 0.0083,
      "step": 1990
    },
    {
      "epoch": 1.3133245382585752,
      "grad_norm": 0.09136722981929779,
      "learning_rate": 0.00011248900615655233,
      "loss": 0.0068,
      "step": 1991
    },
    {
      "epoch": 1.3139841688654355,
      "grad_norm": 0.04159439727663994,
      "learning_rate": 0.00011244503078276165,
      "loss": 0.0021,
      "step": 1992
    },
    {
      "epoch": 1.3146437994722955,
      "grad_norm": 0.037187930196523666,
      "learning_rate": 0.00011240105540897099,
      "loss": 0.0017,
      "step": 1993
    },
    {
      "epoch": 1.3153034300791557,
      "grad_norm": 0.14092302322387695,
      "learning_rate": 0.0001123570800351803,
      "loss": 0.0103,
      "step": 1994
    },
    {
      "epoch": 1.3159630606860158,
      "grad_norm": 0.10241100192070007,
      "learning_rate": 0.00011231310466138963,
      "loss": 0.011,
      "step": 1995
    },
    {
      "epoch": 1.316622691292876,
      "grad_norm": 0.0881560668349266,
      "learning_rate": 0.00011226912928759894,
      "loss": 0.0076,
      "step": 1996
    },
    {
      "epoch": 1.3172823218997363,
      "grad_norm": 0.11245658248662949,
      "learning_rate": 0.00011222515391380828,
      "loss": 0.0124,
      "step": 1997
    },
    {
      "epoch": 1.3179419525065963,
      "grad_norm": 0.19672340154647827,
      "learning_rate": 0.0001121811785400176,
      "loss": 0.0433,
      "step": 1998
    },
    {
      "epoch": 1.3186015831134565,
      "grad_norm": 0.04643542692065239,
      "learning_rate": 0.00011213720316622691,
      "loss": 0.003,
      "step": 1999
    },
    {
      "epoch": 1.3192612137203166,
      "grad_norm": 0.3735794723033905,
      "learning_rate": 0.00011209322779243625,
      "loss": 0.062,
      "step": 2000
    },
    {
      "epoch": 1.3199208443271768,
      "grad_norm": 0.1618492305278778,
      "learning_rate": 0.00011204925241864557,
      "loss": 0.0133,
      "step": 2001
    },
    {
      "epoch": 1.320580474934037,
      "grad_norm": 0.23598258197307587,
      "learning_rate": 0.00011200527704485488,
      "loss": 0.0241,
      "step": 2002
    },
    {
      "epoch": 1.321240105540897,
      "grad_norm": 0.1875707358121872,
      "learning_rate": 0.0001119613016710642,
      "loss": 0.0215,
      "step": 2003
    },
    {
      "epoch": 1.3218997361477571,
      "grad_norm": 0.05551941692829132,
      "learning_rate": 0.00011191732629727354,
      "loss": 0.0053,
      "step": 2004
    },
    {
      "epoch": 1.3225593667546174,
      "grad_norm": 0.31713688373565674,
      "learning_rate": 0.00011187335092348285,
      "loss": 0.0365,
      "step": 2005
    },
    {
      "epoch": 1.3232189973614776,
      "grad_norm": 0.08236110210418701,
      "learning_rate": 0.00011182937554969218,
      "loss": 0.0052,
      "step": 2006
    },
    {
      "epoch": 1.3238786279683377,
      "grad_norm": 0.2086339145898819,
      "learning_rate": 0.0001117854001759015,
      "loss": 0.0285,
      "step": 2007
    },
    {
      "epoch": 1.324538258575198,
      "grad_norm": 0.11173447966575623,
      "learning_rate": 0.00011174142480211083,
      "loss": 0.0069,
      "step": 2008
    },
    {
      "epoch": 1.325197889182058,
      "grad_norm": 0.33653339743614197,
      "learning_rate": 0.00011169744942832015,
      "loss": 0.0415,
      "step": 2009
    },
    {
      "epoch": 1.3258575197889182,
      "grad_norm": 0.2004612386226654,
      "learning_rate": 0.00011165347405452946,
      "loss": 0.0151,
      "step": 2010
    },
    {
      "epoch": 1.3265171503957784,
      "grad_norm": 0.40993091464042664,
      "learning_rate": 0.0001116094986807388,
      "loss": 0.0711,
      "step": 2011
    },
    {
      "epoch": 1.3271767810026385,
      "grad_norm": 0.12625949084758759,
      "learning_rate": 0.00011156552330694812,
      "loss": 0.0064,
      "step": 2012
    },
    {
      "epoch": 1.3278364116094987,
      "grad_norm": 0.12359727174043655,
      "learning_rate": 0.00011152154793315743,
      "loss": 0.0111,
      "step": 2013
    },
    {
      "epoch": 1.3284960422163588,
      "grad_norm": 0.28057047724723816,
      "learning_rate": 0.00011147757255936674,
      "loss": 0.0413,
      "step": 2014
    },
    {
      "epoch": 1.329155672823219,
      "grad_norm": 0.18996232748031616,
      "learning_rate": 0.00011143359718557608,
      "loss": 0.0106,
      "step": 2015
    },
    {
      "epoch": 1.3298153034300793,
      "grad_norm": 0.18943065404891968,
      "learning_rate": 0.00011138962181178541,
      "loss": 0.0161,
      "step": 2016
    },
    {
      "epoch": 1.3304749340369393,
      "grad_norm": 0.1880556046962738,
      "learning_rate": 0.00011134564643799473,
      "loss": 0.0405,
      "step": 2017
    },
    {
      "epoch": 1.3311345646437995,
      "grad_norm": 0.1601225584745407,
      "learning_rate": 0.00011130167106420407,
      "loss": 0.0261,
      "step": 2018
    },
    {
      "epoch": 1.3317941952506596,
      "grad_norm": 0.16800956428050995,
      "learning_rate": 0.00011125769569041338,
      "loss": 0.0146,
      "step": 2019
    },
    {
      "epoch": 1.3324538258575198,
      "grad_norm": 0.21391135454177856,
      "learning_rate": 0.0001112137203166227,
      "loss": 0.0065,
      "step": 2020
    },
    {
      "epoch": 1.33311345646438,
      "grad_norm": 0.22288623452186584,
      "learning_rate": 0.00011116974494283201,
      "loss": 0.028,
      "step": 2021
    },
    {
      "epoch": 1.33377308707124,
      "grad_norm": 0.18179483711719513,
      "learning_rate": 0.00011112576956904135,
      "loss": 0.0231,
      "step": 2022
    },
    {
      "epoch": 1.3344327176781003,
      "grad_norm": 0.19965453445911407,
      "learning_rate": 0.00011108179419525066,
      "loss": 0.0207,
      "step": 2023
    },
    {
      "epoch": 1.3350923482849604,
      "grad_norm": 0.07168827950954437,
      "learning_rate": 0.00011103781882145998,
      "loss": 0.0044,
      "step": 2024
    },
    {
      "epoch": 1.3357519788918206,
      "grad_norm": 0.055556122213602066,
      "learning_rate": 0.00011099384344766932,
      "loss": 0.003,
      "step": 2025
    },
    {
      "epoch": 1.3364116094986809,
      "grad_norm": 0.22211183607578278,
      "learning_rate": 0.00011094986807387863,
      "loss": 0.0191,
      "step": 2026
    },
    {
      "epoch": 1.337071240105541,
      "grad_norm": 0.1127411350607872,
      "learning_rate": 0.00011090589270008796,
      "loss": 0.0063,
      "step": 2027
    },
    {
      "epoch": 1.337730870712401,
      "grad_norm": 0.04884229972958565,
      "learning_rate": 0.00011086191732629727,
      "loss": 0.0032,
      "step": 2028
    },
    {
      "epoch": 1.3383905013192612,
      "grad_norm": 0.1246853843331337,
      "learning_rate": 0.00011081794195250662,
      "loss": 0.0115,
      "step": 2029
    },
    {
      "epoch": 1.3390501319261214,
      "grad_norm": 0.17091797292232513,
      "learning_rate": 0.00011077396657871593,
      "loss": 0.0198,
      "step": 2030
    },
    {
      "epoch": 1.3397097625329815,
      "grad_norm": 0.2160463184118271,
      "learning_rate": 0.00011072999120492524,
      "loss": 0.017,
      "step": 2031
    },
    {
      "epoch": 1.3403693931398417,
      "grad_norm": 0.043360404670238495,
      "learning_rate": 0.00011068601583113456,
      "loss": 0.0021,
      "step": 2032
    },
    {
      "epoch": 1.3410290237467017,
      "grad_norm": 0.1543617844581604,
      "learning_rate": 0.0001106420404573439,
      "loss": 0.0125,
      "step": 2033
    },
    {
      "epoch": 1.341688654353562,
      "grad_norm": 0.29571810364723206,
      "learning_rate": 0.00011059806508355321,
      "loss": 0.0376,
      "step": 2034
    },
    {
      "epoch": 1.3423482849604222,
      "grad_norm": 0.30434486269950867,
      "learning_rate": 0.00011055408970976253,
      "loss": 0.0296,
      "step": 2035
    },
    {
      "epoch": 1.3430079155672823,
      "grad_norm": 0.1258779615163803,
      "learning_rate": 0.00011051011433597187,
      "loss": 0.0124,
      "step": 2036
    },
    {
      "epoch": 1.3436675461741425,
      "grad_norm": 0.20987454056739807,
      "learning_rate": 0.00011046613896218118,
      "loss": 0.0428,
      "step": 2037
    },
    {
      "epoch": 1.3443271767810026,
      "grad_norm": 0.23277239501476288,
      "learning_rate": 0.00011042216358839051,
      "loss": 0.0379,
      "step": 2038
    },
    {
      "epoch": 1.3449868073878628,
      "grad_norm": 0.28502318263053894,
      "learning_rate": 0.00011037818821459982,
      "loss": 0.0576,
      "step": 2039
    },
    {
      "epoch": 1.345646437994723,
      "grad_norm": 0.1505460888147354,
      "learning_rate": 0.00011033421284080916,
      "loss": 0.0151,
      "step": 2040
    },
    {
      "epoch": 1.346306068601583,
      "grad_norm": 0.1587887853384018,
      "learning_rate": 0.00011029023746701848,
      "loss": 0.0124,
      "step": 2041
    },
    {
      "epoch": 1.3469656992084433,
      "grad_norm": 0.17380861937999725,
      "learning_rate": 0.00011024626209322779,
      "loss": 0.0109,
      "step": 2042
    },
    {
      "epoch": 1.3476253298153034,
      "grad_norm": 0.13027238845825195,
      "learning_rate": 0.00011020228671943713,
      "loss": 0.014,
      "step": 2043
    },
    {
      "epoch": 1.3482849604221636,
      "grad_norm": 0.1537400186061859,
      "learning_rate": 0.00011015831134564645,
      "loss": 0.0147,
      "step": 2044
    },
    {
      "epoch": 1.3489445910290239,
      "grad_norm": 0.1382700502872467,
      "learning_rate": 0.00011011433597185576,
      "loss": 0.0175,
      "step": 2045
    },
    {
      "epoch": 1.349604221635884,
      "grad_norm": 0.1891116052865982,
      "learning_rate": 0.00011007036059806507,
      "loss": 0.0214,
      "step": 2046
    },
    {
      "epoch": 1.3502638522427441,
      "grad_norm": 0.21408261358737946,
      "learning_rate": 0.00011002638522427442,
      "loss": 0.0161,
      "step": 2047
    },
    {
      "epoch": 1.3509234828496042,
      "grad_norm": 0.1519257128238678,
      "learning_rate": 0.00010998240985048374,
      "loss": 0.0152,
      "step": 2048
    },
    {
      "epoch": 1.3515831134564644,
      "grad_norm": 0.37356311082839966,
      "learning_rate": 0.00010993843447669306,
      "loss": 0.019,
      "step": 2049
    },
    {
      "epoch": 1.3522427440633247,
      "grad_norm": 0.21650616824626923,
      "learning_rate": 0.00010989445910290237,
      "loss": 0.0379,
      "step": 2050
    },
    {
      "epoch": 1.3529023746701847,
      "grad_norm": 0.20201200246810913,
      "learning_rate": 0.00010985048372911171,
      "loss": 0.0276,
      "step": 2051
    },
    {
      "epoch": 1.3535620052770447,
      "grad_norm": 0.14682462811470032,
      "learning_rate": 0.00010980650835532103,
      "loss": 0.0156,
      "step": 2052
    },
    {
      "epoch": 1.354221635883905,
      "grad_norm": 0.09125271439552307,
      "learning_rate": 0.00010976253298153034,
      "loss": 0.007,
      "step": 2053
    },
    {
      "epoch": 1.3548812664907652,
      "grad_norm": 0.16473856568336487,
      "learning_rate": 0.00010971855760773968,
      "loss": 0.0153,
      "step": 2054
    },
    {
      "epoch": 1.3555408970976253,
      "grad_norm": 0.04912237450480461,
      "learning_rate": 0.000109674582233949,
      "loss": 0.0023,
      "step": 2055
    },
    {
      "epoch": 1.3562005277044855,
      "grad_norm": 0.2077733874320984,
      "learning_rate": 0.00010963060686015831,
      "loss": 0.0356,
      "step": 2056
    },
    {
      "epoch": 1.3568601583113455,
      "grad_norm": 0.1009669378399849,
      "learning_rate": 0.00010958663148636764,
      "loss": 0.0065,
      "step": 2057
    },
    {
      "epoch": 1.3575197889182058,
      "grad_norm": 0.28295809030532837,
      "learning_rate": 0.00010954265611257696,
      "loss": 0.0352,
      "step": 2058
    },
    {
      "epoch": 1.358179419525066,
      "grad_norm": 0.034556806087493896,
      "learning_rate": 0.00010949868073878629,
      "loss": 0.002,
      "step": 2059
    },
    {
      "epoch": 1.358839050131926,
      "grad_norm": 0.03822408244013786,
      "learning_rate": 0.0001094547053649956,
      "loss": 0.0018,
      "step": 2060
    },
    {
      "epoch": 1.3594986807387863,
      "grad_norm": 0.11473006755113602,
      "learning_rate": 0.00010941072999120495,
      "loss": 0.0161,
      "step": 2061
    },
    {
      "epoch": 1.3601583113456464,
      "grad_norm": 0.16608504951000214,
      "learning_rate": 0.00010936675461741426,
      "loss": 0.0313,
      "step": 2062
    },
    {
      "epoch": 1.3608179419525066,
      "grad_norm": 0.33714622259140015,
      "learning_rate": 0.00010932277924362357,
      "loss": 0.0325,
      "step": 2063
    },
    {
      "epoch": 1.3614775725593669,
      "grad_norm": 0.163321852684021,
      "learning_rate": 0.00010927880386983289,
      "loss": 0.0121,
      "step": 2064
    },
    {
      "epoch": 1.3621372031662269,
      "grad_norm": 0.22862555086612701,
      "learning_rate": 0.00010923482849604223,
      "loss": 0.0308,
      "step": 2065
    },
    {
      "epoch": 1.3627968337730871,
      "grad_norm": 0.19710718095302582,
      "learning_rate": 0.00010919085312225154,
      "loss": 0.0259,
      "step": 2066
    },
    {
      "epoch": 1.3634564643799472,
      "grad_norm": 0.14543788135051727,
      "learning_rate": 0.00010914687774846086,
      "loss": 0.0207,
      "step": 2067
    },
    {
      "epoch": 1.3641160949868074,
      "grad_norm": 0.13523074984550476,
      "learning_rate": 0.00010910290237467018,
      "loss": 0.011,
      "step": 2068
    },
    {
      "epoch": 1.3647757255936677,
      "grad_norm": 0.20199726521968842,
      "learning_rate": 0.00010905892700087951,
      "loss": 0.0185,
      "step": 2069
    },
    {
      "epoch": 1.3654353562005277,
      "grad_norm": 0.10041981935501099,
      "learning_rate": 0.00010901495162708884,
      "loss": 0.0087,
      "step": 2070
    },
    {
      "epoch": 1.366094986807388,
      "grad_norm": 0.25952261686325073,
      "learning_rate": 0.00010897097625329815,
      "loss": 0.041,
      "step": 2071
    },
    {
      "epoch": 1.366754617414248,
      "grad_norm": 0.18755652010440826,
      "learning_rate": 0.0001089270008795075,
      "loss": 0.0228,
      "step": 2072
    },
    {
      "epoch": 1.3674142480211082,
      "grad_norm": 0.1216297447681427,
      "learning_rate": 0.00010888302550571681,
      "loss": 0.0071,
      "step": 2073
    },
    {
      "epoch": 1.3680738786279685,
      "grad_norm": 0.14432358741760254,
      "learning_rate": 0.00010883905013192612,
      "loss": 0.0107,
      "step": 2074
    },
    {
      "epoch": 1.3687335092348285,
      "grad_norm": 0.22548213601112366,
      "learning_rate": 0.00010879507475813544,
      "loss": 0.033,
      "step": 2075
    },
    {
      "epoch": 1.3693931398416885,
      "grad_norm": 0.2364843636751175,
      "learning_rate": 0.00010875109938434478,
      "loss": 0.037,
      "step": 2076
    },
    {
      "epoch": 1.3700527704485488,
      "grad_norm": 0.20019082725048065,
      "learning_rate": 0.00010870712401055409,
      "loss": 0.0122,
      "step": 2077
    },
    {
      "epoch": 1.370712401055409,
      "grad_norm": 0.25785577297210693,
      "learning_rate": 0.0001086631486367634,
      "loss": 0.0497,
      "step": 2078
    },
    {
      "epoch": 1.371372031662269,
      "grad_norm": 0.13881421089172363,
      "learning_rate": 0.00010861917326297275,
      "loss": 0.0116,
      "step": 2079
    },
    {
      "epoch": 1.3720316622691293,
      "grad_norm": 0.12357491254806519,
      "learning_rate": 0.00010857519788918206,
      "loss": 0.0062,
      "step": 2080
    },
    {
      "epoch": 1.3726912928759893,
      "grad_norm": 0.14464573562145233,
      "learning_rate": 0.00010853122251539139,
      "loss": 0.0162,
      "step": 2081
    },
    {
      "epoch": 1.3733509234828496,
      "grad_norm": 0.13984833657741547,
      "learning_rate": 0.0001084872471416007,
      "loss": 0.0184,
      "step": 2082
    },
    {
      "epoch": 1.3740105540897098,
      "grad_norm": 0.12410672754049301,
      "learning_rate": 0.00010844327176781004,
      "loss": 0.01,
      "step": 2083
    },
    {
      "epoch": 1.3746701846965699,
      "grad_norm": 0.329876571893692,
      "learning_rate": 0.00010839929639401936,
      "loss": 0.0265,
      "step": 2084
    },
    {
      "epoch": 1.3753298153034301,
      "grad_norm": 0.30940526723861694,
      "learning_rate": 0.00010835532102022867,
      "loss": 0.0423,
      "step": 2085
    },
    {
      "epoch": 1.3759894459102902,
      "grad_norm": 0.17291200160980225,
      "learning_rate": 0.00010831134564643799,
      "loss": 0.0136,
      "step": 2086
    },
    {
      "epoch": 1.3766490765171504,
      "grad_norm": 0.1464800238609314,
      "learning_rate": 0.00010826737027264733,
      "loss": 0.0113,
      "step": 2087
    },
    {
      "epoch": 1.3773087071240107,
      "grad_norm": 0.27843090891838074,
      "learning_rate": 0.00010822339489885664,
      "loss": 0.0372,
      "step": 2088
    },
    {
      "epoch": 1.3779683377308707,
      "grad_norm": 0.20495925843715668,
      "learning_rate": 0.00010817941952506597,
      "loss": 0.0209,
      "step": 2089
    },
    {
      "epoch": 1.378627968337731,
      "grad_norm": 0.10885069519281387,
      "learning_rate": 0.0001081354441512753,
      "loss": 0.005,
      "step": 2090
    },
    {
      "epoch": 1.379287598944591,
      "grad_norm": 0.2372528612613678,
      "learning_rate": 0.00010809146877748462,
      "loss": 0.0199,
      "step": 2091
    },
    {
      "epoch": 1.3799472295514512,
      "grad_norm": 0.17041224241256714,
      "learning_rate": 0.00010804749340369394,
      "loss": 0.0218,
      "step": 2092
    },
    {
      "epoch": 1.3806068601583115,
      "grad_norm": 0.06121878698468208,
      "learning_rate": 0.00010800351802990325,
      "loss": 0.0027,
      "step": 2093
    },
    {
      "epoch": 1.3812664907651715,
      "grad_norm": 0.15076923370361328,
      "learning_rate": 0.00010795954265611259,
      "loss": 0.0152,
      "step": 2094
    },
    {
      "epoch": 1.3819261213720315,
      "grad_norm": 0.14457343518733978,
      "learning_rate": 0.0001079155672823219,
      "loss": 0.0151,
      "step": 2095
    },
    {
      "epoch": 1.3825857519788918,
      "grad_norm": 0.1449626386165619,
      "learning_rate": 0.00010787159190853122,
      "loss": 0.0131,
      "step": 2096
    },
    {
      "epoch": 1.383245382585752,
      "grad_norm": 0.20550015568733215,
      "learning_rate": 0.00010782761653474056,
      "loss": 0.0129,
      "step": 2097
    },
    {
      "epoch": 1.383905013192612,
      "grad_norm": 0.40897420048713684,
      "learning_rate": 0.00010778364116094987,
      "loss": 0.0258,
      "step": 2098
    },
    {
      "epoch": 1.3845646437994723,
      "grad_norm": 0.21025756001472473,
      "learning_rate": 0.00010773966578715919,
      "loss": 0.0241,
      "step": 2099
    },
    {
      "epoch": 1.3852242744063323,
      "grad_norm": 0.11015691608190536,
      "learning_rate": 0.00010769569041336852,
      "loss": 0.0047,
      "step": 2100
    },
    {
      "epoch": 1.3858839050131926,
      "grad_norm": 0.17006464302539825,
      "learning_rate": 0.00010765171503957784,
      "loss": 0.0231,
      "step": 2101
    },
    {
      "epoch": 1.3865435356200528,
      "grad_norm": 0.17778415977954865,
      "learning_rate": 0.00010760773966578717,
      "loss": 0.0152,
      "step": 2102
    },
    {
      "epoch": 1.3872031662269129,
      "grad_norm": 0.3252599239349365,
      "learning_rate": 0.00010756376429199649,
      "loss": 0.0148,
      "step": 2103
    },
    {
      "epoch": 1.3878627968337731,
      "grad_norm": 0.2698167860507965,
      "learning_rate": 0.0001075197889182058,
      "loss": 0.0214,
      "step": 2104
    },
    {
      "epoch": 1.3885224274406331,
      "grad_norm": 0.15259213745594025,
      "learning_rate": 0.00010747581354441514,
      "loss": 0.0122,
      "step": 2105
    },
    {
      "epoch": 1.3891820580474934,
      "grad_norm": 0.11614292114973068,
      "learning_rate": 0.00010743183817062445,
      "loss": 0.0064,
      "step": 2106
    },
    {
      "epoch": 1.3898416886543536,
      "grad_norm": 0.2082459181547165,
      "learning_rate": 0.00010738786279683377,
      "loss": 0.014,
      "step": 2107
    },
    {
      "epoch": 1.3905013192612137,
      "grad_norm": 0.49648746848106384,
      "learning_rate": 0.00010734388742304311,
      "loss": 0.0469,
      "step": 2108
    },
    {
      "epoch": 1.391160949868074,
      "grad_norm": 0.13951703906059265,
      "learning_rate": 0.00010729991204925242,
      "loss": 0.0059,
      "step": 2109
    },
    {
      "epoch": 1.391820580474934,
      "grad_norm": 0.14098136126995087,
      "learning_rate": 0.00010725593667546174,
      "loss": 0.0042,
      "step": 2110
    },
    {
      "epoch": 1.3924802110817942,
      "grad_norm": 0.4288976788520813,
      "learning_rate": 0.00010721196130167106,
      "loss": 0.0236,
      "step": 2111
    },
    {
      "epoch": 1.3931398416886545,
      "grad_norm": 0.38446277379989624,
      "learning_rate": 0.00010716798592788039,
      "loss": 0.0464,
      "step": 2112
    },
    {
      "epoch": 1.3937994722955145,
      "grad_norm": 0.06306403130292892,
      "learning_rate": 0.00010712401055408972,
      "loss": 0.005,
      "step": 2113
    },
    {
      "epoch": 1.3944591029023747,
      "grad_norm": 0.09106796979904175,
      "learning_rate": 0.00010708003518029903,
      "loss": 0.0027,
      "step": 2114
    },
    {
      "epoch": 1.3951187335092348,
      "grad_norm": 0.23336251080036163,
      "learning_rate": 0.00010703605980650837,
      "loss": 0.0148,
      "step": 2115
    },
    {
      "epoch": 1.395778364116095,
      "grad_norm": 0.2808469235897064,
      "learning_rate": 0.00010699208443271769,
      "loss": 0.0271,
      "step": 2116
    },
    {
      "epoch": 1.3964379947229553,
      "grad_norm": 0.2818354666233063,
      "learning_rate": 0.000106948109058927,
      "loss": 0.0448,
      "step": 2117
    },
    {
      "epoch": 1.3970976253298153,
      "grad_norm": 0.20222820341587067,
      "learning_rate": 0.00010690413368513632,
      "loss": 0.0085,
      "step": 2118
    },
    {
      "epoch": 1.3977572559366753,
      "grad_norm": 0.4545062184333801,
      "learning_rate": 0.00010686015831134566,
      "loss": 0.0264,
      "step": 2119
    },
    {
      "epoch": 1.3984168865435356,
      "grad_norm": 0.5839635133743286,
      "learning_rate": 0.00010681618293755497,
      "loss": 0.0376,
      "step": 2120
    },
    {
      "epoch": 1.3990765171503958,
      "grad_norm": 0.2489938735961914,
      "learning_rate": 0.0001067722075637643,
      "loss": 0.0104,
      "step": 2121
    },
    {
      "epoch": 1.3997361477572559,
      "grad_norm": 0.09330667555332184,
      "learning_rate": 0.00010672823218997361,
      "loss": 0.0063,
      "step": 2122
    },
    {
      "epoch": 1.400395778364116,
      "grad_norm": 0.2458137422800064,
      "learning_rate": 0.00010668425681618295,
      "loss": 0.0219,
      "step": 2123
    },
    {
      "epoch": 1.4010554089709761,
      "grad_norm": 0.13626421988010406,
      "learning_rate": 0.00010664028144239227,
      "loss": 0.0051,
      "step": 2124
    },
    {
      "epoch": 1.4017150395778364,
      "grad_norm": 0.06122765317559242,
      "learning_rate": 0.00010659630606860158,
      "loss": 0.0022,
      "step": 2125
    },
    {
      "epoch": 1.4023746701846966,
      "grad_norm": 0.4085473418235779,
      "learning_rate": 0.00010655233069481092,
      "loss": 0.0657,
      "step": 2126
    },
    {
      "epoch": 1.4030343007915567,
      "grad_norm": 0.27815839648246765,
      "learning_rate": 0.00010650835532102024,
      "loss": 0.0267,
      "step": 2127
    },
    {
      "epoch": 1.403693931398417,
      "grad_norm": 0.2180618941783905,
      "learning_rate": 0.00010646437994722955,
      "loss": 0.0199,
      "step": 2128
    },
    {
      "epoch": 1.404353562005277,
      "grad_norm": 0.12721768021583557,
      "learning_rate": 0.00010642040457343887,
      "loss": 0.0122,
      "step": 2129
    },
    {
      "epoch": 1.4050131926121372,
      "grad_norm": 0.2742673456668854,
      "learning_rate": 0.0001063764291996482,
      "loss": 0.0317,
      "step": 2130
    },
    {
      "epoch": 1.4056728232189974,
      "grad_norm": 0.3667301833629608,
      "learning_rate": 0.00010633245382585752,
      "loss": 0.0553,
      "step": 2131
    },
    {
      "epoch": 1.4063324538258575,
      "grad_norm": 0.26082420349121094,
      "learning_rate": 0.00010628847845206685,
      "loss": 0.0461,
      "step": 2132
    },
    {
      "epoch": 1.4069920844327177,
      "grad_norm": 0.188258558511734,
      "learning_rate": 0.00010624450307827618,
      "loss": 0.0248,
      "step": 2133
    },
    {
      "epoch": 1.4076517150395778,
      "grad_norm": 0.1205693930387497,
      "learning_rate": 0.0001062005277044855,
      "loss": 0.0098,
      "step": 2134
    },
    {
      "epoch": 1.408311345646438,
      "grad_norm": 0.47114041447639465,
      "learning_rate": 0.00010615655233069482,
      "loss": 0.0203,
      "step": 2135
    },
    {
      "epoch": 1.4089709762532983,
      "grad_norm": 0.18047748506069183,
      "learning_rate": 0.00010611257695690413,
      "loss": 0.0178,
      "step": 2136
    },
    {
      "epoch": 1.4096306068601583,
      "grad_norm": 0.18114428222179413,
      "learning_rate": 0.00010606860158311347,
      "loss": 0.022,
      "step": 2137
    },
    {
      "epoch": 1.4102902374670185,
      "grad_norm": 0.18266774713993073,
      "learning_rate": 0.00010602462620932279,
      "loss": 0.0112,
      "step": 2138
    },
    {
      "epoch": 1.4109498680738786,
      "grad_norm": 0.23667161166667938,
      "learning_rate": 0.0001059806508355321,
      "loss": 0.0352,
      "step": 2139
    },
    {
      "epoch": 1.4116094986807388,
      "grad_norm": 0.230092853307724,
      "learning_rate": 0.00010593667546174141,
      "loss": 0.0279,
      "step": 2140
    },
    {
      "epoch": 1.412269129287599,
      "grad_norm": 0.14537526667118073,
      "learning_rate": 0.00010589270008795075,
      "loss": 0.021,
      "step": 2141
    },
    {
      "epoch": 1.412928759894459,
      "grad_norm": 0.1802864521741867,
      "learning_rate": 0.00010584872471416007,
      "loss": 0.0326,
      "step": 2142
    },
    {
      "epoch": 1.4135883905013191,
      "grad_norm": 0.1767529398202896,
      "learning_rate": 0.0001058047493403694,
      "loss": 0.0237,
      "step": 2143
    },
    {
      "epoch": 1.4142480211081794,
      "grad_norm": 0.10941952466964722,
      "learning_rate": 0.00010576077396657872,
      "loss": 0.0082,
      "step": 2144
    },
    {
      "epoch": 1.4149076517150396,
      "grad_norm": 0.06962587684392929,
      "learning_rate": 0.00010571679859278805,
      "loss": 0.0068,
      "step": 2145
    },
    {
      "epoch": 1.4155672823218997,
      "grad_norm": 0.1568218320608139,
      "learning_rate": 0.00010567282321899737,
      "loss": 0.0282,
      "step": 2146
    },
    {
      "epoch": 1.41622691292876,
      "grad_norm": 0.18818894028663635,
      "learning_rate": 0.00010562884784520668,
      "loss": 0.0331,
      "step": 2147
    },
    {
      "epoch": 1.41688654353562,
      "grad_norm": 0.21701651811599731,
      "learning_rate": 0.00010558487247141602,
      "loss": 0.0324,
      "step": 2148
    },
    {
      "epoch": 1.4175461741424802,
      "grad_norm": 0.21480654180049896,
      "learning_rate": 0.00010554089709762533,
      "loss": 0.021,
      "step": 2149
    },
    {
      "epoch": 1.4182058047493404,
      "grad_norm": 0.11054698377847672,
      "learning_rate": 0.00010549692172383465,
      "loss": 0.0076,
      "step": 2150
    },
    {
      "epoch": 1.4188654353562005,
      "grad_norm": 0.07468391954898834,
      "learning_rate": 0.00010545294635004399,
      "loss": 0.0044,
      "step": 2151
    },
    {
      "epoch": 1.4195250659630607,
      "grad_norm": 0.22326204180717468,
      "learning_rate": 0.0001054089709762533,
      "loss": 0.0591,
      "step": 2152
    },
    {
      "epoch": 1.4201846965699207,
      "grad_norm": 0.23531530797481537,
      "learning_rate": 0.00010536499560246262,
      "loss": 0.0386,
      "step": 2153
    },
    {
      "epoch": 1.420844327176781,
      "grad_norm": 0.038621678948402405,
      "learning_rate": 0.00010532102022867194,
      "loss": 0.0021,
      "step": 2154
    },
    {
      "epoch": 1.4215039577836412,
      "grad_norm": 0.09695898741483688,
      "learning_rate": 0.00010527704485488129,
      "loss": 0.0188,
      "step": 2155
    },
    {
      "epoch": 1.4221635883905013,
      "grad_norm": 0.2903863191604614,
      "learning_rate": 0.0001052330694810906,
      "loss": 0.0324,
      "step": 2156
    },
    {
      "epoch": 1.4228232189973615,
      "grad_norm": 0.09362392872571945,
      "learning_rate": 0.00010518909410729991,
      "loss": 0.0105,
      "step": 2157
    },
    {
      "epoch": 1.4234828496042216,
      "grad_norm": 0.19588059186935425,
      "learning_rate": 0.00010514511873350923,
      "loss": 0.0146,
      "step": 2158
    },
    {
      "epoch": 1.4241424802110818,
      "grad_norm": 0.21150381863117218,
      "learning_rate": 0.00010510114335971857,
      "loss": 0.0267,
      "step": 2159
    },
    {
      "epoch": 1.424802110817942,
      "grad_norm": 0.15701423585414886,
      "learning_rate": 0.00010505716798592788,
      "loss": 0.0236,
      "step": 2160
    },
    {
      "epoch": 1.425461741424802,
      "grad_norm": 0.14876718819141388,
      "learning_rate": 0.0001050131926121372,
      "loss": 0.0199,
      "step": 2161
    },
    {
      "epoch": 1.4261213720316623,
      "grad_norm": 0.12013079971075058,
      "learning_rate": 0.00010496921723834654,
      "loss": 0.0239,
      "step": 2162
    },
    {
      "epoch": 1.4267810026385224,
      "grad_norm": 0.24047231674194336,
      "learning_rate": 0.00010492524186455585,
      "loss": 0.0391,
      "step": 2163
    },
    {
      "epoch": 1.4274406332453826,
      "grad_norm": 0.1586984246969223,
      "learning_rate": 0.00010488126649076518,
      "loss": 0.0311,
      "step": 2164
    },
    {
      "epoch": 1.4281002638522429,
      "grad_norm": 0.08716081082820892,
      "learning_rate": 0.00010483729111697449,
      "loss": 0.0084,
      "step": 2165
    },
    {
      "epoch": 1.428759894459103,
      "grad_norm": 0.30585557222366333,
      "learning_rate": 0.00010479331574318383,
      "loss": 0.0533,
      "step": 2166
    },
    {
      "epoch": 1.429419525065963,
      "grad_norm": 0.12337017059326172,
      "learning_rate": 0.00010474934036939315,
      "loss": 0.0076,
      "step": 2167
    },
    {
      "epoch": 1.4300791556728232,
      "grad_norm": 0.1256432980298996,
      "learning_rate": 0.00010470536499560246,
      "loss": 0.009,
      "step": 2168
    },
    {
      "epoch": 1.4307387862796834,
      "grad_norm": 0.14625094830989838,
      "learning_rate": 0.0001046613896218118,
      "loss": 0.0199,
      "step": 2169
    },
    {
      "epoch": 1.4313984168865435,
      "grad_norm": 0.14561986923217773,
      "learning_rate": 0.00010461741424802112,
      "loss": 0.0278,
      "step": 2170
    },
    {
      "epoch": 1.4320580474934037,
      "grad_norm": 0.13086314499378204,
      "learning_rate": 0.00010457343887423043,
      "loss": 0.0076,
      "step": 2171
    },
    {
      "epoch": 1.4327176781002637,
      "grad_norm": 0.22278888523578644,
      "learning_rate": 0.00010452946350043975,
      "loss": 0.0429,
      "step": 2172
    },
    {
      "epoch": 1.433377308707124,
      "grad_norm": 0.13849423825740814,
      "learning_rate": 0.00010448548812664909,
      "loss": 0.0158,
      "step": 2173
    },
    {
      "epoch": 1.4340369393139842,
      "grad_norm": 0.11780798435211182,
      "learning_rate": 0.0001044415127528584,
      "loss": 0.0137,
      "step": 2174
    },
    {
      "epoch": 1.4346965699208443,
      "grad_norm": 0.14834442734718323,
      "learning_rate": 0.00010439753737906773,
      "loss": 0.0103,
      "step": 2175
    },
    {
      "epoch": 1.4353562005277045,
      "grad_norm": 0.22144433856010437,
      "learning_rate": 0.00010435356200527704,
      "loss": 0.0302,
      "step": 2176
    },
    {
      "epoch": 1.4360158311345645,
      "grad_norm": 0.22135716676712036,
      "learning_rate": 0.00010430958663148638,
      "loss": 0.0261,
      "step": 2177
    },
    {
      "epoch": 1.4366754617414248,
      "grad_norm": 0.07947096973657608,
      "learning_rate": 0.0001042656112576957,
      "loss": 0.0044,
      "step": 2178
    },
    {
      "epoch": 1.437335092348285,
      "grad_norm": 0.2329157292842865,
      "learning_rate": 0.00010422163588390501,
      "loss": 0.0424,
      "step": 2179
    },
    {
      "epoch": 1.437994722955145,
      "grad_norm": 0.16374017298221588,
      "learning_rate": 0.00010417766051011435,
      "loss": 0.0194,
      "step": 2180
    },
    {
      "epoch": 1.4386543535620053,
      "grad_norm": 0.1483132243156433,
      "learning_rate": 0.00010413368513632367,
      "loss": 0.0115,
      "step": 2181
    },
    {
      "epoch": 1.4393139841688654,
      "grad_norm": 0.09863612800836563,
      "learning_rate": 0.00010408970976253298,
      "loss": 0.0118,
      "step": 2182
    },
    {
      "epoch": 1.4399736147757256,
      "grad_norm": 0.1677493005990982,
      "learning_rate": 0.0001040457343887423,
      "loss": 0.0293,
      "step": 2183
    },
    {
      "epoch": 1.4406332453825859,
      "grad_norm": 0.3152305483818054,
      "learning_rate": 0.00010400175901495163,
      "loss": 0.035,
      "step": 2184
    },
    {
      "epoch": 1.4412928759894459,
      "grad_norm": 0.23512040078639984,
      "learning_rate": 0.00010395778364116095,
      "loss": 0.0442,
      "step": 2185
    },
    {
      "epoch": 1.4419525065963061,
      "grad_norm": 0.17921525239944458,
      "learning_rate": 0.00010391380826737028,
      "loss": 0.0314,
      "step": 2186
    },
    {
      "epoch": 1.4426121372031662,
      "grad_norm": 0.2806720733642578,
      "learning_rate": 0.00010386983289357962,
      "loss": 0.0287,
      "step": 2187
    },
    {
      "epoch": 1.4432717678100264,
      "grad_norm": 0.12769915163516998,
      "learning_rate": 0.00010382585751978893,
      "loss": 0.0129,
      "step": 2188
    },
    {
      "epoch": 1.4439313984168867,
      "grad_norm": 0.12275905162096024,
      "learning_rate": 0.00010378188214599825,
      "loss": 0.0174,
      "step": 2189
    },
    {
      "epoch": 1.4445910290237467,
      "grad_norm": 0.16301821172237396,
      "learning_rate": 0.00010373790677220756,
      "loss": 0.0217,
      "step": 2190
    },
    {
      "epoch": 1.4452506596306067,
      "grad_norm": 0.38515159487724304,
      "learning_rate": 0.0001036939313984169,
      "loss": 0.0078,
      "step": 2191
    },
    {
      "epoch": 1.445910290237467,
      "grad_norm": 0.1267624795436859,
      "learning_rate": 0.00010364995602462621,
      "loss": 0.0105,
      "step": 2192
    },
    {
      "epoch": 1.4465699208443272,
      "grad_norm": 0.4131496846675873,
      "learning_rate": 0.00010360598065083553,
      "loss": 0.0457,
      "step": 2193
    },
    {
      "epoch": 1.4472295514511873,
      "grad_norm": 0.1331387758255005,
      "learning_rate": 0.00010356200527704486,
      "loss": 0.0145,
      "step": 2194
    },
    {
      "epoch": 1.4478891820580475,
      "grad_norm": 0.256919264793396,
      "learning_rate": 0.00010351802990325418,
      "loss": 0.0325,
      "step": 2195
    },
    {
      "epoch": 1.4485488126649075,
      "grad_norm": 0.2558099031448364,
      "learning_rate": 0.00010347405452946351,
      "loss": 0.0372,
      "step": 2196
    },
    {
      "epoch": 1.4492084432717678,
      "grad_norm": 0.14161793887615204,
      "learning_rate": 0.00010343007915567282,
      "loss": 0.0128,
      "step": 2197
    },
    {
      "epoch": 1.449868073878628,
      "grad_norm": 0.09223639965057373,
      "learning_rate": 0.00010338610378188217,
      "loss": 0.0105,
      "step": 2198
    },
    {
      "epoch": 1.450527704485488,
      "grad_norm": 0.3199845552444458,
      "learning_rate": 0.00010334212840809148,
      "loss": 0.0237,
      "step": 2199
    },
    {
      "epoch": 1.4511873350923483,
      "grad_norm": 0.2336978167295456,
      "learning_rate": 0.0001032981530343008,
      "loss": 0.0368,
      "step": 2200
    },
    {
      "epoch": 1.4518469656992083,
      "grad_norm": 0.18491844832897186,
      "learning_rate": 0.00010325417766051011,
      "loss": 0.0249,
      "step": 2201
    },
    {
      "epoch": 1.4525065963060686,
      "grad_norm": 0.07707696408033371,
      "learning_rate": 0.00010321020228671945,
      "loss": 0.0039,
      "step": 2202
    },
    {
      "epoch": 1.4531662269129288,
      "grad_norm": 0.1839364618062973,
      "learning_rate": 0.00010316622691292876,
      "loss": 0.0361,
      "step": 2203
    },
    {
      "epoch": 1.4538258575197889,
      "grad_norm": 0.1406610608100891,
      "learning_rate": 0.00010312225153913808,
      "loss": 0.0228,
      "step": 2204
    },
    {
      "epoch": 1.4544854881266491,
      "grad_norm": 0.15596908330917358,
      "learning_rate": 0.00010307827616534742,
      "loss": 0.0188,
      "step": 2205
    },
    {
      "epoch": 1.4551451187335092,
      "grad_norm": 0.17682147026062012,
      "learning_rate": 0.00010303430079155673,
      "loss": 0.0229,
      "step": 2206
    },
    {
      "epoch": 1.4558047493403694,
      "grad_norm": 0.1313202679157257,
      "learning_rate": 0.00010299032541776606,
      "loss": 0.0076,
      "step": 2207
    },
    {
      "epoch": 1.4564643799472297,
      "grad_norm": 0.19191013276576996,
      "learning_rate": 0.00010294635004397537,
      "loss": 0.0203,
      "step": 2208
    },
    {
      "epoch": 1.4571240105540897,
      "grad_norm": 0.222987100481987,
      "learning_rate": 0.00010290237467018471,
      "loss": 0.052,
      "step": 2209
    },
    {
      "epoch": 1.45778364116095,
      "grad_norm": 0.16363030672073364,
      "learning_rate": 0.00010285839929639403,
      "loss": 0.0224,
      "step": 2210
    },
    {
      "epoch": 1.45844327176781,
      "grad_norm": 0.3409494161605835,
      "learning_rate": 0.00010281442392260334,
      "loss": 0.0611,
      "step": 2211
    },
    {
      "epoch": 1.4591029023746702,
      "grad_norm": 0.11843609064817429,
      "learning_rate": 0.00010277044854881266,
      "loss": 0.0165,
      "step": 2212
    },
    {
      "epoch": 1.4597625329815305,
      "grad_norm": 0.09491096436977386,
      "learning_rate": 0.000102726473175022,
      "loss": 0.0094,
      "step": 2213
    },
    {
      "epoch": 1.4604221635883905,
      "grad_norm": 0.17560768127441406,
      "learning_rate": 0.00010268249780123131,
      "loss": 0.021,
      "step": 2214
    },
    {
      "epoch": 1.4610817941952505,
      "grad_norm": 0.11819537729024887,
      "learning_rate": 0.00010263852242744062,
      "loss": 0.0162,
      "step": 2215
    },
    {
      "epoch": 1.4617414248021108,
      "grad_norm": 0.13117386400699615,
      "learning_rate": 0.00010259454705364997,
      "loss": 0.0206,
      "step": 2216
    },
    {
      "epoch": 1.462401055408971,
      "grad_norm": 0.30160126090049744,
      "learning_rate": 0.00010255057167985928,
      "loss": 0.0614,
      "step": 2217
    },
    {
      "epoch": 1.463060686015831,
      "grad_norm": 0.12923666834831238,
      "learning_rate": 0.00010250659630606861,
      "loss": 0.0137,
      "step": 2218
    },
    {
      "epoch": 1.4637203166226913,
      "grad_norm": 0.18722717463970184,
      "learning_rate": 0.00010246262093227792,
      "loss": 0.0211,
      "step": 2219
    },
    {
      "epoch": 1.4643799472295513,
      "grad_norm": 0.16855716705322266,
      "learning_rate": 0.00010241864555848726,
      "loss": 0.0127,
      "step": 2220
    },
    {
      "epoch": 1.4650395778364116,
      "grad_norm": 0.13826125860214233,
      "learning_rate": 0.00010237467018469658,
      "loss": 0.0177,
      "step": 2221
    },
    {
      "epoch": 1.4656992084432718,
      "grad_norm": 0.15056255459785461,
      "learning_rate": 0.00010233069481090589,
      "loss": 0.0286,
      "step": 2222
    },
    {
      "epoch": 1.4663588390501319,
      "grad_norm": 0.2286316603422165,
      "learning_rate": 0.00010228671943711523,
      "loss": 0.0263,
      "step": 2223
    },
    {
      "epoch": 1.4670184696569921,
      "grad_norm": 0.15612025558948517,
      "learning_rate": 0.00010224274406332455,
      "loss": 0.0286,
      "step": 2224
    },
    {
      "epoch": 1.4676781002638521,
      "grad_norm": 0.14891786873340607,
      "learning_rate": 0.00010219876868953386,
      "loss": 0.0175,
      "step": 2225
    },
    {
      "epoch": 1.4683377308707124,
      "grad_norm": 0.10007581114768982,
      "learning_rate": 0.00010215479331574319,
      "loss": 0.012,
      "step": 2226
    },
    {
      "epoch": 1.4689973614775726,
      "grad_norm": 0.12112882733345032,
      "learning_rate": 0.00010211081794195251,
      "loss": 0.0158,
      "step": 2227
    },
    {
      "epoch": 1.4696569920844327,
      "grad_norm": 1.1335031986236572,
      "learning_rate": 0.00010206684256816184,
      "loss": 0.078,
      "step": 2228
    },
    {
      "epoch": 1.470316622691293,
      "grad_norm": 0.09280600398778915,
      "learning_rate": 0.00010202286719437116,
      "loss": 0.0094,
      "step": 2229
    },
    {
      "epoch": 1.470976253298153,
      "grad_norm": 0.10725639760494232,
      "learning_rate": 0.00010197889182058047,
      "loss": 0.0181,
      "step": 2230
    },
    {
      "epoch": 1.4716358839050132,
      "grad_norm": 0.10879095643758774,
      "learning_rate": 0.00010193491644678981,
      "loss": 0.0198,
      "step": 2231
    },
    {
      "epoch": 1.4722955145118735,
      "grad_norm": 0.13066497445106506,
      "learning_rate": 0.00010189094107299912,
      "loss": 0.0188,
      "step": 2232
    },
    {
      "epoch": 1.4729551451187335,
      "grad_norm": 0.12206855416297913,
      "learning_rate": 0.00010184696569920844,
      "loss": 0.0145,
      "step": 2233
    },
    {
      "epoch": 1.4736147757255937,
      "grad_norm": 0.12723734974861145,
      "learning_rate": 0.00010180299032541778,
      "loss": 0.0146,
      "step": 2234
    },
    {
      "epoch": 1.4742744063324538,
      "grad_norm": 0.1625661700963974,
      "learning_rate": 0.0001017590149516271,
      "loss": 0.0268,
      "step": 2235
    },
    {
      "epoch": 1.474934036939314,
      "grad_norm": 0.15153194963932037,
      "learning_rate": 0.00010171503957783641,
      "loss": 0.0168,
      "step": 2236
    },
    {
      "epoch": 1.4755936675461743,
      "grad_norm": 0.14447511732578278,
      "learning_rate": 0.00010167106420404574,
      "loss": 0.0171,
      "step": 2237
    },
    {
      "epoch": 1.4762532981530343,
      "grad_norm": 0.0658499151468277,
      "learning_rate": 0.00010162708883025506,
      "loss": 0.0035,
      "step": 2238
    },
    {
      "epoch": 1.4769129287598943,
      "grad_norm": 0.2710281312465668,
      "learning_rate": 0.00010158311345646439,
      "loss": 0.0359,
      "step": 2239
    },
    {
      "epoch": 1.4775725593667546,
      "grad_norm": 0.11314639449119568,
      "learning_rate": 0.0001015391380826737,
      "loss": 0.0099,
      "step": 2240
    },
    {
      "epoch": 1.4782321899736148,
      "grad_norm": 0.0296635664999485,
      "learning_rate": 0.00010149516270888305,
      "loss": 0.0015,
      "step": 2241
    },
    {
      "epoch": 1.4788918205804749,
      "grad_norm": 0.21908128261566162,
      "learning_rate": 0.00010145118733509236,
      "loss": 0.0411,
      "step": 2242
    },
    {
      "epoch": 1.479551451187335,
      "grad_norm": 0.17807503044605255,
      "learning_rate": 0.00010140721196130167,
      "loss": 0.0241,
      "step": 2243
    },
    {
      "epoch": 1.4802110817941951,
      "grad_norm": 0.1773519366979599,
      "learning_rate": 0.00010136323658751099,
      "loss": 0.0214,
      "step": 2244
    },
    {
      "epoch": 1.4808707124010554,
      "grad_norm": 0.19901034235954285,
      "learning_rate": 0.00010131926121372033,
      "loss": 0.0183,
      "step": 2245
    },
    {
      "epoch": 1.4815303430079156,
      "grad_norm": 0.14812341332435608,
      "learning_rate": 0.00010127528583992964,
      "loss": 0.0188,
      "step": 2246
    },
    {
      "epoch": 1.4821899736147757,
      "grad_norm": 0.2563003897666931,
      "learning_rate": 0.00010123131046613896,
      "loss": 0.0388,
      "step": 2247
    },
    {
      "epoch": 1.482849604221636,
      "grad_norm": 0.08010882884263992,
      "learning_rate": 0.00010118733509234828,
      "loss": 0.0065,
      "step": 2248
    },
    {
      "epoch": 1.483509234828496,
      "grad_norm": 0.15236100554466248,
      "learning_rate": 0.00010114335971855761,
      "loss": 0.0125,
      "step": 2249
    },
    {
      "epoch": 1.4841688654353562,
      "grad_norm": 0.09682334214448929,
      "learning_rate": 0.00010109938434476694,
      "loss": 0.007,
      "step": 2250
    },
    {
      "epoch": 1.4848284960422165,
      "grad_norm": 0.25937750935554504,
      "learning_rate": 0.00010105540897097625,
      "loss": 0.0341,
      "step": 2251
    },
    {
      "epoch": 1.4854881266490765,
      "grad_norm": 0.2466520071029663,
      "learning_rate": 0.0001010114335971856,
      "loss": 0.0298,
      "step": 2252
    },
    {
      "epoch": 1.4861477572559367,
      "grad_norm": 0.06857985258102417,
      "learning_rate": 0.00010096745822339491,
      "loss": 0.0074,
      "step": 2253
    },
    {
      "epoch": 1.4868073878627968,
      "grad_norm": 0.16446834802627563,
      "learning_rate": 0.00010092348284960422,
      "loss": 0.0202,
      "step": 2254
    },
    {
      "epoch": 1.487467018469657,
      "grad_norm": 0.07532766461372375,
      "learning_rate": 0.00010087950747581354,
      "loss": 0.0067,
      "step": 2255
    },
    {
      "epoch": 1.4881266490765173,
      "grad_norm": 0.12575708329677582,
      "learning_rate": 0.00010083553210202288,
      "loss": 0.0109,
      "step": 2256
    },
    {
      "epoch": 1.4887862796833773,
      "grad_norm": 0.2533518671989441,
      "learning_rate": 0.00010079155672823219,
      "loss": 0.0487,
      "step": 2257
    },
    {
      "epoch": 1.4894459102902375,
      "grad_norm": 0.16113948822021484,
      "learning_rate": 0.0001007475813544415,
      "loss": 0.0265,
      "step": 2258
    },
    {
      "epoch": 1.4901055408970976,
      "grad_norm": 0.1849280297756195,
      "learning_rate": 0.00010070360598065085,
      "loss": 0.0359,
      "step": 2259
    },
    {
      "epoch": 1.4907651715039578,
      "grad_norm": 0.0981520563364029,
      "learning_rate": 0.00010065963060686017,
      "loss": 0.0109,
      "step": 2260
    },
    {
      "epoch": 1.491424802110818,
      "grad_norm": 0.23880842328071594,
      "learning_rate": 0.00010061565523306949,
      "loss": 0.0344,
      "step": 2261
    },
    {
      "epoch": 1.492084432717678,
      "grad_norm": 0.20598839223384857,
      "learning_rate": 0.0001005716798592788,
      "loss": 0.0358,
      "step": 2262
    },
    {
      "epoch": 1.4927440633245381,
      "grad_norm": 0.14241839945316315,
      "learning_rate": 0.00010052770448548814,
      "loss": 0.0221,
      "step": 2263
    },
    {
      "epoch": 1.4934036939313984,
      "grad_norm": 0.06905533373355865,
      "learning_rate": 0.00010048372911169746,
      "loss": 0.0031,
      "step": 2264
    },
    {
      "epoch": 1.4940633245382586,
      "grad_norm": 0.14320574700832367,
      "learning_rate": 0.00010043975373790677,
      "loss": 0.0145,
      "step": 2265
    },
    {
      "epoch": 1.4947229551451187,
      "grad_norm": 0.19433681666851044,
      "learning_rate": 0.00010039577836411608,
      "loss": 0.0266,
      "step": 2266
    },
    {
      "epoch": 1.495382585751979,
      "grad_norm": 0.25864601135253906,
      "learning_rate": 0.00010035180299032543,
      "loss": 0.0283,
      "step": 2267
    },
    {
      "epoch": 1.496042216358839,
      "grad_norm": 0.23573443293571472,
      "learning_rate": 0.00010030782761653474,
      "loss": 0.0321,
      "step": 2268
    },
    {
      "epoch": 1.4967018469656992,
      "grad_norm": 0.16533099114894867,
      "learning_rate": 0.00010026385224274407,
      "loss": 0.012,
      "step": 2269
    },
    {
      "epoch": 1.4973614775725594,
      "grad_norm": 0.20044614374637604,
      "learning_rate": 0.0001002198768689534,
      "loss": 0.0278,
      "step": 2270
    },
    {
      "epoch": 1.4980211081794195,
      "grad_norm": 0.1558765172958374,
      "learning_rate": 0.00010017590149516272,
      "loss": 0.0091,
      "step": 2271
    },
    {
      "epoch": 1.4986807387862797,
      "grad_norm": 0.11893809586763382,
      "learning_rate": 0.00010013192612137204,
      "loss": 0.015,
      "step": 2272
    },
    {
      "epoch": 1.4993403693931397,
      "grad_norm": 0.18846671283245087,
      "learning_rate": 0.00010008795074758135,
      "loss": 0.0193,
      "step": 2273
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.11883412301540375,
      "learning_rate": 0.00010004397537379069,
      "loss": 0.0061,
      "step": 2274
    },
    {
      "epoch": 1.5006596306068603,
      "grad_norm": 0.11363106966018677,
      "learning_rate": 0.0001,
      "loss": 0.0127,
      "step": 2275
    },
    {
      "epoch": 1.5013192612137203,
      "grad_norm": 0.23938246071338654,
      "learning_rate": 9.995602462620933e-05,
      "loss": 0.0491,
      "step": 2276
    },
    {
      "epoch": 1.5019788918205803,
      "grad_norm": 0.3012019395828247,
      "learning_rate": 9.991204925241865e-05,
      "loss": 0.0487,
      "step": 2277
    },
    {
      "epoch": 1.5026385224274406,
      "grad_norm": 0.18173770606517792,
      "learning_rate": 9.986807387862797e-05,
      "loss": 0.0106,
      "step": 2278
    },
    {
      "epoch": 1.5032981530343008,
      "grad_norm": 0.26813870668411255,
      "learning_rate": 9.982409850483729e-05,
      "loss": 0.0215,
      "step": 2279
    },
    {
      "epoch": 1.503957783641161,
      "grad_norm": 0.08570205420255661,
      "learning_rate": 9.978012313104662e-05,
      "loss": 0.005,
      "step": 2280
    },
    {
      "epoch": 1.504617414248021,
      "grad_norm": 0.09963896125555038,
      "learning_rate": 9.973614775725594e-05,
      "loss": 0.0108,
      "step": 2281
    },
    {
      "epoch": 1.5052770448548811,
      "grad_norm": 0.09115888178348541,
      "learning_rate": 9.969217238346527e-05,
      "loss": 0.0117,
      "step": 2282
    },
    {
      "epoch": 1.5059366754617414,
      "grad_norm": 0.15755482017993927,
      "learning_rate": 9.964819700967458e-05,
      "loss": 0.0133,
      "step": 2283
    },
    {
      "epoch": 1.5065963060686016,
      "grad_norm": 0.202826589345932,
      "learning_rate": 9.960422163588391e-05,
      "loss": 0.0261,
      "step": 2284
    },
    {
      "epoch": 1.5072559366754619,
      "grad_norm": 0.13739916682243347,
      "learning_rate": 9.956024626209324e-05,
      "loss": 0.016,
      "step": 2285
    },
    {
      "epoch": 1.507915567282322,
      "grad_norm": 0.1054035946726799,
      "learning_rate": 9.951627088830255e-05,
      "loss": 0.0069,
      "step": 2286
    },
    {
      "epoch": 1.508575197889182,
      "grad_norm": 0.13402962684631348,
      "learning_rate": 9.947229551451188e-05,
      "loss": 0.0081,
      "step": 2287
    },
    {
      "epoch": 1.5092348284960422,
      "grad_norm": 0.10973530262708664,
      "learning_rate": 9.94283201407212e-05,
      "loss": 0.0101,
      "step": 2288
    },
    {
      "epoch": 1.5098944591029024,
      "grad_norm": 0.41241374611854553,
      "learning_rate": 9.938434476693052e-05,
      "loss": 0.063,
      "step": 2289
    },
    {
      "epoch": 1.5105540897097627,
      "grad_norm": 0.12199129164218903,
      "learning_rate": 9.934036939313984e-05,
      "loss": 0.0091,
      "step": 2290
    },
    {
      "epoch": 1.5112137203166227,
      "grad_norm": 0.09937892854213715,
      "learning_rate": 9.929639401934916e-05,
      "loss": 0.0063,
      "step": 2291
    },
    {
      "epoch": 1.5118733509234827,
      "grad_norm": 0.44267863035202026,
      "learning_rate": 9.925241864555849e-05,
      "loss": 0.0401,
      "step": 2292
    },
    {
      "epoch": 1.512532981530343,
      "grad_norm": 0.1077684760093689,
      "learning_rate": 9.920844327176782e-05,
      "loss": 0.0063,
      "step": 2293
    },
    {
      "epoch": 1.5131926121372032,
      "grad_norm": 0.1070098802447319,
      "learning_rate": 9.916446789797715e-05,
      "loss": 0.0054,
      "step": 2294
    },
    {
      "epoch": 1.5138522427440633,
      "grad_norm": 0.15277284383773804,
      "learning_rate": 9.912049252418646e-05,
      "loss": 0.0216,
      "step": 2295
    },
    {
      "epoch": 1.5145118733509235,
      "grad_norm": 0.17278258502483368,
      "learning_rate": 9.907651715039579e-05,
      "loss": 0.0135,
      "step": 2296
    },
    {
      "epoch": 1.5151715039577835,
      "grad_norm": 0.1401127129793167,
      "learning_rate": 9.90325417766051e-05,
      "loss": 0.0135,
      "step": 2297
    },
    {
      "epoch": 1.5158311345646438,
      "grad_norm": 0.15005554258823395,
      "learning_rate": 9.898856640281443e-05,
      "loss": 0.0104,
      "step": 2298
    },
    {
      "epoch": 1.516490765171504,
      "grad_norm": 0.1431414783000946,
      "learning_rate": 9.894459102902374e-05,
      "loss": 0.0079,
      "step": 2299
    },
    {
      "epoch": 1.517150395778364,
      "grad_norm": 0.254458487033844,
      "learning_rate": 9.890061565523307e-05,
      "loss": 0.0446,
      "step": 2300
    },
    {
      "epoch": 1.517810026385224,
      "grad_norm": 0.31731918454170227,
      "learning_rate": 9.88566402814424e-05,
      "loss": 0.0337,
      "step": 2301
    },
    {
      "epoch": 1.5184696569920844,
      "grad_norm": 0.2086222767829895,
      "learning_rate": 9.881266490765173e-05,
      "loss": 0.0233,
      "step": 2302
    },
    {
      "epoch": 1.5191292875989446,
      "grad_norm": 0.22169139981269836,
      "learning_rate": 9.876868953386105e-05,
      "loss": 0.0305,
      "step": 2303
    },
    {
      "epoch": 1.5197889182058049,
      "grad_norm": 0.32321101427078247,
      "learning_rate": 9.872471416007037e-05,
      "loss": 0.0333,
      "step": 2304
    },
    {
      "epoch": 1.520448548812665,
      "grad_norm": 0.06547419726848602,
      "learning_rate": 9.86807387862797e-05,
      "loss": 0.0029,
      "step": 2305
    },
    {
      "epoch": 1.521108179419525,
      "grad_norm": 0.1656629741191864,
      "learning_rate": 9.863676341248901e-05,
      "loss": 0.0088,
      "step": 2306
    },
    {
      "epoch": 1.5217678100263852,
      "grad_norm": 0.16532710194587708,
      "learning_rate": 9.859278803869834e-05,
      "loss": 0.0132,
      "step": 2307
    },
    {
      "epoch": 1.5224274406332454,
      "grad_norm": 0.08655235916376114,
      "learning_rate": 9.854881266490765e-05,
      "loss": 0.0077,
      "step": 2308
    },
    {
      "epoch": 1.5230870712401057,
      "grad_norm": 0.15553972125053406,
      "learning_rate": 9.850483729111698e-05,
      "loss": 0.0094,
      "step": 2309
    },
    {
      "epoch": 1.5237467018469657,
      "grad_norm": 0.1486598700284958,
      "learning_rate": 9.846086191732629e-05,
      "loss": 0.018,
      "step": 2310
    },
    {
      "epoch": 1.5244063324538257,
      "grad_norm": 0.15025918185710907,
      "learning_rate": 9.841688654353562e-05,
      "loss": 0.0218,
      "step": 2311
    },
    {
      "epoch": 1.525065963060686,
      "grad_norm": 0.21049124002456665,
      "learning_rate": 9.837291116974495e-05,
      "loss": 0.0193,
      "step": 2312
    },
    {
      "epoch": 1.5257255936675462,
      "grad_norm": 0.15765559673309326,
      "learning_rate": 9.832893579595427e-05,
      "loss": 0.0248,
      "step": 2313
    },
    {
      "epoch": 1.5263852242744065,
      "grad_norm": 0.26597481966018677,
      "learning_rate": 9.82849604221636e-05,
      "loss": 0.0405,
      "step": 2314
    },
    {
      "epoch": 1.5270448548812665,
      "grad_norm": 0.2305215448141098,
      "learning_rate": 9.824098504837292e-05,
      "loss": 0.0194,
      "step": 2315
    },
    {
      "epoch": 1.5277044854881265,
      "grad_norm": 0.16884110867977142,
      "learning_rate": 9.819700967458224e-05,
      "loss": 0.017,
      "step": 2316
    },
    {
      "epoch": 1.5283641160949868,
      "grad_norm": 0.15297161042690277,
      "learning_rate": 9.815303430079156e-05,
      "loss": 0.0207,
      "step": 2317
    },
    {
      "epoch": 1.529023746701847,
      "grad_norm": 0.17925557494163513,
      "learning_rate": 9.810905892700088e-05,
      "loss": 0.0173,
      "step": 2318
    },
    {
      "epoch": 1.529683377308707,
      "grad_norm": 0.19812153279781342,
      "learning_rate": 9.80650835532102e-05,
      "loss": 0.0272,
      "step": 2319
    },
    {
      "epoch": 1.5303430079155673,
      "grad_norm": 0.14896658062934875,
      "learning_rate": 9.802110817941953e-05,
      "loss": 0.017,
      "step": 2320
    },
    {
      "epoch": 1.5310026385224274,
      "grad_norm": 0.07893869280815125,
      "learning_rate": 9.797713280562885e-05,
      "loss": 0.0066,
      "step": 2321
    },
    {
      "epoch": 1.5316622691292876,
      "grad_norm": 0.23147989809513092,
      "learning_rate": 9.793315743183817e-05,
      "loss": 0.0272,
      "step": 2322
    },
    {
      "epoch": 1.5323218997361479,
      "grad_norm": 0.2723081707954407,
      "learning_rate": 9.78891820580475e-05,
      "loss": 0.0296,
      "step": 2323
    },
    {
      "epoch": 1.5329815303430079,
      "grad_norm": 0.07740925252437592,
      "learning_rate": 9.784520668425682e-05,
      "loss": 0.0042,
      "step": 2324
    },
    {
      "epoch": 1.533641160949868,
      "grad_norm": 0.2726486623287201,
      "learning_rate": 9.780123131046615e-05,
      "loss": 0.0461,
      "step": 2325
    },
    {
      "epoch": 1.5343007915567282,
      "grad_norm": 0.16731473803520203,
      "learning_rate": 9.775725593667546e-05,
      "loss": 0.0346,
      "step": 2326
    },
    {
      "epoch": 1.5349604221635884,
      "grad_norm": 0.16084788739681244,
      "learning_rate": 9.771328056288479e-05,
      "loss": 0.0195,
      "step": 2327
    },
    {
      "epoch": 1.5356200527704487,
      "grad_norm": 0.21921265125274658,
      "learning_rate": 9.76693051890941e-05,
      "loss": 0.0241,
      "step": 2328
    },
    {
      "epoch": 1.5362796833773087,
      "grad_norm": 0.09811312705278397,
      "learning_rate": 9.762532981530343e-05,
      "loss": 0.0127,
      "step": 2329
    },
    {
      "epoch": 1.5369393139841687,
      "grad_norm": 0.2748267650604248,
      "learning_rate": 9.758135444151276e-05,
      "loss": 0.0444,
      "step": 2330
    },
    {
      "epoch": 1.537598944591029,
      "grad_norm": 0.17644141614437103,
      "learning_rate": 9.753737906772207e-05,
      "loss": 0.0235,
      "step": 2331
    },
    {
      "epoch": 1.5382585751978892,
      "grad_norm": 0.1756352186203003,
      "learning_rate": 9.74934036939314e-05,
      "loss": 0.021,
      "step": 2332
    },
    {
      "epoch": 1.5389182058047495,
      "grad_norm": 0.25544342398643494,
      "learning_rate": 9.744942832014073e-05,
      "loss": 0.0376,
      "step": 2333
    },
    {
      "epoch": 1.5395778364116095,
      "grad_norm": 0.19621030986309052,
      "learning_rate": 9.740545294635006e-05,
      "loss": 0.0344,
      "step": 2334
    },
    {
      "epoch": 1.5402374670184695,
      "grad_norm": 0.2336917370557785,
      "learning_rate": 9.736147757255937e-05,
      "loss": 0.0389,
      "step": 2335
    },
    {
      "epoch": 1.5408970976253298,
      "grad_norm": 0.06756545603275299,
      "learning_rate": 9.73175021987687e-05,
      "loss": 0.0058,
      "step": 2336
    },
    {
      "epoch": 1.54155672823219,
      "grad_norm": 0.19205287098884583,
      "learning_rate": 9.727352682497801e-05,
      "loss": 0.026,
      "step": 2337
    },
    {
      "epoch": 1.5422163588390503,
      "grad_norm": 0.12248429656028748,
      "learning_rate": 9.722955145118734e-05,
      "loss": 0.0143,
      "step": 2338
    },
    {
      "epoch": 1.5428759894459103,
      "grad_norm": 0.19174042344093323,
      "learning_rate": 9.718557607739667e-05,
      "loss": 0.0119,
      "step": 2339
    },
    {
      "epoch": 1.5435356200527703,
      "grad_norm": 0.3507598340511322,
      "learning_rate": 9.714160070360598e-05,
      "loss": 0.0416,
      "step": 2340
    },
    {
      "epoch": 1.5441952506596306,
      "grad_norm": 0.08846919983625412,
      "learning_rate": 9.709762532981531e-05,
      "loss": 0.0063,
      "step": 2341
    },
    {
      "epoch": 1.5448548812664908,
      "grad_norm": 0.15299083292484283,
      "learning_rate": 9.705364995602462e-05,
      "loss": 0.0181,
      "step": 2342
    },
    {
      "epoch": 1.5455145118733509,
      "grad_norm": 0.06806095689535141,
      "learning_rate": 9.700967458223395e-05,
      "loss": 0.0037,
      "step": 2343
    },
    {
      "epoch": 1.5461741424802111,
      "grad_norm": 0.22314995527267456,
      "learning_rate": 9.696569920844328e-05,
      "loss": 0.0428,
      "step": 2344
    },
    {
      "epoch": 1.5468337730870712,
      "grad_norm": 0.31822389364242554,
      "learning_rate": 9.69217238346526e-05,
      "loss": 0.0316,
      "step": 2345
    },
    {
      "epoch": 1.5474934036939314,
      "grad_norm": 0.2306964099407196,
      "learning_rate": 9.687774846086192e-05,
      "loss": 0.035,
      "step": 2346
    },
    {
      "epoch": 1.5481530343007917,
      "grad_norm": 0.15715858340263367,
      "learning_rate": 9.683377308707125e-05,
      "loss": 0.0218,
      "step": 2347
    },
    {
      "epoch": 1.5488126649076517,
      "grad_norm": 0.25201618671417236,
      "learning_rate": 9.678979771328057e-05,
      "loss": 0.028,
      "step": 2348
    },
    {
      "epoch": 1.5494722955145117,
      "grad_norm": 0.2241787165403366,
      "learning_rate": 9.674582233948989e-05,
      "loss": 0.0365,
      "step": 2349
    },
    {
      "epoch": 1.550131926121372,
      "grad_norm": 0.09742574393749237,
      "learning_rate": 9.670184696569922e-05,
      "loss": 0.0051,
      "step": 2350
    },
    {
      "epoch": 1.5507915567282322,
      "grad_norm": 0.2114849090576172,
      "learning_rate": 9.665787159190853e-05,
      "loss": 0.0471,
      "step": 2351
    },
    {
      "epoch": 1.5514511873350925,
      "grad_norm": 0.2689625024795532,
      "learning_rate": 9.661389621811786e-05,
      "loss": 0.0279,
      "step": 2352
    },
    {
      "epoch": 1.5521108179419525,
      "grad_norm": 0.21059632301330566,
      "learning_rate": 9.656992084432717e-05,
      "loss": 0.0307,
      "step": 2353
    },
    {
      "epoch": 1.5527704485488125,
      "grad_norm": 0.10360333323478699,
      "learning_rate": 9.65259454705365e-05,
      "loss": 0.0075,
      "step": 2354
    },
    {
      "epoch": 1.5534300791556728,
      "grad_norm": 0.1005290076136589,
      "learning_rate": 9.648197009674583e-05,
      "loss": 0.0047,
      "step": 2355
    },
    {
      "epoch": 1.554089709762533,
      "grad_norm": 0.1958446204662323,
      "learning_rate": 9.643799472295515e-05,
      "loss": 0.0164,
      "step": 2356
    },
    {
      "epoch": 1.5547493403693933,
      "grad_norm": 0.1936633288860321,
      "learning_rate": 9.639401934916448e-05,
      "loss": 0.0263,
      "step": 2357
    },
    {
      "epoch": 1.5554089709762533,
      "grad_norm": 0.11908873170614243,
      "learning_rate": 9.63500439753738e-05,
      "loss": 0.0141,
      "step": 2358
    },
    {
      "epoch": 1.5560686015831133,
      "grad_norm": 0.17715834081172943,
      "learning_rate": 9.630606860158312e-05,
      "loss": 0.0274,
      "step": 2359
    },
    {
      "epoch": 1.5567282321899736,
      "grad_norm": 0.16387999057769775,
      "learning_rate": 9.626209322779244e-05,
      "loss": 0.0197,
      "step": 2360
    },
    {
      "epoch": 1.5573878627968338,
      "grad_norm": 0.13180018961429596,
      "learning_rate": 9.621811785400176e-05,
      "loss": 0.0164,
      "step": 2361
    },
    {
      "epoch": 1.558047493403694,
      "grad_norm": 0.15855704247951508,
      "learning_rate": 9.617414248021108e-05,
      "loss": 0.0227,
      "step": 2362
    },
    {
      "epoch": 1.5587071240105541,
      "grad_norm": 0.2017734795808792,
      "learning_rate": 9.61301671064204e-05,
      "loss": 0.0267,
      "step": 2363
    },
    {
      "epoch": 1.5593667546174141,
      "grad_norm": 0.22683843970298767,
      "learning_rate": 9.608619173262972e-05,
      "loss": 0.019,
      "step": 2364
    },
    {
      "epoch": 1.5600263852242744,
      "grad_norm": 0.15423037111759186,
      "learning_rate": 9.604221635883906e-05,
      "loss": 0.0197,
      "step": 2365
    },
    {
      "epoch": 1.5606860158311346,
      "grad_norm": 0.15581995248794556,
      "learning_rate": 9.599824098504839e-05,
      "loss": 0.0152,
      "step": 2366
    },
    {
      "epoch": 1.5613456464379947,
      "grad_norm": 0.14392462372779846,
      "learning_rate": 9.59542656112577e-05,
      "loss": 0.0138,
      "step": 2367
    },
    {
      "epoch": 1.562005277044855,
      "grad_norm": 0.1136845201253891,
      "learning_rate": 9.591029023746703e-05,
      "loss": 0.0073,
      "step": 2368
    },
    {
      "epoch": 1.562664907651715,
      "grad_norm": 0.1645091474056244,
      "learning_rate": 9.586631486367634e-05,
      "loss": 0.0163,
      "step": 2369
    },
    {
      "epoch": 1.5633245382585752,
      "grad_norm": 0.19218499958515167,
      "learning_rate": 9.582233948988567e-05,
      "loss": 0.0193,
      "step": 2370
    },
    {
      "epoch": 1.5639841688654355,
      "grad_norm": 0.052257366478443146,
      "learning_rate": 9.577836411609499e-05,
      "loss": 0.0028,
      "step": 2371
    },
    {
      "epoch": 1.5646437994722955,
      "grad_norm": 0.22069168090820312,
      "learning_rate": 9.573438874230431e-05,
      "loss": 0.0267,
      "step": 2372
    },
    {
      "epoch": 1.5653034300791555,
      "grad_norm": 0.2972804605960846,
      "learning_rate": 9.569041336851363e-05,
      "loss": 0.0428,
      "step": 2373
    },
    {
      "epoch": 1.5659630606860158,
      "grad_norm": 0.12375979870557785,
      "learning_rate": 9.564643799472295e-05,
      "loss": 0.0083,
      "step": 2374
    },
    {
      "epoch": 1.566622691292876,
      "grad_norm": 0.16087573766708374,
      "learning_rate": 9.560246262093228e-05,
      "loss": 0.0154,
      "step": 2375
    },
    {
      "epoch": 1.5672823218997363,
      "grad_norm": 0.23150424659252167,
      "learning_rate": 9.555848724714161e-05,
      "loss": 0.0311,
      "step": 2376
    },
    {
      "epoch": 1.5679419525065963,
      "grad_norm": 0.25045377016067505,
      "learning_rate": 9.551451187335094e-05,
      "loss": 0.0161,
      "step": 2377
    },
    {
      "epoch": 1.5686015831134563,
      "grad_norm": 0.14852522313594818,
      "learning_rate": 9.547053649956025e-05,
      "loss": 0.0193,
      "step": 2378
    },
    {
      "epoch": 1.5692612137203166,
      "grad_norm": 0.3261190950870514,
      "learning_rate": 9.542656112576958e-05,
      "loss": 0.0223,
      "step": 2379
    },
    {
      "epoch": 1.5699208443271768,
      "grad_norm": 0.16460028290748596,
      "learning_rate": 9.538258575197889e-05,
      "loss": 0.0176,
      "step": 2380
    },
    {
      "epoch": 1.570580474934037,
      "grad_norm": 0.38297298550605774,
      "learning_rate": 9.533861037818822e-05,
      "loss": 0.0389,
      "step": 2381
    },
    {
      "epoch": 1.571240105540897,
      "grad_norm": 0.2218182533979416,
      "learning_rate": 9.529463500439753e-05,
      "loss": 0.0195,
      "step": 2382
    },
    {
      "epoch": 1.5718997361477571,
      "grad_norm": 0.09355141222476959,
      "learning_rate": 9.525065963060686e-05,
      "loss": 0.0074,
      "step": 2383
    },
    {
      "epoch": 1.5725593667546174,
      "grad_norm": 0.1919308304786682,
      "learning_rate": 9.520668425681619e-05,
      "loss": 0.0143,
      "step": 2384
    },
    {
      "epoch": 1.5732189973614776,
      "grad_norm": 0.1448487788438797,
      "learning_rate": 9.51627088830255e-05,
      "loss": 0.009,
      "step": 2385
    },
    {
      "epoch": 1.5738786279683379,
      "grad_norm": 0.3041685223579407,
      "learning_rate": 9.511873350923483e-05,
      "loss": 0.0263,
      "step": 2386
    },
    {
      "epoch": 1.574538258575198,
      "grad_norm": 0.23492684960365295,
      "learning_rate": 9.507475813544416e-05,
      "loss": 0.0294,
      "step": 2387
    },
    {
      "epoch": 1.575197889182058,
      "grad_norm": 0.14835555851459503,
      "learning_rate": 9.503078276165349e-05,
      "loss": 0.0127,
      "step": 2388
    },
    {
      "epoch": 1.5758575197889182,
      "grad_norm": 0.13490764796733856,
      "learning_rate": 9.49868073878628e-05,
      "loss": 0.0136,
      "step": 2389
    },
    {
      "epoch": 1.5765171503957784,
      "grad_norm": 0.2606726288795471,
      "learning_rate": 9.494283201407213e-05,
      "loss": 0.0217,
      "step": 2390
    },
    {
      "epoch": 1.5771767810026385,
      "grad_norm": 0.3345660865306854,
      "learning_rate": 9.489885664028144e-05,
      "loss": 0.0501,
      "step": 2391
    },
    {
      "epoch": 1.5778364116094987,
      "grad_norm": 0.16315105557441711,
      "learning_rate": 9.485488126649077e-05,
      "loss": 0.0143,
      "step": 2392
    },
    {
      "epoch": 1.5784960422163588,
      "grad_norm": 0.289102703332901,
      "learning_rate": 9.48109058927001e-05,
      "loss": 0.0364,
      "step": 2393
    },
    {
      "epoch": 1.579155672823219,
      "grad_norm": 0.27864891290664673,
      "learning_rate": 9.476693051890941e-05,
      "loss": 0.0319,
      "step": 2394
    },
    {
      "epoch": 1.5798153034300793,
      "grad_norm": 0.1695263683795929,
      "learning_rate": 9.472295514511874e-05,
      "loss": 0.015,
      "step": 2395
    },
    {
      "epoch": 1.5804749340369393,
      "grad_norm": 0.14664438366889954,
      "learning_rate": 9.467897977132805e-05,
      "loss": 0.0126,
      "step": 2396
    },
    {
      "epoch": 1.5811345646437993,
      "grad_norm": 0.1259928196668625,
      "learning_rate": 9.463500439753739e-05,
      "loss": 0.0123,
      "step": 2397
    },
    {
      "epoch": 1.5817941952506596,
      "grad_norm": 0.1715879589319229,
      "learning_rate": 9.45910290237467e-05,
      "loss": 0.0099,
      "step": 2398
    },
    {
      "epoch": 1.5824538258575198,
      "grad_norm": 0.2765316367149353,
      "learning_rate": 9.454705364995603e-05,
      "loss": 0.0232,
      "step": 2399
    },
    {
      "epoch": 1.58311345646438,
      "grad_norm": 0.09410207718610764,
      "learning_rate": 9.450307827616535e-05,
      "loss": 0.0037,
      "step": 2400
    },
    {
      "epoch": 1.58377308707124,
      "grad_norm": 0.21255946159362793,
      "learning_rate": 9.445910290237468e-05,
      "loss": 0.0179,
      "step": 2401
    },
    {
      "epoch": 1.5844327176781001,
      "grad_norm": 0.27267956733703613,
      "learning_rate": 9.4415127528584e-05,
      "loss": 0.0401,
      "step": 2402
    },
    {
      "epoch": 1.5850923482849604,
      "grad_norm": 0.1540307253599167,
      "learning_rate": 9.437115215479332e-05,
      "loss": 0.0093,
      "step": 2403
    },
    {
      "epoch": 1.5857519788918206,
      "grad_norm": 0.06478501856327057,
      "learning_rate": 9.432717678100264e-05,
      "loss": 0.003,
      "step": 2404
    },
    {
      "epoch": 1.5864116094986809,
      "grad_norm": 0.12231462448835373,
      "learning_rate": 9.428320140721196e-05,
      "loss": 0.0052,
      "step": 2405
    },
    {
      "epoch": 1.587071240105541,
      "grad_norm": 0.12211970239877701,
      "learning_rate": 9.423922603342129e-05,
      "loss": 0.0069,
      "step": 2406
    },
    {
      "epoch": 1.587730870712401,
      "grad_norm": 0.17683206498622894,
      "learning_rate": 9.419525065963061e-05,
      "loss": 0.0167,
      "step": 2407
    },
    {
      "epoch": 1.5883905013192612,
      "grad_norm": 0.3260849714279175,
      "learning_rate": 9.415127528583994e-05,
      "loss": 0.0329,
      "step": 2408
    },
    {
      "epoch": 1.5890501319261214,
      "grad_norm": 0.43431463837623596,
      "learning_rate": 9.410729991204925e-05,
      "loss": 0.0474,
      "step": 2409
    },
    {
      "epoch": 1.5897097625329817,
      "grad_norm": 0.3056662678718567,
      "learning_rate": 9.406332453825858e-05,
      "loss": 0.0323,
      "step": 2410
    },
    {
      "epoch": 1.5903693931398417,
      "grad_norm": 0.13712841272354126,
      "learning_rate": 9.401934916446791e-05,
      "loss": 0.0124,
      "step": 2411
    },
    {
      "epoch": 1.5910290237467017,
      "grad_norm": 0.338641494512558,
      "learning_rate": 9.397537379067722e-05,
      "loss": 0.0502,
      "step": 2412
    },
    {
      "epoch": 1.591688654353562,
      "grad_norm": 0.26218104362487793,
      "learning_rate": 9.393139841688655e-05,
      "loss": 0.0264,
      "step": 2413
    },
    {
      "epoch": 1.5923482849604222,
      "grad_norm": 0.11376649886369705,
      "learning_rate": 9.388742304309587e-05,
      "loss": 0.0108,
      "step": 2414
    },
    {
      "epoch": 1.5930079155672823,
      "grad_norm": 0.13282105326652527,
      "learning_rate": 9.384344766930519e-05,
      "loss": 0.0128,
      "step": 2415
    },
    {
      "epoch": 1.5936675461741425,
      "grad_norm": 0.21690090000629425,
      "learning_rate": 9.379947229551451e-05,
      "loss": 0.0147,
      "step": 2416
    },
    {
      "epoch": 1.5943271767810026,
      "grad_norm": 0.15419606864452362,
      "learning_rate": 9.375549692172383e-05,
      "loss": 0.0196,
      "step": 2417
    },
    {
      "epoch": 1.5949868073878628,
      "grad_norm": 0.1854340136051178,
      "learning_rate": 9.371152154793316e-05,
      "loss": 0.0115,
      "step": 2418
    },
    {
      "epoch": 1.595646437994723,
      "grad_norm": 0.32705754041671753,
      "learning_rate": 9.366754617414249e-05,
      "loss": 0.0604,
      "step": 2419
    },
    {
      "epoch": 1.596306068601583,
      "grad_norm": 0.10596080869436264,
      "learning_rate": 9.362357080035182e-05,
      "loss": 0.0081,
      "step": 2420
    },
    {
      "epoch": 1.5969656992084431,
      "grad_norm": 0.1531231701374054,
      "learning_rate": 9.357959542656113e-05,
      "loss": 0.013,
      "step": 2421
    },
    {
      "epoch": 1.5976253298153034,
      "grad_norm": 0.38211914896965027,
      "learning_rate": 9.353562005277046e-05,
      "loss": 0.0416,
      "step": 2422
    },
    {
      "epoch": 1.5982849604221636,
      "grad_norm": 0.09685737639665604,
      "learning_rate": 9.349164467897977e-05,
      "loss": 0.0088,
      "step": 2423
    },
    {
      "epoch": 1.5989445910290239,
      "grad_norm": 0.2785378396511078,
      "learning_rate": 9.34476693051891e-05,
      "loss": 0.0292,
      "step": 2424
    },
    {
      "epoch": 1.599604221635884,
      "grad_norm": 0.1274077147245407,
      "learning_rate": 9.340369393139841e-05,
      "loss": 0.0071,
      "step": 2425
    },
    {
      "epoch": 1.600263852242744,
      "grad_norm": 0.1712624728679657,
      "learning_rate": 9.335971855760774e-05,
      "loss": 0.0154,
      "step": 2426
    },
    {
      "epoch": 1.6009234828496042,
      "grad_norm": 0.18679924309253693,
      "learning_rate": 9.331574318381706e-05,
      "loss": 0.0265,
      "step": 2427
    },
    {
      "epoch": 1.6015831134564644,
      "grad_norm": 0.220694899559021,
      "learning_rate": 9.327176781002638e-05,
      "loss": 0.0261,
      "step": 2428
    },
    {
      "epoch": 1.6022427440633247,
      "grad_norm": 0.17641501128673553,
      "learning_rate": 9.322779243623571e-05,
      "loss": 0.0226,
      "step": 2429
    },
    {
      "epoch": 1.6029023746701847,
      "grad_norm": 0.19436576962471008,
      "learning_rate": 9.318381706244504e-05,
      "loss": 0.0184,
      "step": 2430
    },
    {
      "epoch": 1.6035620052770447,
      "grad_norm": 0.07702253758907318,
      "learning_rate": 9.313984168865437e-05,
      "loss": 0.0056,
      "step": 2431
    },
    {
      "epoch": 1.604221635883905,
      "grad_norm": 0.13205845654010773,
      "learning_rate": 9.309586631486368e-05,
      "loss": 0.0136,
      "step": 2432
    },
    {
      "epoch": 1.6048812664907652,
      "grad_norm": 0.25212258100509644,
      "learning_rate": 9.305189094107301e-05,
      "loss": 0.0426,
      "step": 2433
    },
    {
      "epoch": 1.6055408970976255,
      "grad_norm": 0.12453874200582504,
      "learning_rate": 9.300791556728232e-05,
      "loss": 0.0111,
      "step": 2434
    },
    {
      "epoch": 1.6062005277044855,
      "grad_norm": 0.2556183338165283,
      "learning_rate": 9.296394019349165e-05,
      "loss": 0.031,
      "step": 2435
    },
    {
      "epoch": 1.6068601583113455,
      "grad_norm": 0.08457767218351364,
      "learning_rate": 9.291996481970096e-05,
      "loss": 0.0048,
      "step": 2436
    },
    {
      "epoch": 1.6075197889182058,
      "grad_norm": 0.3328068256378174,
      "learning_rate": 9.287598944591029e-05,
      "loss": 0.0573,
      "step": 2437
    },
    {
      "epoch": 1.608179419525066,
      "grad_norm": 0.17725463211536407,
      "learning_rate": 9.283201407211962e-05,
      "loss": 0.0283,
      "step": 2438
    },
    {
      "epoch": 1.608839050131926,
      "grad_norm": 0.14336895942687988,
      "learning_rate": 9.278803869832894e-05,
      "loss": 0.0146,
      "step": 2439
    },
    {
      "epoch": 1.6094986807387863,
      "grad_norm": 0.032862767577171326,
      "learning_rate": 9.274406332453827e-05,
      "loss": 0.0014,
      "step": 2440
    },
    {
      "epoch": 1.6101583113456464,
      "grad_norm": 0.16879838705062866,
      "learning_rate": 9.270008795074759e-05,
      "loss": 0.0222,
      "step": 2441
    },
    {
      "epoch": 1.6108179419525066,
      "grad_norm": 0.12094852328300476,
      "learning_rate": 9.265611257695691e-05,
      "loss": 0.0111,
      "step": 2442
    },
    {
      "epoch": 1.6114775725593669,
      "grad_norm": 0.08645505458116531,
      "learning_rate": 9.261213720316623e-05,
      "loss": 0.0089,
      "step": 2443
    },
    {
      "epoch": 1.6121372031662269,
      "grad_norm": 0.24590373039245605,
      "learning_rate": 9.256816182937556e-05,
      "loss": 0.0254,
      "step": 2444
    },
    {
      "epoch": 1.612796833773087,
      "grad_norm": 0.12772251665592194,
      "learning_rate": 9.252418645558487e-05,
      "loss": 0.0136,
      "step": 2445
    },
    {
      "epoch": 1.6134564643799472,
      "grad_norm": 0.1371418535709381,
      "learning_rate": 9.24802110817942e-05,
      "loss": 0.0195,
      "step": 2446
    },
    {
      "epoch": 1.6141160949868074,
      "grad_norm": 0.2351863980293274,
      "learning_rate": 9.243623570800352e-05,
      "loss": 0.0448,
      "step": 2447
    },
    {
      "epoch": 1.6147757255936677,
      "grad_norm": 0.15415950119495392,
      "learning_rate": 9.239226033421284e-05,
      "loss": 0.0142,
      "step": 2448
    },
    {
      "epoch": 1.6154353562005277,
      "grad_norm": 0.25680750608444214,
      "learning_rate": 9.234828496042217e-05,
      "loss": 0.0336,
      "step": 2449
    },
    {
      "epoch": 1.6160949868073877,
      "grad_norm": 0.10691695660352707,
      "learning_rate": 9.23043095866315e-05,
      "loss": 0.0096,
      "step": 2450
    },
    {
      "epoch": 1.616754617414248,
      "grad_norm": 0.2808123230934143,
      "learning_rate": 9.226033421284082e-05,
      "loss": 0.0354,
      "step": 2451
    },
    {
      "epoch": 1.6174142480211082,
      "grad_norm": 0.18304264545440674,
      "learning_rate": 9.221635883905013e-05,
      "loss": 0.0303,
      "step": 2452
    },
    {
      "epoch": 1.6180738786279685,
      "grad_norm": 0.18066899478435516,
      "learning_rate": 9.217238346525946e-05,
      "loss": 0.0155,
      "step": 2453
    },
    {
      "epoch": 1.6187335092348285,
      "grad_norm": 0.2742932140827179,
      "learning_rate": 9.212840809146878e-05,
      "loss": 0.0266,
      "step": 2454
    },
    {
      "epoch": 1.6193931398416885,
      "grad_norm": 0.2835165858268738,
      "learning_rate": 9.20844327176781e-05,
      "loss": 0.0205,
      "step": 2455
    },
    {
      "epoch": 1.6200527704485488,
      "grad_norm": 0.13096566498279572,
      "learning_rate": 9.204045734388743e-05,
      "loss": 0.0185,
      "step": 2456
    },
    {
      "epoch": 1.620712401055409,
      "grad_norm": 0.17106422781944275,
      "learning_rate": 9.199648197009675e-05,
      "loss": 0.0238,
      "step": 2457
    },
    {
      "epoch": 1.6213720316622693,
      "grad_norm": 0.1575346738100052,
      "learning_rate": 9.195250659630607e-05,
      "loss": 0.0232,
      "step": 2458
    },
    {
      "epoch": 1.6220316622691293,
      "grad_norm": 0.19328495860099792,
      "learning_rate": 9.190853122251539e-05,
      "loss": 0.0116,
      "step": 2459
    },
    {
      "epoch": 1.6226912928759893,
      "grad_norm": 0.19412697851657867,
      "learning_rate": 9.186455584872471e-05,
      "loss": 0.0233,
      "step": 2460
    },
    {
      "epoch": 1.6233509234828496,
      "grad_norm": 0.2990759015083313,
      "learning_rate": 9.182058047493404e-05,
      "loss": 0.0435,
      "step": 2461
    },
    {
      "epoch": 1.6240105540897098,
      "grad_norm": 0.12441185861825943,
      "learning_rate": 9.177660510114337e-05,
      "loss": 0.0095,
      "step": 2462
    },
    {
      "epoch": 1.6246701846965699,
      "grad_norm": 0.2724146544933319,
      "learning_rate": 9.173262972735268e-05,
      "loss": 0.0312,
      "step": 2463
    },
    {
      "epoch": 1.6253298153034301,
      "grad_norm": 0.25934362411499023,
      "learning_rate": 9.168865435356201e-05,
      "loss": 0.0381,
      "step": 2464
    },
    {
      "epoch": 1.6259894459102902,
      "grad_norm": 0.15409910678863525,
      "learning_rate": 9.164467897977134e-05,
      "loss": 0.0179,
      "step": 2465
    },
    {
      "epoch": 1.6266490765171504,
      "grad_norm": 0.17487357556819916,
      "learning_rate": 9.160070360598065e-05,
      "loss": 0.0117,
      "step": 2466
    },
    {
      "epoch": 1.6273087071240107,
      "grad_norm": 0.14990665018558502,
      "learning_rate": 9.155672823218998e-05,
      "loss": 0.0076,
      "step": 2467
    },
    {
      "epoch": 1.6279683377308707,
      "grad_norm": 0.2992219626903534,
      "learning_rate": 9.15127528583993e-05,
      "loss": 0.018,
      "step": 2468
    },
    {
      "epoch": 1.6286279683377307,
      "grad_norm": 0.19507159292697906,
      "learning_rate": 9.146877748460862e-05,
      "loss": 0.0217,
      "step": 2469
    },
    {
      "epoch": 1.629287598944591,
      "grad_norm": 0.17453020811080933,
      "learning_rate": 9.142480211081795e-05,
      "loss": 0.0085,
      "step": 2470
    },
    {
      "epoch": 1.6299472295514512,
      "grad_norm": 0.2180546224117279,
      "learning_rate": 9.138082673702728e-05,
      "loss": 0.0187,
      "step": 2471
    },
    {
      "epoch": 1.6306068601583115,
      "grad_norm": 0.15931127965450287,
      "learning_rate": 9.13368513632366e-05,
      "loss": 0.0112,
      "step": 2472
    },
    {
      "epoch": 1.6312664907651715,
      "grad_norm": 0.1414521187543869,
      "learning_rate": 9.129287598944592e-05,
      "loss": 0.0115,
      "step": 2473
    },
    {
      "epoch": 1.6319261213720315,
      "grad_norm": 0.1443435251712799,
      "learning_rate": 9.124890061565525e-05,
      "loss": 0.0171,
      "step": 2474
    },
    {
      "epoch": 1.6325857519788918,
      "grad_norm": 0.21710824966430664,
      "learning_rate": 9.120492524186456e-05,
      "loss": 0.0194,
      "step": 2475
    },
    {
      "epoch": 1.633245382585752,
      "grad_norm": 0.21221409738063812,
      "learning_rate": 9.116094986807389e-05,
      "loss": 0.0284,
      "step": 2476
    },
    {
      "epoch": 1.6339050131926123,
      "grad_norm": 0.05927341803908348,
      "learning_rate": 9.11169744942832e-05,
      "loss": 0.0047,
      "step": 2477
    },
    {
      "epoch": 1.6345646437994723,
      "grad_norm": 0.16572999954223633,
      "learning_rate": 9.107299912049253e-05,
      "loss": 0.0184,
      "step": 2478
    },
    {
      "epoch": 1.6352242744063323,
      "grad_norm": 0.32710766792297363,
      "learning_rate": 9.102902374670184e-05,
      "loss": 0.0358,
      "step": 2479
    },
    {
      "epoch": 1.6358839050131926,
      "grad_norm": 0.1356254667043686,
      "learning_rate": 9.098504837291117e-05,
      "loss": 0.0112,
      "step": 2480
    },
    {
      "epoch": 1.6365435356200528,
      "grad_norm": 0.08299818634986877,
      "learning_rate": 9.09410729991205e-05,
      "loss": 0.0063,
      "step": 2481
    },
    {
      "epoch": 1.637203166226913,
      "grad_norm": 0.17409437894821167,
      "learning_rate": 9.089709762532982e-05,
      "loss": 0.0221,
      "step": 2482
    },
    {
      "epoch": 1.6378627968337731,
      "grad_norm": 0.15554475784301758,
      "learning_rate": 9.085312225153915e-05,
      "loss": 0.0071,
      "step": 2483
    },
    {
      "epoch": 1.6385224274406331,
      "grad_norm": 0.24480687081813812,
      "learning_rate": 9.080914687774847e-05,
      "loss": 0.0337,
      "step": 2484
    },
    {
      "epoch": 1.6391820580474934,
      "grad_norm": 0.27721381187438965,
      "learning_rate": 9.07651715039578e-05,
      "loss": 0.0391,
      "step": 2485
    },
    {
      "epoch": 1.6398416886543536,
      "grad_norm": 0.26980969309806824,
      "learning_rate": 9.072119613016711e-05,
      "loss": 0.0177,
      "step": 2486
    },
    {
      "epoch": 1.6405013192612137,
      "grad_norm": 0.3072410225868225,
      "learning_rate": 9.067722075637644e-05,
      "loss": 0.026,
      "step": 2487
    },
    {
      "epoch": 1.641160949868074,
      "grad_norm": 0.15684105455875397,
      "learning_rate": 9.063324538258575e-05,
      "loss": 0.0122,
      "step": 2488
    },
    {
      "epoch": 1.641820580474934,
      "grad_norm": 0.28842782974243164,
      "learning_rate": 9.058927000879508e-05,
      "loss": 0.0337,
      "step": 2489
    },
    {
      "epoch": 1.6424802110817942,
      "grad_norm": 0.2787590026855469,
      "learning_rate": 9.05452946350044e-05,
      "loss": 0.0408,
      "step": 2490
    },
    {
      "epoch": 1.6431398416886545,
      "grad_norm": 0.2202538549900055,
      "learning_rate": 9.050131926121372e-05,
      "loss": 0.0393,
      "step": 2491
    },
    {
      "epoch": 1.6437994722955145,
      "grad_norm": 0.2616989314556122,
      "learning_rate": 9.045734388742305e-05,
      "loss": 0.0438,
      "step": 2492
    },
    {
      "epoch": 1.6444591029023745,
      "grad_norm": 0.40169280767440796,
      "learning_rate": 9.041336851363237e-05,
      "loss": 0.0261,
      "step": 2493
    },
    {
      "epoch": 1.6451187335092348,
      "grad_norm": 0.18325789272785187,
      "learning_rate": 9.03693931398417e-05,
      "loss": 0.0181,
      "step": 2494
    },
    {
      "epoch": 1.645778364116095,
      "grad_norm": 0.30653926730155945,
      "learning_rate": 9.032541776605101e-05,
      "loss": 0.0204,
      "step": 2495
    },
    {
      "epoch": 1.6464379947229553,
      "grad_norm": 0.21604257822036743,
      "learning_rate": 9.028144239226034e-05,
      "loss": 0.0209,
      "step": 2496
    },
    {
      "epoch": 1.6470976253298153,
      "grad_norm": 0.21949926018714905,
      "learning_rate": 9.023746701846966e-05,
      "loss": 0.0217,
      "step": 2497
    },
    {
      "epoch": 1.6477572559366753,
      "grad_norm": 0.22029465436935425,
      "learning_rate": 9.019349164467898e-05,
      "loss": 0.0168,
      "step": 2498
    },
    {
      "epoch": 1.6484168865435356,
      "grad_norm": 0.22088362276554108,
      "learning_rate": 9.014951627088831e-05,
      "loss": 0.0266,
      "step": 2499
    },
    {
      "epoch": 1.6490765171503958,
      "grad_norm": 0.17147399485111237,
      "learning_rate": 9.010554089709763e-05,
      "loss": 0.0362,
      "step": 2500
    },
    {
      "epoch": 1.649736147757256,
      "grad_norm": 0.3542076647281647,
      "learning_rate": 9.006156552330695e-05,
      "loss": 0.0554,
      "step": 2501
    },
    {
      "epoch": 1.650395778364116,
      "grad_norm": 0.060558099299669266,
      "learning_rate": 9.001759014951627e-05,
      "loss": 0.005,
      "step": 2502
    },
    {
      "epoch": 1.6510554089709761,
      "grad_norm": 0.24234050512313843,
      "learning_rate": 8.997361477572561e-05,
      "loss": 0.0317,
      "step": 2503
    },
    {
      "epoch": 1.6517150395778364,
      "grad_norm": 0.24386201798915863,
      "learning_rate": 8.992963940193492e-05,
      "loss": 0.0241,
      "step": 2504
    },
    {
      "epoch": 1.6523746701846966,
      "grad_norm": 0.0645606741309166,
      "learning_rate": 8.988566402814425e-05,
      "loss": 0.0031,
      "step": 2505
    },
    {
      "epoch": 1.6530343007915569,
      "grad_norm": 0.09330610930919647,
      "learning_rate": 8.984168865435356e-05,
      "loss": 0.0084,
      "step": 2506
    },
    {
      "epoch": 1.653693931398417,
      "grad_norm": 0.13521961867809296,
      "learning_rate": 8.979771328056289e-05,
      "loss": 0.0133,
      "step": 2507
    },
    {
      "epoch": 1.654353562005277,
      "grad_norm": 0.1947251856327057,
      "learning_rate": 8.975373790677222e-05,
      "loss": 0.0267,
      "step": 2508
    },
    {
      "epoch": 1.6550131926121372,
      "grad_norm": 0.029748784378170967,
      "learning_rate": 8.970976253298153e-05,
      "loss": 0.0015,
      "step": 2509
    },
    {
      "epoch": 1.6556728232189974,
      "grad_norm": 0.09827499091625214,
      "learning_rate": 8.966578715919086e-05,
      "loss": 0.0167,
      "step": 2510
    },
    {
      "epoch": 1.6563324538258575,
      "grad_norm": 0.1781948059797287,
      "learning_rate": 8.962181178540017e-05,
      "loss": 0.0268,
      "step": 2511
    },
    {
      "epoch": 1.6569920844327177,
      "grad_norm": 0.2827456593513489,
      "learning_rate": 8.95778364116095e-05,
      "loss": 0.0337,
      "step": 2512
    },
    {
      "epoch": 1.6576517150395778,
      "grad_norm": 0.30435171723365784,
      "learning_rate": 8.953386103781883e-05,
      "loss": 0.0382,
      "step": 2513
    },
    {
      "epoch": 1.658311345646438,
      "grad_norm": 0.2703988254070282,
      "learning_rate": 8.948988566402816e-05,
      "loss": 0.049,
      "step": 2514
    },
    {
      "epoch": 1.6589709762532983,
      "grad_norm": 0.10415419936180115,
      "learning_rate": 8.944591029023747e-05,
      "loss": 0.0115,
      "step": 2515
    },
    {
      "epoch": 1.6596306068601583,
      "grad_norm": 0.1338416039943695,
      "learning_rate": 8.94019349164468e-05,
      "loss": 0.0188,
      "step": 2516
    },
    {
      "epoch": 1.6602902374670183,
      "grad_norm": 0.19127210974693298,
      "learning_rate": 8.935795954265613e-05,
      "loss": 0.0242,
      "step": 2517
    },
    {
      "epoch": 1.6609498680738786,
      "grad_norm": 0.30568549036979675,
      "learning_rate": 8.931398416886544e-05,
      "loss": 0.0444,
      "step": 2518
    },
    {
      "epoch": 1.6616094986807388,
      "grad_norm": 0.19486220180988312,
      "learning_rate": 8.927000879507477e-05,
      "loss": 0.0205,
      "step": 2519
    },
    {
      "epoch": 1.662269129287599,
      "grad_norm": 0.13209877908229828,
      "learning_rate": 8.922603342128408e-05,
      "loss": 0.0173,
      "step": 2520
    },
    {
      "epoch": 1.662928759894459,
      "grad_norm": 0.23671141266822815,
      "learning_rate": 8.918205804749341e-05,
      "loss": 0.0188,
      "step": 2521
    },
    {
      "epoch": 1.6635883905013191,
      "grad_norm": 0.2636582851409912,
      "learning_rate": 8.913808267370272e-05,
      "loss": 0.0184,
      "step": 2522
    },
    {
      "epoch": 1.6642480211081794,
      "grad_norm": 0.18454515933990479,
      "learning_rate": 8.909410729991205e-05,
      "loss": 0.0255,
      "step": 2523
    },
    {
      "epoch": 1.6649076517150396,
      "grad_norm": 0.11524981260299683,
      "learning_rate": 8.905013192612138e-05,
      "loss": 0.0068,
      "step": 2524
    },
    {
      "epoch": 1.6655672823218999,
      "grad_norm": 0.21708208322525024,
      "learning_rate": 8.90061565523307e-05,
      "loss": 0.0425,
      "step": 2525
    },
    {
      "epoch": 1.66622691292876,
      "grad_norm": 0.15205563604831696,
      "learning_rate": 8.896218117854003e-05,
      "loss": 0.0204,
      "step": 2526
    },
    {
      "epoch": 1.66688654353562,
      "grad_norm": 0.10449889302253723,
      "learning_rate": 8.891820580474935e-05,
      "loss": 0.0112,
      "step": 2527
    },
    {
      "epoch": 1.6675461741424802,
      "grad_norm": 0.07151570171117783,
      "learning_rate": 8.887423043095867e-05,
      "loss": 0.0075,
      "step": 2528
    },
    {
      "epoch": 1.6682058047493404,
      "grad_norm": 0.14357799291610718,
      "learning_rate": 8.883025505716799e-05,
      "loss": 0.0211,
      "step": 2529
    },
    {
      "epoch": 1.6688654353562007,
      "grad_norm": 0.1806122213602066,
      "learning_rate": 8.878627968337731e-05,
      "loss": 0.0136,
      "step": 2530
    },
    {
      "epoch": 1.6695250659630607,
      "grad_norm": 0.14971113204956055,
      "learning_rate": 8.874230430958663e-05,
      "loss": 0.0107,
      "step": 2531
    },
    {
      "epoch": 1.6701846965699207,
      "grad_norm": 0.1542816311120987,
      "learning_rate": 8.869832893579596e-05,
      "loss": 0.0119,
      "step": 2532
    },
    {
      "epoch": 1.670844327176781,
      "grad_norm": 0.11693241447210312,
      "learning_rate": 8.865435356200527e-05,
      "loss": 0.0126,
      "step": 2533
    },
    {
      "epoch": 1.6715039577836412,
      "grad_norm": 0.24900610744953156,
      "learning_rate": 8.86103781882146e-05,
      "loss": 0.029,
      "step": 2534
    },
    {
      "epoch": 1.6721635883905013,
      "grad_norm": 0.17235170304775238,
      "learning_rate": 8.856640281442393e-05,
      "loss": 0.0181,
      "step": 2535
    },
    {
      "epoch": 1.6728232189973615,
      "grad_norm": 0.08578512817621231,
      "learning_rate": 8.852242744063325e-05,
      "loss": 0.0079,
      "step": 2536
    },
    {
      "epoch": 1.6734828496042216,
      "grad_norm": 0.1005692407488823,
      "learning_rate": 8.847845206684258e-05,
      "loss": 0.01,
      "step": 2537
    },
    {
      "epoch": 1.6741424802110818,
      "grad_norm": 0.1619136482477188,
      "learning_rate": 8.84344766930519e-05,
      "loss": 0.0156,
      "step": 2538
    },
    {
      "epoch": 1.674802110817942,
      "grad_norm": 0.17582276463508606,
      "learning_rate": 8.839050131926122e-05,
      "loss": 0.0393,
      "step": 2539
    },
    {
      "epoch": 1.675461741424802,
      "grad_norm": 0.1249421164393425,
      "learning_rate": 8.834652594547054e-05,
      "loss": 0.0089,
      "step": 2540
    },
    {
      "epoch": 1.6761213720316621,
      "grad_norm": 0.030835432931780815,
      "learning_rate": 8.830255057167986e-05,
      "loss": 0.0014,
      "step": 2541
    },
    {
      "epoch": 1.6767810026385224,
      "grad_norm": 0.23703649640083313,
      "learning_rate": 8.825857519788918e-05,
      "loss": 0.0161,
      "step": 2542
    },
    {
      "epoch": 1.6774406332453826,
      "grad_norm": 0.36425232887268066,
      "learning_rate": 8.82145998240985e-05,
      "loss": 0.0365,
      "step": 2543
    },
    {
      "epoch": 1.6781002638522429,
      "grad_norm": 0.32191723585128784,
      "learning_rate": 8.817062445030783e-05,
      "loss": 0.0172,
      "step": 2544
    },
    {
      "epoch": 1.678759894459103,
      "grad_norm": 0.05050342530012131,
      "learning_rate": 8.812664907651716e-05,
      "loss": 0.0029,
      "step": 2545
    },
    {
      "epoch": 1.679419525065963,
      "grad_norm": 0.2261616736650467,
      "learning_rate": 8.808267370272649e-05,
      "loss": 0.0277,
      "step": 2546
    },
    {
      "epoch": 1.6800791556728232,
      "grad_norm": 0.098316490650177,
      "learning_rate": 8.80386983289358e-05,
      "loss": 0.0062,
      "step": 2547
    },
    {
      "epoch": 1.6807387862796834,
      "grad_norm": 0.13162048161029816,
      "learning_rate": 8.799472295514513e-05,
      "loss": 0.0062,
      "step": 2548
    },
    {
      "epoch": 1.6813984168865437,
      "grad_norm": 0.1425958126783371,
      "learning_rate": 8.795074758135444e-05,
      "loss": 0.0243,
      "step": 2549
    },
    {
      "epoch": 1.6820580474934037,
      "grad_norm": 0.34405651688575745,
      "learning_rate": 8.790677220756377e-05,
      "loss": 0.0504,
      "step": 2550
    },
    {
      "epoch": 1.6827176781002637,
      "grad_norm": 0.20269285142421722,
      "learning_rate": 8.786279683377308e-05,
      "loss": 0.0248,
      "step": 2551
    },
    {
      "epoch": 1.683377308707124,
      "grad_norm": 0.2122715711593628,
      "learning_rate": 8.781882145998241e-05,
      "loss": 0.0157,
      "step": 2552
    },
    {
      "epoch": 1.6840369393139842,
      "grad_norm": 0.1344209909439087,
      "learning_rate": 8.777484608619174e-05,
      "loss": 0.0125,
      "step": 2553
    },
    {
      "epoch": 1.6846965699208445,
      "grad_norm": 0.1416677087545395,
      "learning_rate": 8.773087071240105e-05,
      "loss": 0.0176,
      "step": 2554
    },
    {
      "epoch": 1.6853562005277045,
      "grad_norm": 0.16399234533309937,
      "learning_rate": 8.768689533861038e-05,
      "loss": 0.015,
      "step": 2555
    },
    {
      "epoch": 1.6860158311345645,
      "grad_norm": 0.13055072724819183,
      "learning_rate": 8.764291996481971e-05,
      "loss": 0.01,
      "step": 2556
    },
    {
      "epoch": 1.6866754617414248,
      "grad_norm": 0.4798520505428314,
      "learning_rate": 8.759894459102904e-05,
      "loss": 0.0528,
      "step": 2557
    },
    {
      "epoch": 1.687335092348285,
      "grad_norm": 0.3657957911491394,
      "learning_rate": 8.755496921723835e-05,
      "loss": 0.0317,
      "step": 2558
    },
    {
      "epoch": 1.687994722955145,
      "grad_norm": 0.2342081069946289,
      "learning_rate": 8.751099384344768e-05,
      "loss": 0.0265,
      "step": 2559
    },
    {
      "epoch": 1.6886543535620053,
      "grad_norm": 0.18572081625461578,
      "learning_rate": 8.746701846965699e-05,
      "loss": 0.0188,
      "step": 2560
    },
    {
      "epoch": 1.6893139841688654,
      "grad_norm": 0.2432936131954193,
      "learning_rate": 8.742304309586632e-05,
      "loss": 0.0258,
      "step": 2561
    },
    {
      "epoch": 1.6899736147757256,
      "grad_norm": 0.20999126136302948,
      "learning_rate": 8.737906772207565e-05,
      "loss": 0.0317,
      "step": 2562
    },
    {
      "epoch": 1.6906332453825859,
      "grad_norm": 0.26751068234443665,
      "learning_rate": 8.733509234828496e-05,
      "loss": 0.0373,
      "step": 2563
    },
    {
      "epoch": 1.6912928759894459,
      "grad_norm": 0.16772693395614624,
      "learning_rate": 8.729111697449429e-05,
      "loss": 0.0116,
      "step": 2564
    },
    {
      "epoch": 1.691952506596306,
      "grad_norm": 0.2939163148403168,
      "learning_rate": 8.72471416007036e-05,
      "loss": 0.031,
      "step": 2565
    },
    {
      "epoch": 1.6926121372031662,
      "grad_norm": 0.28378424048423767,
      "learning_rate": 8.720316622691293e-05,
      "loss": 0.0479,
      "step": 2566
    },
    {
      "epoch": 1.6932717678100264,
      "grad_norm": 0.18093249201774597,
      "learning_rate": 8.715919085312226e-05,
      "loss": 0.0238,
      "step": 2567
    },
    {
      "epoch": 1.6939313984168867,
      "grad_norm": 0.3060324192047119,
      "learning_rate": 8.711521547933158e-05,
      "loss": 0.037,
      "step": 2568
    },
    {
      "epoch": 1.6945910290237467,
      "grad_norm": 0.23163263499736786,
      "learning_rate": 8.70712401055409e-05,
      "loss": 0.0189,
      "step": 2569
    },
    {
      "epoch": 1.6952506596306067,
      "grad_norm": 0.1299501657485962,
      "learning_rate": 8.702726473175023e-05,
      "loss": 0.0136,
      "step": 2570
    },
    {
      "epoch": 1.695910290237467,
      "grad_norm": 0.14010745286941528,
      "learning_rate": 8.698328935795955e-05,
      "loss": 0.0135,
      "step": 2571
    },
    {
      "epoch": 1.6965699208443272,
      "grad_norm": 0.2746185064315796,
      "learning_rate": 8.693931398416887e-05,
      "loss": 0.0143,
      "step": 2572
    },
    {
      "epoch": 1.6972295514511875,
      "grad_norm": 0.18429292738437653,
      "learning_rate": 8.68953386103782e-05,
      "loss": 0.0206,
      "step": 2573
    },
    {
      "epoch": 1.6978891820580475,
      "grad_norm": 0.2589872479438782,
      "learning_rate": 8.685136323658751e-05,
      "loss": 0.0215,
      "step": 2574
    },
    {
      "epoch": 1.6985488126649075,
      "grad_norm": 0.34432294964790344,
      "learning_rate": 8.680738786279684e-05,
      "loss": 0.0542,
      "step": 2575
    },
    {
      "epoch": 1.6992084432717678,
      "grad_norm": 0.1719633936882019,
      "learning_rate": 8.676341248900616e-05,
      "loss": 0.0138,
      "step": 2576
    },
    {
      "epoch": 1.699868073878628,
      "grad_norm": 0.23892416059970856,
      "learning_rate": 8.671943711521549e-05,
      "loss": 0.0363,
      "step": 2577
    },
    {
      "epoch": 1.7005277044854883,
      "grad_norm": 0.09841315448284149,
      "learning_rate": 8.66754617414248e-05,
      "loss": 0.0068,
      "step": 2578
    },
    {
      "epoch": 1.7011873350923483,
      "grad_norm": 0.20702238380908966,
      "learning_rate": 8.663148636763413e-05,
      "loss": 0.0233,
      "step": 2579
    },
    {
      "epoch": 1.7018469656992083,
      "grad_norm": 0.22870725393295288,
      "learning_rate": 8.658751099384346e-05,
      "loss": 0.0223,
      "step": 2580
    },
    {
      "epoch": 1.7025065963060686,
      "grad_norm": 0.236893430352211,
      "learning_rate": 8.654353562005277e-05,
      "loss": 0.0342,
      "step": 2581
    },
    {
      "epoch": 1.7031662269129288,
      "grad_norm": 0.10825249552726746,
      "learning_rate": 8.64995602462621e-05,
      "loss": 0.0136,
      "step": 2582
    },
    {
      "epoch": 1.7038258575197889,
      "grad_norm": 0.2752154767513275,
      "learning_rate": 8.645558487247142e-05,
      "loss": 0.0368,
      "step": 2583
    },
    {
      "epoch": 1.7044854881266491,
      "grad_norm": 0.13759967684745789,
      "learning_rate": 8.641160949868074e-05,
      "loss": 0.0121,
      "step": 2584
    },
    {
      "epoch": 1.7051451187335092,
      "grad_norm": 0.12623171508312225,
      "learning_rate": 8.636763412489006e-05,
      "loss": 0.016,
      "step": 2585
    },
    {
      "epoch": 1.7058047493403694,
      "grad_norm": 0.10019439458847046,
      "learning_rate": 8.632365875109938e-05,
      "loss": 0.0124,
      "step": 2586
    },
    {
      "epoch": 1.7064643799472297,
      "grad_norm": 0.17447827756404877,
      "learning_rate": 8.627968337730871e-05,
      "loss": 0.0122,
      "step": 2587
    },
    {
      "epoch": 1.7071240105540897,
      "grad_norm": 0.230885311961174,
      "learning_rate": 8.623570800351804e-05,
      "loss": 0.0384,
      "step": 2588
    },
    {
      "epoch": 1.7077836411609497,
      "grad_norm": 0.212541401386261,
      "learning_rate": 8.619173262972737e-05,
      "loss": 0.0306,
      "step": 2589
    },
    {
      "epoch": 1.70844327176781,
      "grad_norm": 0.21659544110298157,
      "learning_rate": 8.614775725593668e-05,
      "loss": 0.0246,
      "step": 2590
    },
    {
      "epoch": 1.7091029023746702,
      "grad_norm": 0.12420065701007843,
      "learning_rate": 8.610378188214601e-05,
      "loss": 0.0095,
      "step": 2591
    },
    {
      "epoch": 1.7097625329815305,
      "grad_norm": 0.1579732447862625,
      "learning_rate": 8.605980650835532e-05,
      "loss": 0.0184,
      "step": 2592
    },
    {
      "epoch": 1.7104221635883905,
      "grad_norm": 0.23067699372768402,
      "learning_rate": 8.601583113456465e-05,
      "loss": 0.0368,
      "step": 2593
    },
    {
      "epoch": 1.7110817941952505,
      "grad_norm": 0.2660946547985077,
      "learning_rate": 8.597185576077396e-05,
      "loss": 0.0441,
      "step": 2594
    },
    {
      "epoch": 1.7117414248021108,
      "grad_norm": 0.18903306126594543,
      "learning_rate": 8.592788038698329e-05,
      "loss": 0.0198,
      "step": 2595
    },
    {
      "epoch": 1.712401055408971,
      "grad_norm": 0.17342151701450348,
      "learning_rate": 8.58839050131926e-05,
      "loss": 0.0278,
      "step": 2596
    },
    {
      "epoch": 1.7130606860158313,
      "grad_norm": 0.08033183962106705,
      "learning_rate": 8.583992963940193e-05,
      "loss": 0.0076,
      "step": 2597
    },
    {
      "epoch": 1.7137203166226913,
      "grad_norm": 0.13227680325508118,
      "learning_rate": 8.579595426561126e-05,
      "loss": 0.0149,
      "step": 2598
    },
    {
      "epoch": 1.7143799472295513,
      "grad_norm": 0.0832870751619339,
      "learning_rate": 8.575197889182059e-05,
      "loss": 0.008,
      "step": 2599
    },
    {
      "epoch": 1.7150395778364116,
      "grad_norm": 0.08899569511413574,
      "learning_rate": 8.570800351802992e-05,
      "loss": 0.0085,
      "step": 2600
    },
    {
      "epoch": 1.7156992084432718,
      "grad_norm": 0.12190728634595871,
      "learning_rate": 8.566402814423923e-05,
      "loss": 0.0167,
      "step": 2601
    },
    {
      "epoch": 1.716358839050132,
      "grad_norm": 0.15095633268356323,
      "learning_rate": 8.562005277044856e-05,
      "loss": 0.0229,
      "step": 2602
    },
    {
      "epoch": 1.7170184696569921,
      "grad_norm": 0.15428988635540009,
      "learning_rate": 8.557607739665787e-05,
      "loss": 0.0125,
      "step": 2603
    },
    {
      "epoch": 1.7176781002638521,
      "grad_norm": 0.14005666971206665,
      "learning_rate": 8.55321020228672e-05,
      "loss": 0.0172,
      "step": 2604
    },
    {
      "epoch": 1.7183377308707124,
      "grad_norm": 0.09253767877817154,
      "learning_rate": 8.548812664907651e-05,
      "loss": 0.0123,
      "step": 2605
    },
    {
      "epoch": 1.7189973614775726,
      "grad_norm": 0.10263921320438385,
      "learning_rate": 8.544415127528584e-05,
      "loss": 0.0072,
      "step": 2606
    },
    {
      "epoch": 1.7196569920844327,
      "grad_norm": 0.33513954281806946,
      "learning_rate": 8.540017590149517e-05,
      "loss": 0.0548,
      "step": 2607
    },
    {
      "epoch": 1.720316622691293,
      "grad_norm": 0.13675129413604736,
      "learning_rate": 8.53562005277045e-05,
      "loss": 0.0163,
      "step": 2608
    },
    {
      "epoch": 1.720976253298153,
      "grad_norm": 0.26511457562446594,
      "learning_rate": 8.531222515391382e-05,
      "loss": 0.0188,
      "step": 2609
    },
    {
      "epoch": 1.7216358839050132,
      "grad_norm": 0.16507256031036377,
      "learning_rate": 8.526824978012314e-05,
      "loss": 0.0153,
      "step": 2610
    },
    {
      "epoch": 1.7222955145118735,
      "grad_norm": 0.16853970289230347,
      "learning_rate": 8.522427440633246e-05,
      "loss": 0.0167,
      "step": 2611
    },
    {
      "epoch": 1.7229551451187335,
      "grad_norm": 0.17806614935398102,
      "learning_rate": 8.518029903254178e-05,
      "loss": 0.0215,
      "step": 2612
    },
    {
      "epoch": 1.7236147757255935,
      "grad_norm": 1.1784781217575073,
      "learning_rate": 8.51363236587511e-05,
      "loss": 0.0431,
      "step": 2613
    },
    {
      "epoch": 1.7242744063324538,
      "grad_norm": 0.11643394827842712,
      "learning_rate": 8.509234828496042e-05,
      "loss": 0.0115,
      "step": 2614
    },
    {
      "epoch": 1.724934036939314,
      "grad_norm": 0.11151441931724548,
      "learning_rate": 8.504837291116975e-05,
      "loss": 0.0126,
      "step": 2615
    },
    {
      "epoch": 1.7255936675461743,
      "grad_norm": 0.10203991830348969,
      "learning_rate": 8.500439753737907e-05,
      "loss": 0.0054,
      "step": 2616
    },
    {
      "epoch": 1.7262532981530343,
      "grad_norm": 0.1367480307817459,
      "learning_rate": 8.496042216358839e-05,
      "loss": 0.0117,
      "step": 2617
    },
    {
      "epoch": 1.7269129287598943,
      "grad_norm": 0.11539769917726517,
      "learning_rate": 8.491644678979772e-05,
      "loss": 0.0115,
      "step": 2618
    },
    {
      "epoch": 1.7275725593667546,
      "grad_norm": 0.12177950143814087,
      "learning_rate": 8.487247141600704e-05,
      "loss": 0.0174,
      "step": 2619
    },
    {
      "epoch": 1.7282321899736148,
      "grad_norm": 0.13036218285560608,
      "learning_rate": 8.482849604221637e-05,
      "loss": 0.0093,
      "step": 2620
    },
    {
      "epoch": 1.728891820580475,
      "grad_norm": 0.13206712901592255,
      "learning_rate": 8.478452066842569e-05,
      "loss": 0.013,
      "step": 2621
    },
    {
      "epoch": 1.729551451187335,
      "grad_norm": 0.3279782831668854,
      "learning_rate": 8.474054529463501e-05,
      "loss": 0.0265,
      "step": 2622
    },
    {
      "epoch": 1.7302110817941951,
      "grad_norm": 0.23605069518089294,
      "learning_rate": 8.469656992084433e-05,
      "loss": 0.0225,
      "step": 2623
    },
    {
      "epoch": 1.7308707124010554,
      "grad_norm": 0.07838115841150284,
      "learning_rate": 8.465259454705365e-05,
      "loss": 0.0057,
      "step": 2624
    },
    {
      "epoch": 1.7315303430079156,
      "grad_norm": 0.10387418419122696,
      "learning_rate": 8.460861917326298e-05,
      "loss": 0.0085,
      "step": 2625
    },
    {
      "epoch": 1.732189973614776,
      "grad_norm": 0.10958954691886902,
      "learning_rate": 8.45646437994723e-05,
      "loss": 0.0104,
      "step": 2626
    },
    {
      "epoch": 1.732849604221636,
      "grad_norm": 0.10486926883459091,
      "learning_rate": 8.452066842568162e-05,
      "loss": 0.0072,
      "step": 2627
    },
    {
      "epoch": 1.733509234828496,
      "grad_norm": 0.6256232261657715,
      "learning_rate": 8.447669305189094e-05,
      "loss": 0.0338,
      "step": 2628
    },
    {
      "epoch": 1.7341688654353562,
      "grad_norm": 0.1569070816040039,
      "learning_rate": 8.443271767810026e-05,
      "loss": 0.0136,
      "step": 2629
    },
    {
      "epoch": 1.7348284960422165,
      "grad_norm": 0.3481392562389374,
      "learning_rate": 8.438874230430959e-05,
      "loss": 0.0489,
      "step": 2630
    },
    {
      "epoch": 1.7354881266490765,
      "grad_norm": 0.3268278241157532,
      "learning_rate": 8.434476693051892e-05,
      "loss": 0.0349,
      "step": 2631
    },
    {
      "epoch": 1.7361477572559367,
      "grad_norm": 0.25478312373161316,
      "learning_rate": 8.430079155672823e-05,
      "loss": 0.0247,
      "step": 2632
    },
    {
      "epoch": 1.7368073878627968,
      "grad_norm": 0.22996027767658234,
      "learning_rate": 8.425681618293756e-05,
      "loss": 0.0299,
      "step": 2633
    },
    {
      "epoch": 1.737467018469657,
      "grad_norm": 0.11592461168766022,
      "learning_rate": 8.421284080914689e-05,
      "loss": 0.0119,
      "step": 2634
    },
    {
      "epoch": 1.7381266490765173,
      "grad_norm": 0.24483145773410797,
      "learning_rate": 8.41688654353562e-05,
      "loss": 0.019,
      "step": 2635
    },
    {
      "epoch": 1.7387862796833773,
      "grad_norm": 0.18991951644420624,
      "learning_rate": 8.412489006156553e-05,
      "loss": 0.0096,
      "step": 2636
    },
    {
      "epoch": 1.7394459102902373,
      "grad_norm": 0.16778874397277832,
      "learning_rate": 8.408091468777484e-05,
      "loss": 0.0114,
      "step": 2637
    },
    {
      "epoch": 1.7401055408970976,
      "grad_norm": 0.28546229004859924,
      "learning_rate": 8.403693931398417e-05,
      "loss": 0.0281,
      "step": 2638
    },
    {
      "epoch": 1.7407651715039578,
      "grad_norm": 0.31752729415893555,
      "learning_rate": 8.399296394019349e-05,
      "loss": 0.0235,
      "step": 2639
    },
    {
      "epoch": 1.741424802110818,
      "grad_norm": 0.16414791345596313,
      "learning_rate": 8.394898856640281e-05,
      "loss": 0.0111,
      "step": 2640
    },
    {
      "epoch": 1.742084432717678,
      "grad_norm": 0.11975624412298203,
      "learning_rate": 8.390501319261214e-05,
      "loss": 0.0068,
      "step": 2641
    },
    {
      "epoch": 1.7427440633245381,
      "grad_norm": 0.16998395323753357,
      "learning_rate": 8.386103781882147e-05,
      "loss": 0.0127,
      "step": 2642
    },
    {
      "epoch": 1.7434036939313984,
      "grad_norm": 0.12988309562206268,
      "learning_rate": 8.38170624450308e-05,
      "loss": 0.013,
      "step": 2643
    },
    {
      "epoch": 1.7440633245382586,
      "grad_norm": 0.1401234120130539,
      "learning_rate": 8.377308707124011e-05,
      "loss": 0.0124,
      "step": 2644
    },
    {
      "epoch": 1.7447229551451189,
      "grad_norm": 0.14594095945358276,
      "learning_rate": 8.372911169744944e-05,
      "loss": 0.0098,
      "step": 2645
    },
    {
      "epoch": 1.745382585751979,
      "grad_norm": 0.08739394694566727,
      "learning_rate": 8.368513632365875e-05,
      "loss": 0.0027,
      "step": 2646
    },
    {
      "epoch": 1.746042216358839,
      "grad_norm": 0.22884057462215424,
      "learning_rate": 8.364116094986808e-05,
      "loss": 0.0311,
      "step": 2647
    },
    {
      "epoch": 1.7467018469656992,
      "grad_norm": 0.2535199224948883,
      "learning_rate": 8.359718557607739e-05,
      "loss": 0.0221,
      "step": 2648
    },
    {
      "epoch": 1.7473614775725594,
      "grad_norm": 0.34043630957603455,
      "learning_rate": 8.355321020228672e-05,
      "loss": 0.0314,
      "step": 2649
    },
    {
      "epoch": 1.7480211081794197,
      "grad_norm": 0.13010981678962708,
      "learning_rate": 8.350923482849605e-05,
      "loss": 0.0063,
      "step": 2650
    },
    {
      "epoch": 1.7486807387862797,
      "grad_norm": 0.38096338510513306,
      "learning_rate": 8.346525945470538e-05,
      "loss": 0.0313,
      "step": 2651
    },
    {
      "epoch": 1.7493403693931397,
      "grad_norm": 0.19104844331741333,
      "learning_rate": 8.34212840809147e-05,
      "loss": 0.0156,
      "step": 2652
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.20591527223587036,
      "learning_rate": 8.337730870712402e-05,
      "loss": 0.0139,
      "step": 2653
    },
    {
      "epoch": 1.7506596306068603,
      "grad_norm": 0.3037547469139099,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.0197,
      "step": 2654
    },
    {
      "epoch": 1.7513192612137203,
      "grad_norm": 0.23967167735099792,
      "learning_rate": 8.328935795954266e-05,
      "loss": 0.0119,
      "step": 2655
    },
    {
      "epoch": 1.7519788918205803,
      "grad_norm": 0.1312522292137146,
      "learning_rate": 8.324538258575199e-05,
      "loss": 0.0078,
      "step": 2656
    },
    {
      "epoch": 1.7526385224274406,
      "grad_norm": 0.054022207856178284,
      "learning_rate": 8.32014072119613e-05,
      "loss": 0.0017,
      "step": 2657
    },
    {
      "epoch": 1.7532981530343008,
      "grad_norm": 0.21766117215156555,
      "learning_rate": 8.315743183817063e-05,
      "loss": 0.0187,
      "step": 2658
    },
    {
      "epoch": 1.753957783641161,
      "grad_norm": 0.17108388245105743,
      "learning_rate": 8.311345646437994e-05,
      "loss": 0.0268,
      "step": 2659
    },
    {
      "epoch": 1.754617414248021,
      "grad_norm": 0.2707836925983429,
      "learning_rate": 8.306948109058927e-05,
      "loss": 0.038,
      "step": 2660
    },
    {
      "epoch": 1.7552770448548811,
      "grad_norm": 0.2546124756336212,
      "learning_rate": 8.30255057167986e-05,
      "loss": 0.0382,
      "step": 2661
    },
    {
      "epoch": 1.7559366754617414,
      "grad_norm": 0.12794668972492218,
      "learning_rate": 8.298153034300792e-05,
      "loss": 0.0068,
      "step": 2662
    },
    {
      "epoch": 1.7565963060686016,
      "grad_norm": 0.24405032396316528,
      "learning_rate": 8.293755496921725e-05,
      "loss": 0.0187,
      "step": 2663
    },
    {
      "epoch": 1.7572559366754619,
      "grad_norm": 0.18743260204792023,
      "learning_rate": 8.289357959542657e-05,
      "loss": 0.0164,
      "step": 2664
    },
    {
      "epoch": 1.757915567282322,
      "grad_norm": 0.16447892785072327,
      "learning_rate": 8.284960422163589e-05,
      "loss": 0.0144,
      "step": 2665
    },
    {
      "epoch": 1.758575197889182,
      "grad_norm": 0.4965267479419708,
      "learning_rate": 8.28056288478452e-05,
      "loss": 0.0798,
      "step": 2666
    },
    {
      "epoch": 1.7592348284960422,
      "grad_norm": 0.12044331431388855,
      "learning_rate": 8.276165347405453e-05,
      "loss": 0.0135,
      "step": 2667
    },
    {
      "epoch": 1.7598944591029024,
      "grad_norm": 0.20993193984031677,
      "learning_rate": 8.271767810026385e-05,
      "loss": 0.0254,
      "step": 2668
    },
    {
      "epoch": 1.7605540897097627,
      "grad_norm": 0.26059848070144653,
      "learning_rate": 8.267370272647318e-05,
      "loss": 0.0298,
      "step": 2669
    },
    {
      "epoch": 1.7612137203166227,
      "grad_norm": 0.15003421902656555,
      "learning_rate": 8.26297273526825e-05,
      "loss": 0.0148,
      "step": 2670
    },
    {
      "epoch": 1.7618733509234827,
      "grad_norm": 0.11888007074594498,
      "learning_rate": 8.258575197889182e-05,
      "loss": 0.0057,
      "step": 2671
    },
    {
      "epoch": 1.762532981530343,
      "grad_norm": 0.12157242000102997,
      "learning_rate": 8.254177660510114e-05,
      "loss": 0.0054,
      "step": 2672
    },
    {
      "epoch": 1.7631926121372032,
      "grad_norm": 0.0912046730518341,
      "learning_rate": 8.249780123131047e-05,
      "loss": 0.0047,
      "step": 2673
    },
    {
      "epoch": 1.7638522427440633,
      "grad_norm": 0.24987365305423737,
      "learning_rate": 8.24538258575198e-05,
      "loss": 0.0218,
      "step": 2674
    },
    {
      "epoch": 1.7645118733509235,
      "grad_norm": 0.2752772569656372,
      "learning_rate": 8.240985048372911e-05,
      "loss": 0.0311,
      "step": 2675
    },
    {
      "epoch": 1.7651715039577835,
      "grad_norm": 0.11848806589841843,
      "learning_rate": 8.236587510993844e-05,
      "loss": 0.0064,
      "step": 2676
    },
    {
      "epoch": 1.7658311345646438,
      "grad_norm": 0.08379074186086655,
      "learning_rate": 8.232189973614775e-05,
      "loss": 0.006,
      "step": 2677
    },
    {
      "epoch": 1.766490765171504,
      "grad_norm": 0.2328851968050003,
      "learning_rate": 8.227792436235708e-05,
      "loss": 0.0236,
      "step": 2678
    },
    {
      "epoch": 1.767150395778364,
      "grad_norm": 0.2020098865032196,
      "learning_rate": 8.223394898856641e-05,
      "loss": 0.0368,
      "step": 2679
    },
    {
      "epoch": 1.767810026385224,
      "grad_norm": 0.2524200975894928,
      "learning_rate": 8.218997361477572e-05,
      "loss": 0.0329,
      "step": 2680
    },
    {
      "epoch": 1.7684696569920844,
      "grad_norm": 0.12357255071401596,
      "learning_rate": 8.214599824098505e-05,
      "loss": 0.0122,
      "step": 2681
    },
    {
      "epoch": 1.7691292875989446,
      "grad_norm": 0.1862334907054901,
      "learning_rate": 8.210202286719438e-05,
      "loss": 0.0106,
      "step": 2682
    },
    {
      "epoch": 1.7697889182058049,
      "grad_norm": 0.2684810757637024,
      "learning_rate": 8.20580474934037e-05,
      "loss": 0.0279,
      "step": 2683
    },
    {
      "epoch": 1.770448548812665,
      "grad_norm": 0.3517805337905884,
      "learning_rate": 8.201407211961302e-05,
      "loss": 0.0312,
      "step": 2684
    },
    {
      "epoch": 1.771108179419525,
      "grad_norm": 0.29436177015304565,
      "learning_rate": 8.197009674582235e-05,
      "loss": 0.0354,
      "step": 2685
    },
    {
      "epoch": 1.7717678100263852,
      "grad_norm": 0.1337636411190033,
      "learning_rate": 8.192612137203166e-05,
      "loss": 0.0081,
      "step": 2686
    },
    {
      "epoch": 1.7724274406332454,
      "grad_norm": 0.1527768075466156,
      "learning_rate": 8.188214599824099e-05,
      "loss": 0.0133,
      "step": 2687
    },
    {
      "epoch": 1.7730870712401057,
      "grad_norm": 0.15305568277835846,
      "learning_rate": 8.183817062445032e-05,
      "loss": 0.0079,
      "step": 2688
    },
    {
      "epoch": 1.7737467018469657,
      "grad_norm": 0.19929641485214233,
      "learning_rate": 8.179419525065963e-05,
      "loss": 0.0242,
      "step": 2689
    },
    {
      "epoch": 1.7744063324538257,
      "grad_norm": 0.27757328748703003,
      "learning_rate": 8.175021987686896e-05,
      "loss": 0.0244,
      "step": 2690
    },
    {
      "epoch": 1.775065963060686,
      "grad_norm": 0.2737675905227661,
      "learning_rate": 8.170624450307827e-05,
      "loss": 0.0439,
      "step": 2691
    },
    {
      "epoch": 1.7757255936675462,
      "grad_norm": 0.12371321767568588,
      "learning_rate": 8.16622691292876e-05,
      "loss": 0.0133,
      "step": 2692
    },
    {
      "epoch": 1.7763852242744065,
      "grad_norm": 0.1639021635055542,
      "learning_rate": 8.161829375549693e-05,
      "loss": 0.0199,
      "step": 2693
    },
    {
      "epoch": 1.7770448548812665,
      "grad_norm": 0.29395952820777893,
      "learning_rate": 8.157431838170625e-05,
      "loss": 0.0395,
      "step": 2694
    },
    {
      "epoch": 1.7777044854881265,
      "grad_norm": 0.2098071575164795,
      "learning_rate": 8.153034300791557e-05,
      "loss": 0.022,
      "step": 2695
    },
    {
      "epoch": 1.7783641160949868,
      "grad_norm": 0.16236433386802673,
      "learning_rate": 8.14863676341249e-05,
      "loss": 0.0099,
      "step": 2696
    },
    {
      "epoch": 1.779023746701847,
      "grad_norm": 0.20432880520820618,
      "learning_rate": 8.144239226033422e-05,
      "loss": 0.0131,
      "step": 2697
    },
    {
      "epoch": 1.779683377308707,
      "grad_norm": 0.35126549005508423,
      "learning_rate": 8.139841688654354e-05,
      "loss": 0.0619,
      "step": 2698
    },
    {
      "epoch": 1.7803430079155673,
      "grad_norm": 0.4875448942184448,
      "learning_rate": 8.135444151275287e-05,
      "loss": 0.0419,
      "step": 2699
    },
    {
      "epoch": 1.7810026385224274,
      "grad_norm": 0.20042921602725983,
      "learning_rate": 8.131046613896218e-05,
      "loss": 0.0229,
      "step": 2700
    },
    {
      "epoch": 1.7816622691292876,
      "grad_norm": 0.15065503120422363,
      "learning_rate": 8.126649076517151e-05,
      "loss": 0.0104,
      "step": 2701
    },
    {
      "epoch": 1.7823218997361479,
      "grad_norm": 0.24476441740989685,
      "learning_rate": 8.122251539138082e-05,
      "loss": 0.0388,
      "step": 2702
    },
    {
      "epoch": 1.7829815303430079,
      "grad_norm": 0.31234943866729736,
      "learning_rate": 8.117854001759015e-05,
      "loss": 0.0454,
      "step": 2703
    },
    {
      "epoch": 1.783641160949868,
      "grad_norm": 0.26681336760520935,
      "learning_rate": 8.113456464379948e-05,
      "loss": 0.0294,
      "step": 2704
    },
    {
      "epoch": 1.7843007915567282,
      "grad_norm": 0.09280791133642197,
      "learning_rate": 8.10905892700088e-05,
      "loss": 0.0071,
      "step": 2705
    },
    {
      "epoch": 1.7849604221635884,
      "grad_norm": 0.13339495658874512,
      "learning_rate": 8.104661389621813e-05,
      "loss": 0.0135,
      "step": 2706
    },
    {
      "epoch": 1.7856200527704487,
      "grad_norm": 0.22311390936374664,
      "learning_rate": 8.100263852242744e-05,
      "loss": 0.0266,
      "step": 2707
    },
    {
      "epoch": 1.7862796833773087,
      "grad_norm": 0.2342304289340973,
      "learning_rate": 8.095866314863677e-05,
      "loss": 0.0385,
      "step": 2708
    },
    {
      "epoch": 1.7869393139841687,
      "grad_norm": 0.16534537076950073,
      "learning_rate": 8.091468777484609e-05,
      "loss": 0.019,
      "step": 2709
    },
    {
      "epoch": 1.787598944591029,
      "grad_norm": 0.1222333088517189,
      "learning_rate": 8.087071240105541e-05,
      "loss": 0.0098,
      "step": 2710
    },
    {
      "epoch": 1.7882585751978892,
      "grad_norm": 0.2885783612728119,
      "learning_rate": 8.082673702726473e-05,
      "loss": 0.0332,
      "step": 2711
    },
    {
      "epoch": 1.7889182058047495,
      "grad_norm": 0.15848667919635773,
      "learning_rate": 8.078276165347406e-05,
      "loss": 0.0188,
      "step": 2712
    },
    {
      "epoch": 1.7895778364116095,
      "grad_norm": 0.13873352110385895,
      "learning_rate": 8.073878627968337e-05,
      "loss": 0.0185,
      "step": 2713
    },
    {
      "epoch": 1.7902374670184695,
      "grad_norm": 0.21197614073753357,
      "learning_rate": 8.069481090589271e-05,
      "loss": 0.0318,
      "step": 2714
    },
    {
      "epoch": 1.7908970976253298,
      "grad_norm": 0.11623699218034744,
      "learning_rate": 8.065083553210204e-05,
      "loss": 0.017,
      "step": 2715
    },
    {
      "epoch": 1.79155672823219,
      "grad_norm": 0.11721924692392349,
      "learning_rate": 8.060686015831135e-05,
      "loss": 0.0068,
      "step": 2716
    },
    {
      "epoch": 1.7922163588390503,
      "grad_norm": 0.14912286400794983,
      "learning_rate": 8.056288478452068e-05,
      "loss": 0.0175,
      "step": 2717
    },
    {
      "epoch": 1.7928759894459103,
      "grad_norm": 0.1503412425518036,
      "learning_rate": 8.051890941073e-05,
      "loss": 0.0136,
      "step": 2718
    },
    {
      "epoch": 1.7935356200527703,
      "grad_norm": 0.07899495959281921,
      "learning_rate": 8.047493403693932e-05,
      "loss": 0.0042,
      "step": 2719
    },
    {
      "epoch": 1.7941952506596306,
      "grad_norm": 0.337274968624115,
      "learning_rate": 8.043095866314863e-05,
      "loss": 0.0456,
      "step": 2720
    },
    {
      "epoch": 1.7948548812664908,
      "grad_norm": 0.12414707243442535,
      "learning_rate": 8.038698328935796e-05,
      "loss": 0.015,
      "step": 2721
    },
    {
      "epoch": 1.7955145118733509,
      "grad_norm": 0.22975657880306244,
      "learning_rate": 8.034300791556728e-05,
      "loss": 0.0328,
      "step": 2722
    },
    {
      "epoch": 1.7961741424802111,
      "grad_norm": 0.12438766658306122,
      "learning_rate": 8.02990325417766e-05,
      "loss": 0.017,
      "step": 2723
    },
    {
      "epoch": 1.7968337730870712,
      "grad_norm": 0.26890987157821655,
      "learning_rate": 8.025505716798593e-05,
      "loss": 0.02,
      "step": 2724
    },
    {
      "epoch": 1.7974934036939314,
      "grad_norm": 0.25577834248542786,
      "learning_rate": 8.021108179419526e-05,
      "loss": 0.0432,
      "step": 2725
    },
    {
      "epoch": 1.7981530343007917,
      "grad_norm": 0.3468242585659027,
      "learning_rate": 8.016710642040459e-05,
      "loss": 0.0235,
      "step": 2726
    },
    {
      "epoch": 1.7988126649076517,
      "grad_norm": 0.3127930164337158,
      "learning_rate": 8.01231310466139e-05,
      "loss": 0.0612,
      "step": 2727
    },
    {
      "epoch": 1.7994722955145117,
      "grad_norm": 0.16250993311405182,
      "learning_rate": 8.007915567282323e-05,
      "loss": 0.01,
      "step": 2728
    },
    {
      "epoch": 1.800131926121372,
      "grad_norm": 0.3241233229637146,
      "learning_rate": 8.003518029903254e-05,
      "loss": 0.0328,
      "step": 2729
    },
    {
      "epoch": 1.8007915567282322,
      "grad_norm": 0.27200496196746826,
      "learning_rate": 7.999120492524187e-05,
      "loss": 0.043,
      "step": 2730
    },
    {
      "epoch": 1.8014511873350925,
      "grad_norm": 0.16929303109645844,
      "learning_rate": 7.994722955145118e-05,
      "loss": 0.0121,
      "step": 2731
    },
    {
      "epoch": 1.8021108179419525,
      "grad_norm": 0.1295406073331833,
      "learning_rate": 7.990325417766051e-05,
      "loss": 0.0133,
      "step": 2732
    },
    {
      "epoch": 1.8027704485488125,
      "grad_norm": 0.18378539383411407,
      "learning_rate": 7.985927880386984e-05,
      "loss": 0.0231,
      "step": 2733
    },
    {
      "epoch": 1.8034300791556728,
      "grad_norm": 0.19356593489646912,
      "learning_rate": 7.981530343007915e-05,
      "loss": 0.0112,
      "step": 2734
    },
    {
      "epoch": 1.804089709762533,
      "grad_norm": 0.20679447054862976,
      "learning_rate": 7.977132805628848e-05,
      "loss": 0.0187,
      "step": 2735
    },
    {
      "epoch": 1.8047493403693933,
      "grad_norm": 0.08901474624872208,
      "learning_rate": 7.972735268249781e-05,
      "loss": 0.0045,
      "step": 2736
    },
    {
      "epoch": 1.8054089709762533,
      "grad_norm": 0.2945713996887207,
      "learning_rate": 7.968337730870713e-05,
      "loss": 0.0335,
      "step": 2737
    },
    {
      "epoch": 1.8060686015831133,
      "grad_norm": 0.41549861431121826,
      "learning_rate": 7.963940193491645e-05,
      "loss": 0.0751,
      "step": 2738
    },
    {
      "epoch": 1.8067282321899736,
      "grad_norm": 0.13735973834991455,
      "learning_rate": 7.959542656112578e-05,
      "loss": 0.0106,
      "step": 2739
    },
    {
      "epoch": 1.8073878627968338,
      "grad_norm": 0.12344290316104889,
      "learning_rate": 7.955145118733509e-05,
      "loss": 0.0119,
      "step": 2740
    },
    {
      "epoch": 1.808047493403694,
      "grad_norm": 0.1866416484117508,
      "learning_rate": 7.950747581354442e-05,
      "loss": 0.0239,
      "step": 2741
    },
    {
      "epoch": 1.8087071240105541,
      "grad_norm": 0.23007646203041077,
      "learning_rate": 7.946350043975375e-05,
      "loss": 0.034,
      "step": 2742
    },
    {
      "epoch": 1.8093667546174141,
      "grad_norm": 0.2352345585823059,
      "learning_rate": 7.941952506596306e-05,
      "loss": 0.0243,
      "step": 2743
    },
    {
      "epoch": 1.8100263852242744,
      "grad_norm": 0.29979637265205383,
      "learning_rate": 7.937554969217239e-05,
      "loss": 0.0392,
      "step": 2744
    },
    {
      "epoch": 1.8106860158311346,
      "grad_norm": 0.12986642122268677,
      "learning_rate": 7.93315743183817e-05,
      "loss": 0.0131,
      "step": 2745
    },
    {
      "epoch": 1.8113456464379947,
      "grad_norm": 0.33265289664268494,
      "learning_rate": 7.928759894459103e-05,
      "loss": 0.0558,
      "step": 2746
    },
    {
      "epoch": 1.812005277044855,
      "grad_norm": 0.28785765171051025,
      "learning_rate": 7.924362357080036e-05,
      "loss": 0.045,
      "step": 2747
    },
    {
      "epoch": 1.812664907651715,
      "grad_norm": 0.12722782790660858,
      "learning_rate": 7.919964819700968e-05,
      "loss": 0.0133,
      "step": 2748
    },
    {
      "epoch": 1.8133245382585752,
      "grad_norm": 0.17607155442237854,
      "learning_rate": 7.9155672823219e-05,
      "loss": 0.0357,
      "step": 2749
    },
    {
      "epoch": 1.8139841688654355,
      "grad_norm": 0.18099474906921387,
      "learning_rate": 7.911169744942832e-05,
      "loss": 0.0173,
      "step": 2750
    },
    {
      "epoch": 1.8146437994722955,
      "grad_norm": 0.146961510181427,
      "learning_rate": 7.906772207563765e-05,
      "loss": 0.0189,
      "step": 2751
    },
    {
      "epoch": 1.8153034300791555,
      "grad_norm": 0.14975321292877197,
      "learning_rate": 7.902374670184697e-05,
      "loss": 0.0169,
      "step": 2752
    },
    {
      "epoch": 1.8159630606860158,
      "grad_norm": 0.15772411227226257,
      "learning_rate": 7.89797713280563e-05,
      "loss": 0.011,
      "step": 2753
    },
    {
      "epoch": 1.816622691292876,
      "grad_norm": 0.1442575305700302,
      "learning_rate": 7.893579595426561e-05,
      "loss": 0.0154,
      "step": 2754
    },
    {
      "epoch": 1.8172823218997363,
      "grad_norm": 0.17567983269691467,
      "learning_rate": 7.889182058047494e-05,
      "loss": 0.0117,
      "step": 2755
    },
    {
      "epoch": 1.8179419525065963,
      "grad_norm": 0.150907501578331,
      "learning_rate": 7.884784520668426e-05,
      "loss": 0.0149,
      "step": 2756
    },
    {
      "epoch": 1.8186015831134563,
      "grad_norm": 0.16949275135993958,
      "learning_rate": 7.880386983289359e-05,
      "loss": 0.0136,
      "step": 2757
    },
    {
      "epoch": 1.8192612137203166,
      "grad_norm": 0.2028064727783203,
      "learning_rate": 7.87598944591029e-05,
      "loss": 0.0203,
      "step": 2758
    },
    {
      "epoch": 1.8199208443271768,
      "grad_norm": 0.09157130867242813,
      "learning_rate": 7.871591908531223e-05,
      "loss": 0.0043,
      "step": 2759
    },
    {
      "epoch": 1.820580474934037,
      "grad_norm": 0.24332042038440704,
      "learning_rate": 7.867194371152156e-05,
      "loss": 0.0285,
      "step": 2760
    },
    {
      "epoch": 1.821240105540897,
      "grad_norm": 0.16444915533065796,
      "learning_rate": 7.862796833773087e-05,
      "loss": 0.0149,
      "step": 2761
    },
    {
      "epoch": 1.8218997361477571,
      "grad_norm": 0.20262329280376434,
      "learning_rate": 7.85839929639402e-05,
      "loss": 0.0274,
      "step": 2762
    },
    {
      "epoch": 1.8225593667546174,
      "grad_norm": 0.17623576521873474,
      "learning_rate": 7.854001759014951e-05,
      "loss": 0.0165,
      "step": 2763
    },
    {
      "epoch": 1.8232189973614776,
      "grad_norm": 0.18654415011405945,
      "learning_rate": 7.849604221635884e-05,
      "loss": 0.0144,
      "step": 2764
    },
    {
      "epoch": 1.8238786279683379,
      "grad_norm": 0.3619517683982849,
      "learning_rate": 7.845206684256816e-05,
      "loss": 0.0453,
      "step": 2765
    },
    {
      "epoch": 1.824538258575198,
      "grad_norm": 0.22341744601726532,
      "learning_rate": 7.840809146877748e-05,
      "loss": 0.0278,
      "step": 2766
    },
    {
      "epoch": 1.825197889182058,
      "grad_norm": 0.13852159678936005,
      "learning_rate": 7.836411609498681e-05,
      "loss": 0.0093,
      "step": 2767
    },
    {
      "epoch": 1.8258575197889182,
      "grad_norm": 0.3025662302970886,
      "learning_rate": 7.832014072119614e-05,
      "loss": 0.0223,
      "step": 2768
    },
    {
      "epoch": 1.8265171503957784,
      "grad_norm": 0.21195976436138153,
      "learning_rate": 7.827616534740547e-05,
      "loss": 0.0166,
      "step": 2769
    },
    {
      "epoch": 1.8271767810026385,
      "grad_norm": 0.12848341464996338,
      "learning_rate": 7.823218997361478e-05,
      "loss": 0.0096,
      "step": 2770
    },
    {
      "epoch": 1.8278364116094987,
      "grad_norm": 0.23055890202522278,
      "learning_rate": 7.818821459982411e-05,
      "loss": 0.0393,
      "step": 2771
    },
    {
      "epoch": 1.8284960422163588,
      "grad_norm": 0.19791820645332336,
      "learning_rate": 7.814423922603342e-05,
      "loss": 0.0167,
      "step": 2772
    },
    {
      "epoch": 1.829155672823219,
      "grad_norm": 0.3850221037864685,
      "learning_rate": 7.810026385224275e-05,
      "loss": 0.0583,
      "step": 2773
    },
    {
      "epoch": 1.8298153034300793,
      "grad_norm": 0.48800262808799744,
      "learning_rate": 7.805628847845206e-05,
      "loss": 0.073,
      "step": 2774
    },
    {
      "epoch": 1.8304749340369393,
      "grad_norm": 0.17612482607364655,
      "learning_rate": 7.801231310466139e-05,
      "loss": 0.0098,
      "step": 2775
    },
    {
      "epoch": 1.8311345646437993,
      "grad_norm": 0.14257752895355225,
      "learning_rate": 7.79683377308707e-05,
      "loss": 0.0074,
      "step": 2776
    },
    {
      "epoch": 1.8317941952506596,
      "grad_norm": 0.2532043755054474,
      "learning_rate": 7.792436235708003e-05,
      "loss": 0.0192,
      "step": 2777
    },
    {
      "epoch": 1.8324538258575198,
      "grad_norm": 0.23519526422023773,
      "learning_rate": 7.788038698328936e-05,
      "loss": 0.0281,
      "step": 2778
    },
    {
      "epoch": 1.83311345646438,
      "grad_norm": 0.2683464586734772,
      "learning_rate": 7.783641160949869e-05,
      "loss": 0.0226,
      "step": 2779
    },
    {
      "epoch": 1.83377308707124,
      "grad_norm": 0.20405696332454681,
      "learning_rate": 7.779243623570801e-05,
      "loss": 0.0232,
      "step": 2780
    },
    {
      "epoch": 1.8344327176781001,
      "grad_norm": 0.2711271345615387,
      "learning_rate": 7.774846086191733e-05,
      "loss": 0.034,
      "step": 2781
    },
    {
      "epoch": 1.8350923482849604,
      "grad_norm": 0.1427929401397705,
      "learning_rate": 7.770448548812666e-05,
      "loss": 0.0089,
      "step": 2782
    },
    {
      "epoch": 1.8357519788918206,
      "grad_norm": 0.3366852402687073,
      "learning_rate": 7.766051011433597e-05,
      "loss": 0.0334,
      "step": 2783
    },
    {
      "epoch": 1.8364116094986809,
      "grad_norm": 0.17214982211589813,
      "learning_rate": 7.76165347405453e-05,
      "loss": 0.0185,
      "step": 2784
    },
    {
      "epoch": 1.837071240105541,
      "grad_norm": 0.238674134016037,
      "learning_rate": 7.757255936675461e-05,
      "loss": 0.0243,
      "step": 2785
    },
    {
      "epoch": 1.837730870712401,
      "grad_norm": 0.36783891916275024,
      "learning_rate": 7.752858399296394e-05,
      "loss": 0.0504,
      "step": 2786
    },
    {
      "epoch": 1.8383905013192612,
      "grad_norm": 0.06602689623832703,
      "learning_rate": 7.748460861917327e-05,
      "loss": 0.0049,
      "step": 2787
    },
    {
      "epoch": 1.8390501319261214,
      "grad_norm": 0.28541818261146545,
      "learning_rate": 7.74406332453826e-05,
      "loss": 0.0221,
      "step": 2788
    },
    {
      "epoch": 1.8397097625329817,
      "grad_norm": 0.07639995217323303,
      "learning_rate": 7.739665787159192e-05,
      "loss": 0.0035,
      "step": 2789
    },
    {
      "epoch": 1.8403693931398417,
      "grad_norm": 0.21889354288578033,
      "learning_rate": 7.735268249780124e-05,
      "loss": 0.0331,
      "step": 2790
    },
    {
      "epoch": 1.8410290237467017,
      "grad_norm": 0.2134886384010315,
      "learning_rate": 7.730870712401056e-05,
      "loss": 0.0236,
      "step": 2791
    },
    {
      "epoch": 1.841688654353562,
      "grad_norm": 0.20619119703769684,
      "learning_rate": 7.726473175021988e-05,
      "loss": 0.0187,
      "step": 2792
    },
    {
      "epoch": 1.8423482849604222,
      "grad_norm": 0.21676355600357056,
      "learning_rate": 7.72207563764292e-05,
      "loss": 0.0306,
      "step": 2793
    },
    {
      "epoch": 1.8430079155672823,
      "grad_norm": 0.4106104373931885,
      "learning_rate": 7.717678100263852e-05,
      "loss": 0.0702,
      "step": 2794
    },
    {
      "epoch": 1.8436675461741425,
      "grad_norm": 0.18373677134513855,
      "learning_rate": 7.713280562884785e-05,
      "loss": 0.0197,
      "step": 2795
    },
    {
      "epoch": 1.8443271767810026,
      "grad_norm": 0.12523841857910156,
      "learning_rate": 7.708883025505717e-05,
      "loss": 0.0093,
      "step": 2796
    },
    {
      "epoch": 1.8449868073878628,
      "grad_norm": 0.16157236695289612,
      "learning_rate": 7.704485488126649e-05,
      "loss": 0.0257,
      "step": 2797
    },
    {
      "epoch": 1.845646437994723,
      "grad_norm": 0.20758870244026184,
      "learning_rate": 7.700087950747582e-05,
      "loss": 0.0357,
      "step": 2798
    },
    {
      "epoch": 1.846306068601583,
      "grad_norm": 0.13454288244247437,
      "learning_rate": 7.695690413368514e-05,
      "loss": 0.012,
      "step": 2799
    },
    {
      "epoch": 1.8469656992084431,
      "grad_norm": 0.106239452958107,
      "learning_rate": 7.691292875989447e-05,
      "loss": 0.0076,
      "step": 2800
    },
    {
      "epoch": 1.8476253298153034,
      "grad_norm": 0.26041093468666077,
      "learning_rate": 7.686895338610378e-05,
      "loss": 0.016,
      "step": 2801
    },
    {
      "epoch": 1.8482849604221636,
      "grad_norm": 0.13161644339561462,
      "learning_rate": 7.682497801231311e-05,
      "loss": 0.007,
      "step": 2802
    },
    {
      "epoch": 1.8489445910290239,
      "grad_norm": 0.1474287360906601,
      "learning_rate": 7.678100263852243e-05,
      "loss": 0.0145,
      "step": 2803
    },
    {
      "epoch": 1.849604221635884,
      "grad_norm": 0.25932133197784424,
      "learning_rate": 7.673702726473175e-05,
      "loss": 0.0429,
      "step": 2804
    },
    {
      "epoch": 1.850263852242744,
      "grad_norm": 0.15210750699043274,
      "learning_rate": 7.669305189094108e-05,
      "loss": 0.0141,
      "step": 2805
    },
    {
      "epoch": 1.8509234828496042,
      "grad_norm": 0.24407535791397095,
      "learning_rate": 7.66490765171504e-05,
      "loss": 0.0323,
      "step": 2806
    },
    {
      "epoch": 1.8515831134564644,
      "grad_norm": 0.16145657002925873,
      "learning_rate": 7.660510114335972e-05,
      "loss": 0.0107,
      "step": 2807
    },
    {
      "epoch": 1.8522427440633247,
      "grad_norm": 0.11881151795387268,
      "learning_rate": 7.656112576956904e-05,
      "loss": 0.0105,
      "step": 2808
    },
    {
      "epoch": 1.8529023746701847,
      "grad_norm": 0.2334231585264206,
      "learning_rate": 7.651715039577836e-05,
      "loss": 0.0171,
      "step": 2809
    },
    {
      "epoch": 1.8535620052770447,
      "grad_norm": 0.33000943064689636,
      "learning_rate": 7.647317502198769e-05,
      "loss": 0.0222,
      "step": 2810
    },
    {
      "epoch": 1.854221635883905,
      "grad_norm": 0.18606333434581757,
      "learning_rate": 7.642919964819702e-05,
      "loss": 0.0128,
      "step": 2811
    },
    {
      "epoch": 1.8548812664907652,
      "grad_norm": 0.24701932072639465,
      "learning_rate": 7.638522427440633e-05,
      "loss": 0.0247,
      "step": 2812
    },
    {
      "epoch": 1.8555408970976255,
      "grad_norm": 0.32284510135650635,
      "learning_rate": 7.634124890061566e-05,
      "loss": 0.0546,
      "step": 2813
    },
    {
      "epoch": 1.8562005277044855,
      "grad_norm": 0.08574432134628296,
      "learning_rate": 7.629727352682499e-05,
      "loss": 0.0082,
      "step": 2814
    },
    {
      "epoch": 1.8568601583113455,
      "grad_norm": 0.14602836966514587,
      "learning_rate": 7.62532981530343e-05,
      "loss": 0.0128,
      "step": 2815
    },
    {
      "epoch": 1.8575197889182058,
      "grad_norm": 0.16012640297412872,
      "learning_rate": 7.620932277924363e-05,
      "loss": 0.0156,
      "step": 2816
    },
    {
      "epoch": 1.858179419525066,
      "grad_norm": 0.1468902826309204,
      "learning_rate": 7.616534740545294e-05,
      "loss": 0.0107,
      "step": 2817
    },
    {
      "epoch": 1.858839050131926,
      "grad_norm": 0.35945749282836914,
      "learning_rate": 7.612137203166227e-05,
      "loss": 0.0305,
      "step": 2818
    },
    {
      "epoch": 1.8594986807387863,
      "grad_norm": 0.28958430886268616,
      "learning_rate": 7.60773966578716e-05,
      "loss": 0.0277,
      "step": 2819
    },
    {
      "epoch": 1.8601583113456464,
      "grad_norm": 0.1918850988149643,
      "learning_rate": 7.603342128408093e-05,
      "loss": 0.0262,
      "step": 2820
    },
    {
      "epoch": 1.8608179419525066,
      "grad_norm": 0.2893976867198944,
      "learning_rate": 7.598944591029024e-05,
      "loss": 0.0352,
      "step": 2821
    },
    {
      "epoch": 1.8614775725593669,
      "grad_norm": 0.09686340391635895,
      "learning_rate": 7.594547053649957e-05,
      "loss": 0.0089,
      "step": 2822
    },
    {
      "epoch": 1.8621372031662269,
      "grad_norm": 0.12034756690263748,
      "learning_rate": 7.59014951627089e-05,
      "loss": 0.0165,
      "step": 2823
    },
    {
      "epoch": 1.862796833773087,
      "grad_norm": 0.3034087121486664,
      "learning_rate": 7.585751978891821e-05,
      "loss": 0.0246,
      "step": 2824
    },
    {
      "epoch": 1.8634564643799472,
      "grad_norm": 0.4927346110343933,
      "learning_rate": 7.581354441512754e-05,
      "loss": 0.0599,
      "step": 2825
    },
    {
      "epoch": 1.8641160949868074,
      "grad_norm": 0.3847402334213257,
      "learning_rate": 7.576956904133685e-05,
      "loss": 0.0201,
      "step": 2826
    },
    {
      "epoch": 1.8647757255936677,
      "grad_norm": 0.45093464851379395,
      "learning_rate": 7.572559366754618e-05,
      "loss": 0.0362,
      "step": 2827
    },
    {
      "epoch": 1.8654353562005277,
      "grad_norm": 0.23050084710121155,
      "learning_rate": 7.568161829375549e-05,
      "loss": 0.0189,
      "step": 2828
    },
    {
      "epoch": 1.8660949868073877,
      "grad_norm": 0.18512016534805298,
      "learning_rate": 7.563764291996482e-05,
      "loss": 0.0084,
      "step": 2829
    },
    {
      "epoch": 1.866754617414248,
      "grad_norm": 0.25253385305404663,
      "learning_rate": 7.559366754617415e-05,
      "loss": 0.0396,
      "step": 2830
    },
    {
      "epoch": 1.8674142480211082,
      "grad_norm": 0.2025977373123169,
      "learning_rate": 7.554969217238347e-05,
      "loss": 0.0108,
      "step": 2831
    },
    {
      "epoch": 1.8680738786279685,
      "grad_norm": 0.16080817580223083,
      "learning_rate": 7.55057167985928e-05,
      "loss": 0.0134,
      "step": 2832
    },
    {
      "epoch": 1.8687335092348285,
      "grad_norm": 0.10139831155538559,
      "learning_rate": 7.546174142480212e-05,
      "loss": 0.0059,
      "step": 2833
    },
    {
      "epoch": 1.8693931398416885,
      "grad_norm": 0.15412548184394836,
      "learning_rate": 7.541776605101144e-05,
      "loss": 0.0133,
      "step": 2834
    },
    {
      "epoch": 1.8700527704485488,
      "grad_norm": 0.09235629439353943,
      "learning_rate": 7.537379067722076e-05,
      "loss": 0.0046,
      "step": 2835
    },
    {
      "epoch": 1.870712401055409,
      "grad_norm": 0.1445881873369217,
      "learning_rate": 7.532981530343008e-05,
      "loss": 0.0219,
      "step": 2836
    },
    {
      "epoch": 1.8713720316622693,
      "grad_norm": 0.2993033528327942,
      "learning_rate": 7.52858399296394e-05,
      "loss": 0.036,
      "step": 2837
    },
    {
      "epoch": 1.8720316622691293,
      "grad_norm": 0.19003605842590332,
      "learning_rate": 7.524186455584873e-05,
      "loss": 0.0126,
      "step": 2838
    },
    {
      "epoch": 1.8726912928759893,
      "grad_norm": 0.3888677656650543,
      "learning_rate": 7.519788918205804e-05,
      "loss": 0.0337,
      "step": 2839
    },
    {
      "epoch": 1.8733509234828496,
      "grad_norm": 0.16194216907024384,
      "learning_rate": 7.515391380826737e-05,
      "loss": 0.0111,
      "step": 2840
    },
    {
      "epoch": 1.8740105540897098,
      "grad_norm": 0.1396002173423767,
      "learning_rate": 7.51099384344767e-05,
      "loss": 0.0094,
      "step": 2841
    },
    {
      "epoch": 1.8746701846965699,
      "grad_norm": 0.19259139895439148,
      "learning_rate": 7.506596306068602e-05,
      "loss": 0.0231,
      "step": 2842
    },
    {
      "epoch": 1.8753298153034301,
      "grad_norm": 0.17200006544589996,
      "learning_rate": 7.502198768689535e-05,
      "loss": 0.0137,
      "step": 2843
    },
    {
      "epoch": 1.8759894459102902,
      "grad_norm": 0.10354752093553543,
      "learning_rate": 7.497801231310466e-05,
      "loss": 0.0071,
      "step": 2844
    },
    {
      "epoch": 1.8766490765171504,
      "grad_norm": 0.19826409220695496,
      "learning_rate": 7.493403693931399e-05,
      "loss": 0.02,
      "step": 2845
    },
    {
      "epoch": 1.8773087071240107,
      "grad_norm": 0.19735245406627655,
      "learning_rate": 7.48900615655233e-05,
      "loss": 0.013,
      "step": 2846
    },
    {
      "epoch": 1.8779683377308707,
      "grad_norm": 0.16423310339450836,
      "learning_rate": 7.484608619173263e-05,
      "loss": 0.0176,
      "step": 2847
    },
    {
      "epoch": 1.8786279683377307,
      "grad_norm": 0.28111645579338074,
      "learning_rate": 7.480211081794196e-05,
      "loss": 0.0326,
      "step": 2848
    },
    {
      "epoch": 1.879287598944591,
      "grad_norm": 0.11786750704050064,
      "learning_rate": 7.475813544415127e-05,
      "loss": 0.0072,
      "step": 2849
    },
    {
      "epoch": 1.8799472295514512,
      "grad_norm": 0.092871755361557,
      "learning_rate": 7.47141600703606e-05,
      "loss": 0.007,
      "step": 2850
    },
    {
      "epoch": 1.8806068601583115,
      "grad_norm": 0.21385978162288666,
      "learning_rate": 7.467018469656992e-05,
      "loss": 0.0151,
      "step": 2851
    },
    {
      "epoch": 1.8812664907651715,
      "grad_norm": 0.07873031497001648,
      "learning_rate": 7.462620932277926e-05,
      "loss": 0.008,
      "step": 2852
    },
    {
      "epoch": 1.8819261213720315,
      "grad_norm": 0.0804000198841095,
      "learning_rate": 7.458223394898857e-05,
      "loss": 0.0073,
      "step": 2853
    },
    {
      "epoch": 1.8825857519788918,
      "grad_norm": 0.17292432487010956,
      "learning_rate": 7.45382585751979e-05,
      "loss": 0.0163,
      "step": 2854
    },
    {
      "epoch": 1.883245382585752,
      "grad_norm": 0.22032998502254486,
      "learning_rate": 7.449428320140721e-05,
      "loss": 0.0274,
      "step": 2855
    },
    {
      "epoch": 1.8839050131926123,
      "grad_norm": 0.28404390811920166,
      "learning_rate": 7.445030782761654e-05,
      "loss": 0.0449,
      "step": 2856
    },
    {
      "epoch": 1.8845646437994723,
      "grad_norm": 0.41523051261901855,
      "learning_rate": 7.440633245382587e-05,
      "loss": 0.0628,
      "step": 2857
    },
    {
      "epoch": 1.8852242744063323,
      "grad_norm": 0.18648652732372284,
      "learning_rate": 7.436235708003518e-05,
      "loss": 0.0229,
      "step": 2858
    },
    {
      "epoch": 1.8858839050131926,
      "grad_norm": 0.13943637907505035,
      "learning_rate": 7.431838170624451e-05,
      "loss": 0.0184,
      "step": 2859
    },
    {
      "epoch": 1.8865435356200528,
      "grad_norm": 0.184025838971138,
      "learning_rate": 7.427440633245382e-05,
      "loss": 0.0222,
      "step": 2860
    },
    {
      "epoch": 1.887203166226913,
      "grad_norm": 0.13453878462314606,
      "learning_rate": 7.423043095866315e-05,
      "loss": 0.0068,
      "step": 2861
    },
    {
      "epoch": 1.8878627968337731,
      "grad_norm": 0.2350127398967743,
      "learning_rate": 7.418645558487248e-05,
      "loss": 0.0302,
      "step": 2862
    },
    {
      "epoch": 1.8885224274406331,
      "grad_norm": 0.2160026729106903,
      "learning_rate": 7.41424802110818e-05,
      "loss": 0.0379,
      "step": 2863
    },
    {
      "epoch": 1.8891820580474934,
      "grad_norm": 0.10692550987005234,
      "learning_rate": 7.409850483729112e-05,
      "loss": 0.0091,
      "step": 2864
    },
    {
      "epoch": 1.8898416886543536,
      "grad_norm": 0.24343867599964142,
      "learning_rate": 7.405452946350045e-05,
      "loss": 0.0292,
      "step": 2865
    },
    {
      "epoch": 1.8905013192612137,
      "grad_norm": 0.2097318172454834,
      "learning_rate": 7.401055408970977e-05,
      "loss": 0.0115,
      "step": 2866
    },
    {
      "epoch": 1.891160949868074,
      "grad_norm": 0.1536915898323059,
      "learning_rate": 7.396657871591909e-05,
      "loss": 0.0169,
      "step": 2867
    },
    {
      "epoch": 1.891820580474934,
      "grad_norm": 0.22072221338748932,
      "learning_rate": 7.392260334212842e-05,
      "loss": 0.0281,
      "step": 2868
    },
    {
      "epoch": 1.8924802110817942,
      "grad_norm": 0.15706124901771545,
      "learning_rate": 7.387862796833773e-05,
      "loss": 0.0158,
      "step": 2869
    },
    {
      "epoch": 1.8931398416886545,
      "grad_norm": 0.10294493287801743,
      "learning_rate": 7.383465259454706e-05,
      "loss": 0.0059,
      "step": 2870
    },
    {
      "epoch": 1.8937994722955145,
      "grad_norm": 0.3270883560180664,
      "learning_rate": 7.379067722075637e-05,
      "loss": 0.0665,
      "step": 2871
    },
    {
      "epoch": 1.8944591029023745,
      "grad_norm": 0.22944344580173492,
      "learning_rate": 7.37467018469657e-05,
      "loss": 0.0311,
      "step": 2872
    },
    {
      "epoch": 1.8951187335092348,
      "grad_norm": 0.13771496713161469,
      "learning_rate": 7.370272647317503e-05,
      "loss": 0.0156,
      "step": 2873
    },
    {
      "epoch": 1.895778364116095,
      "grad_norm": 0.2809918224811554,
      "learning_rate": 7.365875109938435e-05,
      "loss": 0.0314,
      "step": 2874
    },
    {
      "epoch": 1.8964379947229553,
      "grad_norm": 0.16855289041996002,
      "learning_rate": 7.361477572559368e-05,
      "loss": 0.0163,
      "step": 2875
    },
    {
      "epoch": 1.8970976253298153,
      "grad_norm": 0.180150106549263,
      "learning_rate": 7.3570800351803e-05,
      "loss": 0.0211,
      "step": 2876
    },
    {
      "epoch": 1.8977572559366753,
      "grad_norm": 0.3133273720741272,
      "learning_rate": 7.352682497801232e-05,
      "loss": 0.0496,
      "step": 2877
    },
    {
      "epoch": 1.8984168865435356,
      "grad_norm": 0.13801166415214539,
      "learning_rate": 7.348284960422164e-05,
      "loss": 0.0102,
      "step": 2878
    },
    {
      "epoch": 1.8990765171503958,
      "grad_norm": 0.09991901367902756,
      "learning_rate": 7.343887423043096e-05,
      "loss": 0.0048,
      "step": 2879
    },
    {
      "epoch": 1.899736147757256,
      "grad_norm": 0.0877138301730156,
      "learning_rate": 7.339489885664028e-05,
      "loss": 0.0075,
      "step": 2880
    },
    {
      "epoch": 1.900395778364116,
      "grad_norm": 0.2881017327308655,
      "learning_rate": 7.33509234828496e-05,
      "loss": 0.042,
      "step": 2881
    },
    {
      "epoch": 1.9010554089709761,
      "grad_norm": 0.14600415527820587,
      "learning_rate": 7.330694810905892e-05,
      "loss": 0.0128,
      "step": 2882
    },
    {
      "epoch": 1.9017150395778364,
      "grad_norm": 0.11876318603754044,
      "learning_rate": 7.326297273526825e-05,
      "loss": 0.0068,
      "step": 2883
    },
    {
      "epoch": 1.9023746701846966,
      "grad_norm": 0.10693684220314026,
      "learning_rate": 7.321899736147757e-05,
      "loss": 0.0076,
      "step": 2884
    },
    {
      "epoch": 1.9030343007915569,
      "grad_norm": 0.11803679913282394,
      "learning_rate": 7.31750219876869e-05,
      "loss": 0.01,
      "step": 2885
    },
    {
      "epoch": 1.903693931398417,
      "grad_norm": 0.2070775181055069,
      "learning_rate": 7.313104661389623e-05,
      "loss": 0.0249,
      "step": 2886
    },
    {
      "epoch": 1.904353562005277,
      "grad_norm": 0.32434672117233276,
      "learning_rate": 7.308707124010554e-05,
      "loss": 0.0548,
      "step": 2887
    },
    {
      "epoch": 1.9050131926121372,
      "grad_norm": 0.1529756486415863,
      "learning_rate": 7.304309586631487e-05,
      "loss": 0.0056,
      "step": 2888
    },
    {
      "epoch": 1.9056728232189974,
      "grad_norm": 0.28204354643821716,
      "learning_rate": 7.299912049252419e-05,
      "loss": 0.0307,
      "step": 2889
    },
    {
      "epoch": 1.9063324538258575,
      "grad_norm": 0.0655478686094284,
      "learning_rate": 7.295514511873351e-05,
      "loss": 0.0034,
      "step": 2890
    },
    {
      "epoch": 1.9069920844327177,
      "grad_norm": 0.3371660113334656,
      "learning_rate": 7.291116974494283e-05,
      "loss": 0.0304,
      "step": 2891
    },
    {
      "epoch": 1.9076517150395778,
      "grad_norm": 0.2635439932346344,
      "learning_rate": 7.286719437115215e-05,
      "loss": 0.0324,
      "step": 2892
    },
    {
      "epoch": 1.908311345646438,
      "grad_norm": 0.15766239166259766,
      "learning_rate": 7.282321899736148e-05,
      "loss": 0.0174,
      "step": 2893
    },
    {
      "epoch": 1.9089709762532983,
      "grad_norm": 0.14179065823554993,
      "learning_rate": 7.277924362357081e-05,
      "loss": 0.0122,
      "step": 2894
    },
    {
      "epoch": 1.9096306068601583,
      "grad_norm": 0.20279808342456818,
      "learning_rate": 7.273526824978014e-05,
      "loss": 0.0172,
      "step": 2895
    },
    {
      "epoch": 1.9102902374670183,
      "grad_norm": 0.15429438650608063,
      "learning_rate": 7.269129287598945e-05,
      "loss": 0.0152,
      "step": 2896
    },
    {
      "epoch": 1.9109498680738786,
      "grad_norm": 0.12876877188682556,
      "learning_rate": 7.264731750219878e-05,
      "loss": 0.0227,
      "step": 2897
    },
    {
      "epoch": 1.9116094986807388,
      "grad_norm": 0.3660869002342224,
      "learning_rate": 7.260334212840809e-05,
      "loss": 0.0582,
      "step": 2898
    },
    {
      "epoch": 1.912269129287599,
      "grad_norm": 0.12763187289237976,
      "learning_rate": 7.255936675461742e-05,
      "loss": 0.0156,
      "step": 2899
    },
    {
      "epoch": 1.912928759894459,
      "grad_norm": 0.2533419132232666,
      "learning_rate": 7.251539138082673e-05,
      "loss": 0.0258,
      "step": 2900
    },
    {
      "epoch": 1.9135883905013191,
      "grad_norm": 0.18534891307353973,
      "learning_rate": 7.247141600703606e-05,
      "loss": 0.0167,
      "step": 2901
    },
    {
      "epoch": 1.9142480211081794,
      "grad_norm": 0.32120102643966675,
      "learning_rate": 7.242744063324539e-05,
      "loss": 0.0343,
      "step": 2902
    },
    {
      "epoch": 1.9149076517150396,
      "grad_norm": 0.17889007925987244,
      "learning_rate": 7.23834652594547e-05,
      "loss": 0.0093,
      "step": 2903
    },
    {
      "epoch": 1.9155672823218999,
      "grad_norm": 0.22613419592380524,
      "learning_rate": 7.233948988566403e-05,
      "loss": 0.0274,
      "step": 2904
    },
    {
      "epoch": 1.91622691292876,
      "grad_norm": 0.07854067534208298,
      "learning_rate": 7.229551451187336e-05,
      "loss": 0.0069,
      "step": 2905
    },
    {
      "epoch": 1.91688654353562,
      "grad_norm": 0.19310425221920013,
      "learning_rate": 7.225153913808269e-05,
      "loss": 0.03,
      "step": 2906
    },
    {
      "epoch": 1.9175461741424802,
      "grad_norm": 0.17199349403381348,
      "learning_rate": 7.2207563764292e-05,
      "loss": 0.0153,
      "step": 2907
    },
    {
      "epoch": 1.9182058047493404,
      "grad_norm": 0.0825711041688919,
      "learning_rate": 7.216358839050133e-05,
      "loss": 0.0051,
      "step": 2908
    },
    {
      "epoch": 1.9188654353562007,
      "grad_norm": 0.2465319186449051,
      "learning_rate": 7.211961301671064e-05,
      "loss": 0.0279,
      "step": 2909
    },
    {
      "epoch": 1.9195250659630607,
      "grad_norm": 0.16191814839839935,
      "learning_rate": 7.207563764291997e-05,
      "loss": 0.0152,
      "step": 2910
    },
    {
      "epoch": 1.9201846965699207,
      "grad_norm": 0.22056709229946136,
      "learning_rate": 7.20316622691293e-05,
      "loss": 0.0245,
      "step": 2911
    },
    {
      "epoch": 1.920844327176781,
      "grad_norm": 0.21312682330608368,
      "learning_rate": 7.198768689533861e-05,
      "loss": 0.027,
      "step": 2912
    },
    {
      "epoch": 1.9215039577836412,
      "grad_norm": 0.14792752265930176,
      "learning_rate": 7.194371152154794e-05,
      "loss": 0.0066,
      "step": 2913
    },
    {
      "epoch": 1.9221635883905013,
      "grad_norm": 0.2229294627904892,
      "learning_rate": 7.189973614775725e-05,
      "loss": 0.0222,
      "step": 2914
    },
    {
      "epoch": 1.9228232189973615,
      "grad_norm": 0.10122574120759964,
      "learning_rate": 7.185576077396658e-05,
      "loss": 0.0073,
      "step": 2915
    },
    {
      "epoch": 1.9234828496042216,
      "grad_norm": 0.111105777323246,
      "learning_rate": 7.18117854001759e-05,
      "loss": 0.006,
      "step": 2916
    },
    {
      "epoch": 1.9241424802110818,
      "grad_norm": 0.249180406332016,
      "learning_rate": 7.176781002638523e-05,
      "loss": 0.0348,
      "step": 2917
    },
    {
      "epoch": 1.924802110817942,
      "grad_norm": 0.2529471814632416,
      "learning_rate": 7.172383465259455e-05,
      "loss": 0.0306,
      "step": 2918
    },
    {
      "epoch": 1.925461741424802,
      "grad_norm": 0.1605866700410843,
      "learning_rate": 7.167985927880388e-05,
      "loss": 0.0224,
      "step": 2919
    },
    {
      "epoch": 1.9261213720316621,
      "grad_norm": 0.13728179037570953,
      "learning_rate": 7.16358839050132e-05,
      "loss": 0.0085,
      "step": 2920
    },
    {
      "epoch": 1.9267810026385224,
      "grad_norm": 0.16104823350906372,
      "learning_rate": 7.159190853122252e-05,
      "loss": 0.0141,
      "step": 2921
    },
    {
      "epoch": 1.9274406332453826,
      "grad_norm": 0.46136143803596497,
      "learning_rate": 7.154793315743184e-05,
      "loss": 0.0647,
      "step": 2922
    },
    {
      "epoch": 1.9281002638522429,
      "grad_norm": 0.12149818986654282,
      "learning_rate": 7.150395778364116e-05,
      "loss": 0.0141,
      "step": 2923
    },
    {
      "epoch": 1.928759894459103,
      "grad_norm": 0.17970506846904755,
      "learning_rate": 7.145998240985049e-05,
      "loss": 0.0109,
      "step": 2924
    },
    {
      "epoch": 1.929419525065963,
      "grad_norm": 0.047779928892850876,
      "learning_rate": 7.141600703605981e-05,
      "loss": 0.0021,
      "step": 2925
    },
    {
      "epoch": 1.9300791556728232,
      "grad_norm": 0.2621977925300598,
      "learning_rate": 7.137203166226914e-05,
      "loss": 0.0377,
      "step": 2926
    },
    {
      "epoch": 1.9307387862796834,
      "grad_norm": 0.05099867284297943,
      "learning_rate": 7.132805628847845e-05,
      "loss": 0.0029,
      "step": 2927
    },
    {
      "epoch": 1.9313984168865437,
      "grad_norm": 0.28660792112350464,
      "learning_rate": 7.128408091468778e-05,
      "loss": 0.0154,
      "step": 2928
    },
    {
      "epoch": 1.9320580474934037,
      "grad_norm": 0.23628336191177368,
      "learning_rate": 7.124010554089711e-05,
      "loss": 0.0175,
      "step": 2929
    },
    {
      "epoch": 1.9327176781002637,
      "grad_norm": 0.2031453400850296,
      "learning_rate": 7.119613016710642e-05,
      "loss": 0.0194,
      "step": 2930
    },
    {
      "epoch": 1.933377308707124,
      "grad_norm": 0.1407802551984787,
      "learning_rate": 7.115215479331575e-05,
      "loss": 0.0065,
      "step": 2931
    },
    {
      "epoch": 1.9340369393139842,
      "grad_norm": 0.43895167112350464,
      "learning_rate": 7.110817941952507e-05,
      "loss": 0.0241,
      "step": 2932
    },
    {
      "epoch": 1.9346965699208445,
      "grad_norm": 0.14472289383411407,
      "learning_rate": 7.106420404573439e-05,
      "loss": 0.0141,
      "step": 2933
    },
    {
      "epoch": 1.9353562005277045,
      "grad_norm": 0.2860003113746643,
      "learning_rate": 7.10202286719437e-05,
      "loss": 0.0421,
      "step": 2934
    },
    {
      "epoch": 1.9360158311345645,
      "grad_norm": 0.28837892413139343,
      "learning_rate": 7.097625329815303e-05,
      "loss": 0.0377,
      "step": 2935
    },
    {
      "epoch": 1.9366754617414248,
      "grad_norm": 0.1749301701784134,
      "learning_rate": 7.093227792436236e-05,
      "loss": 0.0253,
      "step": 2936
    },
    {
      "epoch": 1.937335092348285,
      "grad_norm": 0.2277473360300064,
      "learning_rate": 7.088830255057169e-05,
      "loss": 0.0231,
      "step": 2937
    },
    {
      "epoch": 1.937994722955145,
      "grad_norm": 0.19682970643043518,
      "learning_rate": 7.084432717678102e-05,
      "loss": 0.0353,
      "step": 2938
    },
    {
      "epoch": 1.9386543535620053,
      "grad_norm": 0.37570247054100037,
      "learning_rate": 7.080035180299033e-05,
      "loss": 0.03,
      "step": 2939
    },
    {
      "epoch": 1.9393139841688654,
      "grad_norm": 0.1742970049381256,
      "learning_rate": 7.075637642919966e-05,
      "loss": 0.0108,
      "step": 2940
    },
    {
      "epoch": 1.9399736147757256,
      "grad_norm": 0.17638802528381348,
      "learning_rate": 7.071240105540897e-05,
      "loss": 0.0201,
      "step": 2941
    },
    {
      "epoch": 1.9406332453825859,
      "grad_norm": 0.21158526837825775,
      "learning_rate": 7.06684256816183e-05,
      "loss": 0.0282,
      "step": 2942
    },
    {
      "epoch": 1.9412928759894459,
      "grad_norm": 0.15666839480400085,
      "learning_rate": 7.062445030782761e-05,
      "loss": 0.0082,
      "step": 2943
    },
    {
      "epoch": 1.941952506596306,
      "grad_norm": 0.29743412137031555,
      "learning_rate": 7.058047493403694e-05,
      "loss": 0.0442,
      "step": 2944
    },
    {
      "epoch": 1.9426121372031662,
      "grad_norm": 0.2694702744483948,
      "learning_rate": 7.053649956024626e-05,
      "loss": 0.0284,
      "step": 2945
    },
    {
      "epoch": 1.9432717678100264,
      "grad_norm": 0.2728174924850464,
      "learning_rate": 7.049252418645558e-05,
      "loss": 0.0202,
      "step": 2946
    },
    {
      "epoch": 1.9439313984168867,
      "grad_norm": 0.2754025161266327,
      "learning_rate": 7.044854881266491e-05,
      "loss": 0.0223,
      "step": 2947
    },
    {
      "epoch": 1.9445910290237467,
      "grad_norm": 0.26483267545700073,
      "learning_rate": 7.040457343887424e-05,
      "loss": 0.0335,
      "step": 2948
    },
    {
      "epoch": 1.9452506596306067,
      "grad_norm": 0.2181401252746582,
      "learning_rate": 7.036059806508357e-05,
      "loss": 0.0113,
      "step": 2949
    },
    {
      "epoch": 1.945910290237467,
      "grad_norm": 0.18007490038871765,
      "learning_rate": 7.031662269129288e-05,
      "loss": 0.0152,
      "step": 2950
    },
    {
      "epoch": 1.9465699208443272,
      "grad_norm": 0.07463125139474869,
      "learning_rate": 7.02726473175022e-05,
      "loss": 0.0039,
      "step": 2951
    },
    {
      "epoch": 1.9472295514511875,
      "grad_norm": 0.29378682374954224,
      "learning_rate": 7.022867194371152e-05,
      "loss": 0.0342,
      "step": 2952
    },
    {
      "epoch": 1.9478891820580475,
      "grad_norm": 0.16628804802894592,
      "learning_rate": 7.018469656992085e-05,
      "loss": 0.0103,
      "step": 2953
    },
    {
      "epoch": 1.9485488126649075,
      "grad_norm": 0.3508700430393219,
      "learning_rate": 7.014072119613016e-05,
      "loss": 0.0448,
      "step": 2954
    },
    {
      "epoch": 1.9492084432717678,
      "grad_norm": 0.11937953531742096,
      "learning_rate": 7.009674582233949e-05,
      "loss": 0.0058,
      "step": 2955
    },
    {
      "epoch": 1.949868073878628,
      "grad_norm": 0.23947545886039734,
      "learning_rate": 7.005277044854882e-05,
      "loss": 0.0225,
      "step": 2956
    },
    {
      "epoch": 1.9505277044854883,
      "grad_norm": 0.22055967152118683,
      "learning_rate": 7.000879507475813e-05,
      "loss": 0.0338,
      "step": 2957
    },
    {
      "epoch": 1.9511873350923483,
      "grad_norm": 0.21334172785282135,
      "learning_rate": 6.996481970096747e-05,
      "loss": 0.0178,
      "step": 2958
    },
    {
      "epoch": 1.9518469656992083,
      "grad_norm": 0.20147818326950073,
      "learning_rate": 6.992084432717679e-05,
      "loss": 0.0096,
      "step": 2959
    },
    {
      "epoch": 1.9525065963060686,
      "grad_norm": 0.2729618549346924,
      "learning_rate": 6.987686895338611e-05,
      "loss": 0.0277,
      "step": 2960
    },
    {
      "epoch": 1.9531662269129288,
      "grad_norm": 0.1506851762533188,
      "learning_rate": 6.983289357959543e-05,
      "loss": 0.0208,
      "step": 2961
    },
    {
      "epoch": 1.9538258575197889,
      "grad_norm": 0.04038381204009056,
      "learning_rate": 6.978891820580476e-05,
      "loss": 0.0016,
      "step": 2962
    },
    {
      "epoch": 1.9544854881266491,
      "grad_norm": 0.1499798446893692,
      "learning_rate": 6.974494283201407e-05,
      "loss": 0.0116,
      "step": 2963
    },
    {
      "epoch": 1.9551451187335092,
      "grad_norm": 0.1280926614999771,
      "learning_rate": 6.97009674582234e-05,
      "loss": 0.0086,
      "step": 2964
    },
    {
      "epoch": 1.9558047493403694,
      "grad_norm": 0.2520540952682495,
      "learning_rate": 6.965699208443272e-05,
      "loss": 0.0424,
      "step": 2965
    },
    {
      "epoch": 1.9564643799472297,
      "grad_norm": 0.11058218777179718,
      "learning_rate": 6.961301671064204e-05,
      "loss": 0.0057,
      "step": 2966
    },
    {
      "epoch": 1.9571240105540897,
      "grad_norm": 0.2484280914068222,
      "learning_rate": 6.956904133685137e-05,
      "loss": 0.0347,
      "step": 2967
    },
    {
      "epoch": 1.9577836411609497,
      "grad_norm": 0.17642061412334442,
      "learning_rate": 6.952506596306069e-05,
      "loss": 0.0234,
      "step": 2968
    },
    {
      "epoch": 1.95844327176781,
      "grad_norm": 0.07907155156135559,
      "learning_rate": 6.948109058927002e-05,
      "loss": 0.005,
      "step": 2969
    },
    {
      "epoch": 1.9591029023746702,
      "grad_norm": 0.1505434811115265,
      "learning_rate": 6.943711521547933e-05,
      "loss": 0.015,
      "step": 2970
    },
    {
      "epoch": 1.9597625329815305,
      "grad_norm": 0.041935309767723083,
      "learning_rate": 6.939313984168866e-05,
      "loss": 0.0018,
      "step": 2971
    },
    {
      "epoch": 1.9604221635883905,
      "grad_norm": 0.19211220741271973,
      "learning_rate": 6.934916446789798e-05,
      "loss": 0.0211,
      "step": 2972
    },
    {
      "epoch": 1.9610817941952505,
      "grad_norm": 0.29570892453193665,
      "learning_rate": 6.93051890941073e-05,
      "loss": 0.0342,
      "step": 2973
    },
    {
      "epoch": 1.9617414248021108,
      "grad_norm": 0.12242046743631363,
      "learning_rate": 6.926121372031663e-05,
      "loss": 0.0092,
      "step": 2974
    },
    {
      "epoch": 1.962401055408971,
      "grad_norm": 0.2033359855413437,
      "learning_rate": 6.921723834652595e-05,
      "loss": 0.0158,
      "step": 2975
    },
    {
      "epoch": 1.9630606860158313,
      "grad_norm": 0.23931747674942017,
      "learning_rate": 6.917326297273527e-05,
      "loss": 0.017,
      "step": 2976
    },
    {
      "epoch": 1.9637203166226913,
      "grad_norm": 0.16890238225460052,
      "learning_rate": 6.912928759894459e-05,
      "loss": 0.0152,
      "step": 2977
    },
    {
      "epoch": 1.9643799472295513,
      "grad_norm": 0.332594633102417,
      "learning_rate": 6.908531222515391e-05,
      "loss": 0.0381,
      "step": 2978
    },
    {
      "epoch": 1.9650395778364116,
      "grad_norm": 0.34289899468421936,
      "learning_rate": 6.904133685136324e-05,
      "loss": 0.0278,
      "step": 2979
    },
    {
      "epoch": 1.9656992084432718,
      "grad_norm": 0.16652153432369232,
      "learning_rate": 6.899736147757257e-05,
      "loss": 0.02,
      "step": 2980
    },
    {
      "epoch": 1.966358839050132,
      "grad_norm": 0.26240772008895874,
      "learning_rate": 6.895338610378188e-05,
      "loss": 0.0451,
      "step": 2981
    },
    {
      "epoch": 1.9670184696569921,
      "grad_norm": 0.2182132452726364,
      "learning_rate": 6.890941072999121e-05,
      "loss": 0.0214,
      "step": 2982
    },
    {
      "epoch": 1.9676781002638521,
      "grad_norm": 0.20795932412147522,
      "learning_rate": 6.886543535620054e-05,
      "loss": 0.0201,
      "step": 2983
    },
    {
      "epoch": 1.9683377308707124,
      "grad_norm": 0.1849905252456665,
      "learning_rate": 6.882145998240985e-05,
      "loss": 0.0154,
      "step": 2984
    },
    {
      "epoch": 1.9689973614775726,
      "grad_norm": 0.14260658621788025,
      "learning_rate": 6.877748460861918e-05,
      "loss": 0.0107,
      "step": 2985
    },
    {
      "epoch": 1.9696569920844327,
      "grad_norm": 0.2104092687368393,
      "learning_rate": 6.87335092348285e-05,
      "loss": 0.0229,
      "step": 2986
    },
    {
      "epoch": 1.970316622691293,
      "grad_norm": 0.3626486361026764,
      "learning_rate": 6.868953386103782e-05,
      "loss": 0.0617,
      "step": 2987
    },
    {
      "epoch": 1.970976253298153,
      "grad_norm": 0.14923593401908875,
      "learning_rate": 6.864555848724713e-05,
      "loss": 0.0157,
      "step": 2988
    },
    {
      "epoch": 1.9716358839050132,
      "grad_norm": 0.14062491059303284,
      "learning_rate": 6.860158311345646e-05,
      "loss": 0.0087,
      "step": 2989
    },
    {
      "epoch": 1.9722955145118735,
      "grad_norm": 0.19384847581386566,
      "learning_rate": 6.855760773966579e-05,
      "loss": 0.0228,
      "step": 2990
    },
    {
      "epoch": 1.9729551451187335,
      "grad_norm": 0.22148042917251587,
      "learning_rate": 6.851363236587512e-05,
      "loss": 0.0163,
      "step": 2991
    },
    {
      "epoch": 1.9736147757255935,
      "grad_norm": 0.17843545973300934,
      "learning_rate": 6.846965699208445e-05,
      "loss": 0.0227,
      "step": 2992
    },
    {
      "epoch": 1.9742744063324538,
      "grad_norm": 0.3592085540294647,
      "learning_rate": 6.842568161829376e-05,
      "loss": 0.0609,
      "step": 2993
    },
    {
      "epoch": 1.974934036939314,
      "grad_norm": 0.19764181971549988,
      "learning_rate": 6.838170624450309e-05,
      "loss": 0.0083,
      "step": 2994
    },
    {
      "epoch": 1.9755936675461743,
      "grad_norm": 0.18136391043663025,
      "learning_rate": 6.83377308707124e-05,
      "loss": 0.0366,
      "step": 2995
    },
    {
      "epoch": 1.9762532981530343,
      "grad_norm": 0.09767881780862808,
      "learning_rate": 6.829375549692173e-05,
      "loss": 0.007,
      "step": 2996
    },
    {
      "epoch": 1.9769129287598943,
      "grad_norm": 0.2846497595310211,
      "learning_rate": 6.824978012313104e-05,
      "loss": 0.0521,
      "step": 2997
    },
    {
      "epoch": 1.9775725593667546,
      "grad_norm": 0.21922342479228973,
      "learning_rate": 6.820580474934037e-05,
      "loss": 0.0142,
      "step": 2998
    },
    {
      "epoch": 1.9782321899736148,
      "grad_norm": 0.05860615149140358,
      "learning_rate": 6.81618293755497e-05,
      "loss": 0.0035,
      "step": 2999
    },
    {
      "epoch": 1.978891820580475,
      "grad_norm": 0.12740202248096466,
      "learning_rate": 6.811785400175902e-05,
      "loss": 0.0105,
      "step": 3000
    },
    {
      "epoch": 1.979551451187335,
      "grad_norm": 0.14641861617565155,
      "learning_rate": 6.807387862796835e-05,
      "loss": 0.0142,
      "step": 3001
    },
    {
      "epoch": 1.9802110817941951,
      "grad_norm": 0.13161900639533997,
      "learning_rate": 6.802990325417767e-05,
      "loss": 0.0098,
      "step": 3002
    },
    {
      "epoch": 1.9808707124010554,
      "grad_norm": 0.10382699221372604,
      "learning_rate": 6.7985927880387e-05,
      "loss": 0.0066,
      "step": 3003
    },
    {
      "epoch": 1.9815303430079156,
      "grad_norm": 0.2339610755443573,
      "learning_rate": 6.794195250659631e-05,
      "loss": 0.0252,
      "step": 3004
    },
    {
      "epoch": 1.982189973614776,
      "grad_norm": 0.04586242139339447,
      "learning_rate": 6.789797713280563e-05,
      "loss": 0.002,
      "step": 3005
    },
    {
      "epoch": 1.982849604221636,
      "grad_norm": 0.19235509634017944,
      "learning_rate": 6.785400175901495e-05,
      "loss": 0.0328,
      "step": 3006
    },
    {
      "epoch": 1.983509234828496,
      "grad_norm": 0.37389814853668213,
      "learning_rate": 6.781002638522428e-05,
      "loss": 0.0447,
      "step": 3007
    },
    {
      "epoch": 1.9841688654353562,
      "grad_norm": 0.20745636522769928,
      "learning_rate": 6.776605101143359e-05,
      "loss": 0.0215,
      "step": 3008
    },
    {
      "epoch": 1.9848284960422165,
      "grad_norm": 0.16259123384952545,
      "learning_rate": 6.772207563764292e-05,
      "loss": 0.0105,
      "step": 3009
    },
    {
      "epoch": 1.9854881266490765,
      "grad_norm": 0.3052310645580292,
      "learning_rate": 6.767810026385225e-05,
      "loss": 0.0249,
      "step": 3010
    },
    {
      "epoch": 1.9861477572559367,
      "grad_norm": 0.17532971501350403,
      "learning_rate": 6.763412489006157e-05,
      "loss": 0.0175,
      "step": 3011
    },
    {
      "epoch": 1.9868073878627968,
      "grad_norm": 0.109926737844944,
      "learning_rate": 6.75901495162709e-05,
      "loss": 0.0049,
      "step": 3012
    },
    {
      "epoch": 1.987467018469657,
      "grad_norm": 0.16249775886535645,
      "learning_rate": 6.754617414248021e-05,
      "loss": 0.0203,
      "step": 3013
    },
    {
      "epoch": 1.9881266490765173,
      "grad_norm": 0.08253919333219528,
      "learning_rate": 6.750219876868954e-05,
      "loss": 0.0062,
      "step": 3014
    },
    {
      "epoch": 1.9887862796833773,
      "grad_norm": 0.49170103669166565,
      "learning_rate": 6.745822339489886e-05,
      "loss": 0.0564,
      "step": 3015
    },
    {
      "epoch": 1.9894459102902373,
      "grad_norm": 0.0715288370847702,
      "learning_rate": 6.741424802110818e-05,
      "loss": 0.0038,
      "step": 3016
    },
    {
      "epoch": 1.9901055408970976,
      "grad_norm": 0.26855093240737915,
      "learning_rate": 6.73702726473175e-05,
      "loss": 0.0276,
      "step": 3017
    },
    {
      "epoch": 1.9907651715039578,
      "grad_norm": 0.4431125223636627,
      "learning_rate": 6.732629727352682e-05,
      "loss": 0.0633,
      "step": 3018
    },
    {
      "epoch": 1.991424802110818,
      "grad_norm": 0.10355092585086823,
      "learning_rate": 6.728232189973615e-05,
      "loss": 0.0097,
      "step": 3019
    },
    {
      "epoch": 1.992084432717678,
      "grad_norm": 0.15154799818992615,
      "learning_rate": 6.723834652594547e-05,
      "loss": 0.0156,
      "step": 3020
    },
    {
      "epoch": 1.9927440633245381,
      "grad_norm": 0.11612600088119507,
      "learning_rate": 6.71943711521548e-05,
      "loss": 0.0128,
      "step": 3021
    },
    {
      "epoch": 1.9934036939313984,
      "grad_norm": 0.14091430604457855,
      "learning_rate": 6.715039577836412e-05,
      "loss": 0.0178,
      "step": 3022
    },
    {
      "epoch": 1.9940633245382586,
      "grad_norm": 0.1546829789876938,
      "learning_rate": 6.710642040457345e-05,
      "loss": 0.014,
      "step": 3023
    },
    {
      "epoch": 1.9947229551451189,
      "grad_norm": 0.2749294936656952,
      "learning_rate": 6.706244503078276e-05,
      "loss": 0.0305,
      "step": 3024
    },
    {
      "epoch": 1.995382585751979,
      "grad_norm": 0.18040087819099426,
      "learning_rate": 6.701846965699209e-05,
      "loss": 0.0126,
      "step": 3025
    },
    {
      "epoch": 1.996042216358839,
      "grad_norm": 0.30964407324790955,
      "learning_rate": 6.69744942832014e-05,
      "loss": 0.0373,
      "step": 3026
    },
    {
      "epoch": 1.9967018469656992,
      "grad_norm": 0.13901686668395996,
      "learning_rate": 6.693051890941073e-05,
      "loss": 0.0084,
      "step": 3027
    },
    {
      "epoch": 1.9973614775725594,
      "grad_norm": 0.09942308068275452,
      "learning_rate": 6.688654353562006e-05,
      "loss": 0.0043,
      "step": 3028
    },
    {
      "epoch": 1.9980211081794197,
      "grad_norm": 0.19745442271232605,
      "learning_rate": 6.684256816182937e-05,
      "loss": 0.0296,
      "step": 3029
    },
    {
      "epoch": 1.9986807387862797,
      "grad_norm": 0.17693781852722168,
      "learning_rate": 6.67985927880387e-05,
      "loss": 0.025,
      "step": 3030
    },
    {
      "epoch": 1.9993403693931397,
      "grad_norm": 0.13712015748023987,
      "learning_rate": 6.675461741424803e-05,
      "loss": 0.0093,
      "step": 3031
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2903669476509094,
      "learning_rate": 6.671064204045736e-05,
      "loss": 0.0289,
      "step": 3032
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.7011494252873564,
      "eval_f1": 0.5138620245003224,
      "eval_keras_BCE": 1.001787781715393,
      "eval_loss": 1.0017483234405518,
      "eval_precision": 0.7526133406005988,
      "eval_runtime": 1.459,
      "eval_samples_per_second": 119.256,
      "eval_steps_per_second": 15.078,
      "eval_weigthed BCE": 0.6701614260673523,
      "step": 3032
    }
  ],
  "logging_steps": 1,
  "max_steps": 4548,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.08702171789565e+17,
  "train_batch_size": 256,
  "trial_name": null,
  "trial_params": null
}
