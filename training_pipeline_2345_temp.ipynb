{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64371899",
   "metadata": {},
   "source": [
    "To do:\n",
    "- ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "- Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    - In addition to this, look at the implications of gradient accumulation steps\n",
    "    - Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "- Early stopping: load_best_model_at_end=True\n",
    "- Split into validation set as well (80:10:10)\n",
    "- Hyperparameter tuning (Alpha, learning rate, batch size so on - not sure how to figure this out)\n",
    "    - There is precedence for no hyperparameter tuning from the author of the OG NLI model that DEBATE is based on = Due to computational restrains and the points from this paper, no hyperparameter tuning was performed in this case. The model tuning in itself is also not the primary focus in this paper, but simply serves as a tool for the actual inquiry into blame in the Danish Parliament\n",
    "- Context / No context?\n",
    "    - Most likely no context, as we do not have the academic reasons for doing so \n",
    "    - Model actually also seems to perform better without (f1 of .66 for no context on random test set vs. . 55 on gold labelled for context model) This might also make sense as the context is not taken into account in the DEBATE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d09f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "#import scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import Conv1D, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 308,072,450 || trainable%: 0.1755\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",  # Fine-tuning all linear (classification, attention... layers)\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to='wandb',\n",
    "    output_dir='./full_tune_results',\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4, # Learning rate copied from mmBERT paper on embedding sweep of LR (1e-4) as they found this to perform best\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=256, # Batching at 256 to balance generalization and efficient training\n",
    "    gradient_accumulation_steps=1,  # Gradient of 1 as full batch fits in memory, accumulation then only slows\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,  # Enable mixed precision\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    remove_unused_columns=True, # Avoiding manual handling of residual text columns\n",
    "    max_grad_norm=1.0,\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe_2345 = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/subset_2_3_4_5_cleaned_training_data.json\")\n",
    "\n",
    "val_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Model_data/validation_set.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=512, # Padding to 512 to massively cut down on computation compared to base 8,192 tokens. \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4402b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 176/176 [00:00<00:00, 518.66 examples/s] \n",
      "Map: 100%|██████████| 3600/3600 [00:06<00:00, 525.34 examples/s] \n"
     ]
    }
   ],
   "source": [
    "val_dataframe = val_dataframe[['text', 'label']]\n",
    "\n",
    "dataframe_2345 = dataframe_2345[['text', 'label']]\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe_2345)\n",
    "\n",
    "tokenized_val = val_dataset.map(tokenize_function)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2270f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bincrossentropy(true, pred, weight_zero = 99.0, weight_one = 1):\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed to represent class imbalance in the dataset.\n",
    "        \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalized 10 times as much as false negatives.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    #weights /= (weight_one + weight_zero) # Normalizing to be more consistent with regular BCE for comparison \n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return np.mean(weighted_bin_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #From logits to probabilities\n",
    "    probs_2d = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    probs = probs_2d[:, 1]  # positive class extraction\n",
    "    \n",
    "    weigthted_bce = weighted_bincrossentropy(labels, probs)\n",
    "    keras_bce = binary_crossentropy(labels, probs)\n",
    "    keras_bce = float(np.mean(keras_bce.numpy()))  # Converting from keras eagertensor to float value\n",
    "    \n",
    "    # Wrapping all metrics to floats for json serialization during model eval\n",
    "    return {\n",
    "        'keras_BCE': keras_bce,\n",
    "        'weighted BCE': weigthted_bce,\n",
    "        'recall': float(recall_score(labels, probs.round())),\n",
    "        'precision': float(average_precision_score(labels, probs)),\n",
    "        'accuracy': float(accuracy_score(labels, probs.round())), # Need rounding for these two computations (integer required)\n",
    "        'f1': float(f1_score(labels, probs.round(), average='macro')) # macro f1 is better for imbalanced dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca18287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trainer class (weigthed)\n",
    "from collections import Counter\n",
    "\n",
    "labels = test_dataframe['label'].tolist()\n",
    "class_counts = Counter(labels)\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Higher weight = more emphasis\n",
    "weights = [total/class_counts[0], total/class_counts[1]]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "#define custom trainer that uses weigted loss\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Define weighted loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36abd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 1:20:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.476800</td>\n",
       "      <td>0.417300</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.838144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.170700</td>\n",
       "      <td>0.414438</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.831302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.143700</td>\n",
       "      <td>0.463640</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.802818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=5.004009094154625, metrics={'train_runtime': 4854.5266, 'train_samples_per_second': 2.225, 'train_steps_per_second': 0.023, 'total_flos': 5.916996698112e+16, 'train_loss': 5.004009094154625, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Look into learning rates, model is currently overfitting quite drastically (\"small\" test-set)\n",
    "Normalizing weigthed BCE or no?\n",
    "Look into regularization, dropout and early stopping to avoid overfitting\n",
    "'''\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadde19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.463640034198761, 'eval_accuracy': 0.8125, 'eval_f1': 0.8028178577491087, 'eval_runtime': 32.7459, 'eval_samples_per_second': 5.375, 'eval_steps_per_second': 0.672, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "with open(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Evalresult2345templates.txt\", \"w\") as f:\n",
    "    f.write(str(eval_results))\n",
    "\n",
    "# This is where we should very much remember to save the finetuned model locally as this contains the new weights for use in analyzing new text\n",
    "lora_model.save_pretrained(f\"output/mmBERT/template_2_3_4_5_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We would expect to see a gradual decrease in both training and validation loss.\n",
    "If either om them split too far from eachother that indicates issues with the training process.\n",
    "The process itself should be pretty smooth with no dips either up or down.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
