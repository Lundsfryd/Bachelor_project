{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3999eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to train model, first the training data needs to be made\n",
    "\n",
    "# Therefore data before 2000 will be made into sentence and context, \n",
    "#but blame will only be assigned for the sentence in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98d8567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#junk\n",
    "#df['date'] = pd.to_datetime(df['date'])\n",
    "#df.to_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Corp_Folketing_V2.csv\")\n",
    "\n",
    "'''\n",
    "from datetime import date\n",
    "annotation_data = df[(df['date'].dt.date <= date(2000,1,1))]\n",
    "annotation_data.to_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/annotation_data.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768e276d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>agenda</th>\n",
       "      <th>speechnumber</th>\n",
       "      <th>speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>party.facts.id</th>\n",
       "      <th>chair</th>\n",
       "      <th>terms</th>\n",
       "      <th>text</th>\n",
       "      <th>parliament</th>\n",
       "      <th>iso3country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Dagsorden</td>\n",
       "      <td>1</td>\n",
       "      <td>Gert Petersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>191</td>\n",
       "      <td>Mødet er åbnet. I henhold til grundloven er Fo...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Dagsorden</td>\n",
       "      <td>2</td>\n",
       "      <td>Formanden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>182</td>\n",
       "      <td>Jeg vil gerne takke Tinget for den tillid, man...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Statsministerens redegørelse i henhold til gru...</td>\n",
       "      <td>3</td>\n",
       "      <td>Poul Nyrup Rasmussen</td>\n",
       "      <td>S</td>\n",
       "      <td>379.0</td>\n",
       "      <td>False</td>\n",
       "      <td>18662</td>\n",
       "      <td>For 25 år siden sagde et flertal i befolkninge...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1997-10-09</td>\n",
       "      <td>1) Indstilling fra Udvalget til Valgs Prøvelse.</td>\n",
       "      <td>2</td>\n",
       "      <td>Formanden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "      <td>Fra Udvalget til Valgs Prøvelse har jeg modtag...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1997-10-09</td>\n",
       "      <td>2) Forhandling om redegørelse nr. R 1.</td>\n",
       "      <td>3</td>\n",
       "      <td>Torben Lund</td>\n",
       "      <td>S</td>\n",
       "      <td>379.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2865</td>\n",
       "      <td>Vi står over for en meget afgørende folketings...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        date                                             agenda  \\\n",
       "0           0  1997-10-07                                          Dagsorden   \n",
       "1           1  1997-10-07                                          Dagsorden   \n",
       "2           2  1997-10-07  Statsministerens redegørelse i henhold til gru...   \n",
       "3           3  1997-10-09    1) Indstilling fra Udvalget til Valgs Prøvelse.   \n",
       "4           4  1997-10-09             2) Forhandling om redegørelse nr. R 1.   \n",
       "\n",
       "   speechnumber               speaker party  party.facts.id  chair  terms  \\\n",
       "0             1         Gert Petersen   NaN             NaN   True    191   \n",
       "1             2             Formanden   NaN             NaN   True    182   \n",
       "2             3  Poul Nyrup Rasmussen     S           379.0  False  18662   \n",
       "3             2             Formanden   NaN             NaN   True     47   \n",
       "4             3           Torben Lund     S           379.0  False   2865   \n",
       "\n",
       "                                                text    parliament iso3country  \n",
       "0  Mødet er åbnet. I henhold til grundloven er Fo...  DK-Folketing         DNK  \n",
       "1  Jeg vil gerne takke Tinget for den tillid, man...  DK-Folketing         DNK  \n",
       "2  For 25 år siden sagde et flertal i befolkninge...  DK-Folketing         DNK  \n",
       "3  Fra Udvalget til Valgs Prøvelse har jeg modtag...  DK-Folketing         DNK  \n",
       "4  Vi står over for en meget afgørende folketings...  DK-Folketing         DNK  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load in data\n",
    "\n",
    "df = pd.read_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Corp_Folketing_V2.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b050966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def simplify_agenda(text):\n",
    "    text = text.lower()  # make it case-insensitive\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Match anything containing \"behandling af beslutningsforslag\"\n",
    "    if re.search(r'behandling af beslutningsforslag', text):\n",
    "        return 'behandling af beslutningsforslag'\n",
    "    \n",
    "    elif re.search(r'forhandling', text):\n",
    "        return 'forhandling'\n",
    "\n",
    "    elif re.search(r'spørgsmål om fremme af forespørgsel', text):\n",
    "        return 'spørgsmål om fremme af forespørgsel'\n",
    "\n",
    "    elif re.search(r'spørgsmål til ministrene', text):\n",
    "        return 'spørgsmål til ministrene'\n",
    "\n",
    "    elif re.search(r'behandling af lovforslag', text):\n",
    "        return 'behandling af lovforslag'\n",
    "\n",
    "    elif re.search(r'fortsættelse af forespørgsel', text):\n",
    "        return 'fortsættelse af forespørgsel'\n",
    "\n",
    "    elif re.search(r'spørgsmål om meddelelse af orlov til', text):\n",
    "        return 'spørgsmål om meddelelse af orlov til'\n",
    "\n",
    "\n",
    "    elif re.search(r'indstilling fra udvalget til valgs prøvelse', text):\n",
    "        return 'indstilling fra udvalget til valgs prøvelse'\n",
    "\n",
    "    elif re.search(r'til ministeren', text):\n",
    "        return 'til ministeren'\n",
    "\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "# Apply to your dataframe\n",
    "df['agenda_simplified'] = df['agenda'].apply(simplify_agenda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d13388f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dagsorden',\n",
       "       'statsministerens redegørelse i henhold til grundlovens § 38 (mundtlig del)',\n",
       "       'indstilling fra udvalget til valgs prøvelse', ...,\n",
       "       'forespørgsel nr. f 3:  forespørgsel til udlændinge- og integrationsministeren:  kan ministeren oplyse, om regeringen ønsker at forhindre udenlandske pengeoverførsler til byggeri af moskeer og til muslimske kulturelle, ideologiske og religiøse formål i danmark, og i givet fald, hvordan?  af martin henriksen (df), marie krarup (df) og christian langballe (df).  (anmeldelse 03.10.2018. fremme 09.10.2018).  kl. 14:29',\n",
       "       'forespørgsel nr. f 16:  forespørgsel til beskæftigelsesministeren:  hvad kan ministeren oplyse om sin holdning til ekspertudvalget om udredning af arbejdsmiljøindsatsens anbefalinger i rapporten »et nyt og forbedret arbejdsmiljø – overvejelser og anbefalinger«, som blev offentliggjort den 27. september 2018, og hvad vil ministeren konkret gøre for at styrke det forebyggende arbejde og nedbringe antallet af mennesker, der arbejder under belastende arbejdsmiljøforhold?  af christian juhl (el) og karsten hønge (sf).  (anmeldelse 01.11.2018. fremme 06.11.2018).  kl. 15:18',\n",
       "       'forespørgsel nr. f 10:  forespørgsel til ældreministeren:  hvilke tiltag vil regeringen igangsætte inden årsskiftet for at sikre, at danskerne trygt kan gå deres alderdom i møde?  af jeppe jakobsen (df), karina adsbøl (df), pernille bendixen (df), susanne eilersen (df) og karin nødgaard (df).  (anmeldelse 09.10.2018. fremme 11.10.2018).  kl. 10:00'],\n",
       "      shape=(1699,), dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['agenda_simplified'].value_counts()\n",
    "df['agenda_simplified'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccb25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ad81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eaf366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Example: annotation_data is your original DataFrame\n",
    "paragraphs = annotation_data['text'].tolist()\n",
    "original_indices = list(annotation_data.index)  # keep original index for para_id\n",
    "\n",
    "# Shuffle paragraphs (zip together with original index)\n",
    "zipped = list(zip(original_indices, paragraphs))\n",
    "random.shuffle(zipped)\n",
    "\n",
    "# Unzip\n",
    "shuffled_indices, shuffled_paragraphs = zip(*zipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load Danish pipeline\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "def split_paragraph(paragraph: str):\n",
    "    doc = nlp(paragraph)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86b517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_samples(paragraph):\n",
    "    sentences = split_paragraph(paragraph)\n",
    "    samples = []\n",
    "    \n",
    "    for i, sent in enumerate(sentences):\n",
    "        prev_sent = sentences[i-1] if i > 0 else \"\"\n",
    "        next_sent = sentences[i+1] if i < len(sentences)-1 else \"\"\n",
    "        \n",
    "        #text = prev_sent + \" [SEP] \" + sent + \" [SEP] \" + next_sent\n",
    "\n",
    "        sent_id = i\n",
    "\n",
    "        # Three-part input with [SEP] handled by tokenizer later\n",
    "        samples.append({\n",
    "            \"prev\": prev_sent,\n",
    "            \"target\": sent,\n",
    "            \"next\": next_sent\n",
    "        })\n",
    "        \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa9670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_sentence(sentence, para_id, sent_id):\n",
    "    \"\"\"\n",
    "    Annotate a sentence interactively.\n",
    "    \n",
    "    Returns:\n",
    "        label (int) or None if escaped\n",
    "    \"\"\"\n",
    "    print(f\"\\nPara {para_id}, Sent {sent_id}:\")\n",
    "    print(f\"\\\"{sentence}\\\"\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Label this sentence (0 = no blame, 1 = blame, 'q' to quit): \").strip()\n",
    "        if user_input.lower() == 'q':\n",
    "            return None  # escape\n",
    "        elif user_input in ['0', '1']:\n",
    "            return int(user_input)\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 0, 1, or 'q'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_sentences = []\n",
    "\n",
    "for para_id, para in zip(shuffled_indices, shuffled_paragraphs):\n",
    "    samples = build_samples(para)  # build prev/target/next windows\n",
    "\n",
    "    for sent_idx, s in enumerate(samples):\n",
    "        # annotate interactively\n",
    "        label = annotate_sentence(s['target'], para_id, sent_idx)\n",
    "        if label is None:  # user pressed 'q'\n",
    "            print(\"Annotation interrupted. Saving progress...\")\n",
    "            break\n",
    "        \n",
    "        labelled_sentences.append({\n",
    "            \"para_id\": para_id,   # original index\n",
    "            \"sent_id\": sent_idx,\n",
    "            \"prev\": s['prev'],\n",
    "            \"target\": s['target'],\n",
    "            \"next\": s['next'],\n",
    "            \"label\": label\n",
    "        })\n",
    "    else:\n",
    "        continue  # only if inner loop wasn't broken\n",
    "    break  # breaks outer loop if user quits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b48e004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "data = pd.DataFrame(labelled_sentences)\n",
    "data.to_csv(\"annotated_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd9d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#later when tokenizing:\n",
    "#HuggingFace will automatically insert [SEP] between these segments.\n",
    "#For BERT-like models, it also uses segment embeddings to distinguish the three parts.\n",
    "\n",
    "'''\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"prev\"], \n",
    "        batch[\"target\"], \n",
    "        batch[\"next\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"annotated_sentences.csv\")\n",
    "\n",
    "# Keep only the necessary columns\n",
    "df = df[['prev', 'target', 'next', 'label']]\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"  # works for Danish\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['prev'],\n",
    "        example['target'],\n",
    "        example['next'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256 #OBS\n",
    "    )\n",
    "\n",
    "#tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "\n",
    "#initialize model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split tokenized dataset\n",
    "splits = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = splits['train']\n",
    "test_dataset = splits['test']\n",
    "\n",
    "\n",
    "#define metrics / Two options have been listed below. The second i probably preferred as ittakes the amount of 0 to 1' into account\n",
    "# First option\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "#second option\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "labels = df['label'].tolist()\n",
    "class_counts = Counter(labels)\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Higher weight = more emphasis\n",
    "weights = [total/class_counts[0], total/class_counts[1]]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "#define custom trainer that uses weigted loss\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Define weighted loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# And implement the second option in training\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "#set up trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blame_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "#train\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
