{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16609cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import transformers \n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db06738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_json(\"/work/MarkusLundsfrydJensen#1865/Training_data/training_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de0f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_TD = training_data.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c156959d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subset_TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a055c9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>sentence_nr</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>preceding_sentence</th>\n",
       "      <th>succeeding_sent</th>\n",
       "      <th>current_speaker_in_government</th>\n",
       "      <th>parties_in_government</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1442409</th>\n",
       "      <td>22028</td>\n",
       "      <td>34</td>\n",
       "      <td>Der har jo altid  været diskussioner her i Fol...</td>\n",
       "      <td>Poul Nyrup Rasmussen</td>\n",
       "      <td>S</td>\n",
       "      <td>Jeg synes ikke, det er smukt at se på, og jeg ...</td>\n",
       "      <td>Folketinget har været her, om, hvem regeringen...</td>\n",
       "      <td>True</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1999-02-11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268120</th>\n",
       "      <td>23743</td>\n",
       "      <td>1</td>\n",
       "      <td>Når vi også har givet den rabat til DSB, som h...</td>\n",
       "      <td>Erling Christensen</td>\n",
       "      <td>S</td>\n",
       "      <td>Kort og godt handler det om liberaliseringen, ...</td>\n",
       "      <td>Og mit spørgsmål gik jo ganske enkelt på, at d...</td>\n",
       "      <td>True</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1999-03-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813164</th>\n",
       "      <td>1225</td>\n",
       "      <td>5</td>\n",
       "      <td>Et af de centrale punkter i Enhedslistens fors...</td>\n",
       "      <td>Jan Petersen</td>\n",
       "      <td>S</td>\n",
       "      <td>Det er i højere grad en  debat om, hvilke midl...</td>\n",
       "      <td>Ministeren har svaret på spørgsmålet om den st...</td>\n",
       "      <td>True</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1997-10-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010866</th>\n",
       "      <td>19079</td>\n",
       "      <td>15</td>\n",
       "      <td>Panelet er sammensat af detailhandelen,  forbr...</td>\n",
       "      <td>Pia Larsen</td>\n",
       "      <td>V</td>\n",
       "      <td>Det bliver ifølge lovforslaget op til et panel...</td>\n",
       "      <td>Panelets rolle som  stopklods for ophævelse af...</td>\n",
       "      <td>False</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1998-12-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832898</th>\n",
       "      <td>21409</td>\n",
       "      <td>4</td>\n",
       "      <td>Redegørelsen fra undervisningsministeren og  f...</td>\n",
       "      <td>Klaus Kjær</td>\n",
       "      <td>DF</td>\n",
       "      <td>Det er derfor  også vigtigt, at vi lytter til ...</td>\n",
       "      <td>Redegørelsen er således et udtryk for regering...</td>\n",
       "      <td>False</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1999-02-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         paragraph  sentence_nr  \\\n",
       "1442409      22028           34   \n",
       "268120       23743            1   \n",
       "813164        1225            5   \n",
       "1010866      19079           15   \n",
       "1832898      21409            4   \n",
       "\n",
       "                                                      text  \\\n",
       "1442409  Der har jo altid  været diskussioner her i Fol...   \n",
       "268120   Når vi også har givet den rabat til DSB, som h...   \n",
       "813164   Et af de centrale punkter i Enhedslistens fors...   \n",
       "1010866  Panelet er sammensat af detailhandelen,  forbr...   \n",
       "1832898  Redegørelsen fra undervisningsministeren og  f...   \n",
       "\n",
       "                      speaker party  \\\n",
       "1442409  Poul Nyrup Rasmussen     S   \n",
       "268120     Erling Christensen     S   \n",
       "813164           Jan Petersen     S   \n",
       "1010866            Pia Larsen     V   \n",
       "1832898            Klaus Kjær    DF   \n",
       "\n",
       "                                        preceding_sentence  \\\n",
       "1442409  Jeg synes ikke, det er smukt at se på, og jeg ...   \n",
       "268120   Kort og godt handler det om liberaliseringen, ...   \n",
       "813164   Det er i højere grad en  debat om, hvilke midl...   \n",
       "1010866  Det bliver ifølge lovforslaget op til et panel...   \n",
       "1832898  Det er derfor  også vigtigt, at vi lytter til ...   \n",
       "\n",
       "                                           succeeding_sent  \\\n",
       "1442409  Folketinget har været her, om, hvem regeringen...   \n",
       "268120   Og mit spørgsmål gik jo ganske enkelt på, at d...   \n",
       "813164   Ministeren har svaret på spørgsmålet om den st...   \n",
       "1010866  Panelets rolle som  stopklods for ophævelse af...   \n",
       "1832898  Redegørelsen er således et udtryk for regering...   \n",
       "\n",
       "         current_speaker_in_government parties_in_government       date  label  \n",
       "1442409                           True               [S, RV] 1999-02-11      0  \n",
       "268120                            True               [S, RV] 1999-03-19      0  \n",
       "813164                            True               [S, RV] 1997-10-30      0  \n",
       "1010866                          False               [S, RV] 1998-12-15      0  \n",
       "1832898                          False               [S, RV] 1999-02-04      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = subset_TD\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0464b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86fda52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f164fbc52b24cd4a346d5069182a67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep only the necessary columns\n",
    "df = df[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"  # works for Danish\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['preceding_sentence'],\n",
    "        example['text'],\n",
    "        example['succeeding_sent']\n",
    "    )\n",
    "\n",
    "#tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80dcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split tokenized dataset\n",
    "splits = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = splits['train']\n",
    "test_dataset = splits['test']\n",
    "\n",
    "\n",
    "#define metrics / Two options have been listed below. The second i probably preferred as ittakes the amount of 0 to 1' into account\n",
    "# First option\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "#second option\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "labels = df['label'].tolist()\n",
    "class_counts = Counter(labels)\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Higher weight = more emphasis\n",
    "weights = [total/class_counts[0], total/class_counts[1]]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "#define custom trainer that uses weigted loss\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Define weighted loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# And implement the second option in training\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "#set up trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blame_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "#train\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
