{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 158,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012658227848101266,
      "grad_norm": 3.2255325317382812,
      "learning_rate": 0.0001,
      "loss": 0.2717,
      "step": 1
    },
    {
      "epoch": 0.02531645569620253,
      "grad_norm": 5.936131000518799,
      "learning_rate": 9.957805907172997e-05,
      "loss": 0.3278,
      "step": 2
    },
    {
      "epoch": 0.0379746835443038,
      "grad_norm": 3.0019633769989014,
      "learning_rate": 9.915611814345992e-05,
      "loss": 0.3092,
      "step": 3
    },
    {
      "epoch": 0.05063291139240506,
      "grad_norm": 2.0255439281463623,
      "learning_rate": 9.873417721518988e-05,
      "loss": 0.2575,
      "step": 4
    },
    {
      "epoch": 0.06329113924050633,
      "grad_norm": 10.445018768310547,
      "learning_rate": 9.831223628691983e-05,
      "loss": 0.5642,
      "step": 5
    },
    {
      "epoch": 0.0759493670886076,
      "grad_norm": 8.164011001586914,
      "learning_rate": 9.78902953586498e-05,
      "loss": 0.4622,
      "step": 6
    },
    {
      "epoch": 0.08860759493670886,
      "grad_norm": 6.724480152130127,
      "learning_rate": 9.746835443037975e-05,
      "loss": 0.4294,
      "step": 7
    },
    {
      "epoch": 0.10126582278481013,
      "grad_norm": 5.121272087097168,
      "learning_rate": 9.704641350210972e-05,
      "loss": 0.3515,
      "step": 8
    },
    {
      "epoch": 0.11392405063291139,
      "grad_norm": 1.1718209981918335,
      "learning_rate": 9.662447257383967e-05,
      "loss": 0.2361,
      "step": 9
    },
    {
      "epoch": 0.12658227848101267,
      "grad_norm": 5.274511814117432,
      "learning_rate": 9.620253164556962e-05,
      "loss": 0.4183,
      "step": 10
    },
    {
      "epoch": 0.13924050632911392,
      "grad_norm": 3.9509999752044678,
      "learning_rate": 9.578059071729957e-05,
      "loss": 0.3357,
      "step": 11
    },
    {
      "epoch": 0.1518987341772152,
      "grad_norm": 3.816878080368042,
      "learning_rate": 9.535864978902954e-05,
      "loss": 0.3208,
      "step": 12
    },
    {
      "epoch": 0.16455696202531644,
      "grad_norm": 1.7339189052581787,
      "learning_rate": 9.493670886075949e-05,
      "loss": 0.3395,
      "step": 13
    },
    {
      "epoch": 0.17721518987341772,
      "grad_norm": 2.8172805309295654,
      "learning_rate": 9.451476793248946e-05,
      "loss": 0.2951,
      "step": 14
    },
    {
      "epoch": 0.189873417721519,
      "grad_norm": 0.8974289298057556,
      "learning_rate": 9.409282700421943e-05,
      "loss": 0.2062,
      "step": 15
    },
    {
      "epoch": 0.20253164556962025,
      "grad_norm": 5.326119899749756,
      "learning_rate": 9.367088607594936e-05,
      "loss": 0.39,
      "step": 16
    },
    {
      "epoch": 0.21518987341772153,
      "grad_norm": 3.894324541091919,
      "learning_rate": 9.324894514767933e-05,
      "loss": 0.2975,
      "step": 17
    },
    {
      "epoch": 0.22784810126582278,
      "grad_norm": 3.9985718727111816,
      "learning_rate": 9.282700421940928e-05,
      "loss": 0.3816,
      "step": 18
    },
    {
      "epoch": 0.24050632911392406,
      "grad_norm": 5.520809173583984,
      "learning_rate": 9.240506329113925e-05,
      "loss": 0.3398,
      "step": 19
    },
    {
      "epoch": 0.25316455696202533,
      "grad_norm": 2.9872121810913086,
      "learning_rate": 9.19831223628692e-05,
      "loss": 0.2654,
      "step": 20
    },
    {
      "epoch": 0.26582278481012656,
      "grad_norm": 1.3459199666976929,
      "learning_rate": 9.156118143459917e-05,
      "loss": 0.1986,
      "step": 21
    },
    {
      "epoch": 0.27848101265822783,
      "grad_norm": 1.5905078649520874,
      "learning_rate": 9.113924050632912e-05,
      "loss": 0.3124,
      "step": 22
    },
    {
      "epoch": 0.2911392405063291,
      "grad_norm": 1.431283712387085,
      "learning_rate": 9.071729957805908e-05,
      "loss": 0.2068,
      "step": 23
    },
    {
      "epoch": 0.3037974683544304,
      "grad_norm": 2.4091384410858154,
      "learning_rate": 9.029535864978903e-05,
      "loss": 0.2782,
      "step": 24
    },
    {
      "epoch": 0.31645569620253167,
      "grad_norm": 0.92051762342453,
      "learning_rate": 8.9873417721519e-05,
      "loss": 0.2652,
      "step": 25
    },
    {
      "epoch": 0.3291139240506329,
      "grad_norm": 1.7090729475021362,
      "learning_rate": 8.945147679324895e-05,
      "loss": 0.2234,
      "step": 26
    },
    {
      "epoch": 0.34177215189873417,
      "grad_norm": 3.464411973953247,
      "learning_rate": 8.902953586497891e-05,
      "loss": 0.3535,
      "step": 27
    },
    {
      "epoch": 0.35443037974683544,
      "grad_norm": 1.1274164915084839,
      "learning_rate": 8.860759493670887e-05,
      "loss": 0.287,
      "step": 28
    },
    {
      "epoch": 0.3670886075949367,
      "grad_norm": 1.1603777408599854,
      "learning_rate": 8.818565400843882e-05,
      "loss": 0.1936,
      "step": 29
    },
    {
      "epoch": 0.379746835443038,
      "grad_norm": 1.2419159412384033,
      "learning_rate": 8.776371308016879e-05,
      "loss": 0.1703,
      "step": 30
    },
    {
      "epoch": 0.3924050632911392,
      "grad_norm": 3.4801580905914307,
      "learning_rate": 8.734177215189874e-05,
      "loss": 0.2766,
      "step": 31
    },
    {
      "epoch": 0.4050632911392405,
      "grad_norm": 4.317643165588379,
      "learning_rate": 8.69198312236287e-05,
      "loss": 0.2797,
      "step": 32
    },
    {
      "epoch": 0.4177215189873418,
      "grad_norm": 1.5091791152954102,
      "learning_rate": 8.649789029535866e-05,
      "loss": 0.2779,
      "step": 33
    },
    {
      "epoch": 0.43037974683544306,
      "grad_norm": 1.4333523511886597,
      "learning_rate": 8.607594936708861e-05,
      "loss": 0.2189,
      "step": 34
    },
    {
      "epoch": 0.4430379746835443,
      "grad_norm": 5.8908185958862305,
      "learning_rate": 8.565400843881856e-05,
      "loss": 0.338,
      "step": 35
    },
    {
      "epoch": 0.45569620253164556,
      "grad_norm": 2.1405158042907715,
      "learning_rate": 8.523206751054853e-05,
      "loss": 0.262,
      "step": 36
    },
    {
      "epoch": 0.46835443037974683,
      "grad_norm": 1.928255319595337,
      "learning_rate": 8.481012658227848e-05,
      "loss": 0.2735,
      "step": 37
    },
    {
      "epoch": 0.4810126582278481,
      "grad_norm": 1.2634241580963135,
      "learning_rate": 8.438818565400845e-05,
      "loss": 0.2499,
      "step": 38
    },
    {
      "epoch": 0.4936708860759494,
      "grad_norm": 1.8778374195098877,
      "learning_rate": 8.39662447257384e-05,
      "loss": 0.2211,
      "step": 39
    },
    {
      "epoch": 0.5063291139240507,
      "grad_norm": 1.2848247289657593,
      "learning_rate": 8.354430379746835e-05,
      "loss": 0.1968,
      "step": 40
    },
    {
      "epoch": 0.5189873417721519,
      "grad_norm": 1.3075088262557983,
      "learning_rate": 8.312236286919831e-05,
      "loss": 0.2742,
      "step": 41
    },
    {
      "epoch": 0.5316455696202531,
      "grad_norm": 3.525006055831909,
      "learning_rate": 8.270042194092827e-05,
      "loss": 0.2763,
      "step": 42
    },
    {
      "epoch": 0.5443037974683544,
      "grad_norm": 3.5085320472717285,
      "learning_rate": 8.227848101265824e-05,
      "loss": 0.328,
      "step": 43
    },
    {
      "epoch": 0.5569620253164557,
      "grad_norm": 4.752373218536377,
      "learning_rate": 8.185654008438819e-05,
      "loss": 0.4671,
      "step": 44
    },
    {
      "epoch": 0.569620253164557,
      "grad_norm": 1.1499031782150269,
      "learning_rate": 8.143459915611815e-05,
      "loss": 0.2383,
      "step": 45
    },
    {
      "epoch": 0.5822784810126582,
      "grad_norm": 2.0514914989471436,
      "learning_rate": 8.10126582278481e-05,
      "loss": 0.3419,
      "step": 46
    },
    {
      "epoch": 0.5949367088607594,
      "grad_norm": 3.4552736282348633,
      "learning_rate": 8.059071729957806e-05,
      "loss": 0.3781,
      "step": 47
    },
    {
      "epoch": 0.6075949367088608,
      "grad_norm": 1.7675797939300537,
      "learning_rate": 8.016877637130802e-05,
      "loss": 0.2787,
      "step": 48
    },
    {
      "epoch": 0.620253164556962,
      "grad_norm": 2.1336567401885986,
      "learning_rate": 7.974683544303798e-05,
      "loss": 0.2536,
      "step": 49
    },
    {
      "epoch": 0.6329113924050633,
      "grad_norm": 2.004063367843628,
      "learning_rate": 7.932489451476794e-05,
      "loss": 0.2438,
      "step": 50
    },
    {
      "epoch": 0.6455696202531646,
      "grad_norm": 2.421466112136841,
      "learning_rate": 7.89029535864979e-05,
      "loss": 0.2024,
      "step": 51
    },
    {
      "epoch": 0.6582278481012658,
      "grad_norm": 3.015326976776123,
      "learning_rate": 7.848101265822784e-05,
      "loss": 0.289,
      "step": 52
    },
    {
      "epoch": 0.6708860759493671,
      "grad_norm": 2.422560930252075,
      "learning_rate": 7.805907172995781e-05,
      "loss": 0.2855,
      "step": 53
    },
    {
      "epoch": 0.6835443037974683,
      "grad_norm": 1.7738765478134155,
      "learning_rate": 7.763713080168776e-05,
      "loss": 0.2479,
      "step": 54
    },
    {
      "epoch": 0.6962025316455697,
      "grad_norm": 1.198279619216919,
      "learning_rate": 7.721518987341773e-05,
      "loss": 0.2962,
      "step": 55
    },
    {
      "epoch": 0.7088607594936709,
      "grad_norm": 1.0668933391571045,
      "learning_rate": 7.679324894514768e-05,
      "loss": 0.1452,
      "step": 56
    },
    {
      "epoch": 0.7215189873417721,
      "grad_norm": 1.779697299003601,
      "learning_rate": 7.637130801687765e-05,
      "loss": 0.2218,
      "step": 57
    },
    {
      "epoch": 0.7341772151898734,
      "grad_norm": 1.5562379360198975,
      "learning_rate": 7.59493670886076e-05,
      "loss": 0.2024,
      "step": 58
    },
    {
      "epoch": 0.7468354430379747,
      "grad_norm": 2.1289143562316895,
      "learning_rate": 7.552742616033755e-05,
      "loss": 0.2946,
      "step": 59
    },
    {
      "epoch": 0.759493670886076,
      "grad_norm": 2.4055440425872803,
      "learning_rate": 7.510548523206752e-05,
      "loss": 0.3721,
      "step": 60
    },
    {
      "epoch": 0.7721518987341772,
      "grad_norm": 3.3397326469421387,
      "learning_rate": 7.468354430379747e-05,
      "loss": 0.3709,
      "step": 61
    },
    {
      "epoch": 0.7848101265822784,
      "grad_norm": 1.6577234268188477,
      "learning_rate": 7.426160337552744e-05,
      "loss": 0.2243,
      "step": 62
    },
    {
      "epoch": 0.7974683544303798,
      "grad_norm": 6.468565464019775,
      "learning_rate": 7.383966244725739e-05,
      "loss": 0.2324,
      "step": 63
    },
    {
      "epoch": 0.810126582278481,
      "grad_norm": 1.2942875623703003,
      "learning_rate": 7.341772151898734e-05,
      "loss": 0.2007,
      "step": 64
    },
    {
      "epoch": 0.8227848101265823,
      "grad_norm": 2.2885570526123047,
      "learning_rate": 7.29957805907173e-05,
      "loss": 0.2435,
      "step": 65
    },
    {
      "epoch": 0.8354430379746836,
      "grad_norm": 1.4624768495559692,
      "learning_rate": 7.257383966244726e-05,
      "loss": 0.1904,
      "step": 66
    },
    {
      "epoch": 0.8481012658227848,
      "grad_norm": 2.2533915042877197,
      "learning_rate": 7.215189873417722e-05,
      "loss": 0.1891,
      "step": 67
    },
    {
      "epoch": 0.8607594936708861,
      "grad_norm": 1.4079869985580444,
      "learning_rate": 7.172995780590718e-05,
      "loss": 0.1711,
      "step": 68
    },
    {
      "epoch": 0.8734177215189873,
      "grad_norm": 2.078516960144043,
      "learning_rate": 7.130801687763713e-05,
      "loss": 0.2406,
      "step": 69
    },
    {
      "epoch": 0.8860759493670886,
      "grad_norm": 2.026370048522949,
      "learning_rate": 7.088607594936709e-05,
      "loss": 0.2281,
      "step": 70
    },
    {
      "epoch": 0.8987341772151899,
      "grad_norm": 3.581109046936035,
      "learning_rate": 7.046413502109705e-05,
      "loss": 0.286,
      "step": 71
    },
    {
      "epoch": 0.9113924050632911,
      "grad_norm": 1.7375619411468506,
      "learning_rate": 7.0042194092827e-05,
      "loss": 0.238,
      "step": 72
    },
    {
      "epoch": 0.9240506329113924,
      "grad_norm": 1.4049296379089355,
      "learning_rate": 6.962025316455697e-05,
      "loss": 0.2592,
      "step": 73
    },
    {
      "epoch": 0.9367088607594937,
      "grad_norm": 1.7302360534667969,
      "learning_rate": 6.919831223628693e-05,
      "loss": 0.1645,
      "step": 74
    },
    {
      "epoch": 0.9493670886075949,
      "grad_norm": 1.417008876800537,
      "learning_rate": 6.877637130801688e-05,
      "loss": 0.2538,
      "step": 75
    },
    {
      "epoch": 0.9620253164556962,
      "grad_norm": 2.027297258377075,
      "learning_rate": 6.835443037974683e-05,
      "loss": 0.263,
      "step": 76
    },
    {
      "epoch": 0.9746835443037974,
      "grad_norm": 3.7345874309539795,
      "learning_rate": 6.79324894514768e-05,
      "loss": 0.3012,
      "step": 77
    },
    {
      "epoch": 0.9873417721518988,
      "grad_norm": 2.245330572128296,
      "learning_rate": 6.751054852320675e-05,
      "loss": 0.3513,
      "step": 78
    },
    {
      "epoch": 1.0,
      "grad_norm": 2.1530632972717285,
      "learning_rate": 6.708860759493672e-05,
      "loss": 0.0814,
      "step": 79
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8604651162790697,
      "eval_f1": 0.8486803519061583,
      "eval_keras_BCE": 0.3869037330150604,
      "eval_loss": 0.4184030294418335,
      "eval_precision": 0.7653061224489796,
      "eval_recall": 0.8522727272727273,
      "eval_runtime": 1.4754,
      "eval_samples_per_second": 174.865,
      "eval_steps_per_second": 22.366,
      "eval_weighted_BCE": 25.37068748474121,
      "step": 79
    },
    {
      "epoch": 1.0126582278481013,
      "grad_norm": 1.451705813407898,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.2972,
      "step": 80
    },
    {
      "epoch": 1.0253164556962024,
      "grad_norm": 3.4719932079315186,
      "learning_rate": 6.624472573839662e-05,
      "loss": 0.2035,
      "step": 81
    },
    {
      "epoch": 1.0379746835443038,
      "grad_norm": 1.7316818237304688,
      "learning_rate": 6.582278481012658e-05,
      "loss": 0.1667,
      "step": 82
    },
    {
      "epoch": 1.0506329113924051,
      "grad_norm": 2.3351762294769287,
      "learning_rate": 6.540084388185654e-05,
      "loss": 0.2435,
      "step": 83
    },
    {
      "epoch": 1.0632911392405062,
      "grad_norm": 2.69775390625,
      "learning_rate": 6.49789029535865e-05,
      "loss": 0.1465,
      "step": 84
    },
    {
      "epoch": 1.0759493670886076,
      "grad_norm": 1.4519096612930298,
      "learning_rate": 6.455696202531646e-05,
      "loss": 0.1731,
      "step": 85
    },
    {
      "epoch": 1.0886075949367089,
      "grad_norm": 2.5857112407684326,
      "learning_rate": 6.413502109704643e-05,
      "loss": 0.2604,
      "step": 86
    },
    {
      "epoch": 1.1012658227848102,
      "grad_norm": 1.503920316696167,
      "learning_rate": 6.371308016877638e-05,
      "loss": 0.176,
      "step": 87
    },
    {
      "epoch": 1.1139240506329113,
      "grad_norm": 1.257734775543213,
      "learning_rate": 6.329113924050633e-05,
      "loss": 0.1736,
      "step": 88
    },
    {
      "epoch": 1.1265822784810127,
      "grad_norm": 1.7267248630523682,
      "learning_rate": 6.286919831223629e-05,
      "loss": 0.2113,
      "step": 89
    },
    {
      "epoch": 1.139240506329114,
      "grad_norm": 3.7736763954162598,
      "learning_rate": 6.244725738396625e-05,
      "loss": 0.2813,
      "step": 90
    },
    {
      "epoch": 1.1518987341772151,
      "grad_norm": 2.8489019870758057,
      "learning_rate": 6.20253164556962e-05,
      "loss": 0.1731,
      "step": 91
    },
    {
      "epoch": 1.1645569620253164,
      "grad_norm": 2.6046512126922607,
      "learning_rate": 6.160337552742617e-05,
      "loss": 0.2528,
      "step": 92
    },
    {
      "epoch": 1.1772151898734178,
      "grad_norm": 1.0293508768081665,
      "learning_rate": 6.118143459915612e-05,
      "loss": 0.117,
      "step": 93
    },
    {
      "epoch": 1.189873417721519,
      "grad_norm": 3.9925131797790527,
      "learning_rate": 6.0759493670886084e-05,
      "loss": 0.2011,
      "step": 94
    },
    {
      "epoch": 1.2025316455696202,
      "grad_norm": 2.07277774810791,
      "learning_rate": 6.033755274261603e-05,
      "loss": 0.1427,
      "step": 95
    },
    {
      "epoch": 1.2151898734177216,
      "grad_norm": 1.334873914718628,
      "learning_rate": 5.9915611814345996e-05,
      "loss": 0.1518,
      "step": 96
    },
    {
      "epoch": 1.2278481012658227,
      "grad_norm": 1.6838560104370117,
      "learning_rate": 5.949367088607595e-05,
      "loss": 0.2322,
      "step": 97
    },
    {
      "epoch": 1.240506329113924,
      "grad_norm": 1.9884614944458008,
      "learning_rate": 5.907172995780591e-05,
      "loss": 0.1741,
      "step": 98
    },
    {
      "epoch": 1.2531645569620253,
      "grad_norm": 1.3905425071716309,
      "learning_rate": 5.8649789029535875e-05,
      "loss": 0.1519,
      "step": 99
    },
    {
      "epoch": 1.2658227848101267,
      "grad_norm": 1.5218379497528076,
      "learning_rate": 5.822784810126583e-05,
      "loss": 0.1959,
      "step": 100
    },
    {
      "epoch": 1.2784810126582278,
      "grad_norm": 1.6406607627868652,
      "learning_rate": 5.780590717299579e-05,
      "loss": 0.2198,
      "step": 101
    },
    {
      "epoch": 1.2911392405063291,
      "grad_norm": 2.202998161315918,
      "learning_rate": 5.738396624472574e-05,
      "loss": 0.1683,
      "step": 102
    },
    {
      "epoch": 1.3037974683544304,
      "grad_norm": 5.656140327453613,
      "learning_rate": 5.69620253164557e-05,
      "loss": 0.2722,
      "step": 103
    },
    {
      "epoch": 1.3164556962025316,
      "grad_norm": 2.774486780166626,
      "learning_rate": 5.654008438818565e-05,
      "loss": 0.2248,
      "step": 104
    },
    {
      "epoch": 1.3291139240506329,
      "grad_norm": 1.3498039245605469,
      "learning_rate": 5.611814345991562e-05,
      "loss": 0.1647,
      "step": 105
    },
    {
      "epoch": 1.3417721518987342,
      "grad_norm": 2.003150224685669,
      "learning_rate": 5.569620253164557e-05,
      "loss": 0.1261,
      "step": 106
    },
    {
      "epoch": 1.3544303797468356,
      "grad_norm": 2.6935291290283203,
      "learning_rate": 5.527426160337553e-05,
      "loss": 0.1815,
      "step": 107
    },
    {
      "epoch": 1.3670886075949367,
      "grad_norm": 2.774737596511841,
      "learning_rate": 5.4852320675105484e-05,
      "loss": 0.1793,
      "step": 108
    },
    {
      "epoch": 1.379746835443038,
      "grad_norm": 1.7943683862686157,
      "learning_rate": 5.4430379746835444e-05,
      "loss": 0.106,
      "step": 109
    },
    {
      "epoch": 1.3924050632911391,
      "grad_norm": 1.8195860385894775,
      "learning_rate": 5.4008438818565396e-05,
      "loss": 0.2418,
      "step": 110
    },
    {
      "epoch": 1.4050632911392404,
      "grad_norm": 3.1216254234313965,
      "learning_rate": 5.358649789029536e-05,
      "loss": 0.1334,
      "step": 111
    },
    {
      "epoch": 1.4177215189873418,
      "grad_norm": 2.849714517593384,
      "learning_rate": 5.3164556962025316e-05,
      "loss": 0.3014,
      "step": 112
    },
    {
      "epoch": 1.4303797468354431,
      "grad_norm": 3.1816699504852295,
      "learning_rate": 5.2742616033755275e-05,
      "loss": 0.1385,
      "step": 113
    },
    {
      "epoch": 1.4430379746835442,
      "grad_norm": 1.0315524339675903,
      "learning_rate": 5.232067510548524e-05,
      "loss": 0.1538,
      "step": 114
    },
    {
      "epoch": 1.4556962025316456,
      "grad_norm": 1.2459542751312256,
      "learning_rate": 5.1898734177215194e-05,
      "loss": 0.1845,
      "step": 115
    },
    {
      "epoch": 1.4683544303797469,
      "grad_norm": 1.9690216779708862,
      "learning_rate": 5.1476793248945154e-05,
      "loss": 0.2234,
      "step": 116
    },
    {
      "epoch": 1.481012658227848,
      "grad_norm": 1.0645132064819336,
      "learning_rate": 5.105485232067511e-05,
      "loss": 0.135,
      "step": 117
    },
    {
      "epoch": 1.4936708860759493,
      "grad_norm": 2.764749765396118,
      "learning_rate": 5.0632911392405066e-05,
      "loss": 0.1577,
      "step": 118
    },
    {
      "epoch": 1.5063291139240507,
      "grad_norm": 1.134035348892212,
      "learning_rate": 5.021097046413502e-05,
      "loss": 0.1066,
      "step": 119
    },
    {
      "epoch": 1.518987341772152,
      "grad_norm": 4.5565690994262695,
      "learning_rate": 4.9789029535864986e-05,
      "loss": 0.1893,
      "step": 120
    },
    {
      "epoch": 1.5316455696202531,
      "grad_norm": 1.2252404689788818,
      "learning_rate": 4.936708860759494e-05,
      "loss": 0.1329,
      "step": 121
    },
    {
      "epoch": 1.5443037974683544,
      "grad_norm": 2.2628777027130127,
      "learning_rate": 4.89451476793249e-05,
      "loss": 0.1978,
      "step": 122
    },
    {
      "epoch": 1.5569620253164556,
      "grad_norm": 2.5287582874298096,
      "learning_rate": 4.852320675105486e-05,
      "loss": 0.1948,
      "step": 123
    },
    {
      "epoch": 1.5696202531645569,
      "grad_norm": 2.447828531265259,
      "learning_rate": 4.810126582278481e-05,
      "loss": 0.2145,
      "step": 124
    },
    {
      "epoch": 1.5822784810126582,
      "grad_norm": 4.826825141906738,
      "learning_rate": 4.767932489451477e-05,
      "loss": 0.2361,
      "step": 125
    },
    {
      "epoch": 1.5949367088607596,
      "grad_norm": 4.101231575012207,
      "learning_rate": 4.725738396624473e-05,
      "loss": 0.2062,
      "step": 126
    },
    {
      "epoch": 1.6075949367088609,
      "grad_norm": 4.318490505218506,
      "learning_rate": 4.683544303797468e-05,
      "loss": 0.2452,
      "step": 127
    },
    {
      "epoch": 1.620253164556962,
      "grad_norm": 3.3845295906066895,
      "learning_rate": 4.641350210970464e-05,
      "loss": 0.1442,
      "step": 128
    },
    {
      "epoch": 1.6329113924050633,
      "grad_norm": 1.2381218671798706,
      "learning_rate": 4.59915611814346e-05,
      "loss": 0.1366,
      "step": 129
    },
    {
      "epoch": 1.6455696202531644,
      "grad_norm": 1.9544837474822998,
      "learning_rate": 4.556962025316456e-05,
      "loss": 0.1644,
      "step": 130
    },
    {
      "epoch": 1.6582278481012658,
      "grad_norm": 4.154504299163818,
      "learning_rate": 4.5147679324894514e-05,
      "loss": 0.2225,
      "step": 131
    },
    {
      "epoch": 1.6708860759493671,
      "grad_norm": 2.5980825424194336,
      "learning_rate": 4.4725738396624474e-05,
      "loss": 0.2204,
      "step": 132
    },
    {
      "epoch": 1.6835443037974684,
      "grad_norm": 2.4278783798217773,
      "learning_rate": 4.430379746835443e-05,
      "loss": 0.2605,
      "step": 133
    },
    {
      "epoch": 1.6962025316455698,
      "grad_norm": 1.4080636501312256,
      "learning_rate": 4.388185654008439e-05,
      "loss": 0.1962,
      "step": 134
    },
    {
      "epoch": 1.7088607594936709,
      "grad_norm": 1.2110494375228882,
      "learning_rate": 4.345991561181435e-05,
      "loss": 0.1654,
      "step": 135
    },
    {
      "epoch": 1.721518987341772,
      "grad_norm": 3.1014552116394043,
      "learning_rate": 4.3037974683544305e-05,
      "loss": 0.1991,
      "step": 136
    },
    {
      "epoch": 1.7341772151898733,
      "grad_norm": 2.022944211959839,
      "learning_rate": 4.2616033755274265e-05,
      "loss": 0.3058,
      "step": 137
    },
    {
      "epoch": 1.7468354430379747,
      "grad_norm": 1.5577589273452759,
      "learning_rate": 4.2194092827004224e-05,
      "loss": 0.1345,
      "step": 138
    },
    {
      "epoch": 1.759493670886076,
      "grad_norm": 2.31721568107605,
      "learning_rate": 4.177215189873418e-05,
      "loss": 0.2199,
      "step": 139
    },
    {
      "epoch": 1.7721518987341773,
      "grad_norm": 2.324014663696289,
      "learning_rate": 4.135021097046414e-05,
      "loss": 0.2328,
      "step": 140
    },
    {
      "epoch": 1.7848101265822784,
      "grad_norm": 1.2821390628814697,
      "learning_rate": 4.0928270042194096e-05,
      "loss": 0.1392,
      "step": 141
    },
    {
      "epoch": 1.7974683544303798,
      "grad_norm": 3.5407888889312744,
      "learning_rate": 4.050632911392405e-05,
      "loss": 0.1326,
      "step": 142
    },
    {
      "epoch": 1.810126582278481,
      "grad_norm": 2.862133026123047,
      "learning_rate": 4.008438818565401e-05,
      "loss": 0.1949,
      "step": 143
    },
    {
      "epoch": 1.8227848101265822,
      "grad_norm": 2.666989326477051,
      "learning_rate": 3.966244725738397e-05,
      "loss": 0.1334,
      "step": 144
    },
    {
      "epoch": 1.8354430379746836,
      "grad_norm": 1.7119276523590088,
      "learning_rate": 3.924050632911392e-05,
      "loss": 0.229,
      "step": 145
    },
    {
      "epoch": 1.8481012658227849,
      "grad_norm": 1.3591604232788086,
      "learning_rate": 3.881856540084388e-05,
      "loss": 0.1565,
      "step": 146
    },
    {
      "epoch": 1.8607594936708862,
      "grad_norm": 1.4920153617858887,
      "learning_rate": 3.839662447257384e-05,
      "loss": 0.1848,
      "step": 147
    },
    {
      "epoch": 1.8734177215189873,
      "grad_norm": 4.510705471038818,
      "learning_rate": 3.79746835443038e-05,
      "loss": 0.3315,
      "step": 148
    },
    {
      "epoch": 1.8860759493670884,
      "grad_norm": 1.4757369756698608,
      "learning_rate": 3.755274261603376e-05,
      "loss": 0.1263,
      "step": 149
    },
    {
      "epoch": 1.8987341772151898,
      "grad_norm": 1.1211596727371216,
      "learning_rate": 3.713080168776372e-05,
      "loss": 0.1519,
      "step": 150
    },
    {
      "epoch": 1.9113924050632911,
      "grad_norm": 1.2811825275421143,
      "learning_rate": 3.670886075949367e-05,
      "loss": 0.1857,
      "step": 151
    },
    {
      "epoch": 1.9240506329113924,
      "grad_norm": 1.7444310188293457,
      "learning_rate": 3.628691983122363e-05,
      "loss": 0.1415,
      "step": 152
    },
    {
      "epoch": 1.9367088607594938,
      "grad_norm": 2.0658581256866455,
      "learning_rate": 3.586497890295359e-05,
      "loss": 0.1525,
      "step": 153
    },
    {
      "epoch": 1.9493670886075949,
      "grad_norm": 1.6718828678131104,
      "learning_rate": 3.5443037974683544e-05,
      "loss": 0.254,
      "step": 154
    },
    {
      "epoch": 1.9620253164556962,
      "grad_norm": 1.5781428813934326,
      "learning_rate": 3.50210970464135e-05,
      "loss": 0.2375,
      "step": 155
    },
    {
      "epoch": 1.9746835443037973,
      "grad_norm": 1.424481987953186,
      "learning_rate": 3.459915611814346e-05,
      "loss": 0.1166,
      "step": 156
    },
    {
      "epoch": 1.9873417721518987,
      "grad_norm": 1.2330272197723389,
      "learning_rate": 3.4177215189873416e-05,
      "loss": 0.1501,
      "step": 157
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.6336400508880615,
      "learning_rate": 3.3755274261603375e-05,
      "loss": 0.1857,
      "step": 158
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8682170542635659,
      "eval_f1": 0.85837907652567,
      "eval_keras_BCE": 0.4184921085834503,
      "eval_loss": 0.3716869354248047,
      "eval_precision": 0.7647058823529411,
      "eval_recall": 0.8863636363636364,
      "eval_runtime": 1.4869,
      "eval_samples_per_second": 173.51,
      "eval_steps_per_second": 22.193,
      "eval_weighted_BCE": 27.442052841186523,
      "step": 158
    }
  ],
  "logging_steps": 1,
  "max_steps": 237,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7025041858560000.0,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
