{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29be4141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab900d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Here we define a variable that will be passed to our classifier. This will check if a GPU is available, and use the CPU if one is not available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # if you want to use the GPU on a macbook change 'cuda' to 'mps' and make sure you have the 'accelerate' library installed.\n",
    "# This line prints the device that will be used. Make sure it prints 'cuda' or 'mps' if you are trying to use a GPU.\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66fd56a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"zero-shot-classification\", model=\"mlburnham/Political_DEBATE_base_v1.0\", device = device, batch_size = 32) # To use the base model\n",
    "#pipe = pipeline(\"zero-shot-classification\", model='mlburnham/Political_DEBATE_large_v1.0', device = device, batch_size = 32) # To use the large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b88717bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': ' now it is the next one in line for speaking to the parliament', 'labels': ['neutral', 'blame', 'praise'], 'scores': [0.9999026656150818, 0.0004477902839425951, 6.562122962350259e-06]}, {'sequence': 'everything is the fault of the immigrants', 'labels': ['blame', 'neutral', 'praise'], 'scores': [0.9999503493309021, 5.331348347681342e-06, 2.302036136825336e-06]}, {'sequence': 'I just love sharwarmamesteren He makes the best food', 'labels': ['praise', 'blame', 'neutral'], 'scores': [0.9994699358940125, 0.0001050720748025924, 6.623339868383482e-05]}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m (result)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract blame probability\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m label_scores = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, result[\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m      9\u001b[39m blame_prob = label_scores.get(\u001b[33m\"\u001b[39m\u001b[33mblame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.0\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBlame probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblame_prob\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "test_doc = [' now it is the next one in line for speaking to the parliament', 'everything is the fault of the immigrants', \"I just love sharwarmamesteren He makes the best food\"]\n",
    "hypothesis_template = \"Based on this text, the author's attitude towards others is best described as {}.\"\n",
    "test_labels = [\"blame\", \"praise\", \"neutral\"]\n",
    "result = pipe(test_doc, test_labels, hypothesis_template = hypothesis_template, multi_label = True)\n",
    "\n",
    "print (result)\n",
    "# Extract blame probability\n",
    "label_scores = dict(zip(result[\"labels\"], result[\"scores\"]))\n",
    "blame_prob = label_scores.get(\"blame\", 0.0)\n",
    "\n",
    "print(f\"Blame probability: {blame_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84c92574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>agenda</th>\n",
       "      <th>speechnumber</th>\n",
       "      <th>speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>party.facts.id</th>\n",
       "      <th>chair</th>\n",
       "      <th>terms</th>\n",
       "      <th>text</th>\n",
       "      <th>parliament</th>\n",
       "      <th>iso3country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Dagsorden</td>\n",
       "      <td>1</td>\n",
       "      <td>Gert Petersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>191</td>\n",
       "      <td>Mødet er åbnet. I henhold til grundloven er Fo...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Dagsorden</td>\n",
       "      <td>2</td>\n",
       "      <td>Formanden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>182</td>\n",
       "      <td>Jeg vil gerne takke Tinget for den tillid, man...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>Statsministerens redegørelse i henhold til gru...</td>\n",
       "      <td>3</td>\n",
       "      <td>Poul Nyrup Rasmussen</td>\n",
       "      <td>S</td>\n",
       "      <td>379.0</td>\n",
       "      <td>False</td>\n",
       "      <td>18662</td>\n",
       "      <td>For 25 år siden sagde et flertal i befolkninge...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-10-09</td>\n",
       "      <td>1) Indstilling fra Udvalget til Valgs Prøvelse.</td>\n",
       "      <td>2</td>\n",
       "      <td>Formanden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "      <td>Fra Udvalget til Valgs Prøvelse har jeg modtag...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-10-09</td>\n",
       "      <td>2) Forhandling om redegørelse nr. R 1.</td>\n",
       "      <td>3</td>\n",
       "      <td>Torben Lund</td>\n",
       "      <td>S</td>\n",
       "      <td>379.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2865</td>\n",
       "      <td>Vi står over for en meget afgørende folketings...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                             agenda  \\\n",
       "0  1997-10-07                                          Dagsorden   \n",
       "1  1997-10-07                                          Dagsorden   \n",
       "2  1997-10-07  Statsministerens redegørelse i henhold til gru...   \n",
       "3  1997-10-09    1) Indstilling fra Udvalget til Valgs Prøvelse.   \n",
       "4  1997-10-09             2) Forhandling om redegørelse nr. R 1.   \n",
       "\n",
       "   speechnumber               speaker party  party.facts.id  chair  terms  \\\n",
       "0             1         Gert Petersen   NaN             NaN   True    191   \n",
       "1             2             Formanden   NaN             NaN   True    182   \n",
       "2             3  Poul Nyrup Rasmussen     S           379.0  False  18662   \n",
       "3             2             Formanden   NaN             NaN   True     47   \n",
       "4             3           Torben Lund     S           379.0  False   2865   \n",
       "\n",
       "                                                text    parliament iso3country  \n",
       "0  Mødet er åbnet. I henhold til grundloven er Fo...  DK-Folketing         DNK  \n",
       "1  Jeg vil gerne takke Tinget for den tillid, man...  DK-Folketing         DNK  \n",
       "2  For 25 år siden sagde et flertal i befolkninge...  DK-Folketing         DNK  \n",
       "3  Fra Udvalget til Valgs Prøvelse har jeg modtag...  DK-Folketing         DNK  \n",
       "4  Vi står over for en meget afgørende folketings...  DK-Folketing         DNK  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RDS\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/annotation_data.csv\")\n",
    "\n",
    "df.pop('Unnamed: 0')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887c458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, how are you?\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-da-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate(text):\n",
    "    translated = model.generate(**tokenizer(text, return_tensors=\"pt\", padding=True))\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "print(translate(\"Hej, hvordan har du det?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b48951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "def split_paragraph(paragraph: str):\n",
    "    doc = nlp(paragraph)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b1de7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_paragraph(para):\n",
    "\n",
    "    translated_sentences = \"\"\n",
    "\n",
    "    sentences = split_paragraph(para)\n",
    "\n",
    "    for sent in sentences:\n",
    "        #translate sentence\n",
    "        english_sentence = translate(sent)\n",
    "\n",
    "        translated_sentences += english_sentence + \" \"\n",
    "\n",
    "    return translated_sentences\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72a7b390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey, you. How's it going? You bet I've been through a lot these past few days. I'm super busy, but otherwise I'll be fine. I was down shopping the other day. It was really hard to carry so many things up the apartment again. Hey, you. How you doing? You bet I've been through a lot these past few days. I'm super busy, but otherwise I'll be fine. I was down shopping the other day. It was really hard to carry so many things up the apartment again. Hey, you. How you doing? You bet I've been through a lot these past few days. I'm super busy, but otherwise I'll be fine. I was down shopping the other day. It was really hard to carry so many things up the apartment again. Hey, you. How you doing? You bet I've been through a lot these past few days. I'm super busy, but otherwise I'll be fine. I was down shopping the other day. It was really hard to carry so many things up in the apartment again4 Hey, you. How's it going? You bet I've been through a lot these past few days. I'm super busy, but otherwise I'll be fine. I was down shopping the other day. It was really hard to carry so many things up the apartment again. 7.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"Hej med dig, hvordan går det? Du kan tro, jeg har oplevet meget de seneste par dage. Jeg har super travlt, men ellers går det fint. Jeg var nede og handle den anden dag. Det var virkelig hårdt at bære så mange ting op i lejligheden igen. Hej med dig, hvordan går det? Du kan tro, jeg har oplevet meget de seneste par dage. Jeg har super travlt, men ellers går det fint. Jeg var nede og handle den anden dag. Det var virkelig hårdt at bære så mange ting op i lejligheden igen. Hej med dig, hvordan går det? Du kan tro, jeg har oplevet meget de seneste par dage. Jeg har super travlt, men ellers går det fint. Jeg var nede og handle den anden dag. Det var virkelig hårdt at bære så mange ting op i lejligheden igen. Hej med dig, hvordan går det? Du kan tro, jeg har oplevet meget de seneste par dage. Jeg har super travlt, men ellers går det fint. Jeg var nede og handle den anden dag. Det var virkelig hårdt at bære så mange ting op i lejligheden igen4 Hej med dig, hvordan går det? Du kan tro, jeg har oplevet meget de seneste par dage. Jeg har super travlt, men ellers går det fint. Jeg var nede og handle den anden dag. Det var virkelig hårdt at bære så mange ting op i lejligheden igen. 7.\"\n",
    "\n",
    "output = translate_paragraph(paragraph)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64f68302",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/swifter/swifter.py:311\u001b[39m, in \u001b[36mSeriesAccessor.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, **kwds)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr_logging():\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     tmp_df = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     sample_df = sample.apply(func, convert_dtype=convert_dtype, args=args, **kwds)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtranslate_paragraph\u001b[39m\u001b[34m(para)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslate_paragraph\u001b[39m(para):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     sentences = \u001b[43msplit_paragraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     translated_sentences = translate_batch(sentences)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36msplit_paragraph\u001b[39m\u001b[34m(paragraph)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_paragraph\u001b[39m(paragraph: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparagraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [sent.text.strip() \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc.sents]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/spacy/language.py:1041\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1027\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[33;03mand can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[32m   1029\u001b[39m \u001b[33;03mis preserved.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1039\u001b[39m \u001b[33;03mDOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/spacy/language.py:1135\u001b[39m, in \u001b[36mLanguage._ensure_doc\u001b[39m\u001b[34m(self, doc_like)\u001b[39m\n\u001b[32m   1134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m.vocab).from_bytes(doc_like)\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E1041.format(\u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[31mValueError\u001b[39m: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'pandas.core.series.Series'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#df['translated_text'] = df['text'].apply(translate_paragraph)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mswifter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtranslated_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mswifter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate_paragraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/swifter/swifter.py:320\u001b[39m, in \u001b[36mSeriesAccessor.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, **kwds)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ERRORS_TO_HANDLE:  \u001b[38;5;66;03m# if can't vectorize, estimate time to pandas apply\u001b[39;00m\n\u001b[32m    319\u001b[39m     wrapped = \u001b[38;5;28mself\u001b[39m._wrapped_apply(func, convert_dtype=convert_dtype, args=args, **kwds)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     timed = \u001b[43mtimeit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_REPEATS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     sample_proc_est = timed / N_REPEATS\n\u001b[32m    322\u001b[39m     est_apply_duration = sample_proc_est / \u001b[38;5;28mself\u001b[39m._SAMPLE_SIZE * \u001b[38;5;28mself\u001b[39m._nrows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/timeit.py:237\u001b[39m, in \u001b[36mtimeit\u001b[39m\u001b[34m(stmt, setup, timer, number, globals)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtimeit\u001b[39m(stmt=\u001b[33m\"\u001b[39m\u001b[33mpass\u001b[39m\u001b[33m\"\u001b[39m, setup=\u001b[33m\"\u001b[39m\u001b[33mpass\u001b[39m\u001b[33m\"\u001b[39m, timer=default_timer,\n\u001b[32m    235\u001b[39m            number=default_number, \u001b[38;5;28mglobals\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    236\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTimer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/timeit.py:180\u001b[39m, in \u001b[36mTimer.timeit\u001b[39m\u001b[34m(self, number)\u001b[39m\n\u001b[32m    178\u001b[39m gc.disable()\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     timing = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timeit-src>:6\u001b[39m, in \u001b[36minner\u001b[39m\u001b[34m(_it, _timer, _stmt)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/swifter/swifter.py:228\u001b[39m, in \u001b[36mSeriesAccessor._wrapped_apply.<locals>.wrapped\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m():\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m suppress_stdout_stderr_logging():\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_SAMPLE_INDEX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtranslate_paragraph\u001b[39m\u001b[34m(para)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslate_paragraph\u001b[39m(para):\n\u001b[32m     34\u001b[39m     sentences = split_paragraph(para)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     translated_sentences = \u001b[43mtranslate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(translated_sentences)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtranslate_batch\u001b[39m\u001b[34m(sentences, batch_size)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     27\u001b[39m     tokens = {k: v.to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tokens.items()}\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m translated = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m decoded = [tokenizer.decode(t, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m translated]\n\u001b[32m     30\u001b[39m translations.extend(decoded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/transformers/generation/utils.py:2551\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2539\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2540\u001b[39m         input_ids,\n\u001b[32m   2541\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2546\u001b[39m         **model_kwargs,\n\u001b[32m   2547\u001b[39m     )\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2556\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2561\u001b[39m     logger.warning_once(\n\u001b[32m   2562\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2563\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2564\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/transformers/generation/utils.py:3462\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   3460\u001b[39m         model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._reorder_cache(model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m], beam_idx)\n\u001b[32m   3461\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m         \u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_key_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3464\u001b[39m cur_len = cur_len + \u001b[32m1\u001b[39m\n\u001b[32m   3465\u001b[39m is_early_stop_heuristic_unsatisfied = \u001b[38;5;28mself\u001b[39m._check_early_stop_heuristic(\n\u001b[32m   3466\u001b[39m     is_early_stop_heuristic_unsatisfied=is_early_stop_heuristic_unsatisfied,\n\u001b[32m   3467\u001b[39m     running_beam_scores=running_beam_scores,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3474\u001b[39m     length_penalty=length_penalty,\n\u001b[32m   3475\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/transformers/cache_utils.py:1349\u001b[39m, in \u001b[36mEncoderDecoderCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreorder_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, beam_idx: torch.LongTensor):\n\u001b[32m   1348\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attention_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28mself\u001b[39m.cross_attention_cache.reorder_cache(beam_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/transformers/cache_utils.py:884\u001b[39m, in \u001b[36mCache.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reorder the cache for beam search\"\"\"\u001b[39;00m\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers)):\n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.13/site-packages/transformers/cache_utils.py:78\u001b[39m, in \u001b[36mCacheLayerMixin.reorder_cache\u001b[39m\u001b[34m(self, beam_idx)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reorders this layer's cache for beam search.\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_seq_length() > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28mself\u001b[39m.keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m.values = \u001b[38;5;28mself\u001b[39m.values.index_select(\u001b[32m0\u001b[39m, beam_idx.to(\u001b[38;5;28mself\u001b[39m.values.device))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#df['translated_text'] = df['text'].apply(translate_paragraph)\n",
    "\n",
    "import swifter\n",
    "\n",
    "df['translated_text'] = df['text'].swifter.apply(translate_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5584654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import spacy\n",
    "import swifter\n",
    "\n",
    "# Load models\n",
    "model_name = 'Helsinki-NLP/opus-mt-da-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "\n",
    "# SpaCy for sentence splitting (optional)\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "def split_paragraph(paragraph: str):\n",
    "    doc = nlp(paragraph)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def translate_batch(sentences, batch_size=16):\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        if torch.cuda.is_available():\n",
    "            tokens = {k: v.to('cuda') for k, v in tokens.items()}\n",
    "        translated = model.generate(**tokens)\n",
    "        decoded = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "        translations.extend(decoded)\n",
    "    return translations\n",
    "\n",
    "def translate_paragraph(para):\n",
    "    sentences = split_paragraph(para)\n",
    "    translated_sentences = translate_batch(sentences)\n",
    "    return \" \".join(translated_sentences)\n",
    "\n",
    "# Apply to dataframe in parallel\n",
    "df['translated_text'] = df['text'].swifter.apply(translate_paragraph)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
