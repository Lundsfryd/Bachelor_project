{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ae94641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import outlines\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "import gc\n",
    "import ast\n",
    "from outlines import from_transformers, Generator, models\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b08852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = from_transformers(\n",
    "    transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", device_map=\"auto\", dtype=torch.bfloat16),\n",
    "    transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    ")\n",
    "#\"meta-llama/Llama-3.2-1B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1c2998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pydantic class which ensures the structured output from the llm\n",
    "class BlameeDetection(BaseModel):\n",
    "    text: str = Field(description=\"The exact original sentence being analyzed\")\n",
    "    blamee: Optional[str] = Field(default = None, description=\"Who or what is being blamed\")\n",
    "    arguments: Optional[str] = Field(default = None, description=\"What the blamee is being blamed for - the specific negative outcome\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e6941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/annotation_data_translated_version_03_10.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f611a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph_entry = {}\n",
    "for i, text in enumerate(text_data[\"da_segmented_text\"]): #check if i is sctually number\n",
    "\n",
    "\n",
    "    da_segmented_sentences = ast.literal_eval(text_data.loc[i][\"da_segmented_text\"])\n",
    "\n",
    "    sentece_entry = {}\n",
    "    for p, sentence in enumerate(da_segmented_sentences):\n",
    "        sentece_entry[p] = sentence\n",
    "    \n",
    "    paragraph_entry[i] = sentece_entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a231eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/labelstudio_merged.json', 'r') as file:\n",
    "    orig_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9ff4f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph': '2',\n",
       " 'sentence_nr': '235',\n",
       " 'text': 'Regeringen vil også fortsætte sin offensive  miljøpolitik.',\n",
       " 'speaker': 'Poul Nyrup Rasmussen',\n",
       " 'party': 'S'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2599f86",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mjson_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "json_data.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "spoken_sentences =\n",
    "\n",
    "for i in range(len(data)):\n",
    "    text = data[i][\"text\"]\n",
    "    speaker = data[i][\"speaker\"]\n",
    "    spoken_sentences.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71a5423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, BlameeDetection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Blamee detection:   0%|          | 0/4325 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 1/4325 [00:00<14:51,  4.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 2/4325 [00:00<10:49,  6.66it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 3/4325 [00:00<13:32,  5.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 4/4325 [00:00<13:53,  5.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 5/4325 [00:00<12:42,  5.67it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 6/4325 [00:01<12:43,  5.65it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 7/4325 [00:01<28:00,  2.57it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 8/4325 [00:02<27:15,  2.64it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 9/4325 [00:02<24:55,  2.89it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 10/4325 [00:02<24:32,  2.93it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 11/4325 [00:05<1:06:02,  1.09it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 12/4325 [00:05<55:27,  1.30it/s]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 13/4325 [00:05<41:03,  1.75it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 14/4325 [00:06<39:56,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 15/4325 [00:06<45:42,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 16/4325 [00:07<36:11,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 17/4325 [00:07<32:52,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 18/4325 [00:07<25:26,  2.82it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 19/4325 [00:08<33:58,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 20/4325 [00:08<28:44,  2.50it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   0%|          | 21/4325 [00:08<27:16,  2.63it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 22/4325 [00:09<39:01,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 23/4325 [00:10<31:19,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 24/4325 [00:10<27:07,  2.64it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 25/4325 [00:10<25:47,  2.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 26/4325 [00:10<25:10,  2.85it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 27/4325 [00:11<33:44,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 28/4325 [00:11<28:11,  2.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 29/4325 [00:12<31:06,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 30/4325 [00:12<32:45,  2.19it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 32/4325 [00:13<31:48,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 33/4325 [00:14<39:39,  1.80it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 34/4325 [00:15<34:56,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 35/4325 [00:15<35:58,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 36/4325 [00:15<28:44,  2.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 37/4325 [00:16<34:23,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 38/4325 [00:16<34:48,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 39/4325 [00:17<35:44,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 40/4325 [00:17<34:39,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 41/4325 [00:18<29:58,  2.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 42/4325 [00:18<28:10,  2.53it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 43/4325 [00:18<24:03,  2.97it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 44/4325 [00:18<19:12,  3.71it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 45/4325 [00:18<17:12,  4.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 46/4325 [00:19<17:50,  4.00it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 47/4325 [00:19<19:03,  3.74it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 48/4325 [00:19<19:33,  3.65it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 49/4325 [00:19<17:24,  4.10it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 50/4325 [00:20<25:05,  2.84it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 51/4325 [00:21<30:38,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 52/4325 [00:21<30:41,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 53/4325 [00:22<33:01,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|          | 54/4325 [00:23<50:17,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 55/4325 [00:23<46:21,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 56/4325 [00:24<44:09,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 57/4325 [00:25<41:42,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 58/4325 [00:25<41:24,  1.72it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 59/4325 [00:25<35:09,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 60/4325 [00:26<28:17,  2.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 61/4325 [00:26<27:02,  2.63it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 62/4325 [00:26<21:54,  3.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 63/4325 [00:26<22:21,  3.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   1%|▏         | 64/4325 [00:27<28:13,  2.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   2%|▏         | 65/4325 [00:28<31:53,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   2%|▏         | 66/4325 [00:28<35:13,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Blamee detection:   2%|▏         | 66/4325 [00:29<32:12,  2.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      2\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mPerform blamee identification on the following sentence.\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mSentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33m- Blamee: The patient receiving the blame (who or what is being blamed)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m- Argument: What is the blamee being blamed for (the negative outcome)\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():  \u001b[38;5;66;03m# Disable gradient tracking\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     result = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m#print(result)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/outlines/generator.py:297\u001b[39m, in \u001b[36mSteerableGenerator.__call__\u001b[39m\u001b[34m(self, prompt, **inference_kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.logits_processor.reset()\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/outlines/models/transformers.py:326\u001b[39m, in \u001b[36mTransformers.generate\u001b[39m\u001b[34m(self, model_input, output_type, **inference_kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m prompts, inputs = \u001b[38;5;28mself\u001b[39m._prepare_model_inputs(model_input, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    324\u001b[39m logits_processor = \u001b[38;5;28mself\u001b[39m.type_adapter.format_output_type(output_type)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m generated_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_output_seq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# required for multi-modal models that return a 2D tensor even when\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# num_return_sequences is 1\u001b[39;00m\n\u001b[32m    335\u001b[39m num_samples = inference_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mnum_return_sequences\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/outlines/models/transformers.py:375\u001b[39m, in \u001b[36mTransformers._generate_output_seq\u001b[39m\u001b[34m(self, prompts, inputs, **inference_kwargs)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_output_seq\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts, inputs, **inference_kwargs):\n\u001b[32m    373\u001b[39m     input_ids = inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     \u001b[38;5;66;03m# encoder-decoder returns output_ids only, decoder-only returns full seq ids\u001b[39;00m\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/generation/utils.py:2873\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2870\u001b[39m     outputs = model_forward(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2872\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2873\u001b[39m model_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_model_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2874\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2875\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2876\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_encoder_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2877\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[32m   2879\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/generation/utils.py:997\u001b[39m, in \u001b[36mGenerationMixin._update_model_kwargs_for_generation\u001b[39m\u001b[34m(self, outputs, model_kwargs, is_encoder_decoder, num_new_tokens)\u001b[39m\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    996\u001b[39m     past_positions = model_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     new_positions = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_positions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.to(past_positions.device)\n\u001b[32m   1000\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat((past_positions, new_positions))\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for sentence in tqdm.tqdm(spoken_sentences, desc = \"Blamee detection\"):\n",
    "    prompt = f\"\"\"Perform blamee identification on the following sentence.\n",
    "    Sentence: {sentence}\n",
    "\n",
    "    Rules:\n",
    "    - Identify specifically who or what is being blamed for causing a negative outcome in the above sentence\n",
    "    - The \"text\" field must be EXACTLY the sentence provided above - do not modify it\n",
    "    - Identify a specific part of the above sentence which indicates what the blamee is being accused of\n",
    "    - Never leave arguments as an empty string\n",
    "\n",
    "    - Roles:\n",
    "    - Blamee: The patient receiving the blame (who or what is being blamed)\n",
    "    - Argument: What is the blamee being blamed for (the negative outcome)\"\"\"\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        result = generator(prompt, max_new_tokens=256, use_cache=False)\n",
    "        #print(result)\n",
    "    try:\n",
    "        result_out = BlameeDetection.model_validate_json(result)\n",
    "    except (ValidationError, json.JSONDecodeError):\n",
    "        print(\"Skipping invalid entry.\")\n",
    "        continue\n",
    "\n",
    "    with open(\"result_blamee_detection.json\", \"a\") as f:\n",
    "        json.dump(result_out.model_dump(), f, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
