{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71799c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d518315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae515f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS need to be made usable\n",
    "'''class BlameDetectorDa(object):\n",
    "\n",
    "    def __init__(self, model_path, max_length, batch_size = None):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model_initialization()\n",
    "\n",
    "        return\n",
    "\n",
    "    def model_initialization(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            self.text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def run_prediction(self, text):\n",
    "\n",
    "        self.text = text\n",
    "        predicted_class, confidence, probs = self.predict()\n",
    "            \n",
    "        return predicted_class, confidence\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Goddad min kære ven, jeg kan virkelig godt lide dig.\"\n",
    "\n",
    "BD = BlameDetectorDa()\n",
    "BD.run_prediction(text = example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6aa150b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Example usage\\n\\n# Initialize detector\\nMODEL_PATH = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_merged\"\\ndetector = BlameDetectorDa(\\n    model_path=MODEL_PATH,\\n    max_length=512,\\n    batch_size=8  # Process 8 texts at a time\\n)\\n\\nprint(\"Single Prediction Test\")\\n\\n\\n# Single prediction\\nsingle_text = \"Dette er en test sætning.\"\\npred_class, confidence = detector.run_prediction(single_text)\\nprint(f\"\\nText: {single_text}\")\\nprint(f\"Predicted Class: {pred_class}\")\\nprint(f\"Confidence: {confidence:.4f}\")\\n\\nprint(\"Batch Prediction Test\")\\n\\n# Batch prediction\\nbatch_texts = [\\n    \"Første test sætning.\",\\n    \"Anden test sætning.\",\\n    \"Tredje test sætning.\",\\n    \"Fjerde test sætning.\",\\n]\\n\\npred_classes, confidences = detector.run_prediction(batch_texts)\\n\\nprint(\"\\nBatch Results:\")\\nfor i, (text, cls, conf) in enumerate(zip(batch_texts, pred_classes, confidences), 1):\\n    print(f\"\\n{i}. Text: {text}\")\\n    print(f\"   Class: {cls}, Confidence: {conf:.4f}\")'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import Union, List, Tuple\n",
    "\n",
    "class BlameDetectorDa(object):\n",
    "    \"\"\"\n",
    "    Danish Blame Detection classifier with support for single and batch predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, max_length: int = 512, batch_size: int = None, base_model_name = \"jhu-clsp/mmBERT-base\", num_labels = 2):\n",
    "        \"\"\"\n",
    "        Initialize the BlameDetectorDa classifier.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the pretrained model\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "            batch_size: Batch size for batch predictions (None for single predictions)\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.base_model_name = base_model_name #delete when model is ready\n",
    "        self.num_labels = num_labels #delete when model is ready\n",
    "\n",
    "        self.model_initialization_prelim() #change when model is ready\n",
    "\n",
    "    def model_initialization(self):\n",
    "        \"\"\"Initialize the model, tokenizer, and device.\"\"\"\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "        \n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_path,\n",
    "            trust_remote_code=True  # Add this for ModernBERT\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)  # Fixed: was 'device', should be 'self.device'\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "    def model_initialization_prelim(self):\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "    \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
    "        \n",
    "        # Load base model with classification head\n",
    "        print(f\"Loading base model: {self.base_model_name}\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            num_labels=self.num_labels,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load LoRA adapters\n",
    "        print(\"Loading LoRA adapters...\")\n",
    "        self.model = PeftModel.from_pretrained(self.model, self.model_path)\n",
    "        \n",
    "        # Merge LoRA weights with base model for faster inference (optional)\n",
    "        print(\"Merging LoRA weights...\")\n",
    "        self.model = self.model.merge_and_unload()\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        \n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "        return\n",
    "        \n",
    "\n",
    "    def predict_single(self, text: str) -> Tuple[int, float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Make a prediction on a single text input.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to classify\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predicted_class, confidence, probabilities)\n",
    "        \"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def predict_batch(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Make predictions on a batch of text inputs.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts to classify\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predicted_classes, confidences, all_probabilities)\n",
    "        \"\"\"\n",
    "        # Tokenize all inputs\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "            confidences = probabilities.max(dim=1).values.cpu().numpy()\n",
    "            all_probs = probabilities.cpu().numpy()\n",
    "        \n",
    "        return predicted_classes, confidences, all_probs\n",
    "\n",
    "    def run_prediction(\n",
    "        self, \n",
    "        text: Union[str, List[str]]\n",
    "    ) -> Union[Tuple[int, float], Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Run prediction on single text or batch of texts.\n",
    "        \n",
    "        Args:\n",
    "            text: Single text string or list of text strings\n",
    "            \n",
    "        Returns:\n",
    "            For single text: (predicted_class, confidence)\n",
    "            For batch: (predicted_classes, confidences)\n",
    "        \"\"\"\n",
    "        # Check if input is a list (batch) or single text\n",
    "        if isinstance(text, list):\n",
    "            # Batch prediction\n",
    "            if self.batch_size is not None and len(text) > self.batch_size:\n",
    "                # Process in batches\n",
    "                all_classes = []\n",
    "                all_confidences = []\n",
    "                \n",
    "                for i in range(0, len(text), self.batch_size):\n",
    "                    batch = text[i:i + self.batch_size]\n",
    "                    classes, confidences, _ = self.predict_batch(batch)\n",
    "                    all_classes.extend(classes)\n",
    "                    all_confidences.extend(confidences)\n",
    "                \n",
    "                return np.array(all_classes), np.array(all_confidences)\n",
    "            else:\n",
    "                # Single batch prediction\n",
    "                predicted_classes, confidences, _ = self.predict_batch(text)\n",
    "                return predicted_classes, confidences\n",
    "        else:\n",
    "            # Single text prediction\n",
    "            predicted_class, confidence, _ = self.predict_single(text)\n",
    "            return predicted_class, confidence\n",
    "\n",
    "\n",
    "'''# Example usage\n",
    "\n",
    "# Initialize detector\n",
    "MODEL_PATH = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_merged\"\n",
    "detector = BlameDetectorDa(\n",
    "    model_path=MODEL_PATH,\n",
    "    max_length=512,\n",
    "    batch_size=8  # Process 8 texts at a time\n",
    ")\n",
    "\n",
    "print(\"Single Prediction Test\")\n",
    "\n",
    "\n",
    "# Single prediction\n",
    "single_text = \"Dette er en test sætning.\"\n",
    "pred_class, confidence = detector.run_prediction(single_text)\n",
    "print(f\"\\nText: {single_text}\")\n",
    "print(f\"Predicted Class: {pred_class}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "print(\"Batch Prediction Test\")\n",
    "\n",
    "# Batch prediction\n",
    "batch_texts = [\n",
    "    \"Første test sætning.\",\n",
    "    \"Anden test sætning.\",\n",
    "    \"Tredje test sætning.\",\n",
    "    \"Fjerde test sætning.\",\n",
    "]\n",
    "\n",
    "pred_classes, confidences = detector.run_prediction(batch_texts)\n",
    "\n",
    "print(\"\\nBatch Results:\")\n",
    "for i, (text, cls, conf) in enumerate(zip(batch_texts, pred_classes, confidences), 1):\n",
    "    print(f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Class: {cls}, Confidence: {conf:.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59bec3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model...\n",
      "Loading base model: jhu-clsp/mmBERT-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters...\n",
      "Merging LoRA weights...\n",
      "Model loaded successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm  # for progress bar (optional)\n",
    "\n",
    "MODEL_PATH = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model\"\n",
    "detector = BlameDetectorDa(\n",
    "    model_path=MODEL_PATH,\n",
    "    max_length=1024,\n",
    "    batch_size=2  # Process 2 texts at a time\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Load JSON dataset ----\n",
    "import json\n",
    "json_path = \"/work/MarkusLundsfrydJensen#1865/inferece_data/final_inference_data.json\"\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648c74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- 2. Extract texts ----\n",
    "texts = [item[\"text\"] for item in data if \"text\" in item]\n",
    "\n",
    "# ---- 3. Run predictions in batches ----\n",
    "pred_classes, confidences = detector.run_prediction(texts)\n",
    "\n",
    "\n",
    "# ---- 4. Attach predictions back to data ----\n",
    "for item, cls, conf in zip(data, pred_classes, confidences):\n",
    "    item[\"predicted_class\"] = int(cls)\n",
    "    item[\"confidence\"] = float(conf)\n",
    "\n",
    "\n",
    "#save data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c3aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.5366607308387756\n",
      "0\n",
      "0.7602487802505493\n"
     ]
    }
   ],
   "source": [
    "for entry in data:\n",
    "    if entry[\"label\"] == 1:\n",
    "        print(entry[\"predicted_class\"])\n",
    "        print(entry[\"confidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5060a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
