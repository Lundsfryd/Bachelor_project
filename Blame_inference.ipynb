{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71799c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a3eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class BlameDetectorDa(object):\n",
    "\n",
    "    def __init__(self, model_path, max_length=512, batch_size=32):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model_initialization()\n",
    "\n",
    "        return\n",
    "\n",
    "    def model_initialization(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_path,\n",
    "            device_map='auto'\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "       \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def predict_batch(self, texts):\n",
    "        \"\"\"Make predictions on a batch of texts.\"\"\"\n",
    "        # Tokenize all texts in the batch\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "            confidences = probabilities[range(len(predicted_classes)), predicted_classes].cpu().numpy()\n",
    "        \n",
    "        return predicted_classes, confidences, probabilities.cpu().numpy()\n",
    "\n",
    "    def run_prediction(self, text):\n",
    "        \"\"\"Single text prediction (backward compatibility).\"\"\"\n",
    "        predicted_class, confidence, probs = self.predict(text)\n",
    "        return predicted_class, confidence\n",
    "\n",
    "    def run_batch_prediction(self, texts, show_progress=True):\n",
    "        \"\"\"\n",
    "        Run predictions on a list of texts with batching and progress bar.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to predict on\n",
    "            show_progress: Whether to show progress bar (default: True)\n",
    "            \n",
    "        Returns:\n",
    "            predicted_classes: numpy array of predicted class indices\n",
    "            confidences: numpy array of confidence scores\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        \n",
    "        # Create batches\n",
    "        num_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        # Setup progress bar\n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(texts), desc=\"Processing texts\", unit=\"text\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            \n",
    "            # Get predictions for this batch\n",
    "            predicted_classes, confidences, _ = self.predict_batch(batch_texts)\n",
    "            \n",
    "            all_predictions.extend(predicted_classes)\n",
    "            all_confidences.extend(confidences)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if show_progress:\n",
    "                pbar.update(len(batch_texts))\n",
    "        \n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "        \n",
    "        return np.array(all_predictions), np.array(all_confidences)\n",
    "\n",
    "\n",
    "    def predict_from_json(self, json_path, text_key = 'text', output_key='prediction', \n",
    "                          confidence_key='confidence', show_progress=True):\n",
    "        \"\"\"\n",
    "        Load JSON, predict, and return results with predictions added.\n",
    "        \n",
    "        Args:\n",
    "            json_path: Path to JSON file\n",
    "            text_key: Key in JSON objects containing the text to classify\n",
    "            output_key: Key name for storing predictions (default: 'prediction')\n",
    "            confidence_key: Key name for storing confidence scores (default: 'confidence')\n",
    "            show_progress: Whether to show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with predictions added\n",
    "        \"\"\"\n",
    "        import json\n",
    "        \n",
    "        # Load JSON data\n",
    "        print(f\"Loading data from {json_path}...\")\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print('json file loaded')\n",
    "        \n",
    "        # Handle different JSON structures\n",
    "        if isinstance(data, list):\n",
    "            items = data\n",
    "        elif isinstance(data, dict):\n",
    "            # If it's a dict, try to find the list of items\n",
    "            # Adjust this based on your JSON structure\n",
    "            items = list(data.values()) if all(isinstance(v, dict) for v in data.values()) else [data]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported JSON structure\")\n",
    "        \n",
    "        print(f\"Found {len(items)} items. extracting texts...\")\n",
    "        \n",
    "        # Extract texts\n",
    "        texts = [item[text_key] for item in items]\n",
    "\n",
    "        print('start batch prediction')\n",
    "        \n",
    "        # Run predictions\n",
    "        predictions, confidences = self.run_batch_prediction(texts, show_progress=show_progress)\n",
    "        \n",
    "        # Add predictions to original data\n",
    "        for item, pred, conf in zip(items, predictions, confidences):\n",
    "            item[output_key] = int(pred)\n",
    "            item[confidence_key] = float(conf)\n",
    "        \n",
    "        return items\n",
    "\n",
    "    def predict_from_json_to_file(self, input_path, output_path, text_key, \n",
    "                                   output_key='prediction', confidence_key='confidence',\n",
    "                                   show_progress=True):\n",
    "        \"\"\"\n",
    "        Load JSON, predict, and save results to a new file.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input JSON file\n",
    "            output_path: Path to save output JSON file\n",
    "            text_key: Key containing text to classify\n",
    "            output_key: Key name for predictions\n",
    "            confidence_key: Key name for confidence scores\n",
    "            show_progress: Whether to show progress bar\n",
    "        \"\"\"\n",
    "        import json\n",
    "        \n",
    "        # Get predictions\n",
    "        results = self.predict_from_json(\n",
    "            input_path, text_key, output_key, confidence_key, show_progress\n",
    "        )\n",
    "        \n",
    "        # Save to file\n",
    "        print(f\"Saving results to {output_path}...\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Done! Processed {len(results)} items\")\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0eec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "detector = BlameDetectorDa(\n",
    "    model_path=\"Lundsfryd/Pol_Blame_Detection_Da\",\n",
    "    max_length=512,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /work/MarkusLundsfrydJensen#1865/final_inference_data.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:   1%|          | 19328/2191051 [01:03<1:06:37, 543.30text/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json file loaded\n",
      "Found 2191051 items. extracting texts...\n",
      "start batch prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Method 2: Process and save to file directly\n",
    "detector.predict_from_json_to_file(\n",
    "    input_path=\"/work/MarkusLundsfrydJensen#1865/final_inference_data.json\",\n",
    "    output_path=\"/work/MarkusLundsfrydJensen#1865/final_final_inference_data.json\",\n",
    "    text_key=\"text\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
