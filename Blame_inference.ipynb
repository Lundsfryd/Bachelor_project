{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71799c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add3e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define functions\n",
    "\n",
    "def danish_sentences_extraction(data, text_column):\n",
    "\n",
    "\n",
    "    rows = range(len)\n",
    "    sentences = {}\n",
    "\n",
    "    for para in rows:\n",
    "        #sentence_indices = dict[para]\n",
    "        text_sentences = ast.literal_eval(data.loc[para][text_column])\n",
    "        \n",
    "        blame_sentence_dict = {}\n",
    "        for indx in range(len(text_sentences)):\n",
    "            blame_sentence = text_sentences[indx]\n",
    "            blame_sentence_dict[indx] = blame_sentence\n",
    "        \n",
    "        \n",
    "        sentences[para] = blame_sentence_dict\n",
    "\n",
    "    return sentences\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83f0eea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>agenda</th>\n",
       "      <th>speechnumber</th>\n",
       "      <th>speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>party.facts.id</th>\n",
       "      <th>chair</th>\n",
       "      <th>terms</th>\n",
       "      <th>text</th>\n",
       "      <th>parliament</th>\n",
       "      <th>iso3country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36314</th>\n",
       "      <td>36314</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>1) Spørgsmål til ministrene.</td>\n",
       "      <td>2</td>\n",
       "      <td>Henning Grove</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>181</td>\n",
       "      <td>Til at besvare spørgsmål i spørgetimen i dag h...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36315</th>\n",
       "      <td>36315</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>1) Spørgsmål til ministrene.</td>\n",
       "      <td>3</td>\n",
       "      <td>Frank Dahlgaard</td>\n",
       "      <td>UP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>187</td>\n",
       "      <td>Ministeren er jo ikke bare minister for fødeva...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36316</th>\n",
       "      <td>36316</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>1) Spørgsmål til ministrene.</td>\n",
       "      <td>4</td>\n",
       "      <td>Henrik Dam Kristensen</td>\n",
       "      <td>S</td>\n",
       "      <td>379.0</td>\n",
       "      <td>False</td>\n",
       "      <td>175</td>\n",
       "      <td>Først vil jeg gerne sige til hr. Frank Dahlgaa...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36317</th>\n",
       "      <td>36317</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>1) Spørgsmål til ministrene.</td>\n",
       "      <td>5</td>\n",
       "      <td>Frank Dahlgaard</td>\n",
       "      <td>UP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>173</td>\n",
       "      <td>Ja, vi har en stor eksport, men ministeren er ...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36318</th>\n",
       "      <td>36318</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>1) Spørgsmål til ministrene.</td>\n",
       "      <td>6</td>\n",
       "      <td>Henrik Dam Kristensen</td>\n",
       "      <td>S</td>\n",
       "      <td>379.0</td>\n",
       "      <td>False</td>\n",
       "      <td>138</td>\n",
       "      <td>Hr. Frank Dahlgaard har jo fuldstændig ret i, ...</td>\n",
       "      <td>DK-Folketing</td>\n",
       "      <td>DNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0       date                        agenda  speechnumber  \\\n",
       "36314       36314 2000-11-01  1) Spørgsmål til ministrene.             2   \n",
       "36315       36315 2000-11-01  1) Spørgsmål til ministrene.             3   \n",
       "36316       36316 2000-11-01  1) Spørgsmål til ministrene.             4   \n",
       "36317       36317 2000-11-01  1) Spørgsmål til ministrene.             5   \n",
       "36318       36318 2000-11-01  1) Spørgsmål til ministrene.             6   \n",
       "\n",
       "                     speaker party  party.facts.id  chair  terms  \\\n",
       "36314          Henning Grove   NaN             NaN   True    181   \n",
       "36315        Frank Dahlgaard    UP             NaN  False    187   \n",
       "36316  Henrik Dam Kristensen     S           379.0  False    175   \n",
       "36317        Frank Dahlgaard    UP             NaN  False    173   \n",
       "36318  Henrik Dam Kristensen     S           379.0  False    138   \n",
       "\n",
       "                                                    text    parliament  \\\n",
       "36314  Til at besvare spørgsmål i spørgetimen i dag h...  DK-Folketing   \n",
       "36315  Ministeren er jo ikke bare minister for fødeva...  DK-Folketing   \n",
       "36316  Først vil jeg gerne sige til hr. Frank Dahlgaa...  DK-Folketing   \n",
       "36317  Ja, vi har en stor eksport, men ministeren er ...  DK-Folketing   \n",
       "36318  Hr. Frank Dahlgaard har jo fuldstændig ret i, ...  DK-Folketing   \n",
       "\n",
       "      iso3country  \n",
       "36314         DNK  \n",
       "36315         DNK  \n",
       "36316         DNK  \n",
       "36317         DNK  \n",
       "36318         DNK  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fist create inference dataset\n",
    "\n",
    "df = pd.read_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Corp_Folketing_V2.csv\")\n",
    "\n",
    "# Convert the column to datetime (specify format if necessary)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors='coerce', dayfirst=True)\n",
    "\n",
    "# Filter rows after January 1, 2000\n",
    "filtered_df = df[df[\"date\"] > \"2000-01-01\"]\n",
    "\n",
    "#filtered_df.pop(\"Unnamed: 0\")\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28875cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293615"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bf0898",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_sub = filtered_df[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d518315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = danish_sentences_extraction(filtered_df_sub, 'segmented_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/inference_data.json\", mode='w', encoding='utf-8') as jsonfile:\n",
    "    json.dump(filtered_df, jsonfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  {\n",
    "    \"paragraph\": \"2\",\n",
    "    \"sentence_nr\": \"235\",\n",
    "    \"text\": \"Regeringen vil også fortsætte sin offensive  miljøpolitik.\",\n",
    "    \"speaker\": \"Poul Nyrup Rasmussen\",\n",
    "    \"party\": \"S\",\n",
    "    \"preceding_sentence\": \"Der indføres en belønningsordning for virksomheder, der gør  en særlig ekstraindsats ud over det, loven stiller krav om,  for at forbedre arbejdsmiljøet.\",\n",
    "    \"succeeding_sent\": \"Det er vores næste store felt.\",\n",
    "    \"current_speaker_in_government\": true,\n",
    "    \"parties_in_government\": [\n",
    "      \"S\",\n",
    "      \"RV\"\n",
    "    ],\n",
    "    \"date\": \"1997-10-07 00:00:00\",\n",
    "    \"label\": 1\n",
    "  },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe0322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780e97a7bd814cdd8e88c0899ab0b678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81e91d4cab145798e94cb717b68ec9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading adapter weights from /work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model led to unexpected keys not found in the model: model.layers.0.attn.Wqkv.lora_A.default.weight, model.layers.0.attn.Wqkv.lora_B.default.weight, model.layers.1.attn.Wqkv.lora_A.default.weight, model.layers.1.attn.Wqkv.lora_B.default.weight, model.layers.10.attn.Wqkv.lora_A.default.weight, model.layers.10.attn.Wqkv.lora_B.default.weight, model.layers.11.attn.Wqkv.lora_A.default.weight, model.layers.11.attn.Wqkv.lora_B.default.weight, model.layers.12.attn.Wqkv.lora_A.default.weight, model.layers.12.attn.Wqkv.lora_B.default.weight, model.layers.13.attn.Wqkv.lora_A.default.weight, model.layers.13.attn.Wqkv.lora_B.default.weight, model.layers.14.attn.Wqkv.lora_A.default.weight, model.layers.14.attn.Wqkv.lora_B.default.weight, model.layers.15.attn.Wqkv.lora_A.default.weight, model.layers.15.attn.Wqkv.lora_B.default.weight, model.layers.16.attn.Wqkv.lora_A.default.weight, model.layers.16.attn.Wqkv.lora_B.default.weight, model.layers.17.attn.Wqkv.lora_A.default.weight, model.layers.17.attn.Wqkv.lora_B.default.weight, model.layers.18.attn.Wqkv.lora_A.default.weight, model.layers.18.attn.Wqkv.lora_B.default.weight, model.layers.19.attn.Wqkv.lora_A.default.weight, model.layers.19.attn.Wqkv.lora_B.default.weight, model.layers.2.attn.Wqkv.lora_A.default.weight, model.layers.2.attn.Wqkv.lora_B.default.weight, model.layers.20.attn.Wqkv.lora_A.default.weight, model.layers.20.attn.Wqkv.lora_B.default.weight, model.layers.21.attn.Wqkv.lora_A.default.weight, model.layers.21.attn.Wqkv.lora_B.default.weight, model.layers.3.attn.Wqkv.lora_A.default.weight, model.layers.3.attn.Wqkv.lora_B.default.weight, model.layers.4.attn.Wqkv.lora_A.default.weight, model.layers.4.attn.Wqkv.lora_B.default.weight, model.layers.5.attn.Wqkv.lora_A.default.weight, model.layers.5.attn.Wqkv.lora_B.default.weight, model.layers.6.attn.Wqkv.lora_A.default.weight, model.layers.6.attn.Wqkv.lora_B.default.weight, model.layers.7.attn.Wqkv.lora_A.default.weight, model.layers.7.attn.Wqkv.lora_B.default.weight, model.layers.8.attn.Wqkv.lora_A.default.weight, model.layers.8.attn.Wqkv.lora_B.default.weight, model.layers.9.attn.Wqkv.lora_A.default.weight, model.layers.9.attn.Wqkv.lora_B.default.weight. Loading adapter weights from /work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model led to missing keys in the model: layers.0.attn.Wqkv.lora_A.default.weight, layers.0.attn.Wqkv.lora_B.default.weight, layers.1.attn.Wqkv.lora_A.default.weight, layers.1.attn.Wqkv.lora_B.default.weight, layers.2.attn.Wqkv.lora_A.default.weight, layers.2.attn.Wqkv.lora_B.default.weight, layers.3.attn.Wqkv.lora_A.default.weight, layers.3.attn.Wqkv.lora_B.default.weight, layers.4.attn.Wqkv.lora_A.default.weight, layers.4.attn.Wqkv.lora_B.default.weight, layers.5.attn.Wqkv.lora_A.default.weight, layers.5.attn.Wqkv.lora_B.default.weight, layers.6.attn.Wqkv.lora_A.default.weight, layers.6.attn.Wqkv.lora_B.default.weight, layers.7.attn.Wqkv.lora_A.default.weight, layers.7.attn.Wqkv.lora_B.default.weight, layers.8.attn.Wqkv.lora_A.default.weight, layers.8.attn.Wqkv.lora_B.default.weight, layers.9.attn.Wqkv.lora_A.default.weight, layers.9.attn.Wqkv.lora_B.default.weight, layers.10.attn.Wqkv.lora_A.default.weight, layers.10.attn.Wqkv.lora_B.default.weight, layers.11.attn.Wqkv.lora_A.default.weight, layers.11.attn.Wqkv.lora_B.default.weight, layers.12.attn.Wqkv.lora_A.default.weight, layers.12.attn.Wqkv.lora_B.default.weight, layers.13.attn.Wqkv.lora_A.default.weight, layers.13.attn.Wqkv.lora_B.default.weight, layers.14.attn.Wqkv.lora_A.default.weight, layers.14.attn.Wqkv.lora_B.default.weight, layers.15.attn.Wqkv.lora_A.default.weight, layers.15.attn.Wqkv.lora_B.default.weight, layers.16.attn.Wqkv.lora_A.default.weight, layers.16.attn.Wqkv.lora_B.default.weight, layers.17.attn.Wqkv.lora_A.default.weight, layers.17.attn.Wqkv.lora_B.default.weight, layers.18.attn.Wqkv.lora_A.default.weight, layers.18.attn.Wqkv.lora_B.default.weight, layers.19.attn.Wqkv.lora_A.default.weight, layers.19.attn.Wqkv.lora_B.default.weight, layers.20.attn.Wqkv.lora_A.default.weight, layers.20.attn.Wqkv.lora_B.default.weight, layers.21.attn.Wqkv.lora_A.default.weight, layers.21.attn.Wqkv.lora_B.default.weight\n",
      "Loading adapter weights from /work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model led to unexpected keys not found in the model: model.layers.0.attn.Wqkv.lora_A.default.weight, model.layers.0.attn.Wqkv.lora_B.default.weight, model.layers.1.attn.Wqkv.lora_A.default.weight, model.layers.1.attn.Wqkv.lora_B.default.weight, model.layers.10.attn.Wqkv.lora_A.default.weight, model.layers.10.attn.Wqkv.lora_B.default.weight, model.layers.11.attn.Wqkv.lora_A.default.weight, model.layers.11.attn.Wqkv.lora_B.default.weight, model.layers.12.attn.Wqkv.lora_A.default.weight, model.layers.12.attn.Wqkv.lora_B.default.weight, model.layers.13.attn.Wqkv.lora_A.default.weight, model.layers.13.attn.Wqkv.lora_B.default.weight, model.layers.14.attn.Wqkv.lora_A.default.weight, model.layers.14.attn.Wqkv.lora_B.default.weight, model.layers.15.attn.Wqkv.lora_A.default.weight, model.layers.15.attn.Wqkv.lora_B.default.weight, model.layers.16.attn.Wqkv.lora_A.default.weight, model.layers.16.attn.Wqkv.lora_B.default.weight, model.layers.17.attn.Wqkv.lora_A.default.weight, model.layers.17.attn.Wqkv.lora_B.default.weight, model.layers.18.attn.Wqkv.lora_A.default.weight, model.layers.18.attn.Wqkv.lora_B.default.weight, model.layers.19.attn.Wqkv.lora_A.default.weight, model.layers.19.attn.Wqkv.lora_B.default.weight, model.layers.2.attn.Wqkv.lora_A.default.weight, model.layers.2.attn.Wqkv.lora_B.default.weight, model.layers.20.attn.Wqkv.lora_A.default.weight, model.layers.20.attn.Wqkv.lora_B.default.weight, model.layers.21.attn.Wqkv.lora_A.default.weight, model.layers.21.attn.Wqkv.lora_B.default.weight, model.layers.3.attn.Wqkv.lora_A.default.weight, model.layers.3.attn.Wqkv.lora_B.default.weight, model.layers.4.attn.Wqkv.lora_A.default.weight, model.layers.4.attn.Wqkv.lora_B.default.weight, model.layers.5.attn.Wqkv.lora_A.default.weight, model.layers.5.attn.Wqkv.lora_B.default.weight, model.layers.6.attn.Wqkv.lora_A.default.weight, model.layers.6.attn.Wqkv.lora_B.default.weight, model.layers.7.attn.Wqkv.lora_A.default.weight, model.layers.7.attn.Wqkv.lora_B.default.weight, model.layers.8.attn.Wqkv.lora_A.default.weight, model.layers.8.attn.Wqkv.lora_B.default.weight, model.layers.9.attn.Wqkv.lora_A.default.weight, model.layers.9.attn.Wqkv.lora_B.default.weight. Loading adapter weights from /work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model led to missing keys in the model: layers.0.attn.Wqkv.lora_A.default.weight, layers.0.attn.Wqkv.lora_B.default.weight, layers.1.attn.Wqkv.lora_A.default.weight, layers.1.attn.Wqkv.lora_B.default.weight, layers.2.attn.Wqkv.lora_A.default.weight, layers.2.attn.Wqkv.lora_B.default.weight, layers.3.attn.Wqkv.lora_A.default.weight, layers.3.attn.Wqkv.lora_B.default.weight, layers.4.attn.Wqkv.lora_A.default.weight, layers.4.attn.Wqkv.lora_B.default.weight, layers.5.attn.Wqkv.lora_A.default.weight, layers.5.attn.Wqkv.lora_B.default.weight, layers.6.attn.Wqkv.lora_A.default.weight, layers.6.attn.Wqkv.lora_B.default.weight, layers.7.attn.Wqkv.lora_A.default.weight, layers.7.attn.Wqkv.lora_B.default.weight, layers.8.attn.Wqkv.lora_A.default.weight, layers.8.attn.Wqkv.lora_B.default.weight, layers.9.attn.Wqkv.lora_A.default.weight, layers.9.attn.Wqkv.lora_B.default.weight, layers.10.attn.Wqkv.lora_A.default.weight, layers.10.attn.Wqkv.lora_B.default.weight, layers.11.attn.Wqkv.lora_A.default.weight, layers.11.attn.Wqkv.lora_B.default.weight, layers.12.attn.Wqkv.lora_A.default.weight, layers.12.attn.Wqkv.lora_B.default.weight, layers.13.attn.Wqkv.lora_A.default.weight, layers.13.attn.Wqkv.lora_B.default.weight, layers.14.attn.Wqkv.lora_A.default.weight, layers.14.attn.Wqkv.lora_B.default.weight, layers.15.attn.Wqkv.lora_A.default.weight, layers.15.attn.Wqkv.lora_B.default.weight, layers.16.attn.Wqkv.lora_A.default.weight, layers.16.attn.Wqkv.lora_B.default.weight, layers.17.attn.Wqkv.lora_A.default.weight, layers.17.attn.Wqkv.lora_B.default.weight, layers.18.attn.Wqkv.lora_A.default.weight, layers.18.attn.Wqkv.lora_B.default.weight, layers.19.attn.Wqkv.lora_A.default.weight, layers.19.attn.Wqkv.lora_B.default.weight, layers.20.attn.Wqkv.lora_A.default.weight, layers.20.attn.Wqkv.lora_B.default.weight, layers.21.attn.Wqkv.lora_A.default.weight, layers.21.attn.Wqkv.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets\n",
    "#tokenizer = AutoModel.from_pretrained(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model\")\n",
    "model = AutoModel.from_pretrained(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a2739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b97f9e12b94df8b08f6dd804090383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f819fb9ddad64d9db70565aeb809991b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d21a5ab82524f7fa44769b863c8b5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/mmBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1a02b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model...\n",
      "Loading base model: jhu-clsp/mmBERT-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters...\n",
      "Merging LoRA weights...\n",
      "Model loaded successfully on cpu\n",
      "\n",
      "======================================================================\n",
      "Testing ModernBERT Binary Classifier with LoRA\n",
      "======================================================================\n",
      "\n",
      "SINGLE PREDICTIONS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test 1:\n",
      "Input: Goddad min kære ven, jeg kan virkelig godt lide dig.\n",
      "Predicted Class: 0\n",
      "Confidence: 0.7502\n",
      "Probabilities: [Class 0: 0.7502, Class 1: 0.2498]\n",
      "\n",
      "Test 2:\n",
      "Input: Jeg vil blot sige til fødevareministeren, at han er grim .\n",
      "Predicted Class: 0\n",
      "Confidence: 0.5292\n",
      "Probabilities: [Class 0: 0.5292, Class 1: 0.4708]\n",
      "\n",
      "Test 3:\n",
      "Input: Dernædt vil jeg blot understrege de tydelige beviser på immigraternes tydelige skyld i nedgangen i økonomien.\n",
      "Predicted Class: 1\n",
      "Confidence: 0.8764\n",
      "Probabilities: [Class 0: 0.1236, Class 1: 0.8764]\n",
      "\n",
      "======================================================================\n",
      "BATCH PREDICTION:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Text 1: Goddad min kære ven, jeg kan virkelig godt lide dig.\n",
      "Class: 0, Confidence: 0.7502\n",
      "\n",
      "Text 2: Jeg vil blot sige til fødevareministeren, at han er grim .\n",
      "Class: 0, Confidence: 0.5292\n",
      "\n",
      "Text 3: Dernædt vil jeg blot understrege de tydelige beviser på immigraternes tydelige skyld i nedgangen i økonomien.\n",
      "Class: 1, Confidence: 0.8764\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel, PeftConfig\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model\"\n",
    "BASE_MODEL = \"jhu-clsp/mmBERT-base\" \n",
    "MAX_LENGTH = 512  # ModernBERT supports up to 8192\n",
    "NUM_LABELS = 2  # Binary classification\n",
    "\n",
    "def load_model_with_lora(model_path, base_model_name, num_labels=2):\n",
    "    \"\"\"Load the fine-tuned ModernBERT model with LoRA adapters.\"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Load base model with classification head\n",
    "    print(f\"Loading base model: {base_model_name}\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=num_labels,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters\n",
    "    print(\"Loading LoRA adapters...\")\n",
    "    model = PeftModel.from_pretrained(model, model_path)\n",
    "    \n",
    "    # Merge LoRA weights with base model for faster inference (optional)\n",
    "    print(\"Merging LoRA weights...\")\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded successfully on {device}\")\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def predict(text, model, tokenizer, device, max_length=512):\n",
    "    \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "def predict_batch(texts, model, tokenizer, device, max_length=512):\n",
    "    \"\"\"Make predictions on a batch of texts.\"\"\"\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "        confidences = probabilities.max(dim=1).values.cpu().numpy()\n",
    "    \n",
    "    return predicted_classes, confidences, probabilities.cpu().numpy()\n",
    "\n",
    "def main():\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer, device = load_model_with_lora(\n",
    "        MODEL_PATH, \n",
    "        BASE_MODEL, \n",
    "        NUM_LABELS\n",
    "    )\n",
    "    \n",
    "    # Test examples\n",
    "    test_texts = [\n",
    "        \"Goddad min kære ven, jeg kan virkelig godt lide dig.\",\n",
    "        \"Jeg vil blot sige til fødevareministeren, at han er grim .\",\n",
    "        \"Dernædt vil jeg blot understrege de tydelige beviser på immigraternes tydelige skyld i nedgangen i økonomien.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Testing ModernBERT Binary Classifier with LoRA\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Single predictions\n",
    "    print(\"SINGLE PREDICTIONS:\")\n",
    "    print(\"-\" * 70)\n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        print(f\"\\nTest {i}:\")\n",
    "        print(f\"Input: {text}\")\n",
    "        \n",
    "        predicted_class, confidence, probs = predict(\n",
    "            text, model, tokenizer, device, MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        print(f\"Predicted Class: {predicted_class}\")\n",
    "        print(f\"Confidence: {confidence:.4f}\")\n",
    "        print(f\"Probabilities: [Class 0: {probs[0]:.4f}, Class 1: {probs[1]:.4f}]\")\n",
    "    \n",
    "    # Batch prediction example\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BATCH PREDICTION:\")\n",
    "    print(\"-\" * 70)\n",
    "    classes, confidences, probs = predict_batch(\n",
    "        test_texts, model, tokenizer, device, MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    for i, (text, cls, conf) in enumerate(zip(test_texts, classes, confidences), 1):\n",
    "        print(f\"\\nText {i}: {text}\")\n",
    "        print(f\"Class: {cls}, Confidence: {conf:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae515f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS need to be made usable\n",
    "'''class BlameDetectorDa(object):\n",
    "\n",
    "    def __init__(self, model_path, max_length, batch_size = None):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model_initialization()\n",
    "\n",
    "        return\n",
    "\n",
    "    def model_initialization(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            self.text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def run_prediction(self, text):\n",
    "\n",
    "        self.text = text\n",
    "        predicted_class, confidence, probs = self.predict()\n",
    "            \n",
    "        return predicted_class, confidence\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Goddad min kære ven, jeg kan virkelig godt lide dig.\"\n",
    "\n",
    "BD = BlameDetectorDa()\n",
    "BD.run_prediction(text = example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6aa150b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Example usage\\n\\n# Initialize detector\\nMODEL_PATH = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_merged\"\\ndetector = BlameDetectorDa(\\n    model_path=MODEL_PATH,\\n    max_length=512,\\n    batch_size=8  # Process 8 texts at a time\\n)\\n\\nprint(\"Single Prediction Test\")\\n\\n\\n# Single prediction\\nsingle_text = \"Dette er en test sætning.\"\\npred_class, confidence = detector.run_prediction(single_text)\\nprint(f\"\\nText: {single_text}\")\\nprint(f\"Predicted Class: {pred_class}\")\\nprint(f\"Confidence: {confidence:.4f}\")\\n\\nprint(\"Batch Prediction Test\")\\n\\n# Batch prediction\\nbatch_texts = [\\n    \"Første test sætning.\",\\n    \"Anden test sætning.\",\\n    \"Tredje test sætning.\",\\n    \"Fjerde test sætning.\",\\n]\\n\\npred_classes, confidences = detector.run_prediction(batch_texts)\\n\\nprint(\"\\nBatch Results:\")\\nfor i, (text, cls, conf) in enumerate(zip(batch_texts, pred_classes, confidences), 1):\\n    print(f\"\\n{i}. Text: {text}\")\\n    print(f\"   Class: {cls}, Confidence: {conf:.4f}\")'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import Union, List, Tuple\n",
    "\n",
    "class BlameDetectorDa(object):\n",
    "    \"\"\"\n",
    "    Danish Blame Detection classifier with support for single and batch predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, max_length: int = 512, batch_size: int = None, base_model_name = \"jhu-clsp/mmBERT-base\", num_labels = 2):\n",
    "        \"\"\"\n",
    "        Initialize the BlameDetectorDa classifier.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the pretrained model\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "            batch_size: Batch size for batch predictions (None for single predictions)\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.base_model_name = base_model_name #delete when model is ready\n",
    "        self.num_labels = num_labels #delete when model is ready\n",
    "\n",
    "        self.model_initialization_prelim() #change when model is ready\n",
    "\n",
    "    def model_initialization(self):\n",
    "        \"\"\"Initialize the model, tokenizer, and device.\"\"\"\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "        \n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_path,\n",
    "            trust_remote_code=True  # Add this for ModernBERT\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)  # Fixed: was 'device', should be 'self.device'\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "    def model_initialization_prelim(self):\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "    \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
    "        \n",
    "        # Load base model with classification head\n",
    "        print(f\"Loading base model: {self.base_model_name}\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            num_labels=self.num_labels,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load LoRA adapters\n",
    "        print(\"Loading LoRA adapters...\")\n",
    "        self.model = PeftModel.from_pretrained(self.model, self.model_path)\n",
    "        \n",
    "        # Merge LoRA weights with base model for faster inference (optional)\n",
    "        print(\"Merging LoRA weights...\")\n",
    "        self.model = self.model.merge_and_unload()\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        \n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "        return\n",
    "        \n",
    "\n",
    "    def predict_single(self, text: str) -> Tuple[int, float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Make a prediction on a single text input.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to classify\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predicted_class, confidence, probabilities)\n",
    "        \"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def predict_batch(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Make predictions on a batch of text inputs.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts to classify\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predicted_classes, confidences, all_probabilities)\n",
    "        \"\"\"\n",
    "        # Tokenize all inputs\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "            confidences = probabilities.max(dim=1).values.cpu().numpy()\n",
    "            all_probs = probabilities.cpu().numpy()\n",
    "        \n",
    "        return predicted_classes, confidences, all_probs\n",
    "\n",
    "    def run_prediction(\n",
    "        self, \n",
    "        text: Union[str, List[str]]\n",
    "    ) -> Union[Tuple[int, float], Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Run prediction on single text or batch of texts.\n",
    "        \n",
    "        Args:\n",
    "            text: Single text string or list of text strings\n",
    "            \n",
    "        Returns:\n",
    "            For single text: (predicted_class, confidence)\n",
    "            For batch: (predicted_classes, confidences)\n",
    "        \"\"\"\n",
    "        # Check if input is a list (batch) or single text\n",
    "        if isinstance(text, list):\n",
    "            # Batch prediction\n",
    "            if self.batch_size is not None and len(text) > self.batch_size:\n",
    "                # Process in batches\n",
    "                all_classes = []\n",
    "                all_confidences = []\n",
    "                \n",
    "                for i in range(0, len(text), self.batch_size):\n",
    "                    batch = text[i:i + self.batch_size]\n",
    "                    classes, confidences, _ = self.predict_batch(batch)\n",
    "                    all_classes.extend(classes)\n",
    "                    all_confidences.extend(confidences)\n",
    "                \n",
    "                return np.array(all_classes), np.array(all_confidences)\n",
    "            else:\n",
    "                # Single batch prediction\n",
    "                predicted_classes, confidences, _ = self.predict_batch(text)\n",
    "                return predicted_classes, confidences\n",
    "        else:\n",
    "            # Single text prediction\n",
    "            predicted_class, confidence, _ = self.predict_single(text)\n",
    "            return predicted_class, confidence\n",
    "\n",
    "\n",
    "'''# Example usage\n",
    "\n",
    "# Initialize detector\n",
    "MODEL_PATH = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_merged\"\n",
    "detector = BlameDetectorDa(\n",
    "    model_path=MODEL_PATH,\n",
    "    max_length=512,\n",
    "    batch_size=8  # Process 8 texts at a time\n",
    ")\n",
    "\n",
    "print(\"Single Prediction Test\")\n",
    "\n",
    "\n",
    "# Single prediction\n",
    "single_text = \"Dette er en test sætning.\"\n",
    "pred_class, confidence = detector.run_prediction(single_text)\n",
    "print(f\"\\nText: {single_text}\")\n",
    "print(f\"Predicted Class: {pred_class}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "print(\"Batch Prediction Test\")\n",
    "\n",
    "# Batch prediction\n",
    "batch_texts = [\n",
    "    \"Første test sætning.\",\n",
    "    \"Anden test sætning.\",\n",
    "    \"Tredje test sætning.\",\n",
    "    \"Fjerde test sætning.\",\n",
    "]\n",
    "\n",
    "pred_classes, confidences = detector.run_prediction(batch_texts)\n",
    "\n",
    "print(\"\\nBatch Results:\")\n",
    "for i, (text, cls, conf) in enumerate(zip(batch_texts, pred_classes, confidences), 1):\n",
    "    print(f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Class: {cls}, Confidence: {conf:.4f}\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59bec3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model...\n",
      "Loading base model: jhu-clsp/mmBERT-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters...\n",
      "Merging LoRA weights...\n",
      "Model loaded successfully on cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm  # for progress bar (optional)\n",
    "\n",
    "MODEL_PATH = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/output/mmBERT/template_3_4_5_model\"\n",
    "detector = BlameDetectorDa(\n",
    "    model_path=MODEL_PATH,\n",
    "    max_length=1024,\n",
    "    batch_size=2  # Process 2 texts at a time\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5679d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Load JSON dataset ----\n",
    "import json\n",
    "json_path = \"/work/MarkusLundsfrydJensen#1865/Training_data/cleaned_training_data_3_4_5_temps.json\"\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "648c74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- 2. Extract texts ----\n",
    "texts = [item[\"text\"] for item in new_data if \"text\" in item]\n",
    "\n",
    "# ---- 3. Run predictions in batches ----\n",
    "pred_classes, confidences = detector.run_prediction(texts)\n",
    "\n",
    "\n",
    "# ---- 4. Attach predictions back to data ----\n",
    "for item, cls, conf in zip(new_data, pred_classes, confidences):\n",
    "    item[\"predicted_class\"] = int(cls)\n",
    "    item[\"confidence\"] = float(conf)\n",
    "\n",
    "\n",
    "#save data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e60c3aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.5366607308387756\n",
      "0\n",
      "0.7602487802505493\n"
     ]
    }
   ],
   "source": [
    "for entry in new_data:\n",
    "    if entry[\"label\"] == 1:\n",
    "        print(entry[\"predicted_class\"])\n",
    "        print(entry[\"confidence\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
