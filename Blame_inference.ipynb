{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71799c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import Union, List, Tuple\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from peft import PeftModel\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7aaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try again with full model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969c4714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-29 11:49:22.859912: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-29 11:49:22.926017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-29 11:49:24.297223: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load full precision model (deterministic)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"/work/MarkusLundsfrydJensen#1865/test_models_save/Option_2\",\n",
    "    device_map = 'auto'\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/work/MarkusLundsfrydJensen#1865/test_models_save/Option_2\")\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Same predict function as before\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction.item(),\n",
    "        'probability_class_0': probs[0][0].item(),\n",
    "        'probability_class_1': probs[0][1].item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e485a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7635658914728682\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Model_data/validation_set.json\"\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for entry in data:\n",
    "    text = entry[\"text\"]\n",
    "\n",
    "    result = predict(text)\n",
    "\n",
    "    entry[\"pred\"] = result[\"prediction\"]\n",
    "\n",
    "true_labels = 0\n",
    "pred_true = 0\n",
    "false_labels = 0\n",
    "pred_false = 0\n",
    "\n",
    "correct_pred = 0\n",
    "incorrect_pred = 0\n",
    "\n",
    "for entry in data:\n",
    "    if entry[\"label\"] == 1:\n",
    "        true_labels += 1\n",
    "    if entry[\"pred\"] == 1:\n",
    "        pred_true +=1\n",
    "\n",
    "    if entry[\"label\"] == 0:\n",
    "        false_labels += 1\n",
    "    if entry[\"pred\"] == 0:\n",
    "        pred_false +=1\n",
    "\n",
    "    if entry[\"label\"] == entry[\"pred\"]:\n",
    "        correct_pred +=1\n",
    "\n",
    "    if entry[\"label\"] != entry[\"pred\"]:\n",
    "        incorrect_pred +=1\n",
    "\n",
    "\n",
    "print(correct_pred/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa97f962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
