{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71799c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "from typing import Union, List, Tuple\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from peft import PeftModel\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae515f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS need to be made usable\n",
    "'''class BlameDetectorDa(object):\n",
    "\n",
    "    def __init__(self, model_path, max_length, batch_size = None):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model_initialization()\n",
    "\n",
    "        return\n",
    "\n",
    "    def model_initialization(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            self.text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def run_prediction(self, text):\n",
    "\n",
    "        self.text = text\n",
    "        predicted_class, confidence, probs = self.predict()\n",
    "            \n",
    "        return predicted_class, confidence\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''example = \"Goddad min kære ven, jeg kan virkelig godt lide dig.\"\n",
    "\n",
    "BD = BlameDetectorDa()\n",
    "BD.run_prediction(text = example)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6aa150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BlameDetectorDa(object):\n",
    "    \"\"\"\n",
    "    Danish Blame Detection classifier with support for single and batch predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, max_length: int = 512, batch_size: int = None ):\n",
    "        \"\"\"\n",
    "        Initialize the BlameDetectorDa classifier.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the pretrained model\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "            batch_size: Batch size for batch predictions (None for single predictions)\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        #, num_labels = 2, base_model_name = ''\n",
    "        #self.base_model_name = base_model_name\n",
    "        #self.num_labels = num_labels\n",
    "\n",
    "        self.model_initialization()\n",
    "\n",
    "    def model_initialization(self):\n",
    "        \"\"\"Initialize the model, tokenizer, and device.\"\"\"\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "        \n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_path,\n",
    "            trust_remote_code=True  # Add this for ModernBERT\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "\n",
    "        \n",
    "            \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)  # Fixed: was 'device', should be 'self.device'\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "    def model_initialization_prelim(self):\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "    \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
    "        \n",
    "        # Load base model with classification head\n",
    "        print(f\"Loading base model: {self.base_model_name}\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.base_model_name,\n",
    "            num_labels=self.num_labels,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load LoRA adapters\n",
    "        print(\"Loading LoRA adapters...\")\n",
    "        self.model = PeftModel.from_pretrained(self.model, self.model_path)\n",
    "        \n",
    "        # Merge LoRA weights with base model for faster inference (optional)\n",
    "        print(\"Merging LoRA weights...\")\n",
    "        self.model = self.model.merge_and_unload()\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "        return\n",
    "        \n",
    "\n",
    "    def predict_single(self, text: str) -> Tuple[int, float, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Make a prediction on a single text input.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to classify\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predicted_class, confidence, probabilities)\n",
    "        \"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        #with torch.inference_mode():  # Faster than no_grad()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def predict_batch(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Make predictions on a batch of text inputs.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of input texts to classify\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (predicted_classes, confidences, all_probabilities)\n",
    "        \"\"\"\n",
    "        # Tokenize all inputs\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "            confidences = probabilities.max(dim=1).values.cpu().numpy()\n",
    "            all_probs = probabilities.cpu().numpy()\n",
    "        \n",
    "        return predicted_classes, confidences, all_probs\n",
    "\n",
    "    def run_prediction(\n",
    "        self, \n",
    "        text: Union[str, List[str]]\n",
    "    ) -> Union[Tuple[int, float], Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Run prediction on single text or batch of texts.\n",
    "        \n",
    "        Args:\n",
    "            text: Single text string or list of text strings\n",
    "            \n",
    "        Returns:\n",
    "            For single text: (predicted_class, confidence)\n",
    "            For batch: (predicted_classes, confidences)\n",
    "        \"\"\"\n",
    "        # Check if input is a list (batch) or single text\n",
    "        if isinstance(text, list):\n",
    "            # Batch prediction\n",
    "            if self.batch_size is not None and len(text) > self.batch_size:\n",
    "                # Process in batches\n",
    "                all_classes = []\n",
    "                all_confidences = []\n",
    "                \n",
    "                for i in range(0, len(text), self.batch_size):\n",
    "                    batch = text[i:i + self.batch_size]\n",
    "                    classes, confidences, _ = self.predict_batch(batch)\n",
    "                    all_classes.extend(classes)\n",
    "                    all_confidences.extend(confidences)\n",
    "                \n",
    "                return np.array(all_classes), np.array(all_confidences)\n",
    "            else:\n",
    "                # Single batch prediction\n",
    "                predicted_classes, confidences, _ = self.predict_batch(text)\n",
    "                return predicted_classes, confidences\n",
    "        else:\n",
    "            # Single text prediction\n",
    "            predicted_class, confidence, _ = self.predict_single(text)\n",
    "            return predicted_class, confidence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad62f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5679d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1. Load JSON dataset ----\n",
    "import json\n",
    "json_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Model_data/validation_set.json\"\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77d8b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from runetrust/mmBERT-blame-da-test...\n",
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "detector = BlameDetectorDa(\n",
    "        model_path=\"runetrust/mmBERT-blame-da-test\",\n",
    "        max_length=512,\n",
    "        batch_size=224\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0a65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /work/MarkusLundsfrydJensen#1865/Bachelor_project/full_tune_results/checkpoint-3032...\n",
      "Loading base model: jhu-clsp/mmBERT-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters...\n",
      "Merging LoRA weights...\n",
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "detector = BlameDetectorDa(\n",
    "        model_path=\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/full_tune_results/checkpoint-3032\",\n",
    "        max_length=512,\n",
    "        batch_size=128,\n",
    "        num_labels = 2,\n",
    "        base_model_name = \"jhu-clsp/mmBERT-base\"\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "648c74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- 2. Extract texts ----\n",
    "texts = [item[\"text\"] for item in data if \"text\" in item]\n",
    "\n",
    "# ---- 3. Run predictions in batches ----\n",
    "pred_classes, confidences = detector.run_prediction(texts)\n",
    "\n",
    "\n",
    "# ---- 4. Attach predictions back to data ----\n",
    "for item, cls, conf in zip(data, pred_classes, confidences):\n",
    "    item[\"predicted_class\"] = int(cls)\n",
    "    item[\"confidence\"] = float(conf)\n",
    "\n",
    "\n",
    "#save data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e60c3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = 0\n",
    "pred_true = 0\n",
    "false_labels = 0\n",
    "pred_false = 0\n",
    "\n",
    "correct_pred = 0\n",
    "incorrect_pred = 0\n",
    "\n",
    "for entry in data:\n",
    "    if entry[\"label\"] == 1:\n",
    "        true_labels += 1\n",
    "    if entry[\"predicted_class\"] == 1:\n",
    "        pred_true +=1\n",
    "\n",
    "    if entry[\"label\"] == 0:\n",
    "        false_labels += 1\n",
    "    if entry[\"predicted_class\"] == 0:\n",
    "        pred_false +=1\n",
    "\n",
    "    if entry[\"label\"] == entry[\"predicted_class\"]:\n",
    "        correct_pred +=1\n",
    "\n",
    "    if entry[\"label\"] != entry[\"predicted_class\"]:\n",
    "        incorrect_pred +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55e5060a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4844961240310077"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred/len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bincrossentropy(true, pred, weight_zero = 99.0, weight_one = 1):\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed to represent class imbalance in the dataset.\n",
    "        \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalized 10 times as much as false negatives.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    #weights /= (weight_one + weight_zero) # Normalizing to be more consistent with regular BCE for comparison \n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return np.mean(weighted_bin_crossentropy)\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #From logits to probabilities\n",
    "    probs_2d = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    probs = probs_2d[:, 1]  # positive class extraction\n",
    "    \n",
    "    weigthted_bce = weighted_bincrossentropy(labels, probs)\n",
    "    keras_bce = binary_crossentropy(labels, probs)\n",
    "    keras_bce = float(np.mean(keras_bce.numpy()))  # Converting from keras eagertensor to float value\n",
    "    \n",
    "    # Wrapping all metrics to floats for json serialization during model eval\n",
    "    return {\n",
    "        'keras_BCE': keras_bce,\n",
    "        'weighted BCE': weigthted_bce,\n",
    "        'recall': float(recall_score(labels, probs.round())),\n",
    "        'precision': float(average_precision_score(labels, probs)),\n",
    "        'accuracy': float(accuracy_score(labels, probs.round())), # Need rounding for these two computations (integer required)\n",
    "        'f1': float(f1_score(labels, probs.round(), average='macro')) # macro f1 is better for imbalanced dataset\n",
    "    }\n",
    "\n",
    "# Custom trainer class (weigthed)\n",
    "from collections import Counter\n",
    "\n",
    "labels = test_dataframe['label'].tolist()\n",
    "class_counts = Counter(labels)\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Higher weight = more emphasis\n",
    "weights = [total/class_counts[0], total/class_counts[1]]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "#define custom trainer that uses weigted loss\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Define weighted loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "'''\n",
    "Look into learning rates, model is currently overfitting quite drastically (\"small\" test-set)\n",
    "Normalizing weigthed BCE or no?\n",
    "Look into regularization, dropout and early stopping to avoid overfitting\n",
    "'''\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "with open(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Evalresult345templates.txt\", \"w\") as f:\n",
    "    f.write(str(eval_results))\n",
    "\n",
    "# This is where we should very much remember to save the finetuned model locally as this contains the new weights for use in analyzing new text\n",
    "lora_model.save_pretrained(f\"output/mmBERT/template_3_4_5_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2d281d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #From logits to probabilities\n",
    "    probs_2d = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    probs = probs_2d[:, 1]  # positive class extraction\n",
    "    \n",
    "    weigthted_bce = weighted_bincrossentropy(labels, probs)\n",
    "    keras_bce = binary_crossentropy(labels, probs)\n",
    "    keras_bce = float(np.mean(keras_bce.numpy()))  # Converting from keras eagertensor to float value\n",
    "    \n",
    "    # Wrapping all metrics to floats for json serialization during model eval\n",
    "    return {\n",
    "        'keras_BCE': keras_bce,\n",
    "        'weighted BCE': weigthted_bce,\n",
    "        'recall': float(recall_score(labels, probs.round())),\n",
    "        'precision': float(average_precision_score(labels, probs)),\n",
    "        'accuracy': float(accuracy_score(labels, probs.round())), # Need rounding for these two computations (integer required)\n",
    "        'f1': float(f1_score(labels, probs.round(), average='macro')) # macro f1 is better for imbalanced dataset\n",
    "    }\n",
    "\n",
    "def weighted_bincrossentropy(true, pred, weight_zero = 99.0, weight_one = 1):\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed to represent class imbalance in the dataset.\n",
    "        \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalized 10 times as much as false negatives.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    #weights /= (weight_one + weight_zero) # Normalizing to be more consistent with regular BCE for comparison \n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return np.mean(weighted_bin_crossentropy)\n",
    "\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94be7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 258/258 [00:00<00:00, 3970.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "val_dataframe = pd.read_json(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Model_data/validation_set.json\")\n",
    "val_dataframe = val_dataframe[['text', 'label']]\n",
    "val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=512, # Padding to 512 to massively cut down on computation compared to base 8,192 tokens. \n",
    "    )\n",
    "tokenized_val = val_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cc09604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49224806201550386"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for entry in data:\n",
    "    if entry[\"label\"] == entry[\"predicted_class\"]:\n",
    "        correct+=1\n",
    "\n",
    "\n",
    "correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#former was .484\n",
    "#now 4922"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cc1825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual recomputed validation metrics: {'keras_BCE': 0.8103038668632507, 'weighted BCE': np.float32(53.134575), 'recall': 0.8181818181818182, 'precision': 0.40772366052391074, 'accuracy': 0.4844961240310077, 'f1': 0.48168512000966723}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, recall_score\n",
    "\n",
    "# Load model\n",
    "model_name = \"runetrust/mmBERT-blame-da-test\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    tokenized_val.remove_columns([\"text\"]),  # remove raw text if you don't need it\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "# Collect predictions\n",
    "all_logits = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        labels = batch[\"labels\"].cpu().numpy()  # HF Dataset usually uses 'labels' or 'label'\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}  # move to GPU/CPU\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "\n",
    "        all_logits.append(logits)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Compute metrics with your function\n",
    "results = compute_metrics((all_logits, all_labels))\n",
    "print(\"Manual recomputed validation metrics:\", results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
