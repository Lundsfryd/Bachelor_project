{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71799c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#load packages\n",
    "import torch\n",
<<<<<<< HEAD
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next step -> implement batching"
=======
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import json\n"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< HEAD
   "id": "5c7aaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS need to be made usable\n",
    "class BlameDetectorDa(object):\n",
    "\n",
    "    def __init__(self, model_path, max_length = 512):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        #self.batch_size = batch_size\n",
=======
   "id": "a2a3eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class BlameDetectorDa(object):\n",
    "\n",
    "    def __init__(self, model_path, max_length=512, batch_size=32):\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "\n",
    "        self.model_initialization()\n",
    "\n",
    "        return\n",
    "\n",
    "    def model_initialization(self):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_path,\n",
<<<<<<< HEAD
    "            device_map = 'auto'\n",
=======
    "            device_map='auto'\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        \n",
<<<<<<< HEAD
    "            \n",
=======
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "        return\n",
    "\n",
<<<<<<< HEAD
    "    def predict(self):\n",
    "        \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            self.text,\n",
=======
    "    def predict(self, text):\n",
    "        \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "       \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
<<<<<<< HEAD
=======
    "        \n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
<<<<<<< HEAD
    "    def run_prediction(self, text):\n",
    "\n",
    "        self.text = text\n",
    "        predicted_class, confidence, probs = self.predict()\n",
    "            \n",
    "        return predicted_class, confidence\n",
    "\n"
=======
    "    def predict_batch(self, texts):\n",
    "        \"\"\"Make predictions on a batch of texts.\"\"\"\n",
    "        # Tokenize all texts in the batch\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_classes = torch.argmax(probabilities, dim=1).cpu().numpy()\n",
    "            confidences = probabilities[range(len(predicted_classes)), predicted_classes].cpu().numpy()\n",
    "        \n",
    "        return predicted_classes, confidences, probabilities.cpu().numpy()\n",
    "\n",
    "    def run_prediction(self, text):\n",
    "        \"\"\"Single text prediction (backward compatibility).\"\"\"\n",
    "        predicted_class, confidence, probs = self.predict(text)\n",
    "        return predicted_class, confidence\n",
    "\n",
    "    def run_batch_prediction(self, texts, show_progress=True):\n",
    "        \"\"\"\n",
    "        Run predictions on a list of texts with batching and progress bar.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to predict on\n",
    "            show_progress: Whether to show progress bar (default: True)\n",
    "            \n",
    "        Returns:\n",
    "            predicted_classes: numpy array of predicted class indices\n",
    "            confidences: numpy array of confidence scores\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        \n",
    "        # Create batches\n",
    "        num_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        # Setup progress bar\n",
    "        if show_progress:\n",
    "            pbar = tqdm(total=len(texts), desc=\"Processing texts\", unit=\"text\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            \n",
    "            # Get predictions for this batch\n",
    "            predicted_classes, confidences, _ = self.predict_batch(batch_texts)\n",
    "            \n",
    "            all_predictions.extend(predicted_classes)\n",
    "            all_confidences.extend(confidences)\n",
    "            \n",
    "            # Update progress bar\n",
    "            if show_progress:\n",
    "                pbar.update(len(batch_texts))\n",
    "        \n",
    "        if show_progress:\n",
    "            pbar.close()\n",
    "        \n",
    "        return np.array(all_predictions), np.array(all_confidences)\n",
    "\n",
    "\n",
    "    def predict_from_json(self, json_path, text_key = 'text', output_key='prediction', \n",
    "                          confidence_key='confidence', show_progress=True):\n",
    "        \"\"\"\n",
    "        Load JSON, predict, and return results with predictions added.\n",
    "        \n",
    "        Args:\n",
    "            json_path: Path to JSON file\n",
    "            text_key: Key in JSON objects containing the text to classify\n",
    "            output_key: Key name for storing predictions (default: 'prediction')\n",
    "            confidence_key: Key name for storing confidence scores (default: 'confidence')\n",
    "            show_progress: Whether to show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with predictions added\n",
    "        \"\"\"\n",
    "        import json\n",
    "        \n",
    "        # Load JSON data\n",
    "        print(f\"Loading data from {json_path}...\")\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        print('json file loaded')\n",
    "        \n",
    "        # Handle different JSON structures\n",
    "        if isinstance(data, list):\n",
    "            items = data\n",
    "        elif isinstance(data, dict):\n",
    "            # If it's a dict, try to find the list of items\n",
    "            # Adjust this based on your JSON structure\n",
    "            items = list(data.values()) if all(isinstance(v, dict) for v in data.values()) else [data]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported JSON structure\")\n",
    "        \n",
    "        print(f\"Found {len(items)} items. extracting texts...\")\n",
    "        \n",
    "        # Extract texts\n",
    "        texts = [item[text_key] for item in items]\n",
    "\n",
    "        print('start batch prediction')\n",
    "        \n",
    "        # Run predictions\n",
    "        predictions, confidences = self.run_batch_prediction(texts, show_progress=show_progress)\n",
    "        \n",
    "        # Add predictions to original data\n",
    "        for item, pred, conf in zip(items, predictions, confidences):\n",
    "            item[output_key] = int(pred)\n",
    "            item[confidence_key] = float(conf)\n",
    "        \n",
    "        return items\n",
    "\n",
    "    def predict_from_json_to_file(self, input_path, output_path, text_key, \n",
    "                                   output_key='prediction', confidence_key='confidence',\n",
    "                                   show_progress=True):\n",
    "        \"\"\"\n",
    "        Load JSON, predict, and save results to a new file.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input JSON file\n",
    "            output_path: Path to save output JSON file\n",
    "            text_key: Key containing text to classify\n",
    "            output_key: Key name for predictions\n",
    "            confidence_key: Key name for confidence scores\n",
    "            show_progress: Whether to show progress bar\n",
    "        \"\"\"\n",
    "        import json\n",
    "        \n",
    "        # Get predictions\n",
    "        results = self.predict_from_json(\n",
    "            input_path, text_key, output_key, confidence_key, show_progress\n",
    "        )\n",
    "        \n",
    "        # Save to file\n",
    "        print(f\"Saving results to {output_path}...\")\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Done! Processed {len(results)} items\")\n",
    "        \n",
    "        return results"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "a1295b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 14:23:22.938561: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-29 14:23:29.108378: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-29 14:24:02.047962: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
=======
   "execution_count": 5,
   "id": "9d0eec23",
   "metadata": {},
   "outputs": [
    {
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "BD = BlameDetectorDa(model_path = \"/work/MarkusLundsfrydJensen#1865/test_models_save/Option_2\")"
=======
    "detector = BlameDetectorDa(\n",
    "    model_path=\"Lundsfryd/Pol_Blame_Detection_Da\",\n",
    "    max_length=512,\n",
    "    batch_size=64\n",
    ")"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "id": "e485a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
=======
   "execution_count": null,
   "id": "8997798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /work/MarkusLundsfrydJensen#1865/final_inference_data.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts:   1%|          | 19328/2191051 [01:03<1:06:37, 543.30text/s]"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0.7635658914728682\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Model_data/validation_set.json\"\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for entry in data:\n",
    "    text = entry[\"text\"]\n",
    "\n",
    "    entry[\"pred\"], _ = BD.run_prediction(text)\n",
    "\n",
    "    \n",
    "\n",
    "true_labels = 0\n",
    "pred_true = 0\n",
    "false_labels = 0\n",
    "pred_false = 0\n",
    "\n",
    "correct_pred = 0\n",
    "incorrect_pred = 0\n",
    "\n",
    "for entry in data:\n",
    "    if entry[\"label\"] == 1:\n",
    "        true_labels += 1\n",
    "    if entry[\"pred\"] == 1:\n",
    "        pred_true +=1\n",
    "\n",
    "    if entry[\"label\"] == 0:\n",
    "        false_labels += 1\n",
    "    if entry[\"pred\"] == 0:\n",
    "        pred_false +=1\n",
    "\n",
    "    if entry[\"label\"] == entry[\"pred\"]:\n",
    "        correct_pred +=1\n",
    "\n",
    "    if entry[\"label\"] != entry[\"pred\"]:\n",
    "        incorrect_pred +=1\n",
    "\n",
    "\n",
    "print(correct_pred/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa97f962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_true"
=======
      "json file loaded\n",
      "Found 2191051 items. extracting texts...\n",
      "start batch prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# Method 2: Process and save to file directly\n",
    "detector.predict_from_json_to_file(\n",
    "    input_path=\"/work/MarkusLundsfrydJensen#1865/final_inference_data.json\",\n",
    "    output_path=\"/work/MarkusLundsfrydJensen#1865/final_final_inference_data.json\",\n",
    "    text_key=\"text\"\n",
    ")\n"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
