{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654eaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import ast\n",
    "import swifter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "import pandas as pd\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de61dfe",
   "metadata": {},
   "source": [
    "This script extracts data from the Burnham DEBATE runs and processes this in different ways for further analysis and use in Label Studio as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92537f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes:\n",
    "#Blame and praise\n",
    "# entailsments\n",
    "#does not entail an actually entailment\n",
    "\n",
    "#Blame vs endorsement\n",
    "#Also report initial inspection of model wher praise and neutral was added to the hypothesis paramenter instead of blame/not blame.\n",
    "# something about hieracivcal order and the words not being complete opisats and therefore the relative probabilities entails needed information\n",
    "# in addition to the absolute probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "\n",
    "\n",
    "def extract_blame_from_paragraph_lookup(input_str):\n",
    "    \"\"\"\n",
    "    Returns a binary list for blame per sentence:\n",
    "    1 if blame is highest among labels and >= 0.8, else 0\n",
    "    Handles arbitrary label order.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        sentence_list = ast.literal_eval(input_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    # List comprehension is faster than appending in a loop\n",
    "    blame_binary = [\n",
    "        int(\n",
    "            (label_score := {label: score for label, score in zip(sent['labels'], sent['scores'])})['blame']\n",
    "            >= max(label_score.get('praise', 0.0), label_score.get('neutral', 0.0), 0.8)\n",
    "        )\n",
    "        for sent in sentence_list\n",
    "    ]\n",
    "\n",
    "\n",
    "    return blame_binary\n",
    "\n",
    "#example usage: final_data['blame_binary'] = final_data['blame_in_text'].swifter.apply(extract_blame_from_paragraph_lookup)\n",
    "\n",
    "\n",
    "#get row indices (paragraphs) that contain blame\n",
    "def get_rows_with_blame(df, col=\"blame_binary\"):\n",
    "    \"\"\"\n",
    "    Returns row indices where the list in `col` contains at least one 1.\n",
    "    Handles both real lists and stringified lists.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i, values in zip(df.index, df[col]):\n",
    "        if isinstance(values, str):  # convert only if it's a string\n",
    "            values = ast.literal_eval(values)\n",
    "        if 1 in values:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "#example usage: row_indices = get_rows_with_blame(final_data, col=\"blame_binary\")\n",
    "#print(row_indices[:10])\n",
    "\n",
    "\n",
    "#get paragraphs and sentence indices of blame True\n",
    "def get_rows_and_positions(df, col=\"blame_binary\"):\n",
    "    \"\"\"\n",
    "    Returns {row_index: [positions_of_1s]}.\n",
    "    Handles both real lists and stringified lists.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for i, values in zip(df.index, df[col]):\n",
    "        if isinstance(values, str):\n",
    "            values = ast.literal_eval(values)\n",
    "        ones = [j for j, v in enumerate(values) if v == 1]\n",
    "        if ones:\n",
    "            results[i] = ones\n",
    "    return results\n",
    "\n",
    "#example usage: rows_with_positions = get_rows_and_positions(final_data, col=\"blame_binary\")\n",
    "#print(rows_with_positions)\n",
    "\n",
    "#\n",
    "#get danish sentences containing blame from indices extracted as above\n",
    "\n",
    "def danish_sentences_with_blame_extraction(dict, data, text_column):\n",
    "\n",
    "\n",
    "    rows = list(dict.keys())\n",
    "    sentences = {}\n",
    "\n",
    "    for para in rows:\n",
    "        sentence_indices = dict[para]\n",
    "        text_sentences = ast.literal_eval(data.loc[para][text_column])\n",
    "        \n",
    "        blame_sentence_dict = {}\n",
    "        for indx in sentence_indices:\n",
    "            blame_sentence = text_sentences[indx]\n",
    "            blame_sentence_dict[indx] = blame_sentence\n",
    "        \n",
    "        \n",
    "        sentences[para] = blame_sentence_dict\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "#example usage: danish_sentences_with_blame = danish_sentences_with_blame_extraction(rows_with_positions, final_data, 'da_segmented_text')\n",
    "\n",
    "\n",
    "#extract blame percentage:\n",
    "\n",
    "import ast\n",
    "\n",
    "def total_blame_percentage(string_rows):\n",
    "    \"\"\"\n",
    "    Calculate the total percentage of blame-sentences across all rows,\n",
    "    converting string representations of lists into actual lists.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string_rows : list of str\n",
    "        Each element is a string like '[1, 0, 1]' representing a row of blame labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total percentage of blame-sentences (0–100).\n",
    "    \"\"\"\n",
    "    total_sentences = 0\n",
    "    total_blame = 0\n",
    "\n",
    "    for row_str in string_rows:\n",
    "        try:\n",
    "            row = row_str  # convert string to list\n",
    "            #row = ast.literal_eval(row_str)\n",
    "            if not isinstance(row, list):\n",
    "                continue  # skip if not a list\n",
    "            row = [int(val) for val in row]  # ensure integers\n",
    "            total_sentences += len(row)\n",
    "            total_blame += sum(row)\n",
    "        except (ValueError, SyntaxError):\n",
    "            continue  # skip invalid rows\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (total_blame / total_sentences) * 100, total_sentences, total_blame\n",
    "\n",
    "#Example usage: percentage_blame, total_sent, total_blame= total_blame_percentage(final_data['blame_binary'])\n",
    "\n",
    "#extract blame scores\n",
    "\n",
    "def get_blame_scores(data, blame_in_text_column = 'blame_in_text'):\n",
    "    all_blame_scores = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        dict_labels = ast.literal_eval(data.loc[i][blame_in_text_column])\n",
    "\n",
    "        blame_list = [(label_score := {label: score for label, score in zip(sent['labels'], sent['scores'])})['blame'] for sent in dict_labels]\n",
    "\n",
    "        all_blame_scores +=blame_list\n",
    "    return all_blame_scores\n",
    "\n",
    "\n",
    "#make vizualization of the distribution\n",
    "\n",
    "def vizualize_blame_prob(blame_scores):\n",
    "    len_blame_scores = len(blame_scores)\n",
    "    fig, axs = plt.subplots(len_blame_scores, 2, figsize=(12, 10))\n",
    "    for i in range(len_blame_scores):\n",
    "        axs[i,0].hist(blame_scores[i], log = True, bins = 50)\n",
    "        axs[0, 0].set_title('Log transformed y axis')\n",
    "        axs[i,1].hist(blame_scores[i], bins = 50)\n",
    "        axs[0, 1].set_title('absolute y-axis')\n",
    "\n",
    "    fig.suptitle('Distribution of blame probabilities by template (log transformed counts/absolute counts)')\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='probability', ylabel='count')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "#change format from list of paragraph, sentence pairs to dictioranry compatible with other functions\n",
    "\n",
    "def list_to_dict(pairs):\n",
    "    result = {}\n",
    "    for key, value in pairs:\n",
    "        if key in result:\n",
    "            result[key].append(value)\n",
    "        else:\n",
    "            result[key] = [value]\n",
    "    return result\n",
    "\n",
    "# Example usage: converted = list_to_dict(data)\n",
    "\n",
    "\n",
    "\n",
    "def danish_sentences_without_blame_extraction(blame_dict, data, text_column):\n",
    "    \"\"\"\n",
    "    Extract sentences that do NOT contain blame, using the dictionary\n",
    "    returned by get_rows_and_positions.\n",
    "    \n",
    "    blame_dict: {row_index: [positions_of_1s]}\n",
    "    data: DataFrame with a column containing lists of sentences (as lists or stringified lists)\n",
    "    text_column: name of the column in `data` containing the text sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = {}\n",
    "    \n",
    "    for para in data.index:\n",
    "        text_sentences = ast.literal_eval(data.loc[para][text_column])\n",
    "        blame_positions = blame_dict.get(para, [])\n",
    "        \n",
    "        non_blame_sentence_dict = {}\n",
    "        for idx, sentence in enumerate(text_sentences):\n",
    "            if idx not in blame_positions:  # only keep sentences not containing blame\n",
    "                non_blame_sentence_dict[idx] = sentence\n",
    "        \n",
    "        if non_blame_sentence_dict:  # only add if there are non-blame sentences\n",
    "            sentences[para] = non_blame_sentence_dict\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------#\n",
    "#json related functions\n",
    "\n",
    "\n",
    "# Convert dictionary to a JSON string and write to file\n",
    "def convert_to_json_and_write(file_name, sentences):\n",
    "    with open(f'/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/{file_name}.json', 'w') as file:\n",
    "        file.write(json.dumps(sentences, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "    #preprocess data for label studio\n",
    "    # Load your data\n",
    "    with open(f'/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/{file_name}.json', \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    flattened = []\n",
    "\n",
    "    for paragraph, sentences in data.items():\n",
    "        for sentence_nr, text in sentences.items():\n",
    "            flattened.append({\n",
    "                \"paragraph\": paragraph,\n",
    "                \"sentence_nr\": sentence_nr,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "    # Save in a format Label Studio can import\n",
    "    with open(f'/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/{file_name}.json', \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(flattened, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "#final data is csv file as output of PolDebate model\n",
    "\n",
    "def json_append_meta_data(file_name, data):\n",
    "    meta_data = data[['Unnamed: 0','speaker','party']]\n",
    "    meta_data = meta_data.replace({np.nan: None})\n",
    "    meta_data.head()\n",
    "\n",
    "\n",
    "    #connect label studio data with meta data\n",
    "\n",
    "\n",
    "    # Load the flattened sentence JSON\n",
    "    with open(f\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/{file_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = json.load(f)\n",
    "\n",
    "    # Load metadata\n",
    "    meta = meta_data\n",
    "\n",
    "    # Convert metadata to dict for fast lookup\n",
    "    meta_dict = meta.set_index(\"Unnamed: 0\").to_dict(orient=\"index\")\n",
    "\n",
    "    # Merge\n",
    "    for item in sentences:\n",
    "        paragraph = int(item[\"paragraph\"])\n",
    "        if paragraph in meta_dict:\n",
    "\n",
    "            item.update(meta_dict[paragraph])\n",
    "\n",
    "    # Save merged dataset\n",
    "    with open(f'/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/{file_name}.json', \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sentences, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#find parties in government by dataset\n",
    "def find_government(gov_data, date):\n",
    "    match = gov_data[(gov_data[\"Start Date\"] <= date) & (gov_data[\"End Date\"] >= date)]\n",
    "    if not match.empty:\n",
    "\n",
    "        parties = match['Party Letter']\n",
    "        parties = ast.literal_eval(parties.iloc[0])\n",
    "        \n",
    "        return parties\n",
    "    else:\n",
    "        print('empty')\n",
    "        return None\n",
    "\n",
    "\n",
    "#append context and government related data to the json file\n",
    "def json_government_and_context(file_name, data, government_data):\n",
    "\n",
    "    # Load the flattened sentence JSON\n",
    "    with open(f\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/{file_name}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    for entry in json_data:\n",
    "        paragraph_nr = int(entry['paragraph'])\n",
    "        sentence_nr = int(entry['sentence_nr'])\n",
    "\n",
    "        #initialize information on current paragraph\n",
    "        temp_data = data.loc[paragraph_nr]\n",
    "\n",
    "        #get preceding sentence\n",
    "        temp_data_da_text = ast.literal_eval(temp_data['da_segmented_text'])\n",
    "        \n",
    "        pr_sentence_index = sentence_nr-1\n",
    "\n",
    "        if pr_sentence_index < 0:\n",
    "            #do empty string\n",
    "            pr_sent = ''\n",
    "\n",
    "        else:\n",
    "            pr_sent = f'{temp_data_da_text[pr_sentence_index]}'\n",
    "\n",
    "        # find succesding sentence\n",
    "\n",
    "        suc_sent_index = sentence_nr + 1\n",
    "\n",
    "        try:\n",
    "            suc_sent = f'{temp_data_da_text[suc_sent_index]}'\n",
    "\n",
    "        except:\n",
    "            suc_sent = ''\n",
    "\n",
    "        \n",
    "        #Find out if speaker is member of party in Government\n",
    "        party = entry['party']\n",
    "        date = temp_data['date']\n",
    "        parties_gov = find_government(government_data, date)\n",
    "        if party in parties_gov:\n",
    "            in_gov = True\n",
    "        else:\n",
    "            in_gov = False\n",
    "        \n",
    "        #make into dict\n",
    "\n",
    "        context_dict = {'preceding_sentence': pr_sent, \n",
    "                        'succeeding_sent': suc_sent,\n",
    "                        'current_speaker_in_government': in_gov,\n",
    "                        'parties_in_government': parties_gov,\n",
    "                        'date': str(date)\n",
    "                        }\n",
    "\n",
    "        #update\n",
    "        entry.update(context_dict)\n",
    "\n",
    "    #save data\n",
    "    with open(f\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/{file_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def merge_json_files(json_files, output_file_path):\n",
    "    merged_data = []\n",
    "\n",
    "    for file in json_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            merged_data.extend(data)  # add the list from this file to the merged_data\n",
    "\n",
    "    # Save the merged list to a new JSON file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with all templates\n",
    "\n",
    "\n",
    "all_templates_data = pd.read_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/annotation_data_fifth_template_appended.csv\")\n",
    "\n",
    "all_templates_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5634ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "blame_columns = [\"blame_in_text\",\"second_template_blame_in_text\",\"third_template_blame_in_text\",\"fourth_template_blame_in_text\",\"fifth_template_blame_in_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e265a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks:\n",
    "#check if translated, orignal and blame probabilty are same lengt (extend for all templates)\n",
    "\n",
    "for column in blame_columns:\n",
    "    for indx in range(len(all_templates_data)):\n",
    "\n",
    "        temo = all_templates_data.loc[indx]\n",
    "\n",
    "        l_da = len(ast.literal_eval(temo['da_segmented_text']))\n",
    "        l_en = len(ast.literal_eval(temo['translated_text']))\n",
    "        l_bl = len(ast.literal_eval(temo[column]))\n",
    "\n",
    "        if l_da != l_en != l_bl:\n",
    "            print(column)\n",
    "            print(temo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f47e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "# apply extract blame from paragraph lookup function in order to evaulate blame from PolDebate probabilities\n",
    "\n",
    "for i, column in enumerate(blame_columns):\n",
    "    all_templates_data[f'blame_binary_temp_{i+1}'] = all_templates_data[column].swifter.apply(extract_blame_from_paragraph_lookup)\n",
    "\n",
    "all_templates_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do vizualization of blame probability distribution for all templates\n",
    "#alter function to make it subplots\n",
    "\n",
    "all_blame_scores = []\n",
    "\n",
    "for column in blame_columns:\n",
    "    all_blame_scores.append(get_blame_scores(all_templates_data, blame_in_text_column = column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3700df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vizualize_blame_prob(all_blame_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get percentage of blame for each template\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "blame_percentage = []\n",
    "abs_blame = []\n",
    "template_name = []\n",
    "for i in range(1,6):\n",
    "    temp_name = f'blame_binary_temp_{i}'\n",
    "    percentage_blame, total_sent, total_blame= total_blame_percentage(all_templates_data[temp_name])\n",
    "\n",
    "    blame_percentage.append(percentage_blame)\n",
    "    abs_blame.append(total_blame)\n",
    "    template_name.append(temp_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.bar(template_name, blame_percentage)\n",
    "plt.title('Percentage of sentences as blame')\n",
    "\n",
    "\n",
    "plt.suptitle('Bar plot of blame percentage by template')\n",
    "\n",
    "#for ax in axs.flat:\n",
    "#    ax.set(xlabel='probability', ylabel='count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfec571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data and do vizualization of the overlapping blame\n",
    "# How much of the total blame by all templates do 4/3/2/1 templates agree upon\n",
    "# Extract this information on a sentence level df['agreement_degree] maybe in format paragraph_i = [0,0,4,3,2,0,0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blame_indices_template_1 = get_rows_and_positions(all_templates_data, col='blame_binary_temp_1')\n",
    "blame_indices_template_2 = get_rows_and_positions(all_templates_data, col='blame_binary_temp_2')\n",
    "blame_indices_template_3 = get_rows_and_positions(all_templates_data, col='blame_binary_temp_3')\n",
    "blame_indices_template_4 = get_rows_and_positions(all_templates_data, col='blame_binary_temp_4')\n",
    "blame_indices_template_5 = get_rows_and_positions(all_templates_data, col='blame_binary_temp_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of dictioranries holding the paragraph/sentence indices for blame detected\n",
    "dicts = [blame_indices_template_1, blame_indices_template_2, blame_indices_template_3, blame_indices_template_4, blame_indices_template_5]\n",
    "\n",
    "\n",
    "# Step 1: make an empty dictionary to count appearances\n",
    "# structure: {(paragraph, sentence): count}\n",
    "counts = {}\n",
    "\n",
    "# Step 2: loop through all dictionaries one by one\n",
    "for d in dicts:\n",
    "    # For each paragraph in the dictionary\n",
    "    for paragraph, sentences in d.items():\n",
    "        # For each sentence number in the paragraph\n",
    "        for sentence in sentences:\n",
    "            key = (paragraph, sentence)\n",
    "            # If we’ve seen it before, increase count by 1\n",
    "            if key in counts:\n",
    "                counts[key] += 1\n",
    "            # If not, add it and set count to 1\n",
    "            else:\n",
    "                counts[key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Count how many pairs appear in 1–5 dictionaries\n",
    "summary = {i: 0 for i in range(1, 6)}\n",
    "for count in counts.values():\n",
    "    summary[count] += 1\n",
    "\n",
    "print(\"Pairs appearing in N dictionaries:\")\n",
    "for n, total in summary.items():\n",
    "    print(f\"{n}: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a51ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Gather which pairs appear in each count\n",
    "pairs_by_count = {i: [] for i in range(1, 6)}\n",
    "for pair, count in counts.items():\n",
    "    pairs_by_count[count].append(pair)\n",
    "\n",
    "# Example: show all pairs that appear in exactly 3 dictionaries\n",
    "print(\"\\nPairs appearing in exactly 5 dictionaries:\")\n",
    "print(pairs_by_count[5])\n",
    "\n",
    "# Step 5: Total number of unique (paragraph, sentence) pairs\n",
    "n_unique_pairs = len(counts)\n",
    "print(f\"\\nTotal unique (paragraph, sentence) pairs: {n_unique_pairs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a330e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = []\n",
    "for i in summary.values():\n",
    "    percentages.append((float(i)/n_unique_pairs)*100)\n",
    "\n",
    "amount_of_docts = summary.keys()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.bar(amount_of_docts, percentages)\n",
    "\n",
    "plt.title(\"Hello\")\n",
    "plt.xlabel(\"AMount of templates assign True\")\n",
    "plt.ylabel(\"percentage of unique true labels\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_templates_data.to_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/data_ready_for_analysis_10_10.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3941eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_templates_data = pd.read_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/data_ready_for_analysis_10_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dddec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_templates_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db107e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indices_5_temps = list_to_dict(pairs_by_count[5])\n",
    "dict_indices_4_temps = list_to_dict(pairs_by_count[4])\n",
    "dict_indices_3_temps = list_to_dict(pairs_by_count[3])\n",
    "dict_indices_2_temps = list_to_dict(pairs_by_count[2])\n",
    "dict_indices_1_temps = list_to_dict(pairs_by_count[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3684171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now extract danish sentences\n",
    "\n",
    "da_sent_5_temps = danish_sentences_with_blame_extraction(dict_indices_5_temps, all_templates_data, 'da_segmented_text')\n",
    "da_sent_4_temps = danish_sentences_with_blame_extraction(dict_indices_4_temps, all_templates_data, 'da_segmented_text')\n",
    "da_sent_3_temps = danish_sentences_with_blame_extraction(dict_indices_3_temps, all_templates_data, 'da_segmented_text')\n",
    "da_sent_2_temps = danish_sentences_with_blame_extraction(dict_indices_2_temps, all_templates_data, 'da_segmented_text')\n",
    "da_sent_1_temps = danish_sentences_with_blame_extraction(dict_indices_1_temps, all_templates_data, 'da_segmented_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c64bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get government data\n",
    "regerings_data = pd.read_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/danish_govs.csv\")\n",
    "\n",
    "#make date related columns into datetime objects\n",
    "regerings_data[\"Start Date\"] = pd.to_datetime(regerings_data[\"Start Date\"], format=\"%Y-%m-%d\")\n",
    "regerings_data[\"End Date\"]   = pd.to_datetime(regerings_data[\"End Date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "#Also make the date in debate data as datetime object\n",
    "all_templates_data[\"date\"] = pd.to_datetime(all_templates_data[\"date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11762974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by all five templates as blame into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'sentences_in_five_templates',sentences = da_sent_5_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'sentences_in_five_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'sentences_in_five_templates', data = all_templates_data, government_data = regerings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a83187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by four templates as blame into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'sentences_in_four_templates',sentences = da_sent_4_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'sentences_in_four_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'sentences_in_four_templates', data = all_templates_data, government_data = regerings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by three templates as blame into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'sentences_in_three_templates',sentences = da_sent_3_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'sentences_in_three_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'sentences_in_three_templates', data = all_templates_data, government_data = regerings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by two templates as blame into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'sentences_in_two_templates',sentences = da_sent_2_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'sentences_in_two_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'sentences_in_two_templates', data = all_templates_data, government_data = regerings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd29b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by only one templates as blame into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'sentences_in_one_templates',sentences = da_sent_1_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'sentences_in_one_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'sentences_in_one_templates', data = all_templates_data, government_data = regerings_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e60fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge json_files\n",
    "#base directory\n",
    "base_dir = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files\"\n",
    "\n",
    "#save paths in variables\n",
    "five_temps = base_dir + \"/sentences_in_five_templates.json\"\n",
    "four_temps = base_dir + \"/sentences_in_four_templates.json\"\n",
    "three_temps = base_dir + \"/sentences_in_three_templates.json\"\n",
    "two_temps = base_dir + \"/sentences_in_two_templates.json\"\n",
    "one_temps = base_dir + \"/sentences_in_one_templates.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#apply merge\n",
    "\n",
    "merge_json_files(json_files = [five_temps, four_temps], output_file_path = base_dir + '/collapsed_sentences_5_and_4.json')\n",
    "merge_json_files(json_files = [five_temps, four_temps, three_temps], output_file_path = base_dir + '/collapsed_sentences_5_and_4_and_3.json')\n",
    "merge_json_files(json_files = [five_temps, four_temps, three_temps, two_temps], output_file_path = base_dir + '/collapsed_sentences_5_and_4_and_3_and_2.json')\n",
    "merge_json_files(json_files = [five_temps, four_temps, three_temps, two_temps, one_temps], output_file_path = base_dir + '/collapsed_sentences_5_and_4_and_3_and_2_and_1.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b0c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the no blame data\n",
    "\n",
    "#extract sentences with no blame from the dictiorary of blame indices with blame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_no_blame_5_temps = danish_sentences_without_blame_extraction(blame_dict = da_sent_5_temps, data = all_templates_data, text_column = 'da_segmented_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00719dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by all five templates with NO BLAME into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'NO_BLAME_sentences_in_five_templates',sentences = sentences_no_blame_5_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'NO_BLAME_sentences_in_five_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'NO_BLAME_sentences_in_five_templates', data = all_templates_data, government_data = regerings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f65457",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_no_blame_4_temps = danish_sentences_without_blame_extraction(blame_dict = da_sent_4_temps, data = all_templates_data, text_column = 'da_segmented_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by four templates with NO BLAME into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'NO_BLAME_sentences_in_four_templates',sentences = sentences_no_blame_4_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'NO_BLAME_sentences_in_four_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'NO_BLAME_sentences_in_four_templates', data = all_templates_data, government_data = regerings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_no_blame_3_temps = danish_sentences_without_blame_extraction(blame_dict = da_sent_3_temps, data = all_templates_data, text_column = 'da_segmented_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by three templates with NO BLAME into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'NO_BLAME_sentences_in_three_templates',sentences = sentences_no_blame_3_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'NO_BLAME_sentences_in_three_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'NO_BLAME_sentences_in_three_templates', data = all_templates_data, government_data = regerings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c83b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_no_blame_2_temps = danish_sentences_without_blame_extraction(blame_dict = da_sent_2_temps, data = all_templates_data, text_column = 'da_segmented_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by two templates with NO BLAME into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'NO_BLAME_sentences_in_two_templates',sentences = sentences_no_blame_2_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'NO_BLAME_sentences_in_two_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'NO_BLAME_sentences_in_two_templates', data = all_templates_data, government_data = regerings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba404c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_no_blame_1_temps = danish_sentences_without_blame_extraction(blame_dict = da_sent_1_temps, data = all_templates_data, text_column = 'da_segmented_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a57cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all sentences identified by one templates with NO BLAME into json file\n",
    "#make danish sentences into json and write\n",
    "convert_to_json_and_write(file_name = 'NO_BLAME_sentences_in_one_templates',sentences = sentences_no_blame_1_temps)\n",
    "\n",
    "#change format for labelstudio and append meta data\n",
    "json_append_meta_data(file_name = 'NO_BLAME_sentences_in_one_templates', data = all_templates_data)\n",
    "\n",
    "#Now append preceding and succeding sentence for each blame in addition to government meta data\n",
    "json_government_and_context(file_name = 'NO_BLAME_sentences_in_one_templates', data = all_templates_data, government_data = regerings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply merge of 5+4, 5+4+3 etc\n",
    "\n",
    "\n",
    "base_dir = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files\"\n",
    "\n",
    "#save paths in variables\n",
    "no_blame_five_temps = base_dir + \"/NO_BLAME_sentences_in_five_templates.json\"\n",
    "no_blame_four_temps = base_dir + \"/NO_BLAME_sentences_in_four_templates.json\"\n",
    "no_blame_three_temps = base_dir + \"/NO_BLAME_sentences_in_three_templates.json\"\n",
    "no_blame_two_temps = base_dir + \"/NO_BLAME_sentences_in_two_templates.json\"\n",
    "no_blame_one_temps = base_dir + \"/NO_BLAME_sentences_in_one_templates.json\"\n",
    "\n",
    "merge_json_files(json_files = [no_blame_five_temps, no_blame_four_temps], output_file_path = base_dir + '/NO_BLAME_collapsed_sentences_5_and_4.json')\n",
    "merge_json_files(json_files = [no_blame_five_temps, no_blame_four_temps, no_blame_three_temps], output_file_path = base_dir + '/NO_BLAME_collapsed_sentences_5_and_4_and_3.json')\n",
    "merge_json_files(json_files = [no_blame_five_temps, no_blame_four_temps, no_blame_three_temps, no_blame_two_temps], output_file_path = base_dir + '/NO_BLAME_collapsed_sentences_5_and_4_and_3_and_2.json')\n",
    "merge_json_files(json_files = [no_blame_five_temps, no_blame_four_temps, no_blame_three_temps, no_blame_two_temps, no_blame_one_temps], output_file_path = base_dir + '/NO_BLAME_collapsed_sentences_5_and_4_and_3_and_2_and_1.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b419bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Afterwards 50 random samples of sentences which had been classified as containing blame were collected.\n",
    "These sentences were randowmly drawn using xxx from the data file containing merged blame sentences from all model-templates.\n",
    "This was deemed a suffienct step in order to gather a validation set with as little premade assumptions \n",
    "regarding the model performance as possiple. Due to the concern that the sentences which all templates classified as containing\n",
    "blame could share some intrinsic feature of blame, which woudl not caputure the entire spectrum of what blame can be. \n",
    "By drawing the validation set from all detected sentences of blame, the aim was to make the validation set as representative of\n",
    "blame in political speach as possiple. However, due to the relative small number of blame as a proportion of the \n",
    "total amount of sentences, drawing from the pre-classified labels was a nessecary step due to time-constraints, \n",
    "as an enourmous amount of data should be gold-labeled if a true random sample would be drawn from the entire dataset. \n",
    "In this case, if the calculated percentage of sentences containing blame is somewhat accurate \n",
    "(around 1%) 5.000 true random samples would need to be manually labelled if we were to achive a validation \n",
    "set of just 50 labels being true.\n",
    "\n",
    "In addition to the 50 random samples pre-classified as blame drawn, also 50 random samples pre-classified as not blame were drawn.\n",
    "\n",
    "These would be combined to a single set of 100 samples, which would be passed to labelStudio for labelling. It is important to note, that \n",
    "the computationally pre-classified label (decision by the PolDebate) as either blame or no blame, was hid during labelling,\n",
    "as to not influence the authers decision of whether the sentence contained blame or not.\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw 50 random samples from the blame data\n",
    "\n",
    "# Load your flattened JSON\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Training_data/cleaned_training_data_1_2_3_4_5_temps.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Sample 50 random entries\n",
    "sampled_data = random.sample(data, k=50)\n",
    "\n",
    "# Add a new key \"Blame\": 1 to each entry\n",
    "for entry in sampled_data:\n",
    "    entry[\"Blame\"] = 1\n",
    "\n",
    "# Save the modified sampled data\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/blame_true_sampled_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sampled_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edc5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw 50 random samples from the no blame data\n",
    "\n",
    "# Load your flattened JSON\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/NO_BLAME_collapsed_sentences_5_and_4_and_3_and_2_and_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Sample 50 random entries\n",
    "sampled_data = random.sample(data, k=50)\n",
    "\n",
    "# Add a new key \"Blame\": 1 to each entry\n",
    "for entry in sampled_data:\n",
    "    entry[\"Blame\"] = 0\n",
    "\n",
    "# Save the modified sampled data\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/blame_false_sampled_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sampled_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_blame_sample_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/blame_true_sampled_data.json\"\n",
    "false_blame_sample_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/blame_false_sampled_data.json\"\n",
    "path_output = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/Gold_Gold_label_data.json\"\n",
    "\n",
    "merge_json_files(json_files = [true_blame_sample_path, false_blame_sample_path], output_file_path = path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78daab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the above again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92854ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw 50 random samples from the blame data\n",
    "# Load your flattened JSON\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/collapsed_sentences_5_and_4_and_3_and_2_and_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Sample 50 random entries\n",
    "sampled_data = random.sample(data, k=50)\n",
    "\n",
    "# Add a new key \"Blame\": 1 to each entry\n",
    "for entry in sampled_data:\n",
    "    entry[\"Blame\"] = 1\n",
    "\n",
    "# Save the modified sampled data\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/v2_blame_true_sampled_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sampled_data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw 50 random samples from the no blame data\n",
    "\n",
    "# Load your flattened JSON\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/NO_BLAME_collapsed_sentences_5_and_4_and_3_and_2_and_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Sample 50 random entries\n",
    "sampled_data = random.sample(data, k=50)\n",
    "\n",
    "# Add a new key \"Blame\": 1 to each entry\n",
    "for entry in sampled_data:\n",
    "    entry[\"Blame\"] = 0\n",
    "\n",
    "# Save the modified sampled data\n",
    "with open(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/v2_blame_false_sampled_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sampled_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4199bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_blame_sample_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/v2_blame_true_sampled_data.json\"\n",
    "false_blame_sample_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/v2_blame_false_sampled_data.json\"\n",
    "path_output = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/json_files/v2_Gold_Gold_label_data.json\"\n",
    "\n",
    "merge_json_files(json_files = [true_blame_sample_path, false_blame_sample_path], output_file_path = path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a199430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For a thrid time randomly sample data but this time from the correct dataset\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load JSON file\n",
    "with open('/work/MarkusLundsfrydJensen#1865/Training_data/cleaned_training_data_1_2_3_4_5_temps.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter entries by label\n",
    "label_1_entries = [entry for entry in data if entry.get('label') == 1]\n",
    "label_0_entries = [entry for entry in data if entry.get('label') == 0]\n",
    "\n",
    "# Ensure there are enough entries in both categories\n",
    "if len(label_1_entries) < 50:\n",
    "    raise ValueError(f\"Not enough entries with label=1. Only found {len(label_1_entries)}.\")\n",
    "if len(label_0_entries) < 50:\n",
    "    raise ValueError(f\"Not enough entries with label=0. Only found {len(label_0_entries)}.\")\n",
    "\n",
    "# Randomly sample 50 entries from each\n",
    "sampled_1 = random.sample(label_1_entries, 50)\n",
    "sampled_0 = random.sample(label_0_entries, 50)\n",
    "\n",
    "# Combine both samples\n",
    "combined_sample = sampled_1 + sampled_0\n",
    "\n",
    "# (Optional) Shuffle the combined sample so labels are mixed\n",
    "random.shuffle(combined_sample)\n",
    "\n",
    "# (Optional) Save the sample to a new JSON file\n",
    "with open('/work/MarkusLundsfrydJensen#1865/annotation_data_v3.json', 'w') as f:\n",
    "    json.dump(combined_sample, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
