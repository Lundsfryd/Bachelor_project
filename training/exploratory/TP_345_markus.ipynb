{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64371899",
   "metadata": {},
   "source": [
    "To do:\n",
    "- ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "- Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    - In addition to this, look at the implications of gradient accumulation steps\n",
    "    - Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "- Early stopping: load_best_model_at_end=True\n",
    "- Split into validation set as well (80:10:10)\n",
    "- Hyperparameter tuning (Alpha, learning rate, batch size so on - not sure how to figure this out)\n",
    "    - There is precedence for no hyperparameter tuning from the author of the OG NLI model that DEBATE is based on = Due to computational restrains and the points from this paper, no hyperparameter tuning was performed in this case. The model tuning in itself is also not the primary focus in this paper, but simply serves as a tool for the actual inquiry into blame in the Danish Parliament\n",
    "- Context / No context?\n",
    "    - Most likely no context, as we do not have the academic reasons for doing so \n",
    "    - Model actually also seems to perform better without (f1 of .66 for no context on random test set vs. . 55 on gold labelled for context model) This might also make sense as the context is not taken into account in the DEBATE model\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 12,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "18d09f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
<<<<<<< HEAD
      "2025-10-29 14:32:42.088896: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-29 14:32:42.138503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-29 14:32:43.107162: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
=======
      "2025-10-29 16:09:53.790652: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-29 16:09:53.840047: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-29 16:10:33.262942: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
=======
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16, #b addded\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 3,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "trainable params: 3,416,096 || all params: 310,947,874 || trainable%: 1.0986\n"
=======
      "trainable params: 6,832,192 || all params: 314,363,970 || trainable%: 2.1733\n"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,  # Low-rank dimension\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\"  # Fine-tuning all linear (classification, attention... layers)\n",
    ")\n",
    "\n",
    "lora_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Enable gradient computation for LoRA parameters\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        param.requires_grad = True\n",
    "\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=512, # Padding to 512 to massively cut down on computation compared to base 8,192 tokens. \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 5,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "07c61a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Map (num_proc=16): 100%|██████████| 258/258 [00:00<00:00, 554.27 examples/s]\n"
=======
      "Map (num_proc=16): 100%|██████████| 258/258 [00:00<00:00, 568.42 examples/s]\n"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
     ]
    }
   ],
   "source": [
    "val_dataframe = pd.read_json(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Model_data/validation_set.json\")\n",
    "\n",
    "val_dataframe = val_dataframe[['text', 'label']]\n",
    "\n",
    "val_dataframe.rename(columns={'label': 'labels'}, inplace=True)\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, num_proc=16)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 6,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "77531159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/tmp/ipykernel_14772/775643221.py:9: SettingWithCopyWarning: \n",
=======
      "/tmp/ipykernel_62624/645673017.py:9: SettingWithCopyWarning: \n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_dataframe.rename(columns={'label': 'labels'}, inplace=True)\n",
<<<<<<< HEAD
      "Map (num_proc=16): 100%|██████████| 50000/50000 [00:03<00:00, 12907.84 examples/s]\n"
=======
      "Map (num_proc=16): 100%|██████████| 50000/50000 [00:02<00:00, 17143.14 examples/s]\n"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "input_data_dir = \"/work/MarkusLundsfrydJensen#1865/Training_data/cleaned_training_data_3_4_5_temps.json\"\n",
=======
    "input_data_dir = \"/work/MarkusLundsfrydJensen#1865/Training_data/cleaned_training_data.json\"\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "\n",
    "dataframe_3_4_5 = pd.read_json(input_data_dir)\n",
    "\n",
    "dataframe_3_4_5 = dataframe_3_4_5.sample(frac=1)\n",
    "\n",
    "test_dataframe = dataframe_3_4_5[['text', 'label']]\n",
    "\n",
    "test_dataframe.rename(columns={'label': 'labels'}, inplace=True)\n",
    "\n",
    "test_dataframe = test_dataframe[0:50000]\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_dataframe)\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 7,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "e7863f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Smaller batch size with gradient accumulation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/work/MarkusLundsfrydJensen#1865',\n",
    "    #optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,  # Reduced\n",
    "    gradient_accumulation_steps=16,  # Effective batch = 256\n",
    "    logging_steps=1,  # More frequent logging\n",
    "    eval_strategy=\"epoch\",  # Evaluate more frequently\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    remove_unused_columns=True,\n",
    "    max_grad_norm=1.0,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,  # Add this\n",
    "    metric_for_best_model=\"weighted BCE\", # Add this\n",
    "    greater_is_better = False\n",
    "      \n",
    ")\n",
    "\n",
    "#OBS implement early stopping and more epochs, lower learning rate?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "id": "bd22494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bincrossentropy(true, pred, weight_zero = 1, weight_one = 1):\n",
=======
   "execution_count": 8,
   "id": "bd22494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 weight: 0.5024822623761381\n",
      "Class 1 weight: 101.21457489878543\n",
      "Label distribution: labels\n",
      "0    49753\n",
      "1      247\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "label_counts = test_dataframe['labels'].value_counts()\n",
    "total = len(test_dataframe)\n",
    "weight_for_0 = total / (2 * label_counts[0])\n",
    "weight_for_1 = total / (2 * label_counts[1])\n",
    "\n",
    "print(f\"Class 0 weight: {weight_for_0}\")\n",
    "print(f\"Class 1 weight: {weight_for_1}\")\n",
    "print(f\"Label distribution: {label_counts}\")\n",
    "\n",
    "def weighted_bincrossentropy(true, pred, weight_zero = weight_for_0, weight_one = weight_for_1):\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed to represent class imbalance in the dataset.\n",
    "        \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalized 10 times as much as false negatives.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    #weights /= (weight_one + weight_zero) # Normalizing to be more consistent with regular BCE for comparison \n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return np.mean(weighted_bin_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 9,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "de44b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #From logits to probabilities\n",
    "    probs_2d = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    probs = probs_2d[:, 1]  # positive class extraction\n",
    "    \n",
    "    weigthted_bce = weighted_bincrossentropy(labels, probs)\n",
    "    keras_bce = binary_crossentropy(labels, probs)\n",
    "    keras_bce = float(np.mean(keras_bce.numpy()))  # Converting from keras eagertensor to float value\n",
    "    \n",
    "    # Wrapping all metrics to floats for json serialization during model eval\n",
    "    return {\n",
    "        'keras_BCE': keras_bce,\n",
    "        'weighted BCE': weigthted_bce,\n",
    "        'recall': float(recall_score(labels, probs.round())),\n",
    "        'precision': float(average_precision_score(labels, probs)),\n",
    "        'accuracy': float(accuracy_score(labels, probs.round())), # Need rounding for these two computations (integer required)\n",
    "        'f1': float(f1_score(labels, probs.round(), average='macro')), # macro f1 is better for imbalanced dataset\n",
    "        'number_of_true_preds': sum(probs.round()),\n",
    "        'number_of_true_labels': sum(labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 10,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "b2363d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Class 0 weight: 0.5047038397868131\n",
      "Class 1 weight: 53.648068669527895\n",
      "Label distribution: labels\n",
      "0    49534\n",
      "1      466\n",
=======
      "Class 0 weight: 0.5024822623761381\n",
      "Class 1 weight: 101.21457489878543\n",
      "Label distribution: labels\n",
      "0    49753\n",
      "1      247\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create custom trainer with weighted loss\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Calculate class weights\n",
    "label_counts = test_dataframe['labels'].value_counts()\n",
    "total = len(test_dataframe)\n",
    "weight_for_0 = total / (2 * label_counts[0])\n",
    "weight_for_1 = total / (2 * label_counts[1])\n",
    "\n",
    "print(f\"Class 0 weight: {weight_for_0}\")\n",
    "print(f\"Class 1 weight: {weight_for_1}\")\n",
    "print(f\"Label distribution: {label_counts}\")\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(model.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 11,
   "id": "a5b2cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.5025, 101.2146])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='599' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [599/980 18:07 < 11:34, 0.55 it/s, Epoch 3.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Keras Bce</th>\n",
       "      <th>Weighted bce</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number Of True Preds</th>\n",
       "      <th>Number Of True Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>1.560371</td>\n",
       "      <td>0.687709</td>\n",
       "      <td>23.969349</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.807585</td>\n",
       "      <td>0.693798</td>\n",
       "      <td>0.520863</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.184900</td>\n",
       "      <td>1.704087</td>\n",
       "      <td>0.764436</td>\n",
       "      <td>26.643560</td>\n",
       "      <td>0.329545</td>\n",
       "      <td>0.845696</td>\n",
       "      <td>0.751938</td>\n",
       "      <td>0.656487</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.845600</td>\n",
       "      <td>3.560846</td>\n",
       "      <td>1.550281</td>\n",
       "      <td>54.033344</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.781910</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.523444</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1761751031.906956   62624 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Use the custom trainer instead of the regular Trainer\u001b[39;00m\n\u001b[32m      7\u001b[39m trainer = WeightedLossTrainer(\n\u001b[32m      8\u001b[39m     model=lora_model,\n\u001b[32m      9\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     class_weights=class_weights  \u001b[38;5;66;03m# Pass the weights here\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/transformers/trainer.py:2679\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create the class weights tensor\n",
    "\n",
    "#for _5\n",
    "class_weights = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float32)\n",
    "print(class_weights)\n",
    "# Use the custom trainer instead of the regular Trainer\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights  # Pass the weights here\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
   "id": "233b94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "tensor([ 0.5047, 53.6481])\n"
=======
      "tensor([ 0.5034, 75.0751])\n"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
<<<<<<< HEAD
       "      <progress value='106' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [106/980 03:10 < 26:39, 0.55 it/s, Epoch 0.54/5]\n",
=======
       "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [980/980 29:38, Epoch 5/5]\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
<<<<<<< HEAD
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
=======
       "      <th>Keras Bce</th>\n",
       "      <th>Weighted bce</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Number Of True Preds</th>\n",
       "      <th>Number Of True Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.694700</td>\n",
       "      <td>2.017677</td>\n",
       "      <td>0.879203</td>\n",
       "      <td>22.805353</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.794399</td>\n",
       "      <td>0.689922</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>1.140749</td>\n",
       "      <td>0.539143</td>\n",
       "      <td>13.984640</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.836692</td>\n",
       "      <td>0.786822</td>\n",
       "      <td>0.724118</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>3.260417</td>\n",
       "      <td>1.384925</td>\n",
       "      <td>35.923096</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.795126</td>\n",
       "      <td>0.709302</td>\n",
       "      <td>0.545123</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>5.857619</td>\n",
       "      <td>2.489563</td>\n",
       "      <td>64.575935</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.773322</td>\n",
       "      <td>0.693798</td>\n",
       "      <td>0.498511</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.466128</td>\n",
       "      <td>3.089556</td>\n",
       "      <td>80.138969</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.749499</td>\n",
       "      <td>0.701550</td>\n",
       "      <td>0.518785</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
<<<<<<< HEAD
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Use the custom trainer instead of the regular Trainer\u001b[39;00m\n\u001b[32m      5\u001b[39m trainer = WeightedLossTrainer(\n\u001b[32m      6\u001b[39m     model=lora_model,\n\u001b[32m      7\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     class_weights=class_weights  \u001b[38;5;66;03m# Pass the weights here\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/MarkusLundsfrydJensen#1865/miniconda3/envs/blame_bert/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1761748835.069396   44570 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=980, training_loss=1.8795074580366389, metrics={'train_runtime': 1788.3599, 'train_samples_per_second': 139.793, 'train_steps_per_second': 0.548, 'total_flos': 9.043658496e+16, 'train_loss': 1.8795074580366389, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    }
   ],
   "source": [
    "# Create the class weights tensor\n",
<<<<<<< HEAD
=======
    "\n",
    "#for 4_5\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "class_weights = torch.tensor([weight_for_0, weight_for_1], dtype=torch.float32)\n",
    "print(class_weights)\n",
    "# Use the custom trainer instead of the regular Trainer\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights  # Pass the weights here\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11bbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/work/MarkusLundsfrydJensen#1865/test_models_save/Option_2/tokenizer_config.json',\n",
       " '/work/MarkusLundsfrydJensen#1865/test_models_save/Option_2/special_tokens_map.json',\n",
       " '/work/MarkusLundsfrydJensen#1865/test_models_save/Option_2/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge (now clean, no quantization artifacts)\n",
    "model = trainer.model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
<<<<<<< HEAD
    "output_model_dir = \"/work/MarkusLundsfrydJensen#1865/test_models_save/Option_2\"\n",
=======
    "output_model_dir = \"/work/MarkusLundsfrydJensen#1865/Training_data/Prelim_trained_model_345\"\n",
>>>>>>> 3b8aab0faa159bd11ab9e74f8df5d043ccd4a341
    "merged_model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79714b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hold up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98dbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS need to be made usable\n",
    "class BlameDetectorDa_v0(object):\n",
    "\n",
    "    def __init__(self, model, tokenizer, max_length):\n",
    "\n",
    "        self.model = model\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.batch_size = batch_size\n",
    "\n",
    "        self.model_initialization()\n",
    "\n",
    "        return\n",
    "\n",
    "    def model_initialization(self):\n",
    "        #self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_path)\n",
    "        self.model.eval()\n",
    "            \n",
    "        # Move to GPU if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f\"Model loaded successfully on {self.device}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Make a prediction on a single text input.\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            self.text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move inputs to device\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return predicted_class, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "    def run_prediction(self, text):\n",
    "\n",
    "        self.text = text\n",
    "        predicted_class, confidence, probs = self.predict()\n",
    "            \n",
    "        return predicted_class, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7ef1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Model loaded successfully on cuda\n"
     ]
    }
   ],
   "source": [
    "BD = BlameDetectorDa_v0(model = model_tester, tokenizer = tokenizer, max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69371775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_path = \"/work/MarkusLundsfrydJensen#1865/Bachelor_project/Model_data/validation_set.json\"\n",
    "\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45950bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data:\n",
    "    text = entry[\"text\"]\n",
    "\n",
    "    entry[\"predicted_class\"], entry[\"confidence\"] = BD.run_prediction(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c438da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7558139534883721\n"
     ]
    }
   ],
   "source": [
    "true_labels = 0\n",
    "pred_true = 0\n",
    "false_labels = 0\n",
    "pred_false = 0\n",
    "\n",
    "correct_pred = 0\n",
    "incorrect_pred = 0\n",
    "\n",
    "for entry in data:\n",
    "    if entry[\"label\"] == 1:\n",
    "        true_labels += 1\n",
    "    if entry[\"predicted_class\"] == 1:\n",
    "        pred_true +=1\n",
    "\n",
    "    if entry[\"label\"] == 0:\n",
    "        false_labels += 1\n",
    "    if entry[\"predicted_class\"] == 0:\n",
    "        pred_false +=1\n",
    "\n",
    "    if entry[\"label\"] == entry[\"predicted_class\"]:\n",
    "        correct_pred +=1\n",
    "\n",
    "    if entry[\"label\"] != entry[\"predicted_class\"]:\n",
    "        incorrect_pred +=1\n",
    "\n",
    "\n",
    "print(correct_pred/len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
