{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 402,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0075,
      "grad_norm": NaN,
      "learning_rate": 2e-05,
      "loss": 9.0626,
      "step": 1
    },
    {
      "epoch": 0.015,
      "grad_norm": 100.2548599243164,
      "learning_rate": 1.9950248756218905e-05,
      "loss": 15.6883,
      "step": 2
    },
    {
      "epoch": 0.0225,
      "grad_norm": 21.657665252685547,
      "learning_rate": 1.990049751243781e-05,
      "loss": 7.4389,
      "step": 3
    },
    {
      "epoch": 0.03,
      "grad_norm": 45.3979377746582,
      "learning_rate": 1.9850746268656718e-05,
      "loss": 10.5878,
      "step": 4
    },
    {
      "epoch": 0.0375,
      "grad_norm": 38.852577209472656,
      "learning_rate": 1.9800995024875625e-05,
      "loss": 9.3558,
      "step": 5
    },
    {
      "epoch": 0.045,
      "grad_norm": 34.24111557006836,
      "learning_rate": 1.9751243781094528e-05,
      "loss": 12.3966,
      "step": 6
    },
    {
      "epoch": 0.0525,
      "grad_norm": 79.56830596923828,
      "learning_rate": 1.9701492537313435e-05,
      "loss": 10.6013,
      "step": 7
    },
    {
      "epoch": 0.06,
      "grad_norm": 28.965351104736328,
      "learning_rate": 1.965174129353234e-05,
      "loss": 11.2769,
      "step": 8
    },
    {
      "epoch": 0.0675,
      "grad_norm": 60.464622497558594,
      "learning_rate": 1.9601990049751245e-05,
      "loss": 12.5887,
      "step": 9
    },
    {
      "epoch": 0.075,
      "grad_norm": 25.874683380126953,
      "learning_rate": 1.955223880597015e-05,
      "loss": 10.0343,
      "step": 10
    },
    {
      "epoch": 0.0825,
      "grad_norm": NaN,
      "learning_rate": 1.9502487562189055e-05,
      "loss": 12.783,
      "step": 11
    },
    {
      "epoch": 0.09,
      "grad_norm": NaN,
      "learning_rate": 1.945273631840796e-05,
      "loss": 19.488,
      "step": 12
    },
    {
      "epoch": 0.0975,
      "grad_norm": 51.7468147277832,
      "learning_rate": 1.9402985074626868e-05,
      "loss": 11.0022,
      "step": 13
    },
    {
      "epoch": 0.105,
      "grad_norm": 19.61762237548828,
      "learning_rate": 1.935323383084577e-05,
      "loss": 9.4272,
      "step": 14
    },
    {
      "epoch": 0.1125,
      "grad_norm": 66.21730041503906,
      "learning_rate": 1.9303482587064678e-05,
      "loss": 10.0338,
      "step": 15
    },
    {
      "epoch": 0.12,
      "grad_norm": 58.60740280151367,
      "learning_rate": 1.9253731343283585e-05,
      "loss": 10.0696,
      "step": 16
    },
    {
      "epoch": 0.1275,
      "grad_norm": 28.294063568115234,
      "learning_rate": 1.9203980099502488e-05,
      "loss": 10.818,
      "step": 17
    },
    {
      "epoch": 0.135,
      "grad_norm": 52.64224624633789,
      "learning_rate": 1.9154228855721395e-05,
      "loss": 12.2426,
      "step": 18
    },
    {
      "epoch": 0.1425,
      "grad_norm": 17.27118492126465,
      "learning_rate": 1.9104477611940298e-05,
      "loss": 7.7314,
      "step": 19
    },
    {
      "epoch": 0.15,
      "grad_norm": 41.42197799682617,
      "learning_rate": 1.9054726368159208e-05,
      "loss": 10.7449,
      "step": 20
    },
    {
      "epoch": 0.1575,
      "grad_norm": 21.654691696166992,
      "learning_rate": 1.900497512437811e-05,
      "loss": 8.727,
      "step": 21
    },
    {
      "epoch": 0.165,
      "grad_norm": 62.20089340209961,
      "learning_rate": 1.8955223880597015e-05,
      "loss": 12.5781,
      "step": 22
    },
    {
      "epoch": 0.1725,
      "grad_norm": 45.98387908935547,
      "learning_rate": 1.890547263681592e-05,
      "loss": 9.9181,
      "step": 23
    },
    {
      "epoch": 0.18,
      "grad_norm": 29.630626678466797,
      "learning_rate": 1.8855721393034828e-05,
      "loss": 10.8033,
      "step": 24
    },
    {
      "epoch": 0.1875,
      "grad_norm": 21.647010803222656,
      "learning_rate": 1.8805970149253735e-05,
      "loss": 6.9258,
      "step": 25
    },
    {
      "epoch": 0.195,
      "grad_norm": 44.41404342651367,
      "learning_rate": 1.8756218905472638e-05,
      "loss": 10.8324,
      "step": 26
    },
    {
      "epoch": 0.2025,
      "grad_norm": 33.376739501953125,
      "learning_rate": 1.8706467661691545e-05,
      "loss": 8.5274,
      "step": 27
    },
    {
      "epoch": 0.21,
      "grad_norm": 27.93086814880371,
      "learning_rate": 1.865671641791045e-05,
      "loss": 7.5381,
      "step": 28
    },
    {
      "epoch": 0.2175,
      "grad_norm": 16.61469268798828,
      "learning_rate": 1.8606965174129355e-05,
      "loss": 6.7905,
      "step": 29
    },
    {
      "epoch": 0.225,
      "grad_norm": 35.73627853393555,
      "learning_rate": 1.855721393034826e-05,
      "loss": 10.1898,
      "step": 30
    },
    {
      "epoch": 0.2325,
      "grad_norm": 88.77249908447266,
      "learning_rate": 1.8507462686567165e-05,
      "loss": 11.281,
      "step": 31
    },
    {
      "epoch": 0.24,
      "grad_norm": 22.519201278686523,
      "learning_rate": 1.845771144278607e-05,
      "loss": 9.2028,
      "step": 32
    },
    {
      "epoch": 0.2475,
      "grad_norm": 30.45051383972168,
      "learning_rate": 1.8407960199004978e-05,
      "loss": 12.5917,
      "step": 33
    },
    {
      "epoch": 0.255,
      "grad_norm": 18.977733612060547,
      "learning_rate": 1.835820895522388e-05,
      "loss": 9.1142,
      "step": 34
    },
    {
      "epoch": 0.2625,
      "grad_norm": 27.069622039794922,
      "learning_rate": 1.8308457711442788e-05,
      "loss": 10.0263,
      "step": 35
    },
    {
      "epoch": 0.27,
      "grad_norm": 25.025550842285156,
      "learning_rate": 1.8258706467661695e-05,
      "loss": 10.784,
      "step": 36
    },
    {
      "epoch": 0.2775,
      "grad_norm": 16.319324493408203,
      "learning_rate": 1.8208955223880598e-05,
      "loss": 6.5495,
      "step": 37
    },
    {
      "epoch": 0.285,
      "grad_norm": 25.202831268310547,
      "learning_rate": 1.8159203980099505e-05,
      "loss": 8.5724,
      "step": 38
    },
    {
      "epoch": 0.2925,
      "grad_norm": 27.651256561279297,
      "learning_rate": 1.8109452736318408e-05,
      "loss": 11.2236,
      "step": 39
    },
    {
      "epoch": 0.3,
      "grad_norm": 31.416261672973633,
      "learning_rate": 1.8059701492537314e-05,
      "loss": 9.937,
      "step": 40
    },
    {
      "epoch": 0.3075,
      "grad_norm": 31.05480194091797,
      "learning_rate": 1.800995024875622e-05,
      "loss": 11.3937,
      "step": 41
    },
    {
      "epoch": 0.315,
      "grad_norm": 32.72361373901367,
      "learning_rate": 1.7960199004975124e-05,
      "loss": 9.9465,
      "step": 42
    },
    {
      "epoch": 0.3225,
      "grad_norm": 36.1497688293457,
      "learning_rate": 1.791044776119403e-05,
      "loss": 7.38,
      "step": 43
    },
    {
      "epoch": 0.33,
      "grad_norm": 24.330001831054688,
      "learning_rate": 1.7860696517412938e-05,
      "loss": 8.0174,
      "step": 44
    },
    {
      "epoch": 0.3375,
      "grad_norm": 20.591873168945312,
      "learning_rate": 1.7810945273631844e-05,
      "loss": 4.6858,
      "step": 45
    },
    {
      "epoch": 0.345,
      "grad_norm": 32.83747863769531,
      "learning_rate": 1.7761194029850748e-05,
      "loss": 8.6407,
      "step": 46
    },
    {
      "epoch": 0.3525,
      "grad_norm": 25.674787521362305,
      "learning_rate": 1.771144278606965e-05,
      "loss": 8.5029,
      "step": 47
    },
    {
      "epoch": 0.36,
      "grad_norm": 59.82307434082031,
      "learning_rate": 1.7661691542288558e-05,
      "loss": 7.9623,
      "step": 48
    },
    {
      "epoch": 0.3675,
      "grad_norm": 34.72567367553711,
      "learning_rate": 1.7611940298507464e-05,
      "loss": 6.4305,
      "step": 49
    },
    {
      "epoch": 0.375,
      "grad_norm": 47.259124755859375,
      "learning_rate": 1.756218905472637e-05,
      "loss": 6.6104,
      "step": 50
    },
    {
      "epoch": 0.3825,
      "grad_norm": 20.12116050720215,
      "learning_rate": 1.7512437810945274e-05,
      "loss": 9.7729,
      "step": 51
    },
    {
      "epoch": 0.39,
      "grad_norm": 14.2829008102417,
      "learning_rate": 1.746268656716418e-05,
      "loss": 5.8939,
      "step": 52
    },
    {
      "epoch": 0.3975,
      "grad_norm": 79.62437438964844,
      "learning_rate": 1.7412935323383088e-05,
      "loss": 8.6142,
      "step": 53
    },
    {
      "epoch": 0.405,
      "grad_norm": 30.8861026763916,
      "learning_rate": 1.736318407960199e-05,
      "loss": 8.2745,
      "step": 54
    },
    {
      "epoch": 0.4125,
      "grad_norm": 23.435688018798828,
      "learning_rate": 1.7313432835820894e-05,
      "loss": 7.1968,
      "step": 55
    },
    {
      "epoch": 0.42,
      "grad_norm": 14.946676254272461,
      "learning_rate": 1.7263681592039804e-05,
      "loss": 9.0829,
      "step": 56
    },
    {
      "epoch": 0.4275,
      "grad_norm": 39.4263801574707,
      "learning_rate": 1.7213930348258708e-05,
      "loss": 9.8016,
      "step": 57
    },
    {
      "epoch": 0.435,
      "grad_norm": 19.21099853515625,
      "learning_rate": 1.7164179104477614e-05,
      "loss": 7.5221,
      "step": 58
    },
    {
      "epoch": 0.4425,
      "grad_norm": 17.068639755249023,
      "learning_rate": 1.7114427860696518e-05,
      "loss": 7.0226,
      "step": 59
    },
    {
      "epoch": 0.45,
      "grad_norm": 28.776830673217773,
      "learning_rate": 1.7064676616915424e-05,
      "loss": 8.8004,
      "step": 60
    },
    {
      "epoch": 0.4575,
      "grad_norm": 15.761451721191406,
      "learning_rate": 1.701492537313433e-05,
      "loss": 7.1199,
      "step": 61
    },
    {
      "epoch": 0.465,
      "grad_norm": 25.145463943481445,
      "learning_rate": 1.6965174129353234e-05,
      "loss": 7.7005,
      "step": 62
    },
    {
      "epoch": 0.4725,
      "grad_norm": 24.422494888305664,
      "learning_rate": 1.691542288557214e-05,
      "loss": 8.3213,
      "step": 63
    },
    {
      "epoch": 0.48,
      "grad_norm": 28.153934478759766,
      "learning_rate": 1.6865671641791048e-05,
      "loss": 8.3719,
      "step": 64
    },
    {
      "epoch": 0.4875,
      "grad_norm": 39.63351821899414,
      "learning_rate": 1.681592039800995e-05,
      "loss": 8.7767,
      "step": 65
    },
    {
      "epoch": 0.495,
      "grad_norm": 13.164496421813965,
      "learning_rate": 1.6766169154228858e-05,
      "loss": 6.1057,
      "step": 66
    },
    {
      "epoch": 0.5025,
      "grad_norm": 25.706035614013672,
      "learning_rate": 1.671641791044776e-05,
      "loss": 11.2673,
      "step": 67
    },
    {
      "epoch": 0.51,
      "grad_norm": 25.45684242248535,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 7.6786,
      "step": 68
    },
    {
      "epoch": 0.5175,
      "grad_norm": 21.86570167541504,
      "learning_rate": 1.6616915422885574e-05,
      "loss": 11.25,
      "step": 69
    },
    {
      "epoch": 0.525,
      "grad_norm": 25.989151000976562,
      "learning_rate": 1.6567164179104477e-05,
      "loss": 6.6075,
      "step": 70
    },
    {
      "epoch": 0.5325,
      "grad_norm": 37.46907424926758,
      "learning_rate": 1.6517412935323384e-05,
      "loss": 11.0596,
      "step": 71
    },
    {
      "epoch": 0.54,
      "grad_norm": 36.47917175292969,
      "learning_rate": 1.646766169154229e-05,
      "loss": 7.0132,
      "step": 72
    },
    {
      "epoch": 0.5475,
      "grad_norm": 23.211963653564453,
      "learning_rate": 1.6417910447761197e-05,
      "loss": 6.4518,
      "step": 73
    },
    {
      "epoch": 0.555,
      "grad_norm": 21.664081573486328,
      "learning_rate": 1.63681592039801e-05,
      "loss": 6.4688,
      "step": 74
    },
    {
      "epoch": 0.5625,
      "grad_norm": 16.461477279663086,
      "learning_rate": 1.6318407960199004e-05,
      "loss": 7.4322,
      "step": 75
    },
    {
      "epoch": 0.57,
      "grad_norm": 20.48247528076172,
      "learning_rate": 1.626865671641791e-05,
      "loss": 6.3922,
      "step": 76
    },
    {
      "epoch": 0.5775,
      "grad_norm": 26.409486770629883,
      "learning_rate": 1.6218905472636817e-05,
      "loss": 10.0542,
      "step": 77
    },
    {
      "epoch": 0.585,
      "grad_norm": 51.807247161865234,
      "learning_rate": 1.6169154228855724e-05,
      "loss": 6.9009,
      "step": 78
    },
    {
      "epoch": 0.5925,
      "grad_norm": 60.616947174072266,
      "learning_rate": 1.6119402985074627e-05,
      "loss": 8.9386,
      "step": 79
    },
    {
      "epoch": 0.6,
      "grad_norm": 29.81574058532715,
      "learning_rate": 1.6069651741293534e-05,
      "loss": 7.9852,
      "step": 80
    },
    {
      "epoch": 0.6075,
      "grad_norm": 31.11054801940918,
      "learning_rate": 1.601990049751244e-05,
      "loss": 7.1326,
      "step": 81
    },
    {
      "epoch": 0.615,
      "grad_norm": 59.07651138305664,
      "learning_rate": 1.5970149253731344e-05,
      "loss": 6.2053,
      "step": 82
    },
    {
      "epoch": 0.6225,
      "grad_norm": 51.96355056762695,
      "learning_rate": 1.592039800995025e-05,
      "loss": 10.6558,
      "step": 83
    },
    {
      "epoch": 0.63,
      "grad_norm": 56.11465835571289,
      "learning_rate": 1.5870646766169154e-05,
      "loss": 15.1058,
      "step": 84
    },
    {
      "epoch": 0.6375,
      "grad_norm": 24.67856788635254,
      "learning_rate": 1.582089552238806e-05,
      "loss": 9.3093,
      "step": 85
    },
    {
      "epoch": 0.645,
      "grad_norm": 23.62368392944336,
      "learning_rate": 1.5771144278606967e-05,
      "loss": 5.7,
      "step": 86
    },
    {
      "epoch": 0.6525,
      "grad_norm": 37.43406677246094,
      "learning_rate": 1.572139303482587e-05,
      "loss": 8.6442,
      "step": 87
    },
    {
      "epoch": 0.66,
      "grad_norm": 17.68603515625,
      "learning_rate": 1.5671641791044777e-05,
      "loss": 6.6302,
      "step": 88
    },
    {
      "epoch": 0.6675,
      "grad_norm": 33.115577697753906,
      "learning_rate": 1.5621890547263684e-05,
      "loss": 7.6931,
      "step": 89
    },
    {
      "epoch": 0.675,
      "grad_norm": 32.437721252441406,
      "learning_rate": 1.5572139303482587e-05,
      "loss": 11.6377,
      "step": 90
    },
    {
      "epoch": 0.6825,
      "grad_norm": 24.82262420654297,
      "learning_rate": 1.5522388059701494e-05,
      "loss": 7.1797,
      "step": 91
    },
    {
      "epoch": 0.69,
      "grad_norm": 28.860340118408203,
      "learning_rate": 1.5472636815920397e-05,
      "loss": 8.3027,
      "step": 92
    },
    {
      "epoch": 0.6975,
      "grad_norm": 43.25711441040039,
      "learning_rate": 1.5422885572139307e-05,
      "loss": 8.787,
      "step": 93
    },
    {
      "epoch": 0.705,
      "grad_norm": 24.89427375793457,
      "learning_rate": 1.537313432835821e-05,
      "loss": 6.6552,
      "step": 94
    },
    {
      "epoch": 0.7125,
      "grad_norm": 23.01534080505371,
      "learning_rate": 1.5323383084577114e-05,
      "loss": 11.3308,
      "step": 95
    },
    {
      "epoch": 0.72,
      "grad_norm": 45.9493408203125,
      "learning_rate": 1.527363184079602e-05,
      "loss": 9.3732,
      "step": 96
    },
    {
      "epoch": 0.7275,
      "grad_norm": 24.399988174438477,
      "learning_rate": 1.5223880597014925e-05,
      "loss": 6.5884,
      "step": 97
    },
    {
      "epoch": 0.735,
      "grad_norm": 25.305105209350586,
      "learning_rate": 1.5174129353233834e-05,
      "loss": 4.9865,
      "step": 98
    },
    {
      "epoch": 0.7425,
      "grad_norm": 41.85838317871094,
      "learning_rate": 1.5124378109452737e-05,
      "loss": 11.8066,
      "step": 99
    },
    {
      "epoch": 0.75,
      "grad_norm": 68.03023529052734,
      "learning_rate": 1.5074626865671642e-05,
      "loss": 16.9275,
      "step": 100
    },
    {
      "epoch": 0.7575,
      "grad_norm": 18.444759368896484,
      "learning_rate": 1.5024875621890549e-05,
      "loss": 6.4585,
      "step": 101
    },
    {
      "epoch": 0.765,
      "grad_norm": 21.20142936706543,
      "learning_rate": 1.4975124378109454e-05,
      "loss": 6.5606,
      "step": 102
    },
    {
      "epoch": 0.7725,
      "grad_norm": 14.617870330810547,
      "learning_rate": 1.492537313432836e-05,
      "loss": 8.5071,
      "step": 103
    },
    {
      "epoch": 0.78,
      "grad_norm": 26.873567581176758,
      "learning_rate": 1.4875621890547265e-05,
      "loss": 5.823,
      "step": 104
    },
    {
      "epoch": 0.7875,
      "grad_norm": 53.45109558105469,
      "learning_rate": 1.4825870646766169e-05,
      "loss": 8.96,
      "step": 105
    },
    {
      "epoch": 0.795,
      "grad_norm": 25.802339553833008,
      "learning_rate": 1.4776119402985077e-05,
      "loss": 6.9972,
      "step": 106
    },
    {
      "epoch": 0.8025,
      "grad_norm": 18.748937606811523,
      "learning_rate": 1.472636815920398e-05,
      "loss": 9.3385,
      "step": 107
    },
    {
      "epoch": 0.81,
      "grad_norm": 30.682281494140625,
      "learning_rate": 1.4676616915422887e-05,
      "loss": 8.1821,
      "step": 108
    },
    {
      "epoch": 0.8175,
      "grad_norm": 31.01824378967285,
      "learning_rate": 1.4626865671641792e-05,
      "loss": 5.4812,
      "step": 109
    },
    {
      "epoch": 0.825,
      "grad_norm": 36.08597183227539,
      "learning_rate": 1.4577114427860697e-05,
      "loss": 6.9445,
      "step": 110
    },
    {
      "epoch": 0.8325,
      "grad_norm": 15.829289436340332,
      "learning_rate": 1.4527363184079604e-05,
      "loss": 7.8374,
      "step": 111
    },
    {
      "epoch": 0.84,
      "grad_norm": 30.663822174072266,
      "learning_rate": 1.4477611940298509e-05,
      "loss": 7.908,
      "step": 112
    },
    {
      "epoch": 0.8475,
      "grad_norm": 37.69207000732422,
      "learning_rate": 1.4427860696517415e-05,
      "loss": 9.0414,
      "step": 113
    },
    {
      "epoch": 0.855,
      "grad_norm": 35.33003234863281,
      "learning_rate": 1.437810945273632e-05,
      "loss": 10.5025,
      "step": 114
    },
    {
      "epoch": 0.8625,
      "grad_norm": 17.833520889282227,
      "learning_rate": 1.4328358208955224e-05,
      "loss": 7.3152,
      "step": 115
    },
    {
      "epoch": 0.87,
      "grad_norm": 25.151350021362305,
      "learning_rate": 1.427860696517413e-05,
      "loss": 7.0422,
      "step": 116
    },
    {
      "epoch": 0.8775,
      "grad_norm": 54.420597076416016,
      "learning_rate": 1.4228855721393035e-05,
      "loss": 7.1884,
      "step": 117
    },
    {
      "epoch": 0.885,
      "grad_norm": 35.557640075683594,
      "learning_rate": 1.4179104477611942e-05,
      "loss": 8.9646,
      "step": 118
    },
    {
      "epoch": 0.8925,
      "grad_norm": 19.75943374633789,
      "learning_rate": 1.4129353233830847e-05,
      "loss": 8.6153,
      "step": 119
    },
    {
      "epoch": 0.9,
      "grad_norm": 54.16073989868164,
      "learning_rate": 1.4079601990049752e-05,
      "loss": 6.775,
      "step": 120
    },
    {
      "epoch": 0.9075,
      "grad_norm": 16.038286209106445,
      "learning_rate": 1.4029850746268658e-05,
      "loss": 7.1084,
      "step": 121
    },
    {
      "epoch": 0.915,
      "grad_norm": 32.38237762451172,
      "learning_rate": 1.3980099502487563e-05,
      "loss": 7.0232,
      "step": 122
    },
    {
      "epoch": 0.9225,
      "grad_norm": 47.803802490234375,
      "learning_rate": 1.393034825870647e-05,
      "loss": 6.9199,
      "step": 123
    },
    {
      "epoch": 0.93,
      "grad_norm": 20.563671112060547,
      "learning_rate": 1.3880597014925375e-05,
      "loss": 6.8666,
      "step": 124
    },
    {
      "epoch": 0.9375,
      "grad_norm": 31.6651611328125,
      "learning_rate": 1.3830845771144278e-05,
      "loss": 7.1584,
      "step": 125
    },
    {
      "epoch": 0.945,
      "grad_norm": 40.650394439697266,
      "learning_rate": 1.3781094527363185e-05,
      "loss": 6.1821,
      "step": 126
    },
    {
      "epoch": 0.9525,
      "grad_norm": 17.818695068359375,
      "learning_rate": 1.373134328358209e-05,
      "loss": 7.8662,
      "step": 127
    },
    {
      "epoch": 0.96,
      "grad_norm": 37.0513916015625,
      "learning_rate": 1.3681592039800997e-05,
      "loss": 8.4897,
      "step": 128
    },
    {
      "epoch": 0.9675,
      "grad_norm": 26.779916763305664,
      "learning_rate": 1.3631840796019902e-05,
      "loss": 8.7285,
      "step": 129
    },
    {
      "epoch": 0.975,
      "grad_norm": 22.920806884765625,
      "learning_rate": 1.3582089552238807e-05,
      "loss": 7.8308,
      "step": 130
    },
    {
      "epoch": 0.9825,
      "grad_norm": 16.993825912475586,
      "learning_rate": 1.3532338308457713e-05,
      "loss": 8.4368,
      "step": 131
    },
    {
      "epoch": 0.99,
      "grad_norm": 17.32745933532715,
      "learning_rate": 1.3482587064676618e-05,
      "loss": 7.121,
      "step": 132
    },
    {
      "epoch": 0.9975,
      "grad_norm": 68.44009399414062,
      "learning_rate": 1.3432835820895525e-05,
      "loss": 10.0954,
      "step": 133
    },
    {
      "epoch": 1.0,
      "grad_norm": 28.28489875793457,
      "learning_rate": 1.3383084577114428e-05,
      "loss": 1.5474,
      "step": 134
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6075,
      "eval_f1": 0.561938908614039,
      "eval_loss": 0.6354936361312866,
      "eval_runtime": 72.9473,
      "eval_samples_per_second": 5.483,
      "eval_steps_per_second": 0.685,
      "step": 134
    },
    {
      "epoch": 1.0075,
      "grad_norm": 21.8048095703125,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 6.3921,
      "step": 135
    },
    {
      "epoch": 1.015,
      "grad_norm": 20.718963623046875,
      "learning_rate": 1.328358208955224e-05,
      "loss": 6.7405,
      "step": 136
    },
    {
      "epoch": 1.0225,
      "grad_norm": 41.14490509033203,
      "learning_rate": 1.3233830845771145e-05,
      "loss": 11.6148,
      "step": 137
    },
    {
      "epoch": 1.03,
      "grad_norm": 43.570220947265625,
      "learning_rate": 1.3184079601990052e-05,
      "loss": 11.0028,
      "step": 138
    },
    {
      "epoch": 1.0375,
      "grad_norm": 23.1131534576416,
      "learning_rate": 1.3134328358208957e-05,
      "loss": 5.1414,
      "step": 139
    },
    {
      "epoch": 1.045,
      "grad_norm": 39.703094482421875,
      "learning_rate": 1.3084577114427862e-05,
      "loss": 5.1456,
      "step": 140
    },
    {
      "epoch": 1.0525,
      "grad_norm": 15.093818664550781,
      "learning_rate": 1.3034825870646768e-05,
      "loss": 8.071,
      "step": 141
    },
    {
      "epoch": 1.06,
      "grad_norm": 17.311384201049805,
      "learning_rate": 1.2985074626865673e-05,
      "loss": 7.1104,
      "step": 142
    },
    {
      "epoch": 1.0675,
      "grad_norm": 23.477659225463867,
      "learning_rate": 1.293532338308458e-05,
      "loss": 3.1257,
      "step": 143
    },
    {
      "epoch": 1.075,
      "grad_norm": 38.438724517822266,
      "learning_rate": 1.2885572139303483e-05,
      "loss": 5.4707,
      "step": 144
    },
    {
      "epoch": 1.0825,
      "grad_norm": 23.616397857666016,
      "learning_rate": 1.2835820895522388e-05,
      "loss": 6.5425,
      "step": 145
    },
    {
      "epoch": 1.09,
      "grad_norm": 12.37309741973877,
      "learning_rate": 1.2786069651741295e-05,
      "loss": 4.6904,
      "step": 146
    },
    {
      "epoch": 1.0975,
      "grad_norm": 21.663408279418945,
      "learning_rate": 1.27363184079602e-05,
      "loss": 9.1506,
      "step": 147
    },
    {
      "epoch": 1.105,
      "grad_norm": 34.098243713378906,
      "learning_rate": 1.2686567164179107e-05,
      "loss": 8.9672,
      "step": 148
    },
    {
      "epoch": 1.1125,
      "grad_norm": 23.548728942871094,
      "learning_rate": 1.2636815920398011e-05,
      "loss": 6.8627,
      "step": 149
    },
    {
      "epoch": 1.12,
      "grad_norm": 35.71479797363281,
      "learning_rate": 1.2587064676616916e-05,
      "loss": 5.4465,
      "step": 150
    },
    {
      "epoch": 1.1275,
      "grad_norm": 51.86865234375,
      "learning_rate": 1.2537313432835823e-05,
      "loss": 8.2045,
      "step": 151
    },
    {
      "epoch": 1.135,
      "grad_norm": 20.872970581054688,
      "learning_rate": 1.2487562189054726e-05,
      "loss": 8.1157,
      "step": 152
    },
    {
      "epoch": 1.1425,
      "grad_norm": 26.452125549316406,
      "learning_rate": 1.2437810945273631e-05,
      "loss": 7.7974,
      "step": 153
    },
    {
      "epoch": 1.15,
      "grad_norm": 43.17756652832031,
      "learning_rate": 1.2388059701492538e-05,
      "loss": 7.8567,
      "step": 154
    },
    {
      "epoch": 1.1575,
      "grad_norm": 18.319242477416992,
      "learning_rate": 1.2338308457711443e-05,
      "loss": 9.4412,
      "step": 155
    },
    {
      "epoch": 1.165,
      "grad_norm": 21.13210105895996,
      "learning_rate": 1.228855721393035e-05,
      "loss": 8.0804,
      "step": 156
    },
    {
      "epoch": 1.1724999999999999,
      "grad_norm": 38.68110275268555,
      "learning_rate": 1.2238805970149255e-05,
      "loss": 8.5737,
      "step": 157
    },
    {
      "epoch": 1.18,
      "grad_norm": 18.55171775817871,
      "learning_rate": 1.218905472636816e-05,
      "loss": 7.7891,
      "step": 158
    },
    {
      "epoch": 1.1875,
      "grad_norm": 15.467097282409668,
      "learning_rate": 1.2139303482587066e-05,
      "loss": 8.296,
      "step": 159
    },
    {
      "epoch": 1.195,
      "grad_norm": 16.02373695373535,
      "learning_rate": 1.2089552238805971e-05,
      "loss": 8.0142,
      "step": 160
    },
    {
      "epoch": 1.2025000000000001,
      "grad_norm": 15.03539752960205,
      "learning_rate": 1.2039800995024878e-05,
      "loss": 5.1453,
      "step": 161
    },
    {
      "epoch": 1.21,
      "grad_norm": 13.223039627075195,
      "learning_rate": 1.1990049751243781e-05,
      "loss": 4.2064,
      "step": 162
    },
    {
      "epoch": 1.2175,
      "grad_norm": 29.921091079711914,
      "learning_rate": 1.1940298507462686e-05,
      "loss": 10.1191,
      "step": 163
    },
    {
      "epoch": 1.225,
      "grad_norm": 15.049988746643066,
      "learning_rate": 1.1890547263681593e-05,
      "loss": 7.3569,
      "step": 164
    },
    {
      "epoch": 1.2325,
      "grad_norm": 23.30156707763672,
      "learning_rate": 1.1840796019900498e-05,
      "loss": 6.0619,
      "step": 165
    },
    {
      "epoch": 1.24,
      "grad_norm": 17.493349075317383,
      "learning_rate": 1.1791044776119405e-05,
      "loss": 6.7918,
      "step": 166
    },
    {
      "epoch": 1.2475,
      "grad_norm": 22.206836700439453,
      "learning_rate": 1.174129353233831e-05,
      "loss": 7.9917,
      "step": 167
    },
    {
      "epoch": 1.255,
      "grad_norm": 15.1398286819458,
      "learning_rate": 1.1691542288557215e-05,
      "loss": 4.9086,
      "step": 168
    },
    {
      "epoch": 1.2625,
      "grad_norm": 18.365272521972656,
      "learning_rate": 1.1641791044776121e-05,
      "loss": 9.3398,
      "step": 169
    },
    {
      "epoch": 1.27,
      "grad_norm": 15.923727035522461,
      "learning_rate": 1.1592039800995025e-05,
      "loss": 6.0295,
      "step": 170
    },
    {
      "epoch": 1.2775,
      "grad_norm": 50.372737884521484,
      "learning_rate": 1.1542288557213931e-05,
      "loss": 8.266,
      "step": 171
    },
    {
      "epoch": 1.285,
      "grad_norm": 19.067354202270508,
      "learning_rate": 1.1492537313432836e-05,
      "loss": 4.7011,
      "step": 172
    },
    {
      "epoch": 1.2925,
      "grad_norm": 29.11513900756836,
      "learning_rate": 1.1442786069651741e-05,
      "loss": 10.7827,
      "step": 173
    },
    {
      "epoch": 1.3,
      "grad_norm": 16.291074752807617,
      "learning_rate": 1.1393034825870648e-05,
      "loss": 7.0095,
      "step": 174
    },
    {
      "epoch": 1.3075,
      "grad_norm": 28.320255279541016,
      "learning_rate": 1.1343283582089553e-05,
      "loss": 8.6022,
      "step": 175
    },
    {
      "epoch": 1.315,
      "grad_norm": 16.846439361572266,
      "learning_rate": 1.129353233830846e-05,
      "loss": 8.0699,
      "step": 176
    },
    {
      "epoch": 1.3225,
      "grad_norm": 27.560115814208984,
      "learning_rate": 1.1243781094527364e-05,
      "loss": 9.1746,
      "step": 177
    },
    {
      "epoch": 1.33,
      "grad_norm": 45.34199523925781,
      "learning_rate": 1.1194029850746268e-05,
      "loss": 8.5706,
      "step": 178
    },
    {
      "epoch": 1.3375,
      "grad_norm": 21.227651596069336,
      "learning_rate": 1.1144278606965176e-05,
      "loss": 6.9346,
      "step": 179
    },
    {
      "epoch": 1.345,
      "grad_norm": 18.591854095458984,
      "learning_rate": 1.109452736318408e-05,
      "loss": 6.4836,
      "step": 180
    },
    {
      "epoch": 1.3525,
      "grad_norm": 33.59591293334961,
      "learning_rate": 1.1044776119402986e-05,
      "loss": 6.4785,
      "step": 181
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 15.004874229431152,
      "learning_rate": 1.0995024875621891e-05,
      "loss": 5.7717,
      "step": 182
    },
    {
      "epoch": 1.3675,
      "grad_norm": 20.88623046875,
      "learning_rate": 1.0945273631840796e-05,
      "loss": 6.9021,
      "step": 183
    },
    {
      "epoch": 1.375,
      "grad_norm": 31.94795799255371,
      "learning_rate": 1.0895522388059703e-05,
      "loss": 7.0432,
      "step": 184
    },
    {
      "epoch": 1.3825,
      "grad_norm": 31.60441017150879,
      "learning_rate": 1.0845771144278608e-05,
      "loss": 6.6302,
      "step": 185
    },
    {
      "epoch": 1.3900000000000001,
      "grad_norm": 38.355812072753906,
      "learning_rate": 1.0796019900497514e-05,
      "loss": 9.3704,
      "step": 186
    },
    {
      "epoch": 1.3975,
      "grad_norm": 19.245220184326172,
      "learning_rate": 1.074626865671642e-05,
      "loss": 7.2663,
      "step": 187
    },
    {
      "epoch": 1.405,
      "grad_norm": 17.552947998046875,
      "learning_rate": 1.0696517412935323e-05,
      "loss": 7.0701,
      "step": 188
    },
    {
      "epoch": 1.4125,
      "grad_norm": 16.885391235351562,
      "learning_rate": 1.064676616915423e-05,
      "loss": 7.1342,
      "step": 189
    },
    {
      "epoch": 1.42,
      "grad_norm": 29.486068725585938,
      "learning_rate": 1.0597014925373134e-05,
      "loss": 7.6915,
      "step": 190
    },
    {
      "epoch": 1.4275,
      "grad_norm": 14.13184642791748,
      "learning_rate": 1.0547263681592041e-05,
      "loss": 7.136,
      "step": 191
    },
    {
      "epoch": 1.435,
      "grad_norm": 25.83045196533203,
      "learning_rate": 1.0497512437810946e-05,
      "loss": 7.0332,
      "step": 192
    },
    {
      "epoch": 1.4425,
      "grad_norm": 31.09378433227539,
      "learning_rate": 1.0447761194029851e-05,
      "loss": 8.6336,
      "step": 193
    },
    {
      "epoch": 1.45,
      "grad_norm": 31.41608428955078,
      "learning_rate": 1.0398009950248758e-05,
      "loss": 6.8896,
      "step": 194
    },
    {
      "epoch": 1.4575,
      "grad_norm": 13.283350944519043,
      "learning_rate": 1.0348258706467663e-05,
      "loss": 7.0564,
      "step": 195
    },
    {
      "epoch": 1.465,
      "grad_norm": 19.342914581298828,
      "learning_rate": 1.029850746268657e-05,
      "loss": 7.0164,
      "step": 196
    },
    {
      "epoch": 1.4725,
      "grad_norm": 54.5343132019043,
      "learning_rate": 1.0248756218905474e-05,
      "loss": 11.7991,
      "step": 197
    },
    {
      "epoch": 1.48,
      "grad_norm": 15.834651947021484,
      "learning_rate": 1.0199004975124378e-05,
      "loss": 7.0449,
      "step": 198
    },
    {
      "epoch": 1.4875,
      "grad_norm": 14.650225639343262,
      "learning_rate": 1.0149253731343284e-05,
      "loss": 5.9071,
      "step": 199
    },
    {
      "epoch": 1.495,
      "grad_norm": 25.983592987060547,
      "learning_rate": 1.009950248756219e-05,
      "loss": 9.8242,
      "step": 200
    },
    {
      "epoch": 1.5025,
      "grad_norm": 16.385913848876953,
      "learning_rate": 1.0049751243781096e-05,
      "loss": 7.9482,
      "step": 201
    },
    {
      "epoch": 1.51,
      "grad_norm": 51.866554260253906,
      "learning_rate": 1e-05,
      "loss": 8.3957,
      "step": 202
    },
    {
      "epoch": 1.5175,
      "grad_norm": 20.66267967224121,
      "learning_rate": 9.950248756218906e-06,
      "loss": 6.2703,
      "step": 203
    },
    {
      "epoch": 1.525,
      "grad_norm": 21.913820266723633,
      "learning_rate": 9.900497512437812e-06,
      "loss": 2.7396,
      "step": 204
    },
    {
      "epoch": 1.5325,
      "grad_norm": 29.040348052978516,
      "learning_rate": 9.850746268656717e-06,
      "loss": 6.8386,
      "step": 205
    },
    {
      "epoch": 1.54,
      "grad_norm": 28.937885284423828,
      "learning_rate": 9.800995024875622e-06,
      "loss": 7.9331,
      "step": 206
    },
    {
      "epoch": 1.5474999999999999,
      "grad_norm": 39.64885711669922,
      "learning_rate": 9.751243781094527e-06,
      "loss": 6.8863,
      "step": 207
    },
    {
      "epoch": 1.5550000000000002,
      "grad_norm": 8.984557151794434,
      "learning_rate": 9.701492537313434e-06,
      "loss": 2.4821,
      "step": 208
    },
    {
      "epoch": 1.5625,
      "grad_norm": 34.3919563293457,
      "learning_rate": 9.651741293532339e-06,
      "loss": 7.995,
      "step": 209
    },
    {
      "epoch": 1.5699999999999998,
      "grad_norm": 30.613805770874023,
      "learning_rate": 9.601990049751244e-06,
      "loss": 8.297,
      "step": 210
    },
    {
      "epoch": 1.5775000000000001,
      "grad_norm": 40.95920944213867,
      "learning_rate": 9.552238805970149e-06,
      "loss": 9.0611,
      "step": 211
    },
    {
      "epoch": 1.585,
      "grad_norm": 13.995222091674805,
      "learning_rate": 9.502487562189056e-06,
      "loss": 6.7658,
      "step": 212
    },
    {
      "epoch": 1.5925,
      "grad_norm": 28.78339958190918,
      "learning_rate": 9.45273631840796e-06,
      "loss": 8.0583,
      "step": 213
    },
    {
      "epoch": 1.6,
      "grad_norm": 22.00124168395996,
      "learning_rate": 9.402985074626867e-06,
      "loss": 9.9709,
      "step": 214
    },
    {
      "epoch": 1.6075,
      "grad_norm": 34.22935485839844,
      "learning_rate": 9.353233830845772e-06,
      "loss": 9.7541,
      "step": 215
    },
    {
      "epoch": 1.615,
      "grad_norm": 19.476308822631836,
      "learning_rate": 9.303482587064677e-06,
      "loss": 9.1947,
      "step": 216
    },
    {
      "epoch": 1.6225,
      "grad_norm": 23.778493881225586,
      "learning_rate": 9.253731343283582e-06,
      "loss": 7.3918,
      "step": 217
    },
    {
      "epoch": 1.63,
      "grad_norm": 30.43560028076172,
      "learning_rate": 9.203980099502489e-06,
      "loss": 9.8984,
      "step": 218
    },
    {
      "epoch": 1.6375,
      "grad_norm": 28.568025588989258,
      "learning_rate": 9.154228855721394e-06,
      "loss": 10.2383,
      "step": 219
    },
    {
      "epoch": 1.645,
      "grad_norm": 15.705120086669922,
      "learning_rate": 9.104477611940299e-06,
      "loss": 7.2642,
      "step": 220
    },
    {
      "epoch": 1.6524999999999999,
      "grad_norm": 19.00364875793457,
      "learning_rate": 9.054726368159204e-06,
      "loss": 7.035,
      "step": 221
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 24.272581100463867,
      "learning_rate": 9.00497512437811e-06,
      "loss": 7.1954,
      "step": 222
    },
    {
      "epoch": 1.6675,
      "grad_norm": 10.920378684997559,
      "learning_rate": 8.955223880597016e-06,
      "loss": 4.0554,
      "step": 223
    },
    {
      "epoch": 1.675,
      "grad_norm": 23.410737991333008,
      "learning_rate": 8.905472636815922e-06,
      "loss": 6.415,
      "step": 224
    },
    {
      "epoch": 1.6825,
      "grad_norm": 17.936656951904297,
      "learning_rate": 8.855721393034826e-06,
      "loss": 6.0977,
      "step": 225
    },
    {
      "epoch": 1.69,
      "grad_norm": 23.733154296875,
      "learning_rate": 8.805970149253732e-06,
      "loss": 6.8486,
      "step": 226
    },
    {
      "epoch": 1.6975,
      "grad_norm": 22.041576385498047,
      "learning_rate": 8.756218905472637e-06,
      "loss": 7.9913,
      "step": 227
    },
    {
      "epoch": 1.705,
      "grad_norm": 25.79068374633789,
      "learning_rate": 8.706467661691544e-06,
      "loss": 6.1055,
      "step": 228
    },
    {
      "epoch": 1.7125,
      "grad_norm": 21.873388290405273,
      "learning_rate": 8.656716417910447e-06,
      "loss": 8.2784,
      "step": 229
    },
    {
      "epoch": 1.72,
      "grad_norm": 26.4804630279541,
      "learning_rate": 8.606965174129354e-06,
      "loss": 11.3739,
      "step": 230
    },
    {
      "epoch": 1.7275,
      "grad_norm": 14.879560470581055,
      "learning_rate": 8.557213930348259e-06,
      "loss": 5.2851,
      "step": 231
    },
    {
      "epoch": 1.7349999999999999,
      "grad_norm": 30.33489418029785,
      "learning_rate": 8.507462686567165e-06,
      "loss": 9.3843,
      "step": 232
    },
    {
      "epoch": 1.7425000000000002,
      "grad_norm": 27.194686889648438,
      "learning_rate": 8.45771144278607e-06,
      "loss": 8.781,
      "step": 233
    },
    {
      "epoch": 1.75,
      "grad_norm": 10.686748504638672,
      "learning_rate": 8.407960199004975e-06,
      "loss": 4.6982,
      "step": 234
    },
    {
      "epoch": 1.7574999999999998,
      "grad_norm": 13.320880889892578,
      "learning_rate": 8.35820895522388e-06,
      "loss": 4.597,
      "step": 235
    },
    {
      "epoch": 1.7650000000000001,
      "grad_norm": 16.72747230529785,
      "learning_rate": 8.308457711442787e-06,
      "loss": 5.7311,
      "step": 236
    },
    {
      "epoch": 1.7725,
      "grad_norm": 26.784889221191406,
      "learning_rate": 8.258706467661692e-06,
      "loss": 3.8557,
      "step": 237
    },
    {
      "epoch": 1.78,
      "grad_norm": 23.28192710876465,
      "learning_rate": 8.208955223880599e-06,
      "loss": 9.3301,
      "step": 238
    },
    {
      "epoch": 1.7875,
      "grad_norm": 37.040016174316406,
      "learning_rate": 8.159203980099502e-06,
      "loss": 7.9634,
      "step": 239
    },
    {
      "epoch": 1.795,
      "grad_norm": 25.40415382385254,
      "learning_rate": 8.109452736318409e-06,
      "loss": 9.2003,
      "step": 240
    },
    {
      "epoch": 1.8025,
      "grad_norm": 18.693437576293945,
      "learning_rate": 8.059701492537314e-06,
      "loss": 8.8829,
      "step": 241
    },
    {
      "epoch": 1.81,
      "grad_norm": 22.387331008911133,
      "learning_rate": 8.00995024875622e-06,
      "loss": 8.1176,
      "step": 242
    },
    {
      "epoch": 1.8175,
      "grad_norm": 38.62077331542969,
      "learning_rate": 7.960199004975125e-06,
      "loss": 8.217,
      "step": 243
    },
    {
      "epoch": 1.825,
      "grad_norm": 17.523710250854492,
      "learning_rate": 7.91044776119403e-06,
      "loss": 6.4758,
      "step": 244
    },
    {
      "epoch": 1.8325,
      "grad_norm": 19.60209083557129,
      "learning_rate": 7.860696517412935e-06,
      "loss": 9.3735,
      "step": 245
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 19.52773094177246,
      "learning_rate": 7.810945273631842e-06,
      "loss": 4.5269,
      "step": 246
    },
    {
      "epoch": 1.8475000000000001,
      "grad_norm": 23.468578338623047,
      "learning_rate": 7.761194029850747e-06,
      "loss": 8.4642,
      "step": 247
    },
    {
      "epoch": 1.855,
      "grad_norm": 38.496212005615234,
      "learning_rate": 7.711442786069654e-06,
      "loss": 5.5751,
      "step": 248
    },
    {
      "epoch": 1.8625,
      "grad_norm": 26.47999382019043,
      "learning_rate": 7.661691542288557e-06,
      "loss": 7.202,
      "step": 249
    },
    {
      "epoch": 1.87,
      "grad_norm": 29.785980224609375,
      "learning_rate": 7.611940298507463e-06,
      "loss": 5.5261,
      "step": 250
    },
    {
      "epoch": 1.8775,
      "grad_norm": 29.13228988647461,
      "learning_rate": 7.5621890547263685e-06,
      "loss": 10.376,
      "step": 251
    },
    {
      "epoch": 1.885,
      "grad_norm": 27.107797622680664,
      "learning_rate": 7.512437810945274e-06,
      "loss": 3.8005,
      "step": 252
    },
    {
      "epoch": 1.8925,
      "grad_norm": 11.739290237426758,
      "learning_rate": 7.46268656716418e-06,
      "loss": 5.0328,
      "step": 253
    },
    {
      "epoch": 1.9,
      "grad_norm": 20.702102661132812,
      "learning_rate": 7.412935323383084e-06,
      "loss": 7.6542,
      "step": 254
    },
    {
      "epoch": 1.9075,
      "grad_norm": 58.79525375366211,
      "learning_rate": 7.36318407960199e-06,
      "loss": 8.5974,
      "step": 255
    },
    {
      "epoch": 1.915,
      "grad_norm": 75.39005279541016,
      "learning_rate": 7.313432835820896e-06,
      "loss": 10.8879,
      "step": 256
    },
    {
      "epoch": 1.9224999999999999,
      "grad_norm": 15.361435890197754,
      "learning_rate": 7.263681592039802e-06,
      "loss": 5.3811,
      "step": 257
    },
    {
      "epoch": 1.9300000000000002,
      "grad_norm": 19.373353958129883,
      "learning_rate": 7.213930348258708e-06,
      "loss": 8.3788,
      "step": 258
    },
    {
      "epoch": 1.9375,
      "grad_norm": 30.656991958618164,
      "learning_rate": 7.164179104477612e-06,
      "loss": 6.1895,
      "step": 259
    },
    {
      "epoch": 1.9449999999999998,
      "grad_norm": 19.340845108032227,
      "learning_rate": 7.114427860696518e-06,
      "loss": 7.5255,
      "step": 260
    },
    {
      "epoch": 1.9525000000000001,
      "grad_norm": 19.651626586914062,
      "learning_rate": 7.064676616915423e-06,
      "loss": 8.3727,
      "step": 261
    },
    {
      "epoch": 1.96,
      "grad_norm": 23.2816219329834,
      "learning_rate": 7.014925373134329e-06,
      "loss": 7.0986,
      "step": 262
    },
    {
      "epoch": 1.9675,
      "grad_norm": 21.81993293762207,
      "learning_rate": 6.965174129353235e-06,
      "loss": 6.0899,
      "step": 263
    },
    {
      "epoch": 1.975,
      "grad_norm": 18.18334197998047,
      "learning_rate": 6.915422885572139e-06,
      "loss": 7.1409,
      "step": 264
    },
    {
      "epoch": 1.9825,
      "grad_norm": 12.818204879760742,
      "learning_rate": 6.865671641791045e-06,
      "loss": 4.7568,
      "step": 265
    },
    {
      "epoch": 1.99,
      "grad_norm": 15.351046562194824,
      "learning_rate": 6.815920398009951e-06,
      "loss": 7.7841,
      "step": 266
    },
    {
      "epoch": 1.9975,
      "grad_norm": 20.022409439086914,
      "learning_rate": 6.766169154228857e-06,
      "loss": 7.8487,
      "step": 267
    },
    {
      "epoch": 2.0,
      "grad_norm": 29.011016845703125,
      "learning_rate": 6.7164179104477625e-06,
      "loss": 2.2981,
      "step": 268
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.655,
      "eval_f1": 0.6208791208791209,
      "eval_loss": 0.5813692212104797,
      "eval_runtime": 72.2443,
      "eval_samples_per_second": 5.537,
      "eval_steps_per_second": 0.692,
      "step": 268
    },
    {
      "epoch": 2.0075,
      "grad_norm": 27.92513656616211,
      "learning_rate": 6.666666666666667e-06,
      "loss": 5.8526,
      "step": 269
    },
    {
      "epoch": 2.015,
      "grad_norm": 19.649728775024414,
      "learning_rate": 6.6169154228855725e-06,
      "loss": 8.1204,
      "step": 270
    },
    {
      "epoch": 2.0225,
      "grad_norm": 30.78244400024414,
      "learning_rate": 6.567164179104478e-06,
      "loss": 7.7061,
      "step": 271
    },
    {
      "epoch": 2.03,
      "grad_norm": 16.941158294677734,
      "learning_rate": 6.517412935323384e-06,
      "loss": 5.1196,
      "step": 272
    },
    {
      "epoch": 2.0375,
      "grad_norm": 15.178689956665039,
      "learning_rate": 6.46766169154229e-06,
      "loss": 7.7612,
      "step": 273
    },
    {
      "epoch": 2.045,
      "grad_norm": 58.18625259399414,
      "learning_rate": 6.417910447761194e-06,
      "loss": 6.8004,
      "step": 274
    },
    {
      "epoch": 2.0525,
      "grad_norm": 27.602741241455078,
      "learning_rate": 6.3681592039801e-06,
      "loss": 6.262,
      "step": 275
    },
    {
      "epoch": 2.06,
      "grad_norm": 24.349260330200195,
      "learning_rate": 6.318407960199006e-06,
      "loss": 9.5775,
      "step": 276
    },
    {
      "epoch": 2.0675,
      "grad_norm": 11.637301445007324,
      "learning_rate": 6.2686567164179116e-06,
      "loss": 4.458,
      "step": 277
    },
    {
      "epoch": 2.075,
      "grad_norm": 27.13911247253418,
      "learning_rate": 6.218905472636816e-06,
      "loss": 8.7701,
      "step": 278
    },
    {
      "epoch": 2.0825,
      "grad_norm": 44.18229675292969,
      "learning_rate": 6.1691542288557215e-06,
      "loss": 7.6149,
      "step": 279
    },
    {
      "epoch": 2.09,
      "grad_norm": 25.5086612701416,
      "learning_rate": 6.119402985074627e-06,
      "loss": 6.038,
      "step": 280
    },
    {
      "epoch": 2.0975,
      "grad_norm": 18.22816276550293,
      "learning_rate": 6.069651741293533e-06,
      "loss": 8.7931,
      "step": 281
    },
    {
      "epoch": 2.105,
      "grad_norm": 16.139141082763672,
      "learning_rate": 6.019900497512439e-06,
      "loss": 3.373,
      "step": 282
    },
    {
      "epoch": 2.1125,
      "grad_norm": 27.962553024291992,
      "learning_rate": 5.970149253731343e-06,
      "loss": 7.0459,
      "step": 283
    },
    {
      "epoch": 2.12,
      "grad_norm": 61.66249084472656,
      "learning_rate": 5.920398009950249e-06,
      "loss": 11.9463,
      "step": 284
    },
    {
      "epoch": 2.1275,
      "grad_norm": 32.5447883605957,
      "learning_rate": 5.870646766169155e-06,
      "loss": 8.5886,
      "step": 285
    },
    {
      "epoch": 2.135,
      "grad_norm": 41.16959762573242,
      "learning_rate": 5.820895522388061e-06,
      "loss": 7.1619,
      "step": 286
    },
    {
      "epoch": 2.1425,
      "grad_norm": 11.927928924560547,
      "learning_rate": 5.771144278606966e-06,
      "loss": 4.1069,
      "step": 287
    },
    {
      "epoch": 2.15,
      "grad_norm": 23.675893783569336,
      "learning_rate": 5.721393034825871e-06,
      "loss": 4.24,
      "step": 288
    },
    {
      "epoch": 2.1575,
      "grad_norm": 45.987266540527344,
      "learning_rate": 5.671641791044776e-06,
      "loss": 6.7293,
      "step": 289
    },
    {
      "epoch": 2.165,
      "grad_norm": 37.33071517944336,
      "learning_rate": 5.621890547263682e-06,
      "loss": 6.7755,
      "step": 290
    },
    {
      "epoch": 2.1725,
      "grad_norm": 18.371234893798828,
      "learning_rate": 5.572139303482588e-06,
      "loss": 8.0917,
      "step": 291
    },
    {
      "epoch": 2.18,
      "grad_norm": 17.062213897705078,
      "learning_rate": 5.522388059701493e-06,
      "loss": 5.2069,
      "step": 292
    },
    {
      "epoch": 2.1875,
      "grad_norm": 18.505817413330078,
      "learning_rate": 5.472636815920398e-06,
      "loss": 7.0417,
      "step": 293
    },
    {
      "epoch": 2.195,
      "grad_norm": 43.75213623046875,
      "learning_rate": 5.422885572139304e-06,
      "loss": 9.0889,
      "step": 294
    },
    {
      "epoch": 2.2025,
      "grad_norm": 34.92008590698242,
      "learning_rate": 5.37313432835821e-06,
      "loss": 8.2789,
      "step": 295
    },
    {
      "epoch": 2.21,
      "grad_norm": 23.5954647064209,
      "learning_rate": 5.323383084577115e-06,
      "loss": 5.3543,
      "step": 296
    },
    {
      "epoch": 2.2175,
      "grad_norm": 22.844501495361328,
      "learning_rate": 5.2736318407960205e-06,
      "loss": 4.2404,
      "step": 297
    },
    {
      "epoch": 2.225,
      "grad_norm": 35.163185119628906,
      "learning_rate": 5.2238805970149255e-06,
      "loss": 6.269,
      "step": 298
    },
    {
      "epoch": 2.2325,
      "grad_norm": 105.14207458496094,
      "learning_rate": 5.174129353233831e-06,
      "loss": 7.9783,
      "step": 299
    },
    {
      "epoch": 2.24,
      "grad_norm": 15.979594230651855,
      "learning_rate": 5.124378109452737e-06,
      "loss": 5.315,
      "step": 300
    },
    {
      "epoch": 2.2475,
      "grad_norm": 40.121707916259766,
      "learning_rate": 5.074626865671642e-06,
      "loss": 4.4762,
      "step": 301
    },
    {
      "epoch": 2.255,
      "grad_norm": 14.150940895080566,
      "learning_rate": 5.024875621890548e-06,
      "loss": 4.8108,
      "step": 302
    },
    {
      "epoch": 2.2625,
      "grad_norm": 18.21945571899414,
      "learning_rate": 4.975124378109453e-06,
      "loss": 5.6331,
      "step": 303
    },
    {
      "epoch": 2.27,
      "grad_norm": 39.7863655090332,
      "learning_rate": 4.925373134328359e-06,
      "loss": 5.2362,
      "step": 304
    },
    {
      "epoch": 2.2775,
      "grad_norm": 17.06087303161621,
      "learning_rate": 4.875621890547264e-06,
      "loss": 6.6743,
      "step": 305
    },
    {
      "epoch": 2.285,
      "grad_norm": 21.298851013183594,
      "learning_rate": 4.8258706467661695e-06,
      "loss": 5.6479,
      "step": 306
    },
    {
      "epoch": 2.2925,
      "grad_norm": 24.53784942626953,
      "learning_rate": 4.7761194029850745e-06,
      "loss": 4.8649,
      "step": 307
    },
    {
      "epoch": 2.3,
      "grad_norm": 14.416994094848633,
      "learning_rate": 4.72636815920398e-06,
      "loss": 5.844,
      "step": 308
    },
    {
      "epoch": 2.3075,
      "grad_norm": 31.77495765686035,
      "learning_rate": 4.676616915422886e-06,
      "loss": 8.2687,
      "step": 309
    },
    {
      "epoch": 2.315,
      "grad_norm": 33.72322463989258,
      "learning_rate": 4.626865671641791e-06,
      "loss": 8.9749,
      "step": 310
    },
    {
      "epoch": 2.3225,
      "grad_norm": 16.146419525146484,
      "learning_rate": 4.577114427860697e-06,
      "loss": 6.4827,
      "step": 311
    },
    {
      "epoch": 2.33,
      "grad_norm": 42.75928497314453,
      "learning_rate": 4.527363184079602e-06,
      "loss": 8.8948,
      "step": 312
    },
    {
      "epoch": 2.3375,
      "grad_norm": 33.322879791259766,
      "learning_rate": 4.477611940298508e-06,
      "loss": 8.3086,
      "step": 313
    },
    {
      "epoch": 2.3449999999999998,
      "grad_norm": 19.981218338012695,
      "learning_rate": 4.427860696517413e-06,
      "loss": 6.6927,
      "step": 314
    },
    {
      "epoch": 2.3525,
      "grad_norm": 13.085461616516113,
      "learning_rate": 4.378109452736319e-06,
      "loss": 5.0097,
      "step": 315
    },
    {
      "epoch": 2.36,
      "grad_norm": 24.204689025878906,
      "learning_rate": 4.3283582089552236e-06,
      "loss": 5.1235,
      "step": 316
    },
    {
      "epoch": 2.3675,
      "grad_norm": 21.232023239135742,
      "learning_rate": 4.278606965174129e-06,
      "loss": 7.6823,
      "step": 317
    },
    {
      "epoch": 2.375,
      "grad_norm": 39.088443756103516,
      "learning_rate": 4.228855721393035e-06,
      "loss": 8.0951,
      "step": 318
    },
    {
      "epoch": 2.3825,
      "grad_norm": 16.101757049560547,
      "learning_rate": 4.17910447761194e-06,
      "loss": 5.2405,
      "step": 319
    },
    {
      "epoch": 2.39,
      "grad_norm": 46.26877212524414,
      "learning_rate": 4.129353233830846e-06,
      "loss": 7.0149,
      "step": 320
    },
    {
      "epoch": 2.3975,
      "grad_norm": 34.05463790893555,
      "learning_rate": 4.079601990049751e-06,
      "loss": 7.5565,
      "step": 321
    },
    {
      "epoch": 2.4050000000000002,
      "grad_norm": 24.26848793029785,
      "learning_rate": 4.029850746268657e-06,
      "loss": 5.2248,
      "step": 322
    },
    {
      "epoch": 2.4125,
      "grad_norm": 25.347814559936523,
      "learning_rate": 3.980099502487563e-06,
      "loss": 8.5067,
      "step": 323
    },
    {
      "epoch": 2.42,
      "grad_norm": 32.84894561767578,
      "learning_rate": 3.930348258706468e-06,
      "loss": 7.8065,
      "step": 324
    },
    {
      "epoch": 2.4275,
      "grad_norm": 22.245576858520508,
      "learning_rate": 3.8805970149253735e-06,
      "loss": 8.195,
      "step": 325
    },
    {
      "epoch": 2.435,
      "grad_norm": 16.769771575927734,
      "learning_rate": 3.8308457711442784e-06,
      "loss": 4.7903,
      "step": 326
    },
    {
      "epoch": 2.4425,
      "grad_norm": 13.846964836120605,
      "learning_rate": 3.7810945273631843e-06,
      "loss": 5.9638,
      "step": 327
    },
    {
      "epoch": 2.45,
      "grad_norm": 23.30636978149414,
      "learning_rate": 3.73134328358209e-06,
      "loss": 6.2619,
      "step": 328
    },
    {
      "epoch": 2.4575,
      "grad_norm": 14.06575870513916,
      "learning_rate": 3.681592039800995e-06,
      "loss": 6.1876,
      "step": 329
    },
    {
      "epoch": 2.465,
      "grad_norm": 19.100543975830078,
      "learning_rate": 3.631840796019901e-06,
      "loss": 6.9697,
      "step": 330
    },
    {
      "epoch": 2.4725,
      "grad_norm": 17.15780258178711,
      "learning_rate": 3.582089552238806e-06,
      "loss": 6.928,
      "step": 331
    },
    {
      "epoch": 2.48,
      "grad_norm": 37.717872619628906,
      "learning_rate": 3.5323383084577117e-06,
      "loss": 6.1535,
      "step": 332
    },
    {
      "epoch": 2.4875,
      "grad_norm": 17.808786392211914,
      "learning_rate": 3.4825870646766175e-06,
      "loss": 7.4137,
      "step": 333
    },
    {
      "epoch": 2.495,
      "grad_norm": 47.131858825683594,
      "learning_rate": 3.4328358208955225e-06,
      "loss": 7.2585,
      "step": 334
    },
    {
      "epoch": 2.5025,
      "grad_norm": 33.68073272705078,
      "learning_rate": 3.3830845771144283e-06,
      "loss": 9.5108,
      "step": 335
    },
    {
      "epoch": 2.51,
      "grad_norm": 30.105791091918945,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 7.0461,
      "step": 336
    },
    {
      "epoch": 2.5175,
      "grad_norm": 15.2796049118042,
      "learning_rate": 3.283582089552239e-06,
      "loss": 6.7541,
      "step": 337
    },
    {
      "epoch": 2.525,
      "grad_norm": 13.694021224975586,
      "learning_rate": 3.233830845771145e-06,
      "loss": 6.9034,
      "step": 338
    },
    {
      "epoch": 2.5324999999999998,
      "grad_norm": 17.34837532043457,
      "learning_rate": 3.18407960199005e-06,
      "loss": 5.4899,
      "step": 339
    },
    {
      "epoch": 2.54,
      "grad_norm": 39.77907943725586,
      "learning_rate": 3.1343283582089558e-06,
      "loss": 4.6717,
      "step": 340
    },
    {
      "epoch": 2.5475,
      "grad_norm": 19.898359298706055,
      "learning_rate": 3.0845771144278608e-06,
      "loss": 6.5044,
      "step": 341
    },
    {
      "epoch": 2.555,
      "grad_norm": 34.56034469604492,
      "learning_rate": 3.0348258706467666e-06,
      "loss": 8.3383,
      "step": 342
    },
    {
      "epoch": 2.5625,
      "grad_norm": 22.13727378845215,
      "learning_rate": 2.9850746268656716e-06,
      "loss": 7.163,
      "step": 343
    },
    {
      "epoch": 2.57,
      "grad_norm": 33.465797424316406,
      "learning_rate": 2.9353233830845774e-06,
      "loss": 10.0765,
      "step": 344
    },
    {
      "epoch": 2.5775,
      "grad_norm": 27.996877670288086,
      "learning_rate": 2.885572139303483e-06,
      "loss": 6.5268,
      "step": 345
    },
    {
      "epoch": 2.585,
      "grad_norm": 28.954301834106445,
      "learning_rate": 2.835820895522388e-06,
      "loss": 6.9342,
      "step": 346
    },
    {
      "epoch": 2.5925000000000002,
      "grad_norm": 50.0579719543457,
      "learning_rate": 2.786069651741294e-06,
      "loss": 8.3497,
      "step": 347
    },
    {
      "epoch": 2.6,
      "grad_norm": 25.3396053314209,
      "learning_rate": 2.736318407960199e-06,
      "loss": 7.2368,
      "step": 348
    },
    {
      "epoch": 2.6075,
      "grad_norm": 13.951797485351562,
      "learning_rate": 2.686567164179105e-06,
      "loss": 7.0402,
      "step": 349
    },
    {
      "epoch": 2.615,
      "grad_norm": 24.54252815246582,
      "learning_rate": 2.6368159203980102e-06,
      "loss": 7.6127,
      "step": 350
    },
    {
      "epoch": 2.6225,
      "grad_norm": 22.850040435791016,
      "learning_rate": 2.5870646766169156e-06,
      "loss": 10.0877,
      "step": 351
    },
    {
      "epoch": 2.63,
      "grad_norm": 28.262842178344727,
      "learning_rate": 2.537313432835821e-06,
      "loss": 9.0086,
      "step": 352
    },
    {
      "epoch": 2.6375,
      "grad_norm": 45.934814453125,
      "learning_rate": 2.4875621890547264e-06,
      "loss": 7.1792,
      "step": 353
    },
    {
      "epoch": 2.645,
      "grad_norm": 21.861915588378906,
      "learning_rate": 2.437810945273632e-06,
      "loss": 5.5939,
      "step": 354
    },
    {
      "epoch": 2.6525,
      "grad_norm": 21.433805465698242,
      "learning_rate": 2.3880597014925373e-06,
      "loss": 6.4404,
      "step": 355
    },
    {
      "epoch": 2.66,
      "grad_norm": 14.362037658691406,
      "learning_rate": 2.338308457711443e-06,
      "loss": 4.6977,
      "step": 356
    },
    {
      "epoch": 2.6675,
      "grad_norm": 16.11347770690918,
      "learning_rate": 2.2885572139303485e-06,
      "loss": 6.6349,
      "step": 357
    },
    {
      "epoch": 2.675,
      "grad_norm": 24.42821502685547,
      "learning_rate": 2.238805970149254e-06,
      "loss": 7.5293,
      "step": 358
    },
    {
      "epoch": 2.6825,
      "grad_norm": 24.716938018798828,
      "learning_rate": 2.1890547263681593e-06,
      "loss": 9.0789,
      "step": 359
    },
    {
      "epoch": 2.69,
      "grad_norm": 33.687259674072266,
      "learning_rate": 2.1393034825870647e-06,
      "loss": 6.7706,
      "step": 360
    },
    {
      "epoch": 2.6975,
      "grad_norm": 19.493064880371094,
      "learning_rate": 2.08955223880597e-06,
      "loss": 6.0921,
      "step": 361
    },
    {
      "epoch": 2.705,
      "grad_norm": 18.744569778442383,
      "learning_rate": 2.0398009950248755e-06,
      "loss": 6.407,
      "step": 362
    },
    {
      "epoch": 2.7125,
      "grad_norm": 29.870006561279297,
      "learning_rate": 1.9900497512437813e-06,
      "loss": 7.6942,
      "step": 363
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 24.33372688293457,
      "learning_rate": 1.9402985074626867e-06,
      "loss": 6.202,
      "step": 364
    },
    {
      "epoch": 2.7275,
      "grad_norm": 19.0799560546875,
      "learning_rate": 1.8905472636815921e-06,
      "loss": 5.9054,
      "step": 365
    },
    {
      "epoch": 2.735,
      "grad_norm": 18.40586280822754,
      "learning_rate": 1.8407960199004975e-06,
      "loss": 6.9785,
      "step": 366
    },
    {
      "epoch": 2.7425,
      "grad_norm": 19.024864196777344,
      "learning_rate": 1.791044776119403e-06,
      "loss": 6.9126,
      "step": 367
    },
    {
      "epoch": 2.75,
      "grad_norm": 14.753730773925781,
      "learning_rate": 1.7412935323383088e-06,
      "loss": 7.5087,
      "step": 368
    },
    {
      "epoch": 2.7575,
      "grad_norm": 20.431522369384766,
      "learning_rate": 1.6915422885572142e-06,
      "loss": 6.2826,
      "step": 369
    },
    {
      "epoch": 2.765,
      "grad_norm": 18.077045440673828,
      "learning_rate": 1.6417910447761196e-06,
      "loss": 7.5815,
      "step": 370
    },
    {
      "epoch": 2.7725,
      "grad_norm": 22.74675750732422,
      "learning_rate": 1.592039800995025e-06,
      "loss": 8.415,
      "step": 371
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 31.44670867919922,
      "learning_rate": 1.5422885572139304e-06,
      "loss": 5.8582,
      "step": 372
    },
    {
      "epoch": 2.7875,
      "grad_norm": 23.782236099243164,
      "learning_rate": 1.4925373134328358e-06,
      "loss": 5.8906,
      "step": 373
    },
    {
      "epoch": 2.795,
      "grad_norm": 16.397533416748047,
      "learning_rate": 1.4427860696517414e-06,
      "loss": 8.0082,
      "step": 374
    },
    {
      "epoch": 2.8025,
      "grad_norm": 20.685470581054688,
      "learning_rate": 1.393034825870647e-06,
      "loss": 7.1744,
      "step": 375
    },
    {
      "epoch": 2.81,
      "grad_norm": 19.927560806274414,
      "learning_rate": 1.3432835820895524e-06,
      "loss": 4.6129,
      "step": 376
    },
    {
      "epoch": 2.8175,
      "grad_norm": 25.658538818359375,
      "learning_rate": 1.2935323383084578e-06,
      "loss": 3.5955,
      "step": 377
    },
    {
      "epoch": 2.825,
      "grad_norm": 15.939899444580078,
      "learning_rate": 1.2437810945273632e-06,
      "loss": 4.4969,
      "step": 378
    },
    {
      "epoch": 2.8325,
      "grad_norm": 22.23604965209961,
      "learning_rate": 1.1940298507462686e-06,
      "loss": 6.4525,
      "step": 379
    },
    {
      "epoch": 2.84,
      "grad_norm": 30.423213958740234,
      "learning_rate": 1.1442786069651742e-06,
      "loss": 7.2661,
      "step": 380
    },
    {
      "epoch": 2.8475,
      "grad_norm": 46.9858283996582,
      "learning_rate": 1.0945273631840796e-06,
      "loss": 6.7571,
      "step": 381
    },
    {
      "epoch": 2.855,
      "grad_norm": 43.56590270996094,
      "learning_rate": 1.044776119402985e-06,
      "loss": 8.9491,
      "step": 382
    },
    {
      "epoch": 2.8625,
      "grad_norm": 39.40342712402344,
      "learning_rate": 9.950248756218907e-07,
      "loss": 6.8771,
      "step": 383
    },
    {
      "epoch": 2.87,
      "grad_norm": 39.743953704833984,
      "learning_rate": 9.452736318407961e-07,
      "loss": 8.9785,
      "step": 384
    },
    {
      "epoch": 2.8775,
      "grad_norm": 15.604808807373047,
      "learning_rate": 8.955223880597015e-07,
      "loss": 6.8194,
      "step": 385
    },
    {
      "epoch": 2.885,
      "grad_norm": 62.56621170043945,
      "learning_rate": 8.457711442786071e-07,
      "loss": 11.1187,
      "step": 386
    },
    {
      "epoch": 2.8925,
      "grad_norm": 33.044246673583984,
      "learning_rate": 7.960199004975125e-07,
      "loss": 7.8764,
      "step": 387
    },
    {
      "epoch": 2.9,
      "grad_norm": 40.33454513549805,
      "learning_rate": 7.462686567164179e-07,
      "loss": 9.6322,
      "step": 388
    },
    {
      "epoch": 2.9074999999999998,
      "grad_norm": 32.08467102050781,
      "learning_rate": 6.965174129353235e-07,
      "loss": 7.777,
      "step": 389
    },
    {
      "epoch": 2.915,
      "grad_norm": 20.28879165649414,
      "learning_rate": 6.467661691542289e-07,
      "loss": 5.6995,
      "step": 390
    },
    {
      "epoch": 2.9225,
      "grad_norm": 33.023712158203125,
      "learning_rate": 5.970149253731343e-07,
      "loss": 7.2051,
      "step": 391
    },
    {
      "epoch": 2.93,
      "grad_norm": 71.31412506103516,
      "learning_rate": 5.472636815920398e-07,
      "loss": 6.2507,
      "step": 392
    },
    {
      "epoch": 2.9375,
      "grad_norm": 24.08648681640625,
      "learning_rate": 4.975124378109453e-07,
      "loss": 4.5637,
      "step": 393
    },
    {
      "epoch": 2.945,
      "grad_norm": 42.333919525146484,
      "learning_rate": 4.4776119402985074e-07,
      "loss": 5.931,
      "step": 394
    },
    {
      "epoch": 2.9525,
      "grad_norm": 22.59855842590332,
      "learning_rate": 3.9800995024875624e-07,
      "loss": 7.2365,
      "step": 395
    },
    {
      "epoch": 2.96,
      "grad_norm": 51.032649993896484,
      "learning_rate": 3.4825870646766175e-07,
      "loss": 6.351,
      "step": 396
    },
    {
      "epoch": 2.9675000000000002,
      "grad_norm": 13.593828201293945,
      "learning_rate": 2.9850746268656716e-07,
      "loss": 5.6185,
      "step": 397
    },
    {
      "epoch": 2.975,
      "grad_norm": 33.60947799682617,
      "learning_rate": 2.4875621890547267e-07,
      "loss": 4.8231,
      "step": 398
    },
    {
      "epoch": 2.9825,
      "grad_norm": 43.2676887512207,
      "learning_rate": 1.9900497512437812e-07,
      "loss": 12.4573,
      "step": 399
    },
    {
      "epoch": 2.99,
      "grad_norm": NaN,
      "learning_rate": 1.4925373134328358e-07,
      "loss": 6.9484,
      "step": 400
    },
    {
      "epoch": 2.9975,
      "grad_norm": 19.893144607543945,
      "learning_rate": 9.950248756218906e-08,
      "loss": 6.5843,
      "step": 401
    },
    {
      "epoch": 3.0,
      "grad_norm": 7.166378498077393,
      "learning_rate": 4.975124378109453e-08,
      "loss": 2.186,
      "step": 402
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.68,
      "eval_f1": 0.6471788086771961,
      "eval_loss": 0.5645745992660522,
      "eval_runtime": 72.139,
      "eval_samples_per_second": 5.545,
      "eval_steps_per_second": 0.693,
      "step": 402
    }
  ],
  "logging_steps": 1,
  "max_steps": 402,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.629776310272e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
