{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654eaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import ast\n",
    "import swifter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92537f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes:\n",
    "#Blame and praise\n",
    "# entailsments\n",
    "#does not entail an actually entailment\n",
    "\n",
    "#Blame vs endorsement\n",
    "#Also report initial inspection of model wher praise and neutral was added to the hypothesis paramenter instead of blame/not blame.\n",
    "# something about hieracivcal order and the words not being complete opisats and therefore the relative probabilities entails needed information\n",
    "# in addition to the absolute probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "\n",
    "\n",
    "def extract_blame_from_paragraph_lookup(input_str):\n",
    "    \"\"\"\n",
    "    Returns a binary list for blame per sentence:\n",
    "    1 if blame is highest among labels and >= 0.8, else 0\n",
    "    Handles arbitrary label order.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        sentence_list = ast.literal_eval(input_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    # List comprehension is faster than appending in a loop\n",
    "    blame_binary = [\n",
    "        int(\n",
    "            (label_score := {label: score for label, score in zip(sent['labels'], sent['scores'])})['blame']\n",
    "            >= max(label_score.get('praise', 0.0), label_score.get('neutral', 0.0), 0.8)\n",
    "        )\n",
    "        for sent in sentence_list\n",
    "    ]\n",
    "\n",
    "\n",
    "    return blame_binary\n",
    "\n",
    "#example usage: final_data['blame_binary'] = final_data['blame_in_text'].swifter.apply(extract_blame_from_paragraph_lookup)\n",
    "\n",
    "\n",
    "#get row indices (paragraphs) that contain blame\n",
    "def get_rows_with_blame(df, col=\"blame_binary\"):\n",
    "    \"\"\"\n",
    "    Returns row indices where the list in `col` contains at least one 1.\n",
    "    Handles both real lists and stringified lists.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i, values in zip(df.index, df[col]):\n",
    "        if isinstance(values, str):  # convert only if it's a string\n",
    "            values = ast.literal_eval(values)\n",
    "        if 1 in values:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "#example usage: row_indices = get_rows_with_blame(final_data, col=\"blame_binary\")\n",
    "#print(row_indices[:10])\n",
    "\n",
    "\n",
    "#get paragraphs and sentence indices of blame True\n",
    "def get_rows_and_positions(df, col=\"blame_binary\"):\n",
    "    \"\"\"\n",
    "    Returns {row_index: [positions_of_1s]}.\n",
    "    Handles both real lists and stringified lists.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for i, values in zip(df.index, df[col]):\n",
    "        if isinstance(values, str):\n",
    "            values = ast.literal_eval(values)\n",
    "        ones = [j for j, v in enumerate(values) if v == 1]\n",
    "        if ones:\n",
    "            results[i] = ones\n",
    "    return results\n",
    "\n",
    "#example usage: rows_with_positions = get_rows_and_positions(final_data, col=\"blame_binary\")\n",
    "#print(rows_with_positions)\n",
    "\n",
    "#\n",
    "#get danish sentences containing blame from indices extracted as above\n",
    "\n",
    "def danish_sentences_with_blame_extraction(dict, data, text_column):\n",
    "\n",
    "\n",
    "    rows = list(dict.keys())\n",
    "    sentences = {}\n",
    "\n",
    "    for para in rows:\n",
    "        sentence_indices = dict[para]\n",
    "        text_sentences = ast.literal_eval(data.loc[para][text_column])\n",
    "        \n",
    "        blame_sentence_dict = {}\n",
    "        for indx in sentence_indices:\n",
    "            blame_sentence = text_sentences[indx]\n",
    "            blame_sentence_dict[indx] = blame_sentence\n",
    "        \n",
    "        \n",
    "        sentences[para] = blame_sentence_dict\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "#example usage: danish_sentences_with_blame = danish_sentences_with_blame_extraction(rows_with_positions, final_data, 'da_segmented_text')\n",
    "\n",
    "\n",
    "#extract blame percentage:\n",
    "\n",
    "import ast\n",
    "\n",
    "def total_blame_percentage(string_rows):\n",
    "    \"\"\"\n",
    "    Calculate the total percentage of blame-sentences across all rows,\n",
    "    converting string representations of lists into actual lists.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    string_rows : list of str\n",
    "        Each element is a string like '[1, 0, 1]' representing a row of blame labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total percentage of blame-sentences (0â€“100).\n",
    "    \"\"\"\n",
    "    total_sentences = 0\n",
    "    total_blame = 0\n",
    "\n",
    "    for row_str in string_rows:\n",
    "        try:\n",
    "            row = ast.literal_eval(row_str)  # convert string to list\n",
    "            if not isinstance(row, list):\n",
    "                continue  # skip if not a list\n",
    "            row = [int(val) for val in row]  # ensure integers\n",
    "            total_sentences += len(row)\n",
    "            total_blame += sum(row)\n",
    "        except (ValueError, SyntaxError):\n",
    "            continue  # skip invalid rows\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (total_blame / total_sentences) * 100, total_sentences, total_blame\n",
    "\n",
    "#Example usage: percentage_blame, total_sent, total_blame= total_blame_percentage(final_data['blame_binary'])\n",
    "\n",
    "#extract blame scores\n",
    "\n",
    "def get_blame_scores(data = final_data, blame_in_text_column = 'blame_in_text'):\n",
    "    all_blame_scores = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        dict_labels = ast.literal_eval(final_data.loc[i][blame_in_text_column])\n",
    "\n",
    "        blame_list = [(label_score := {label: score for label, score in zip(sent['labels'], sent['scores'])})['blame'] for sent in dict_labels]\n",
    "\n",
    "        all_blame_scores +=blame_list\n",
    "    return all_blame_scores\n",
    "\n",
    "\n",
    "#make vizualization of the distribution\n",
    "\n",
    "def vizualize_blame_prob(blame_scores):\n",
    "    plt.figure()\n",
    "    plt.hist(blame_scores, log = True, bins = 50)\n",
    "    plt.title('Distribution of blame probabilities in training data (log transformed counts)')\n",
    "    plt.xlabel('Probability of blame')\n",
    "    plt.ylabel('Log Counts')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(blame_scores, bins = 50)\n",
    "    plt.title('Distribution of blame probabilities in training data')\n",
    "    plt.xlabel('Probability of blame')\n",
    "\n",
    "    plt.ylabel('Absolute Counts')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity checks:\n",
    "#check if translated, orignal and blame probabilty are same lengt (extend for all templates)\n",
    "for indx in range(len(final_data)):\n",
    "\n",
    "    temo = final_data.loc[indx]\n",
    "\n",
    "    l_da = len(ast.literal_eval(temo['da_segmented_text']))\n",
    "    l_en = len(ast.literal_eval(temo['translated_text']))\n",
    "    l_bl = len(ast.literal_eval(temo['blame_in_text']))\n",
    "\n",
    "    if l_da != l_en != l_bl:\n",
    "        print(temo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with all templates\n",
    "\n",
    "\n",
    "all_templates_data = pd.read_csv(\"/work/MarkusLundsfrydJensen#1865/Bachelor_project/annotation_data_fifth_template_appended.csv\")\n",
    "\n",
    "all_templates_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f47e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "columns = [\"blame_in_text\",\"second_template_blame_in_text\",\"third_template_blame_in_text\",\"fourth_template_blame_in_text\",\"fifth_template_blame_in_text\"]\n",
    "\n",
    "for i, column in enumerate(columns):\n",
    "    all_templates_data[f'blame_binary_temp_{i+1}'] = all_templates_data[column].swifter.apply(extract_blame_from_paragraph_lookup)\n",
    "\n",
    "all_templates_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blame_bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
