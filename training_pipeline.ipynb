{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64371899",
   "metadata": {},
   "source": [
    "To do:\n",
    "- ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "- Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    - In addition to this, look at the implications of gradient accumulation steps\n",
    "    - Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "- Hyperparameter tuning (Alpha, learning rate, batch size so on - not sure how to figure this out)\n",
    "    - There is precedence for no hyperparameter tuning from the author of the OG NLI model that DEBATE is based on = Due to computational restrains and the points from this paper, no hyperparameter tuning was performed in this case. The model tuning in itself is also not the primary focus in this paper, but simply serves as a tool for the actual inquiry into blame in the Danish Parliament\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d09f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-20 22:50:43.562825: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 22:50:43.618092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 22:50:44.549074: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,708,048 || all params: 309,239,826 || trainable%: 0.5523\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",  # Fine-tuning the attention layer specifically\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consider the batch size, could be increased for efficiency purposes.\n",
    "ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    In addition to this, look at the implications of gradient accumulation steps\n",
    "    Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "Early stopping: load_best_model_at_end=True\n",
    "'''\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./full_results',\n",
    "    learning_rate=8e-4, # Learning rate copied from mmBERT paper as they found this to perform best\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=12,\n",
    "    auto_find_batch_size=True, # Allows for auto adjusting of batch to avoid OOM\n",
    "    gradient_accumulation_steps=12,  # Simulate larger batch size\n",
    "\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=True, # Avoiding manual handling of residual text columns\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6b6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94619e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #From logits to probabilities\n",
    "    probs_2d = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    probs = probs_2d[:, 1]  # positive class extraction\n",
    "    \n",
    "    keras_bce = binary_crossentropy(labels, probs)\n",
    "    keras_bce = float(np.mean(keras_bce.numpy()))  # Converting from keras eagertensor to float value\n",
    "    \n",
    "    # Wrapping all metrics to floats for json serialization during model eval\n",
    "    return {\n",
    "        'keras_BCE': keras_bce,\n",
    "        'precision': float(average_precision_score(labels, probs)),\n",
    "        'accuracy': float(accuracy_score(labels, probs.round())), # Need rounding for these two computations (integer required)\n",
    "        'f1': float(f1_score(labels, probs.round(), average='macro')) # macro f1 is better for imbalanced dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/cleaned_training_data_3_4_5_temps.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c6f08eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 176/176 [00:00<00:00, 523.16 examples/s] \n"
     ]
    }
   ],
   "source": [
    "val_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Model_data/validation_set.json\")\n",
    "\n",
    "val_dataframe = val_dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "tokenized_val = val_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00acf585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 546.48 examples/s] \n"
     ]
    }
   ],
   "source": [
    "test_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/subset_cleaned_training_data.json\")\n",
    "\n",
    "test_dataframe = test_dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "test_dataframe = test_dataframe[0:200]\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_dataframe)\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4402b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 398842/398842 [12:29<00:00, 532.46 examples/s] \n"
     ]
    }
   ],
   "source": [
    "#val_dataframe = val_dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "dataframe = dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "#val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe)\n",
    "\n",
    "#tokenized_val = val_dataset.map(tokenize_function)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8c0ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (34/34 shards): 100%|██████████| 398842/398842 [00:13<00:00, 29326.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36abd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='14247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    7/14247 03:10 < 150:58:04, 0.03 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=lora_model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m      7\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/accelerate/utils/memory.py:174\u001b[39m, in \u001b[36mfind_executable_batch_size.<locals>.decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo executable batch size found, reached zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/trainer.py:2677\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2672\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2681\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fadde19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8130114078521729, 'eval_keras_BCE': 0.8130306005477905, 'eval_precision': 0.6156827579404506, 'eval_accuracy': 0.3806818181818182, 'eval_f1': 0.3487014089288746, 'eval_runtime': 32.2089, 'eval_samples_per_second': 5.464, 'eval_steps_per_second': 0.683, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Evalresult5templates.txt\", \"w\") as f:\n",
    "    f.write(str(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We would expect to see a gradual decrease in both training and validation loss.\n",
    "If either om them split too far from eachother that indicates issues with the training process.\n",
    "The process itself should be pretty smooth with no dips either up or down.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we should very much remember to save the finetuned model locally as this contains the new weights for use in analyzing new text\n",
    "lora_model.save_pretrained(f\"output/mmBERT/full_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG function without bce\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        'precision': average_precision_score(labels, predictions),\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro') # Macro is better suited for imbalanced data\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
