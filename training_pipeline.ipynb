{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64371899",
   "metadata": {},
   "source": [
    "To do:\n",
    "- ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "- Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    - In addition to this, look at the implications of gradient accumulation steps\n",
    "    - Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "- Early stopping: load_best_model_at_end=True\n",
    "- Split into validation set as well (80:10:10)\n",
    "- Hyperparameter tuning (Alpha, learning rate, batch size so on - not sure how to figure this out)\n",
    "    - There is precedence for no hyperparameter tuning from the author of the OG NLI model that DEBATE is based on = Due to computational restrains and the points from this paper, no hyperparameter tuning was performed in this case. The model tuning in itself is also not the primary focus in this paper, but simply serves as a tool for the actual inquiry into blame in the Danish Parliament\n",
    "- Context / No context?\n",
    "    - Most likely no context, as we do not have the academic reasons for doing so \n",
    "    - Model actually also seems to perform better without (f1 of .66 for no context on random test set vs. . 55 on gold labelled for context model) This might also make sense as the context is not taken into account in the DEBATE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d09f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: transformers in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 2)) (4.56.2)\n",
      "Requirement already satisfied: peft in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 3)) (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 4)) (0.48.1)\n",
      "Requirement already satisfied: accelerate in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 5)) (1.10.1)\n",
      "Requirement already satisfied: datasets in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 6)) (4.2.0)\n",
      "Requirement already satisfied: scikit-learn in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 7)) (1.7.2)\n",
      "Requirement already satisfied: keras in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 8)) (3.11.3)\n",
      "Collecting tensorflow (from -r requirements_bert.txt (line 9))\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: filelock in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2025.9.18)\n",
      "Requirement already satisfied: requests in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements_bert.txt (line 2)) (1.1.10)\n",
      "Requirement already satisfied: psutil in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from peft->-r requirements_bert.txt (line 3)) (7.1.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: pandas in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (3.13.0)\n",
      "Requirement already satisfied: anyio in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: idna in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from scikit-learn->-r requirements_bert.txt (line 7)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from scikit-learn->-r requirements_bert.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from scikit-learn->-r requirements_bert.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: absl-py in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras->-r requirements_bert.txt (line 8)) (2.3.1)\n",
      "Requirement already satisfied: rich in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras->-r requirements_bert.txt (line 8)) (14.2.0)\n",
      "Requirement already satisfied: namex in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras->-r requirements_bert.txt (line 8)) (0.1.0)\n",
      "Requirement already satisfied: h5py in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras->-r requirements_bert.txt (line 8)) (3.15.1)\n",
      "Requirement already satisfied: optree in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras->-r requirements_bert.txt (line 8)) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras->-r requirements_bert.txt (line 8)) (0.5.3)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: six>=1.12.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading wrapt-2.0.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading grpcio-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (2.5.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9)) (11.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9))\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (1.22.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements_bert.txt (line 9)) (0.45.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements_bert.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9)) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from rich->keras->-r requirements_bert.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from rich->keras->-r requirements_bert.txt (line 8)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->-r requirements_bert.txt (line 8)) (0.1.2)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wrapt-2.0.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, opt_einsum, markdown, grpcio, google_pasta, gast, astunparse, tensorboard, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [tensorflow]5\u001b[0m [tensorflow]]]ata-server]\n",
      "\u001b[1A\u001b[2KSuccessfully installed astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.1 libclang-18.1.1 markdown-3.9 opt_einsum-3.4.0 protobuf-6.33.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "#import scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import Conv1D, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-20T08:02:24.026013Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80A84K4X6QCYH5WJ470PWDZ\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:24.026059Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #0. Sleeping 414.947056ms before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:24.548261Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80A853FAN03F6GCFPDN6R4G\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:24.548280Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #1. Sleeping 3.561067537s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:28.216110Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80A88P20ZPY6EN11D1J3B0Q\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:28.216129Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #2. Sleeping 4.100561118s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:32.424513Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80A8CSKFE4AJ67HDFSD378R\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:32.424533Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #3. Sleeping 12.048990952s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:44.581292Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80A8RNGKRP1BBWJ8RPRRVMS\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n",
      "  \u001b[2m2025-10-20T08:02:44.581310Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mRetry attempt #4. Sleeping 33.408713732s before the next attempt\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs:171\n",
      "\n",
      "  \u001b[2m2025-10-20T08:03:18.096859Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"01K80A9SCVWBSYWD3ENHA3KCB0\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load the model for 'jhu-clsp/mmBERT-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jhu-clsp/mmBERT-base' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/modeling_utils.py:1179\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1178\u001b[39m         filename = _add_variant(WEIGHTS_NAME, variant)\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m         resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(WEIGHTS_NAME, variant):\n\u001b[32m   1183\u001b[39m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/utils/hub.py:321\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[33;03mTries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    270\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m \u001b[33;03m```\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/utils/hub.py:566\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    565\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    568\u001b[39m resolved_files = [\n\u001b[32m    569\u001b[39m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision) \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    570\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/utils/hub.py:478\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/huggingface_hub/file_download.py:1171\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/huggingface_hub/file_download.py:1723\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1722\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1723\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/huggingface_hub/file_download.py:629\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    627\u001b[39m     progress.update(progress_bytes)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Data processing error: CAS service error : Reqwest Error: HTTP status server error (500 Internal Server Error), domain: https://cas-server.xethub.hf.co/reconstructions/38494675f12d761fbd669e6694ef227324f5c94c133631e9d1b6250a6bdb3a78",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mjhu-clsp/mmBERT-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m quantization_config = BitsAndBytesConfig(\n\u001b[32m      4\u001b[39m                                         load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m                                          bnb_4bit_compute_dtype=torch.bfloat16,\n\u001b[32m      6\u001b[39m                                          bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m                                          bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m                                          )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/modeling_utils.py:288\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    290\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/modeling_utils.py:5030\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5020\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   5021\u001b[39m     gguf_file\n\u001b[32m   5022\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5023\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   5024\u001b[39m ):\n\u001b[32m   5025\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   5026\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5027\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5028\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5030\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5032\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5037\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5042\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5043\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5044\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   5047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5048\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5050\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5051\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/transformers/modeling_utils.py:1268\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1265\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1267\u001b[39m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1268\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m   1269\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load the model for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load it\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m from \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1271\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m same name. Otherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1272\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m directory containing a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1273\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local:\n\u001b[32m   1277\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading weights file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Can't load the model for 'jhu-clsp/mmBERT-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'jhu-clsp/mmBERT-base' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m lora_config = LoraConfig(\n\u001b[32m      2\u001b[39m     r=\u001b[32m8\u001b[39m,  \u001b[38;5;66;03m# Low-rank dimension\u001b[39;00m\n\u001b[32m      3\u001b[39m     lora_alpha=\u001b[32m16\u001b[39m,\n\u001b[32m      4\u001b[39m     lora_dropout=\u001b[32m0.05\u001b[39m,\n\u001b[32m      5\u001b[39m     target_modules=[\u001b[33m\"\u001b[39m\u001b[33mWqkv\u001b[39m\u001b[33m\"\u001b[39m],  \u001b[38;5;66;03m# Fine-tuning the attention layer specifically\u001b[39;00m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m lora_model = get_peft_model(\u001b[43mmodel\u001b[49m, lora_config)\n\u001b[32m      9\u001b[39m lora_model.print_trainable_parameters()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"Wqkv\"],  # Fine-tuning the attention layer specifically\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consider the batch size, could be increased for efficiency purposes.\n",
    "ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    In addition to this, look at the implications of gradient accumulation steps\n",
    "    Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "Early stopping: load_best_model_at_end=True\n",
    "'''\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./full_results',\n",
    "    learning_rate=8e-4, # Learning rate copied from mmBERT paper as they found this to perform best\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    auto_find_batch_size=True, # Allows for auto adjusting of batch to avoid OOM\n",
    "    gradient_accumulation_steps=12,  # Simulate larger batch size\n",
    "\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=True, # Avoiding manual handling of residual text columns\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6b6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94619e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'precision:' average_precision_score(labels, predictions),\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro') # Macro is better suited for imbalanced data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/cleaned_training_data_3_4_5_temps.json\")\n",
    "\n",
    "val_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Model_data/validation_set.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4402b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 176/176 [00:00<00:00, 549.70 examples/s] \n",
      "Map: 100%|██████████| 3600/3600 [00:06<00:00, 543.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataframe = val_dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "dataframe = dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe)\n",
    "\n",
    "tokenized_val = val_dataset.map(tokenize_function)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca18287",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This has still not been tested, but will be implemented soon\n",
    "\n",
    "Instead of cross entropy = BCEWITHLOGITSLOSS? https://docs.pytorch.org/docs/1.7.1/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss \n",
    "\n",
    "No matter what, we need to settle on a loss function which allows for weighted loss due to the strong imbalance in the dataset. \n",
    "\n",
    "Binary cross-entropy (log loss) is a loss function used in binary classification problems. It quantifies the difference between the actual class labels (0 or 1) and the predicted probabilities output by the model. The lower the binary cross-entropy value, the better the model’s predictions align with the true labels.\n",
    "\n",
    "Since the model’s output is a probability between 0 and 1, minimizing binary cross-entropy during training helps improve predictive accuracy, ensuring the model effectively distinguishes between two classes.\n",
    "\n",
    "'''\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "labels = random_dataset['label'].tolist()\n",
    "class_counts = Counter(labels)\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Higher weight = more emphasis\n",
    "weights = [total/class_counts[0], total/class_counts[1]]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "#define custom trainer that uses weigted loss\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Define weighted loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3818abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 10:37:54.408384: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 10:37:54.463559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 10:37:56.034189: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Loss (manual calculation): 0.20273661557656092\n",
      "Binary Cross-Entropy Loss (Keras): 0.20273661557656092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1760949476.398954    2662 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# An option for BCE (Binary Cross Entropy implementation), Keras definitely the easy way out - they are essentially the same as manual calc and more efficient.\n",
    "import numpy as np\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# Example true labels and predicted probabilities\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce\n",
    "\n",
    "bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"Binary Cross-Entropy Loss (manual calculation): {bce_loss}\")\n",
    "\n",
    "# Compute Binary Cross-Entropy using Keras\n",
    "bce_loss_keras = binary_crossentropy(y_true, y_pred).numpy()\n",
    "print(f\"Binary Cross-Entropy Loss (Keras): {bce_loss_keras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36abd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 1:20:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.420500</td>\n",
       "      <td>0.508216</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.732810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.795000</td>\n",
       "      <td>0.497293</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.803830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.363200</td>\n",
       "      <td>0.504073</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.800597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=5.090314436377141, metrics={'train_runtime': 4870.4503, 'train_samples_per_second': 2.217, 'train_steps_per_second': 0.023, 'total_flos': 5.916996698112e+16, 'train_loss': 5.090314436377141, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fadde19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5040732026100159, 'eval_accuracy': 0.8125, 'eval_f1': 0.8005973838706355, 'eval_runtime': 32.1592, 'eval_samples_per_second': 5.473, 'eval_steps_per_second': 0.684, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9680560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Evalresult5templates.txt\", \"w\") as f:\n",
    "    f.write(str(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We would expect to see a gradual decrease in both training and validation loss.\n",
    "If either om them split too far from eachother that indicates issues with the training process.\n",
    "The process itself should be pretty smooth with no dips either up or down.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we should very much remember to save the finetuned model locally as this contains the new weights for use in analyzing new text\n",
    "lora_model.save_pretrained(f\"output/mmBERT/full_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
