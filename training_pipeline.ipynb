{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d09f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: transformers in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 2)) (4.56.2)\n",
      "Collecting peft (from -r requirements_bert.txt (line 3))\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting bitsandbytes (from -r requirements_bert.txt (line 4))\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 5)) (1.10.1)\n",
      "Collecting datasets (from -r requirements_bert.txt (line 6))\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting scikit-learn (from -r requirements_bert.txt (line 7))\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2025.9.18)\n",
      "Requirement already satisfied: requests in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements_bert.txt (line 2)) (1.1.10)\n",
      "Requirement already satisfied: psutil in /home/ucloud/.local/lib/python3.12/site-packages (from peft->-r requirements_bert.txt (line 3)) (7.1.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (2.3.3)\n",
      "Collecting httpx<1.0.0 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (2025.8.3)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn->-r requirements_bert.txt (line 7))\n",
      "  Downloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements_bert.txt (line 7))\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements_bert.txt (line 7))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements_bert.txt (line 1)) (1.3.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from jinja2->torch->-r requirements_bert.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ucloud/.local/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements_bert.txt (line 6)) (1.17.0)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, threadpoolctl, sniffio, scipy, pyarrow, propcache, multidict, joblib, h11, frozenlist, dill, aiohappyeyeballs, yarl, scikit-learn, multiprocess, httpcore, anyio, aiosignal, httpx, aiohttp, bitsandbytes, peft, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [datasets]/23\u001b[0m [datasets]tes]lls]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 anyio-4.11.0 bitsandbytes-0.48.1 datasets-4.2.0 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 joblib-1.5.2 multidict-6.7.0 multiprocess-0.70.16 peft-0.17.1 propcache-0.4.1 pyarrow-21.0.0 scikit-learn-1.7.2 scipy-1.16.2 sniffio-1.3.1 threadpoolctl-3.6.0 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "#import scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import Conv1D, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Chunk for looking into trainable layers of the model itself.\n",
    "'''\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "\n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing\n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "\n",
    "    return layer_names\n",
    "\n",
    "list(set(get_specific_layer_names(model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 308,072,450 || trainable%: 0.1755\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"Wqkv\"],  # Fine-tuning the attention layer specifically\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Start small, increase gradually\n",
    "    gradient_accumulation_steps=12,  # Simulate larger batch size\n",
    "\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset generation chunk\n",
    "We need to pass it through the BERT tokenizer here, make a train / test / val split and pass that to the model\n",
    "\n",
    "Below is the structure which worked for the Pol_NLI dataset, we should strive to do the same\n",
    "'''\n",
    "dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/training_data.json\")\n",
    "\n",
    "#tokenized_dataset = tokenized_dataset.rename_column(\"entailment\", \"labels\") # Rename entailment column to labels (which is standard lookup for evaluation in the transmformers trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac481bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"label\"][1900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89907275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataframe[0:5000]\n",
    "\n",
    "random_dataset = dataset.sample(n=2000, axis=0, random_state=40)\n",
    "\n",
    "X = dataset[\"text\"]\n",
    "y = dataset[\"label\"]\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ec45488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>sentence_nr</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>party</th>\n",
       "      <th>preceding_sentence</th>\n",
       "      <th>succeeding_sent</th>\n",
       "      <th>current_speaker_in_government</th>\n",
       "      <th>parties_in_government</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>94</td>\n",
       "      <td>5</td>\n",
       "      <td>Så er fordelen ved retoriske spørgsmål selvføl...</td>\n",
       "      <td>Søren Søndergaard</td>\n",
       "      <td>EL</td>\n",
       "      <td>Man vil både have i pose og sæk, og man vil  i...</td>\n",
       "      <td>Når en eller anden narkoman begår et  indbrud,...</td>\n",
       "      <td>False</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1997-10-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>Vi har vist, at et  flittigt folk er at finde ...</td>\n",
       "      <td>Poul Nyrup Rasmussen</td>\n",
       "      <td>S</td>\n",
       "      <td>Danmark er i fremgang, og  håbet og troen på f...</td>\n",
       "      <td>Regeringen har gennem sin økonomiske politik -...</td>\n",
       "      <td>True</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1997-10-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>183</td>\n",
       "      <td>97</td>\n",
       "      <td>De folk, der står bagest, er taberne.</td>\n",
       "      <td>Poul Nyrup Rasmussen</td>\n",
       "      <td>S</td>\n",
       "      <td>Taberne er de folk, der er  dårligst uddannede...</td>\n",
       "      <td>Jeg siger det for god ordens skyld, for det er...</td>\n",
       "      <td>True</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1997-10-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>13630</td>\n",
       "      <td>3</td>\n",
       "      <td>Men det skal også siges, at  regeringens liv h...</td>\n",
       "      <td>Holger K. Nielsen</td>\n",
       "      <td>SF</td>\n",
       "      <td>Folketingsvalget var udtryk for en kamp om, hv...</td>\n",
       "      <td>Og det bør regeringen have i baghovedet, når d...</td>\n",
       "      <td>False</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1998-06-25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>6487</td>\n",
       "      <td>25</td>\n",
       "      <td>Jeg mener, at det er de i ånden svage  dansker...</td>\n",
       "      <td>Arne Melchior</td>\n",
       "      <td>CD</td>\n",
       "      <td>I øvrigt interesserer jeg mig slet ikke så meg...</td>\n",
       "      <td>Der har heldigvis været en række  ordførere, d...</td>\n",
       "      <td>False</td>\n",
       "      <td>[S, RV]</td>\n",
       "      <td>1998-01-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      paragraph  sentence_nr  \\\n",
       "4088         94            5   \n",
       "2080          2          106   \n",
       "41          183           97   \n",
       "796       13630            3   \n",
       "354        6487           25   \n",
       "\n",
       "                                                   text               speaker  \\\n",
       "4088  Så er fordelen ved retoriske spørgsmål selvføl...     Søren Søndergaard   \n",
       "2080  Vi har vist, at et  flittigt folk er at finde ...  Poul Nyrup Rasmussen   \n",
       "41                De folk, der står bagest, er taberne.  Poul Nyrup Rasmussen   \n",
       "796   Men det skal også siges, at  regeringens liv h...     Holger K. Nielsen   \n",
       "354   Jeg mener, at det er de i ånden svage  dansker...         Arne Melchior   \n",
       "\n",
       "     party                                 preceding_sentence  \\\n",
       "4088    EL  Man vil både have i pose og sæk, og man vil  i...   \n",
       "2080     S  Danmark er i fremgang, og  håbet og troen på f...   \n",
       "41       S  Taberne er de folk, der er  dårligst uddannede...   \n",
       "796     SF  Folketingsvalget var udtryk for en kamp om, hv...   \n",
       "354     CD  I øvrigt interesserer jeg mig slet ikke så meg...   \n",
       "\n",
       "                                        succeeding_sent  \\\n",
       "4088  Når en eller anden narkoman begår et  indbrud,...   \n",
       "2080  Regeringen har gennem sin økonomiske politik -...   \n",
       "41    Jeg siger det for god ordens skyld, for det er...   \n",
       "796   Og det bør regeringen have i baghovedet, når d...   \n",
       "354   Der har heldigvis været en række  ordførere, d...   \n",
       "\n",
       "      current_speaker_in_government parties_in_government       date  label  \n",
       "4088                          False               [S, RV] 1997-10-09      0  \n",
       "2080                           True               [S, RV] 1997-10-07      0  \n",
       "41                             True               [S, RV] 1997-10-09      1  \n",
       "796                           False               [S, RV] 1998-06-25      1  \n",
       "354                           False               [S, RV] 1998-01-22      1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2836e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:04<00:00, 415.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# From dataframe to dataset for mapping tokenizer function \n",
    "dataset = Dataset.from_pandas(random_dataset)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "853ecb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(['Regeringen vil også fortsætte sin offensive  miljøpolitik.', 'Sådan som debatten var foregået, sagde hr. Torben Lund,  var det lavt og nedrigt og pinligt.', 'Jeg vil tillade mig at  sige, at det er lavt og nedrigt og pinligt af den siddende  regering, at den overhovedet ikke har taget debatten op før  nu, når der er et folketingsvalg og et kommunevalg i sigte.', 'Det er oprigtig talt dybt beskæmmende, at Socialdemokratiets  ordfører kan prøve på at bortforklare de internationale  sammenligninger, der har været af skoleelevers kundskaber med  hensyn til læsning, regning, fysik og kemi.', 'Først vil jeg sige til hr. Helge Adam Møller, der jo her  havde en kort bemærkning, der svarede til, tror jeg, næsten  ordret de korte bemærkninger, hr. Helge Adam Møller har haft  til åbningsdebatter og afslutningsdebatter i hvert fald i de  år, jeg har været politisk ordfører, og de går på, at De  Konservative vil hårdere straffe og mere fængsel, og at vi  andre bare er sådan nogle slatne nogle, der går ind for lidt  forebyggelse, men ikke gør noget for at forhindre, at vold og  andet foregår i vort samfund.'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36abd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we should very much remember to save the finetuned model locally as this contains the new weights for use in analyzing new text\n",
    "lora_model.save_pretrained(f\"output/mmBERT/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
