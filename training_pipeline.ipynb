{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64371899",
   "metadata": {},
   "source": [
    "To do:\n",
    "- ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "- Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    - In addition to this, look at the implications of gradient accumulation steps\n",
    "    - Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "- Hyperparameter tuning (Alpha, learning rate, batch size so on - not sure how to figure this out)\n",
    "    - There is precedence for no hyperparameter tuning from the author of the OG NLI model that DEBATE is based on = Due to computational restrains and the points from this paper, no hyperparameter tuning was performed in this case. The model tuning in itself is also not the primary focus in this paper, but simply serves as a tool for the actual inquiry into blame in the Danish Parliament\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d09f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: transformers in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 2)) (4.56.2)\n",
      "Requirement already satisfied: peft in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 3)) (0.17.1)\n",
      "Requirement already satisfied: bitsandbytes in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 4)) (0.48.1)\n",
      "Requirement already satisfied: accelerate in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 5)) (1.10.1)\n",
      "Requirement already satisfied: datasets in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 6)) (4.2.0)\n",
      "Requirement already satisfied: scikit-learn in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 7)) (1.7.2)\n",
      "Requirement already satisfied: tf-keras in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 8)) (2.20.1)\n",
      "Requirement already satisfied: tensorflow in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from -r requirements_bert.txt (line 9)) (2.20.0)\n",
      "Collecting wandb (from -r requirements_bert.txt (line 10))\n",
      "  Downloading wandb-0.22.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from torch->-r requirements_bert.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2025.9.18)\n",
      "Requirement already satisfied: requests in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from transformers->-r requirements_bert.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements_bert.txt (line 2)) (1.1.10)\n",
      "Requirement already satisfied: psutil in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from peft->-r requirements_bert.txt (line 3)) (7.1.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: pandas in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from datasets->-r requirements_bert.txt (line 6)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (3.13.0)\n",
      "Requirement already satisfied: anyio in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: idna in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from scikit-learn->-r requirements_bert.txt (line 7)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from scikit-learn->-r requirements_bert.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from scikit-learn->-r requirements_bert.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (6.33.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (3.11.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorflow->-r requirements_bert.txt (line 9)) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9)) (3.9)\n",
      "Requirement already satisfied: pillow in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9)) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9)) (3.1.3)\n",
      "Collecting click>=8.0.1 (from wandb->-r requirements_bert.txt (line 10))\n",
      "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements_bert.txt (line 10))\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from wandb->-r requirements_bert.txt (line 10)) (4.4.0)\n",
      "Requirement already satisfied: pydantic<3 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from wandb->-r requirements_bert.txt (line 10)) (2.11.9)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb->-r requirements_bert.txt (line 10))\n",
      "  Downloading sentry_sdk-2.42.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements_bert.txt (line 10)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements_bert.txt (line 10)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements_bert.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (1.22.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow->-r requirements_bert.txt (line 9)) (0.45.1)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements_bert.txt (line 10))\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements_bert.txt (line 10))\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: rich in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow->-r requirements_bert.txt (line 9)) (14.2.0)\n",
      "Requirement already satisfied: namex in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow->-r requirements_bert.txt (line 9)) (0.1.0)\n",
      "Requirement already satisfied: optree in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow->-r requirements_bert.txt (line 9)) (0.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements_bert.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow->-r requirements_bert.txt (line 9)) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow->-r requirements_bert.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow->-r requirements_bert.txt (line 9)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow->-r requirements_bert.txt (line 9)) (0.1.2)\n",
      "Downloading wandb-0.22.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sentry_sdk-2.42.1-py2.py3-none-any.whl (380 kB)\n",
      "Installing collected packages: smmap, sentry-sdk, click, gitdb, gitpython, wandb\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [wandb]32m5/6\u001b[0m [wandb]hon]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 gitdb-4.0.12 gitpython-3.1.45 sentry-sdk-2.42.1 smmap-5.0.2 wandb-0.22.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-23 16:02:04.471066: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-23 16:02:10.731076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-23 16:02:49.271953: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d6ca904",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,416,096 || all params: 310,947,874 || trainable%: 1.0986\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",  # Fine-tuning all linear (classification, attention... layers)\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consider the batch size, could be increased for efficiency purposes.\n",
    "ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    In addition to this, look at the implications of gradient accumulation steps\n",
    "    Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "Early stopping: load_best_model_at_end=True\n",
    "'''\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to='wandb',\n",
    "    output_dir='./full_tune_results',\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4, # Learning rate copied from mmBERT paper (8e-4) as they found this to perform best\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=256, # Batching at 256 to balance generalization and efficient training\n",
    "    gradient_accumulation_steps=1,  # Gradient of 1 as full batch fits in memory, accumulation then only slows\n",
    "\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,  # Enable mixed precision\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    remove_unused_columns=True, # Avoiding manual handling of residual text columns\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2139944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_json(input_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Preprocesses a JSON file by filtering out entries based on the 'text' key.\n",
    "    \n",
    "    Criteria for deletion:\n",
    "      - 'text' is missing or empty\n",
    "      - 'text' length is <= 3\n",
    "      - 'text' contains '(' or ')'\n",
    "    \n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        output_path (str, optional): If provided, saves the filtered JSON here.\n",
    "    \n",
    "    Returns:\n",
    "        list: The filtered list of JSON entries.\n",
    "    \"\"\"\n",
    "    # Load JSON file\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Filter entries\n",
    "    filtered_data = [\n",
    "        entry for entry in data\n",
    "        if 'text' in entry\n",
    "        and entry['text']\n",
    "        and len(entry['text']) > 3\n",
    "        and '(' not in entry['text']\n",
    "        and ')' not in entry['text']\n",
    "    ]\n",
    "\n",
    "    # Optionally save to a new file\n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e6b6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=392, # Padding to 392 to massively cut down on computation compared to base 8,192 tokens. \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2af8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bincrossentropy(true, pred, weight_zero = 99.0, weight_one = 1):\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed.\n",
    "        \n",
    "    This can be useful for unbalanced catagories.\n",
    "    \n",
    "    Adjust the weights here depending on what is required.\n",
    "    \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalize 10 times as much as false negatives.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    #weights /= (weight_one + weight_zero) # Normalizing to be more consistent with regular BCE for comparison \n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return np.mean(weighted_bin_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94619e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #From logits to probabilities\n",
    "    probs_2d = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    probs = probs_2d[:, 1]  # positive class extraction\n",
    "    \n",
    "    weigthted_bce = weighted_bincrossentropy(labels, probs)\n",
    "    keras_bce = binary_crossentropy(labels, probs)\n",
    "    keras_bce = float(np.mean(keras_bce.numpy()))  # Converting from keras eagertensor to float value\n",
    "    \n",
    "    # Wrapping all metrics to floats for json serialization during model eval\n",
    "    return {\n",
    "        'keras_BCE': keras_bce,\n",
    "        'weighted BCE (STD)': weigthted_bce, # Normalized to be interpretable compared to regular BCE\n",
    "        'recall': float(recall_score(labels, probs.round())),\n",
    "        'precision': float(average_precision_score(labels, probs)),\n",
    "        'accuracy': float(accuracy_score(labels, probs.round())), # Need rounding for these two computations (integer required)\n",
    "        'f1': float(f1_score(labels, probs.round(), average='macro')) # macro f1 is better for imbalanced dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdaa441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "preprocess_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/cleaned_training_data_3_4_5_temps.json\", \"/work/RuneEgeskovTrust#9638/Bachelor/training_data/preprocessed_data_for_training.json\")\n",
    "#Validation data\n",
    "preprocess_json(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Model_data/validation_set.json\", \"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Model_data/validation_set.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/preprocessed_data_for_training.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71500ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom trainer class (weigthed)\n",
    "from collections import Counter\n",
    "\n",
    "labels = test_dataframe['label'].tolist()\n",
    "class_counts = Counter(labels)\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# Higher weight = more emphasis\n",
    "weights = [total/class_counts[0], total/class_counts[1]]\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "#define custom trainer that uses weigted loss\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Define weighted loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34b33a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.238543472875898, 5.192107995846314]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c6f08eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 174/174 [00:00<00:00, 358.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Model_data/validation_set.json\")\n",
    "\n",
    "val_dataframe = val_dataframe[['text', 'label']]\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00acf585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 5251.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/cleaned_training_data.json\")\n",
    "\n",
    "test_dataframe = test_dataframe[['text', 'label']]\n",
    "\n",
    "test_dataframe = test_dataframe[0:10000]\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_dataframe)\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4402b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 388027/388027 [00:17<00:00, 22653.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#val_dataframe = val_dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "dataframe = dataframe[['text', 'label']]\n",
    "\n",
    "#val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe)\n",
    "\n",
    "#tokenized_val = val_dataset.map(tokenize_function)\n",
    "\n",
    "# I suspect num_proc can be increased after having identified the padding problem\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a36abd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrune-trust\u001b[0m (\u001b[33mrune-trust-aarhus-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/wandb/run-20251023_160428-qudboooz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rune-trust-aarhus-university/huggingface/runs/qudboooz' target=\"_blank\">azure-water-6</a></strong> to <a href='https://wandb.ai/rune-trust-aarhus-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rune-trust-aarhus-university/huggingface' target=\"_blank\">https://wandb.ai/rune-trust-aarhus-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rune-trust-aarhus-university/huggingface/runs/qudboooz' target=\"_blank\">https://wandb.ai/rune-trust-aarhus-university/huggingface/runs/qudboooz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 06:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Keras Bce</th>\n",
       "      <th>Weighted bce (std)</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>0.429027</td>\n",
       "      <td>0.378687</td>\n",
       "      <td>25.332829</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.796601</td>\n",
       "      <td>0.850575</td>\n",
       "      <td>0.833309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.260600</td>\n",
       "      <td>0.346792</td>\n",
       "      <td>0.446387</td>\n",
       "      <td>29.861767</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.801318</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.794811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.215900</td>\n",
       "      <td>0.366908</td>\n",
       "      <td>0.476909</td>\n",
       "      <td>31.903564</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.816188</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.800313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1761228441.798423    2647 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.3324994755287965, metrics={'train_runtime': 448.8311, 'train_samples_per_second': 66.84, 'train_steps_per_second': 0.267, 'total_flos': 8067821509440000.0, 'train_loss': 0.3324994755287965, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Look into learning rates, model is currently overfitting quite drastically (\"small\" test-set)\n",
    "Normalizing weigthed BCE or no?\n",
    "Look into regularization, dropout and early stopping to avoid overfitting\n",
    "'''\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aeaf63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Merged model saved to: /work/RuneEgeskovTrust#9638/Bachelor/mmBlameBERT-pol-DA-merged\n"
     ]
    }
   ],
   "source": [
    "FINE_TUNED_MODEL_NAME = \"mmBlameBERT-pol-DA\"\n",
    "\n",
    "merged_model = model.merge_and_unload()    # PEFT: incorporates LoRA into base weights\n",
    "merged_dir = f\"/work/RuneEgeskovTrust#9638/Bachelor/{FINE_TUNED_MODEL_NAME}-merged\"\n",
    "merged_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "print(\"✓ Merged model saved to:\", merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fadde19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46705466508865356, 'eval_keras_BCE': 0.4670414924621582, 'eval_weigthed BCE': 0.31243467330932617, 'eval_precision': 0.652620435757883, 'eval_accuracy': 0.7586206896551724, 'eval_f1': 0.7180555555555556, 'eval_runtime': 3.2533, 'eval_samples_per_second': 53.485, 'eval_steps_per_second': 6.762, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85270876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9680560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/EvalResultFullData.txt\", \"w\") as f:\n",
    "    f.write(str(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We would expect to see a gradual decrease in both training and validation loss.\n",
    "If either om them split too far from eachother that indicates issues with the training process.\n",
    "The process itself should be pretty smooth with no dips either up or down.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG function without bce\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        'precision': average_precision_score(labels, predictions),\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro') # Macro is better suited for imbalanced data\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
