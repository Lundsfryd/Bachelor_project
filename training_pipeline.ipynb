{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d09f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from -r requirements_bert.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from -r requirements_bert.txt (line 2)) (4.56.2)\n",
      "Collecting peft (from -r requirements_bert.txt (line 3))\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting bitsandbytes (from -r requirements_bert.txt (line 4))\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting accelerate (from -r requirements_bert.txt (line 5))\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets (from -r requirements_bert.txt (line 6))\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting scikit-learn (from -r requirements_bert.txt (line 7))\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from torch->-r requirements_bert.txt (line 1)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from torch->-r requirements_bert.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from torch->-r requirements_bert.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from torch->-r requirements_bert.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from torch->-r requirements_bert.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from torch->-r requirements_bert.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from torch->-r requirements_bert.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\runet\\appdata\\roaming\\python\\python312\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from transformers->-r requirements_bert.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\runet\\appdata\\roaming\\python\\python312\\site-packages (from peft->-r requirements_bert.txt (line 3)) (6.0.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting httpx<1.0.0 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading aiohttp-3.13.0-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (2025.8.3)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements_bert.txt (line 6)) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn->-r requirements_bert.txt (line 7))\n",
      "  Downloading scipy-1.16.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements_bert.txt (line 7))\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements_bert.txt (line 7))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6)) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Downloading yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from requests->transformers->-r requirements_bert.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements_bert.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\runet\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers->-r requirements_bert.txt (line 2)) (0.4.6)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets->-r requirements_bert.txt (line 6))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from jinja2->torch->-r requirements_bert.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\runet\\miniconda3\\envs\\outlining\\lib\\site-packages (from pandas->datasets->-r requirements_bert.txt (line 6)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->-r requirements_bert.txt (line 6))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->-r requirements_bert.txt (line 6))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\runet\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements_bert.txt (line 6)) (1.16.0)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-win_amd64.whl (59.5 MB)\n",
      "   ---------------------------------------- 0.0/59.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/59.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.8/59.5 MB 2.2 MB/s eta 0:00:27\n",
      "    --------------------------------------- 1.3/59.5 MB 2.2 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 2.1/59.5 MB 2.6 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 3.1/59.5 MB 3.1 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 4.5/59.5 MB 3.7 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 6.3/59.5 MB 4.5 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 7.9/59.5 MB 4.9 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 10.2/59.5 MB 5.6 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 12.3/59.5 MB 6.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 14.4/59.5 MB 6.4 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 17.0/59.5 MB 7.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 19.9/59.5 MB 7.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 22.8/59.5 MB 7.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 25.2/59.5 MB 8.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 27.8/59.5 MB 8.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 29.9/59.5 MB 8.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 32.0/59.5 MB 8.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 33.6/59.5 MB 8.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 35.9/59.5 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 38.5/59.5 MB 8.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 40.9/59.5 MB 8.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 41.2/59.5 MB 8.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 41.2/59.5 MB 8.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 41.2/59.5 MB 8.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 43.3/59.5 MB 8.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 44.0/59.5 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 45.1/59.5 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 47.4/59.5 MB 7.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 49.8/59.5 MB 7.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 52.2/59.5 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 54.3/59.5 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 56.6/59.5 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  59.5/59.5 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 59.5/59.5 MB 8.1 MB/s  0:00:07\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.8/8.7 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.9/8.7 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.3/8.7 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 10.6 MB/s  0:00:00\n",
      "Downloading aiohttp-3.13.0-cp312-cp312-win_amd64.whl (451 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/26.2 MB 9.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.7/26.2 MB 9.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.0/26.2 MB 10.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.9/26.2 MB 9.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 10.2/26.2 MB 10.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.7/26.2 MB 9.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.7/26.2 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.6/26.2 MB 7.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.6/26.2 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.7/26.2 MB 7.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 18.1/26.2 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.7/26.2 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 22.0/26.2 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.4/26.2 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.7/26.2 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 8.0 MB/s  0:00:03\n",
      "Downloading scipy-1.16.2-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/38.6 MB 12.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 4.7/38.6 MB 12.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.1/38.6 MB 11.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.2/38.6 MB 11.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 10.7/38.6 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.4/38.6 MB 10.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 17.8/38.6 MB 10.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.4/38.6 MB 11.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.3/38.6 MB 10.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.9/38.6 MB 11.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.7/38.6 MB 10.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.8/38.6 MB 10.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.6 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.5/38.6 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.3/38.6 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.4/38.6 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.2/38.6 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.0/38.6 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 9.2 MB/s  0:00:04\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.0 MB 8.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.7/11.0 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.8/11.0 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 9.0 MB/s  0:00:01\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, threadpoolctl, sniffio, scipy, pyarrow, propcache, multidict, joblib, h11, frozenlist, dill, aiohappyeyeballs, yarl, scikit-learn, pandas, multiprocess, httpcore, anyio, aiosignal, httpx, bitsandbytes, aiohttp, accelerate, peft, datasets\n",
      "\n",
      "   ----------------------------------------  0/27 [pytz]\n",
      "   ----------------------------------------  0/27 [pytz]\n",
      "   ----------------------------------------  0/27 [pytz]\n",
      "   -- -------------------------------------  2/27 [tzdata]\n",
      "   -- -------------------------------------  2/27 [tzdata]\n",
      "   -- -------------------------------------  2/27 [tzdata]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   ------- --------------------------------  5/27 [scipy]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   -------- -------------------------------  6/27 [pyarrow]\n",
      "   ------------- --------------------------  9/27 [joblib]\n",
      "   ------------- --------------------------  9/27 [joblib]\n",
      "   ------------- --------------------------  9/27 [joblib]\n",
      "   ----------------- ---------------------- 12/27 [dill]\n",
      "   ----------------- ---------------------- 12/27 [dill]\n",
      "   -------------------- ------------------- 14/27 [yarl]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ---------------------- ----------------- 15/27 [scikit-learn]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ----------------------- ---------------- 16/27 [pandas]\n",
      "   ------------------------- -------------- 17/27 [multiprocess]\n",
      "   ------------------------- -------------- 17/27 [multiprocess]\n",
      "   -------------------------- ------------- 18/27 [httpcore]\n",
      "   ---------------------------- ----------- 19/27 [anyio]\n",
      "   ------------------------------- -------- 21/27 [httpx]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   -------------------------------- ------- 22/27 [bitsandbytes]\n",
      "   ---------------------------------- ----- 23/27 [aiohttp]\n",
      "   ---------------------------------- ----- 23/27 [aiohttp]\n",
      "   ----------------------------------- ---- 24/27 [accelerate]\n",
      "   ----------------------------------- ---- 24/27 [accelerate]\n",
      "   ----------------------------------- ---- 24/27 [accelerate]\n",
      "   ----------------------------------- ---- 24/27 [accelerate]\n",
      "   ------------------------------------- -- 25/27 [peft]\n",
      "   ------------------------------------- -- 25/27 [peft]\n",
      "   ------------------------------------- -- 25/27 [peft]\n",
      "   ------------------------------------- -- 25/27 [peft]\n",
      "   ------------------------------------- -- 25/27 [peft]\n",
      "   -------------------------------------- - 26/27 [datasets]\n",
      "   -------------------------------------- - 26/27 [datasets]\n",
      "   -------------------------------------- - 26/27 [datasets]\n",
      "   -------------------------------------- - 26/27 [datasets]\n",
      "   ---------------------------------------- 27/27 [datasets]\n",
      "\n",
      "Successfully installed accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 anyio-4.11.0 bitsandbytes-0.48.1 datasets-4.2.0 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 joblib-1.5.2 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 peft-0.17.1 propcache-0.4.1 pyarrow-21.0.0 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.2 sniffio-1.3.1 threadpoolctl-3.6.0 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "#import scikit-learn\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from transformers import Conv1D, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Chunk for looking into trainable layers of the model itself.\n",
    "'''\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "\n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing\n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "\n",
    "    return layer_names\n",
    "\n",
    "list(set(get_specific_layer_names(model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 308,072,450 || trainable%: 0.1755\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"Wqkv\"],  # Fine-tuning the attention layer specifically\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,  # Start small, increase gradually\n",
    "    gradient_accumulation_steps=12,  # Simulate larger batch size\n",
    "\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset generation chunk\n",
    "We need to pass it through the BERT tokenizer here, make a train / test / val split and pass that to the model\n",
    "\n",
    "Below is the structure which worked for the Pol_NLI dataset, we should strive to do the same\n",
    "'''\n",
    "\n",
    "dataset = load_dataset(\"mlburnham/Pol_NLI\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"premise\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"entailment\", \"labels\") # Rename entailment column to labels (which is standard lookup for evaluation in the transmformers trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36abd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we should very much remember to save the finetuned model locally as this contains the new weights for use in analyzing new text\n",
    "lora_model.save_pretrained(f\"output/mmBERT/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}/final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outlining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
