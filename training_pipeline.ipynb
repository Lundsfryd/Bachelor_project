{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64371899",
   "metadata": {},
   "source": [
    "To do:\n",
    "- ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "- Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    - In addition to this, look at the implications of gradient accumulation steps\n",
    "    - Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "- Hyperparameter tuning (Alpha, learning rate, batch size so on - not sure how to figure this out)\n",
    "    - There is precedence for no hyperparameter tuning from the author of the OG NLI model that DEBATE is based on = Due to computational restrains and the points from this paper, no hyperparameter tuning was performed in this case. The model tuning in itself is also not the primary focus in this paper, but simply serves as a tool for the actual inquiry into blame in the Danish Parliament\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d09f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r \"requirements_bert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb794a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6ca904",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbb29e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/mmBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"jhu-clsp/mmBERT-base\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                                        load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                         bnb_4bit_quant_type=\"nf4\",\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2dc543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,416,096 || all params: 310,947,874 || trainable%: 1.0986\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  # Low-rank dimension\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",  # Fine-tuning all linear (classification, attention... layers)\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2659a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consider the batch size, could be increased for efficiency purposes.\n",
    "ADD WEIGHTED LOSS FUNCTION FOR HITS ON 1 FOR POSITIVE LABELLING AS THIS IS WAY MORE RARE = HIGHER \"REWARD\"\n",
    "Have a look at learning rate and gradient norm clipping which I need to read up on.\n",
    "    In addition to this, look at the implications of gradient accumulation steps\n",
    "    Much of this pipeline was constrained due to computational restrictions which I think was caused by errors and not actual training process.\n",
    "Early stopping: load_best_model_at_end=True\n",
    "'''\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./full_results',\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4, # Learning rate copied from mmBERT paper (8e-4) as they found this to perform best\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=256, # Batching at 256 to balance generalization and efficient training\n",
    "    gradient_accumulation_steps=1,  # Gradient of 1 as full batch fits in memory\n",
    "\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,  # Enable mixed precision\n",
    "    fp16=False,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=8,\n",
    "    remove_unused_columns=True, # Avoiding manual handling of residual text columns\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6b6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    max_length=392, # Padding to 392 to massively cut down on computation compared to base 8,192 tokens. \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_bincrossentropy(true, pred, weight_zero = 99.0, weight_one = 1):\n",
    "    \"\"\"\n",
    "    Calculates weighted binary cross entropy. The weights are fixed.\n",
    "        \n",
    "    This can be useful for unbalanced catagories.\n",
    "    \n",
    "    Adjust the weights here depending on what is required.\n",
    "    \n",
    "    For example if there are 10x as many positive classes as negative classes,\n",
    "        if you adjust weight_zero = 1.0, weight_one = 0.1, then false positives \n",
    "        will be penalize 10 times as much as false negatives.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    # calculate the binary cross entropy\n",
    "    bin_crossentropy = binary_crossentropy(true, pred)\n",
    "    \n",
    "    # apply the weights\n",
    "    weights = true * weight_one + (1. - true) * weight_zero\n",
    "    weights /= (weight_one + weight_zero) # Normalizing to be more consistent with regular BCE for comparison \n",
    "    weighted_bin_crossentropy = weights * bin_crossentropy \n",
    "\n",
    "    return np.mean(weighted_bin_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94619e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #From logits to probabilities\n",
    "    probs_2d = np.exp(predictions) / np.exp(predictions).sum(axis=1, keepdims=True)\n",
    "    probs = probs_2d[:, 1]  # positive class extraction\n",
    "    \n",
    "    weigthted_bce = weighted_bincrossentropy(labels, probs)\n",
    "    keras_bce = binary_crossentropy(labels, probs)\n",
    "    keras_bce = float(np.mean(keras_bce.numpy()))  # Converting from keras eagertensor to float value\n",
    "    \n",
    "    # Wrapping all metrics to floats for json serialization during model eval\n",
    "    return {\n",
    "        'keras_BCE': keras_bce,\n",
    "        'weigthed BCE': weigthted_bce,\n",
    "        'precision': float(average_precision_score(labels, probs)),\n",
    "        'accuracy': float(accuracy_score(labels, probs.round())), # Need rounding for these two computations (integer required)\n",
    "        'f1': float(f1_score(labels, probs.round(), average='macro')) # macro f1 is better for imbalanced dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c61a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/cleaned_training_data_3_4_5_temps.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c6f08eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 176/176 [00:00<00:00, 251.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/Model_data/validation_set.json\")\n",
    "\n",
    "val_dataframe = val_dataframe[['text', 'label']]\n",
    "\n",
    "val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00acf585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 5280.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataframe = pd.read_json(\"/work/RuneEgeskovTrust#9638/Bachelor/training_data/cleaned_training_data.json\")\n",
    "\n",
    "test_dataframe = test_dataframe[['text', 'label']]\n",
    "\n",
    "test_dataframe = test_dataframe[0:10000]\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_dataframe)\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "460fc8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4402b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 398842/398842 [00:18<00:00, 21240.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#val_dataframe = val_dataframe[['preceding_sentence', 'text', 'succeeding_sent', 'label']]\n",
    "\n",
    "dataframe = dataframe[['text', 'label']]\n",
    "\n",
    "#val_dataset = Dataset.from_pandas(val_dataframe)\n",
    "\n",
    "dataset = Dataset.from_pandas(dataframe)\n",
    "\n",
    "#tokenized_val = val_dataset.map(tokenize_function)\n",
    "\n",
    "# I suspect num_proc can be increased after having identified the padding problem\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a36abd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 06:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Keras Bce</th>\n",
       "      <th>Weigthed bce</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.185664</td>\n",
       "      <td>1.922872</td>\n",
       "      <td>1.293350</td>\n",
       "      <td>0.763899</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.789628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.989829</td>\n",
       "      <td>1.773121</td>\n",
       "      <td>1.192626</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.797978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.015357</td>\n",
       "      <td>1.814601</td>\n",
       "      <td>1.220526</td>\n",
       "      <td>0.708619</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.794012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.0010901039194626113, metrics={'train_runtime': 413.5878, 'train_samples_per_second': 72.536, 'train_steps_per_second': 0.29, 'total_flos': 8067821509440000.0, 'train_loss': 0.0010901039194626113, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Look into learning rates, model is currently overfitting quite drastically (\"small\" test-set)\n",
    "Normalizing weigthed BCE or no?\n",
    "Look into regularization, dropout and early stopping to avoid overfitting\n",
    "'''\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_test,#dataset,\n",
    "    eval_dataset=tokenized_val,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8aeaf63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/RuneEgeskovTrust#9638/miniconda3/envs/blame/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Merged model saved to: ./mmBlameBERT-pol-DA-merged\n"
     ]
    }
   ],
   "source": [
    "FINE_TUNED_MODEL_NAME = \"mmBlameBERT-pol-DA\"\n",
    "\n",
    "merged_model = model.merge_and_unload()    # PEFT: incorporates LoRA into base weights\n",
    "merged_dir = f\"./{FINE_TUNED_MODEL_NAME}-merged\"\n",
    "merged_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "print(\"✓ Merged model saved to:\", merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual batch size: 20\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "LORA_MODEL_PATH = \"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/output/mmBERT/template_3_4_5_model\"\n",
    "BASE_MODEL = \"jhu-clsp/mmBERT-base\"  # Update if different\n",
    "OUTPUT_PATH = \"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/output/mmBERT/template_3_4_5_merged\"\n",
    "NUM_LABELS = 2  # Binary classification\n",
    "\n",
    "def merge_and_save_model(lora_path, base_model_name, output_path, num_labels=2):\n",
    "    \"\"\"\n",
    "    Merge LoRA adapters with base model and save as a standard model.\n",
    "    This creates a standalone model that can be loaded directly without PEFT.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Merging LoRA Adapters with Base Model\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"\\n1. Loading tokenizer from {base_model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Load base model with classification head\n",
    "    print(f\"2. Loading base model: {base_model_name}\")\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        num_labels=num_labels,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters\n",
    "    print(f\"3. Loading LoRA adapters from {lora_path}...\")\n",
    "    model_with_lora = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    # Merge LoRA weights into base model\n",
    "    print(\"4. Merging LoRA weights with base model...\")\n",
    "    merged_model = model_with_lora.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    print(f\"5. Saving merged model to {output_path}...\")\n",
    "    merged_model.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ Model successfully merged and saved!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nYou can now load it directly with:\")\n",
    "    print(f\"  model = AutoModelForSequenceClassification.from_pretrained('{output_path}')\")\n",
    "    print(f\"  tokenizer = AutoTokenizer.from_pretrained('{output_path}')\")\n",
    "    print(\"\\nNo PEFT library needed for inference!\")\n",
    "\n",
    "\n",
    "# Merge and save the model\n",
    "merge_and_save_model(\n",
    "    lora_path=LORA_MODEL_PATH,\n",
    "    base_model_name=BASE_MODEL,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    num_labels=NUM_LABELS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadde19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/work/RuneEgeskovTrust#9638/Bachelor/Bachelor_project/EvalResultFullData.txt\", \"w\") as f:\n",
    "    f.write(str(eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We would expect to see a gradual decrease in both training and validation loss.\n",
    "If either om them split too far from eachother that indicates issues with the training process.\n",
    "The process itself should be pretty smooth with no dips either up or down.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where we should very much remember to save the finetuned model locally as this contains the new weights for use in analyzing new text\n",
    "model.save_pretrained(f\"output/mmBERT/full_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d73f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OG function without bce\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\n",
    "        'precision': average_precision_score(labels, predictions),\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1': f1_score(labels, predictions, average='macro') # Macro is better suited for imbalanced data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Profile training speed\n",
    "model.train()\n",
    "dummy_batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "for bs in dummy_batch_sizes:\n",
    "    # Create dummy batch\n",
    "    dummy_input = {\n",
    "        'input_ids': torch.randint(0, 30000, (bs, 128)).cuda(),\n",
    "        'attention_mask': torch.ones(bs, 128).cuda(),\n",
    "        'labels': torch.randint(0, model.config.num_labels, (bs,)).cuda(),\n",
    "    }\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            outputs = model(**dummy_input)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Batch size {bs}: OOM\")\n",
    "            break\n",
    "    \n",
    "    # Time it\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(20):\n",
    "        outputs = model(**dummy_input)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    samples_per_sec = (bs * 20) / elapsed\n",
    "    hours_for_400k = 400000 / samples_per_sec / 3600\n",
    "    \n",
    "    print(f\"Batch size {bs}: {samples_per_sec:.1f} samples/sec, Est. time: {hours_for_400k:.1f}h\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
